{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0]))\n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "005A    10\n",
      "071A    10\n",
      "040A    10\n",
      "014B    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "022A     9\n",
      "072A     9\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "031A     7\n",
      "027A     7\n",
      "050A     7\n",
      "109A     6\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "037A     6\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "026A     4\n",
      "035A     4\n",
      "105A     4\n",
      "052A     4\n",
      "003A     4\n",
      "062A     4\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "090A     1\n",
      "100A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "049A     1\n",
      "048A     1\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 988, 1: 805, 2: 600})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1698 - accuracy: 0.4944\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.9299 - accuracy: 0.6059\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.8363 - accuracy: 0.6465\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.7874 - accuracy: 0.6649\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.7624 - accuracy: 0.6657\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.7316 - accuracy: 0.6924\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.7065 - accuracy: 0.7037\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 961us/step - loss: 0.6756 - accuracy: 0.7100\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.6831 - accuracy: 0.7137\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.6402 - accuracy: 0.7317\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.6258 - accuracy: 0.7275\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.6521 - accuracy: 0.7179\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.6255 - accuracy: 0.7409\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.6062 - accuracy: 0.7463\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.5898 - accuracy: 0.7514\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.5731 - accuracy: 0.7618\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.5707 - accuracy: 0.7606\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.5508 - accuracy: 0.7664\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.5580 - accuracy: 0.7668\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.5689 - accuracy: 0.7622\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.5399 - accuracy: 0.7656\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5269 - accuracy: 0.7831\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5091 - accuracy: 0.7885\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.5226 - accuracy: 0.7739\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.5152 - accuracy: 0.7752\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.5053 - accuracy: 0.7936\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.5078 - accuracy: 0.7902\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.5053 - accuracy: 0.7831\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.4883 - accuracy: 0.7848\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.4859 - accuracy: 0.7986\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.4896 - accuracy: 0.7923\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.4814 - accuracy: 0.7919\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 964us/step - loss: 0.4834 - accuracy: 0.7902\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.4780 - accuracy: 0.7881\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.7957\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7957\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.8061\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4728 - accuracy: 0.7931\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7952\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.8103\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4694 - accuracy: 0.8015\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4457 - accuracy: 0.8161\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4551 - accuracy: 0.8028\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.4375 - accuracy: 0.8140\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.4441 - accuracy: 0.8145\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4227 - accuracy: 0.8232\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.4553 - accuracy: 0.8061\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.4335 - accuracy: 0.8220\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.4223 - accuracy: 0.8266\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.4313 - accuracy: 0.8132\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.4129 - accuracy: 0.8228\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.4305 - accuracy: 0.8199\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.4240 - accuracy: 0.8195\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.4252 - accuracy: 0.8249\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.4089 - accuracy: 0.8278\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.4113 - accuracy: 0.8278\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.4124 - accuracy: 0.8191\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4263 - accuracy: 0.8228\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.3968 - accuracy: 0.8324\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.4122 - accuracy: 0.8287\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.4017 - accuracy: 0.8278\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8278\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.8308\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.3791 - accuracy: 0.8420\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.4064 - accuracy: 0.8232\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3965 - accuracy: 0.8328\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3819 - accuracy: 0.8291\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.4022 - accuracy: 0.8299\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.3861 - accuracy: 0.8429\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.3603 - accuracy: 0.8487\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.3946 - accuracy: 0.8374\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.8408\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.3955 - accuracy: 0.8324\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3778 - accuracy: 0.8437\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.3801 - accuracy: 0.8412\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.3675 - accuracy: 0.8391\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.3667 - accuracy: 0.8425\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.3590 - accuracy: 0.8471\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.3647 - accuracy: 0.8408\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3651 - accuracy: 0.8471\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.3571 - accuracy: 0.8508\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.3548 - accuracy: 0.8529\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3574 - accuracy: 0.8537\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.3736 - accuracy: 0.8433\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.3552 - accuracy: 0.8408\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.3507 - accuracy: 0.8533\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3640 - accuracy: 0.8412\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3574 - accuracy: 0.8483\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3536 - accuracy: 0.8525\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.3438 - accuracy: 0.8567\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.3434 - accuracy: 0.8575\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.3442 - accuracy: 0.8558\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.3432 - accuracy: 0.8491\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.3491 - accuracy: 0.8491\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.3184 - accuracy: 0.8650\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.3523 - accuracy: 0.8475\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3382 - accuracy: 0.8533\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3186 - accuracy: 0.8675\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.3463 - accuracy: 0.8546\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.3279 - accuracy: 0.8617\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3388 - accuracy: 0.8621\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3310 - accuracy: 0.8583\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.3318 - accuracy: 0.8654\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3345 - accuracy: 0.8617\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.3191 - accuracy: 0.8613\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.3211 - accuracy: 0.8634\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3196 - accuracy: 0.8696\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.3071 - accuracy: 0.8755\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.3195 - accuracy: 0.8638\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.3304 - accuracy: 0.8575\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.3062 - accuracy: 0.8771\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.3397 - accuracy: 0.8579\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.3075 - accuracy: 0.8667\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.3040 - accuracy: 0.8742\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 911us/step - loss: 0.3126 - accuracy: 0.8625\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.3090 - accuracy: 0.8696\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.3048 - accuracy: 0.8767\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.3080 - accuracy: 0.8700\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.3018 - accuracy: 0.8738\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.3122 - accuracy: 0.8705\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3047 - accuracy: 0.8730\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3075 - accuracy: 0.8792\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.3121 - accuracy: 0.8625\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.3093 - accuracy: 0.8763\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3068 - accuracy: 0.8713\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.3070 - accuracy: 0.8725\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2873 - accuracy: 0.8813\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.3093 - accuracy: 0.8642\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3064 - accuracy: 0.8771\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2927 - accuracy: 0.8717\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2995 - accuracy: 0.8763\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2977 - accuracy: 0.8717\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.3012 - accuracy: 0.8776\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.3080 - accuracy: 0.8751\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2993 - accuracy: 0.8730\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2946 - accuracy: 0.8801\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2832 - accuracy: 0.8884\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2860 - accuracy: 0.8780\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2950 - accuracy: 0.8805\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2778 - accuracy: 0.8801\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.2790 - accuracy: 0.8805\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2919 - accuracy: 0.8851\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2820 - accuracy: 0.8838\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2751 - accuracy: 0.8855\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2816 - accuracy: 0.8842\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2819 - accuracy: 0.8746\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2709 - accuracy: 0.8939\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2851 - accuracy: 0.8851\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2731 - accuracy: 0.8838\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2747 - accuracy: 0.8901\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2861 - accuracy: 0.8809\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2779 - accuracy: 0.8859\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2896 - accuracy: 0.8826\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2755 - accuracy: 0.8876\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2644 - accuracy: 0.8930\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2525 - accuracy: 0.8964\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2810 - accuracy: 0.8897\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2604 - accuracy: 0.8918\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2715 - accuracy: 0.8872\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2678 - accuracy: 0.8888\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2689 - accuracy: 0.8872\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2734 - accuracy: 0.8859\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2670 - accuracy: 0.8855\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2571 - accuracy: 0.8964\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2661 - accuracy: 0.8951\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2513 - accuracy: 0.8993\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2679 - accuracy: 0.8918\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2605 - accuracy: 0.8926\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2550 - accuracy: 0.8989\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2553 - accuracy: 0.9014\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2582 - accuracy: 0.8985\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2540 - accuracy: 0.8972\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2602 - accuracy: 0.8955\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2598 - accuracy: 0.8905\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2520 - accuracy: 0.8926\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2653 - accuracy: 0.8884\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2683 - accuracy: 0.8872\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2481 - accuracy: 0.8989\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2544 - accuracy: 0.8905\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2294 - accuracy: 0.9093\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2572 - accuracy: 0.9010\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2539 - accuracy: 0.8943\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2605 - accuracy: 0.8943\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2564 - accuracy: 0.8968\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2430 - accuracy: 0.9035\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2251 - accuracy: 0.9039\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2564 - accuracy: 0.8934\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2546 - accuracy: 0.8930\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2420 - accuracy: 0.8922\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2303 - accuracy: 0.9056\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2413 - accuracy: 0.9014\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2288 - accuracy: 0.9122\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.2341 - accuracy: 0.8955\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2434 - accuracy: 0.9039\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.2500 - accuracy: 0.9001\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2365 - accuracy: 0.9085\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2238 - accuracy: 0.9056\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2348 - accuracy: 0.9089\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2366 - accuracy: 0.9010\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2349 - accuracy: 0.9081\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2349 - accuracy: 0.9051\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2229 - accuracy: 0.9089\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2419 - accuracy: 0.9005\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2413 - accuracy: 0.9051\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.2497 - accuracy: 0.9018\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.2306 - accuracy: 0.9085\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.2392 - accuracy: 0.9118\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 999us/step - loss: 0.2321 - accuracy: 0.9043\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.9014\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2394 - accuracy: 0.9085\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9072\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2139 - accuracy: 0.9118\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9068\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9081\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2104 - accuracy: 0.9131\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2341 - accuracy: 0.9056\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2254 - accuracy: 0.9135\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2408 - accuracy: 0.9026\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.9102\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.2254 - accuracy: 0.9085\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.2331 - accuracy: 0.9043\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2147 - accuracy: 0.9143\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2243 - accuracy: 0.9097\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2118 - accuracy: 0.9173\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2262 - accuracy: 0.9114\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2057 - accuracy: 0.9177\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2239 - accuracy: 0.9148\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2166 - accuracy: 0.9131\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2197 - accuracy: 0.9152\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2190 - accuracy: 0.9193\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2235 - accuracy: 0.9056\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2076 - accuracy: 0.9168\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2100 - accuracy: 0.9173\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2219 - accuracy: 0.9118\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9160\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.2097 - accuracy: 0.9168\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 996us/step - loss: 0.2045 - accuracy: 0.9260\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.2127 - accuracy: 0.9127\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 981us/step - loss: 0.2140 - accuracy: 0.9156\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.2166 - accuracy: 0.9173\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 988us/step - loss: 0.2119 - accuracy: 0.9173\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.2083 - accuracy: 0.9177\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.2078 - accuracy: 0.9181\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.2102 - accuracy: 0.9168\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2137 - accuracy: 0.9148\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2059 - accuracy: 0.9168\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2088 - accuracy: 0.9177\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2233 - accuracy: 0.9106\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2109 - accuracy: 0.9164\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2074 - accuracy: 0.9239\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2141 - accuracy: 0.9127\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2011 - accuracy: 0.9193\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2062 - accuracy: 0.9227\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.2091 - accuracy: 0.9131\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.2052 - accuracy: 0.9185\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9214\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2012 - accuracy: 0.9193\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2058 - accuracy: 0.9173\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.2040 - accuracy: 0.9219\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.2001 - accuracy: 0.9231\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.1870 - accuracy: 0.9248\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 995us/step - loss: 0.2042 - accuracy: 0.9202\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9239\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.2047 - accuracy: 0.9139\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.2021 - accuracy: 0.9235\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9231\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.1971 - accuracy: 0.9227\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.1893 - accuracy: 0.9231\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1923 - accuracy: 0.9244\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2057 - accuracy: 0.9202\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2035 - accuracy: 0.9252\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9315\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.1960 - accuracy: 0.9231\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1865 - accuracy: 0.9248\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.1920 - accuracy: 0.9248\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.1871 - accuracy: 0.9269\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1946 - accuracy: 0.9210\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2091 - accuracy: 0.9160\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.1920 - accuracy: 0.9227\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9231\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9302\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9198\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1866 - accuracy: 0.9302\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9273\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9277\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.9344\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.2081 - accuracy: 0.9202\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.1894 - accuracy: 0.9256\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.1759 - accuracy: 0.9327\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.1937 - accuracy: 0.9231\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2081 - accuracy: 0.9202\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.1899 - accuracy: 0.9260\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.1955 - accuracy: 0.9235\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.1810 - accuracy: 0.9277\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1947 - accuracy: 0.9277\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.1913 - accuracy: 0.9206\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1758 - accuracy: 0.9319\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1903 - accuracy: 0.9185\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1929 - accuracy: 0.9206\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.1888 - accuracy: 0.9239\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.1929 - accuracy: 0.9227\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.1914 - accuracy: 0.9239\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.1808 - accuracy: 0.9315\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1747 - accuracy: 0.9315\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.1803 - accuracy: 0.9336\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.1902 - accuracy: 0.9248\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.1756 - accuracy: 0.9302\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.1913 - accuracy: 0.9214\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1773 - accuracy: 0.9306\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1873 - accuracy: 0.9310\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1937 - accuracy: 0.9198\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1944 - accuracy: 0.9273\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1711 - accuracy: 0.9352\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1816 - accuracy: 0.9239\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1806 - accuracy: 0.9260\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.1752 - accuracy: 0.9319\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.2009 - accuracy: 0.9202\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1709 - accuracy: 0.9369\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.1794 - accuracy: 0.9285\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1656 - accuracy: 0.9373\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.1736 - accuracy: 0.9356\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1792 - accuracy: 0.9310\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9277\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.1826 - accuracy: 0.9306\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1884 - accuracy: 0.9202\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.1638 - accuracy: 0.9348\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.1698 - accuracy: 0.9327\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.1787 - accuracy: 0.9252\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1699 - accuracy: 0.9323\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.1709 - accuracy: 0.9352\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1748 - accuracy: 0.9277\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.1830 - accuracy: 0.9290\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.1859 - accuracy: 0.9285\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1816 - accuracy: 0.9356\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1871 - accuracy: 0.9231\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1685 - accuracy: 0.9315\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9373\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1625 - accuracy: 0.9382\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9281\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9411\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9361\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9265\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9394\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 965us/step - loss: 0.1622 - accuracy: 0.9377\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9260\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.1792 - accuracy: 0.9294\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.1799 - accuracy: 0.9290\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.1652 - accuracy: 0.9411\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.1774 - accuracy: 0.9277\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.1833 - accuracy: 0.9298\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.1628 - accuracy: 0.9331\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.1840 - accuracy: 0.9294\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.1863 - accuracy: 0.9252\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1756 - accuracy: 0.9252\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.1644 - accuracy: 0.9361\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9306\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.1584 - accuracy: 0.9411\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1701 - accuracy: 0.9319\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.1658 - accuracy: 0.9348\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.1578 - accuracy: 0.9440\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.1595 - accuracy: 0.9382\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1733 - accuracy: 0.9336\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1731 - accuracy: 0.9294\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.1749 - accuracy: 0.9361\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.1579 - accuracy: 0.9377\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.1616 - accuracy: 0.9386\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1614 - accuracy: 0.9394\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.1836 - accuracy: 0.9315\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1576 - accuracy: 0.9394\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.1513 - accuracy: 0.9453\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1709 - accuracy: 0.9344\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9356\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.9411\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.1719 - accuracy: 0.9331\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1704 - accuracy: 0.9315\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1683 - accuracy: 0.9390\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1746 - accuracy: 0.9306\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1641 - accuracy: 0.9348\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.1514 - accuracy: 0.9427\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.1590 - accuracy: 0.9356\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1636 - accuracy: 0.9361\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.1579 - accuracy: 0.9419\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1654 - accuracy: 0.9382\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 964us/step - loss: 0.1648 - accuracy: 0.9411\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9361\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9369\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9432\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 911us/step - loss: 0.1539 - accuracy: 0.9419\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1536 - accuracy: 0.9427\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1559 - accuracy: 0.9419\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1632 - accuracy: 0.9390\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.1603 - accuracy: 0.9398\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9377\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.1701 - accuracy: 0.9344\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1625 - accuracy: 0.9382\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1470 - accuracy: 0.9461\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1581 - accuracy: 0.9377\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1558 - accuracy: 0.9407\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1518 - accuracy: 0.9411\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1546 - accuracy: 0.9369\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1703 - accuracy: 0.9365\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1568 - accuracy: 0.9411\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1353 - accuracy: 0.9490\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.1603 - accuracy: 0.9361\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 679us/step - loss: 0.1472 - accuracy: 0.9415\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.1540 - accuracy: 0.9407\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.1635 - accuracy: 0.9377\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1511 - accuracy: 0.9432\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.1635 - accuracy: 0.9390\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.1369 - accuracy: 0.9453\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.1562 - accuracy: 0.9390\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.1561 - accuracy: 0.9373\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1438 - accuracy: 0.9478\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.1514 - accuracy: 0.9407\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.1443 - accuracy: 0.9478\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.1508 - accuracy: 0.9457\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 957us/step - loss: 0.1543 - accuracy: 0.9365\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1622 - accuracy: 0.9361\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1481 - accuracy: 0.9419\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.1438 - accuracy: 0.9482\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.1602 - accuracy: 0.9411\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.1551 - accuracy: 0.9356\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1483 - accuracy: 0.9373\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.1557 - accuracy: 0.9356\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9402\n",
      "Epoch 426/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9478\n",
      "Epoch 427/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1473 - accuracy: 0.9473\n",
      "Epoch 428/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1535 - accuracy: 0.9398\n",
      "Epoch 429/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1425 - accuracy: 0.9469\n",
      "Epoch 430/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1459 - accuracy: 0.9473\n",
      "Epoch 431/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.1502 - accuracy: 0.9415\n",
      "Epoch 432/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1602 - accuracy: 0.9340\n",
      "Epoch 433/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1618 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 403.\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1444 - accuracy: 0.9423\n",
      "Epoch 433: early stopping\n",
      "6/6 [==============================] - 0s 860us/step - loss: 1.0048 - accuracy: 0.6914\n",
      "6/6 [==============================] - 0s 715us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.68 (17/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 1.0048242807388306, Accuracy: 0.6913580298423767, Precision: 0.6688797313797314, Recall: 0.737882122768403, F1 Score: 0.6956140350877194\n",
      "Confusion Matrix:\n",
      " [[70  5 19]\n",
      " [ 1  9  0]\n",
      " [25  0 33]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "016A    10\n",
      "033A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "108A     6\n",
      "007A     6\n",
      "021A     5\n",
      "034A     5\n",
      "025C     5\n",
      "023B     5\n",
      "075A     5\n",
      "052A     4\n",
      "104A     4\n",
      "026A     4\n",
      "035A     4\n",
      "009A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "056A     3\n",
      "058A     3\n",
      "113A     3\n",
      "038A     2\n",
      "069A     2\n",
      "093A     2\n",
      "087A     2\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "032A     2\n",
      "018A     2\n",
      "115A     1\n",
      "110A     1\n",
      "019B     1\n",
      "090A     1\n",
      "004A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'047A'}\n",
      "Moved to Test Set:\n",
      "{'047A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '047A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "Length of X_train_val:\n",
      "690\n",
      "Length of y_train_val:\n",
      "690\n",
      "Length of groups_train_val:\n",
      "690\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    136\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 1: 680, 2: 630})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 1.1968 - accuracy: 0.4464\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9872 - accuracy: 0.5577\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8752 - accuracy: 0.6136\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8197 - accuracy: 0.6574\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7347 - accuracy: 0.6930\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7259 - accuracy: 0.6925\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.7110\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 974us/step - loss: 0.7011 - accuracy: 0.7101\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.6623 - accuracy: 0.7202\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.6249 - accuracy: 0.7378\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.6156 - accuracy: 0.7465\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 997us/step - loss: 0.6097 - accuracy: 0.7525\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.6101 - accuracy: 0.7424\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5935 - accuracy: 0.7516\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.5897 - accuracy: 0.7548\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.5872 - accuracy: 0.7539\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.5548 - accuracy: 0.7756\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.5552 - accuracy: 0.7742\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.5533 - accuracy: 0.7710\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.5446 - accuracy: 0.7687\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.5320 - accuracy: 0.7825\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.5114 - accuracy: 0.7844\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.5146 - accuracy: 0.7849\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.5120 - accuracy: 0.7779\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 712us/step - loss: 0.5120 - accuracy: 0.7867\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.4925 - accuracy: 0.7959\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.4662 - accuracy: 0.8029\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4939 - accuracy: 0.7969\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.4745 - accuracy: 0.8024\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.4849 - accuracy: 0.8079\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.4773 - accuracy: 0.8019\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.4754 - accuracy: 0.8033\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.4587 - accuracy: 0.8079\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.4522 - accuracy: 0.8079\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.4496 - accuracy: 0.8112\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.4539 - accuracy: 0.8126\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4580 - accuracy: 0.8056\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 993us/step - loss: 0.4314 - accuracy: 0.8195\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.4426 - accuracy: 0.8139\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.4320 - accuracy: 0.8283\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.4352 - accuracy: 0.8130\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.4375 - accuracy: 0.8163\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.4247 - accuracy: 0.8283\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 736us/step - loss: 0.4257 - accuracy: 0.8126\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.4179 - accuracy: 0.8246\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.4268 - accuracy: 0.8232\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.4080 - accuracy: 0.8315\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.4234 - accuracy: 0.8236\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4060 - accuracy: 0.8333\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4107 - accuracy: 0.8361\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3973 - accuracy: 0.8352\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4109 - accuracy: 0.8329\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.3988 - accuracy: 0.8453\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.3729 - accuracy: 0.8398\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.3749 - accuracy: 0.8463\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.3972 - accuracy: 0.8426\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.3690 - accuracy: 0.8467\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.3859 - accuracy: 0.8366\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.3691 - accuracy: 0.8601\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.3800 - accuracy: 0.8463\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 736us/step - loss: 0.3629 - accuracy: 0.8592\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.3710 - accuracy: 0.8518\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.3656 - accuracy: 0.8587\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.3762 - accuracy: 0.8440\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 706us/step - loss: 0.3669 - accuracy: 0.8601\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3695 - accuracy: 0.8500\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.3576 - accuracy: 0.8546\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.3554 - accuracy: 0.8569\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.3486 - accuracy: 0.8610\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.3680 - accuracy: 0.8550\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.3723 - accuracy: 0.8449\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.3404 - accuracy: 0.8689\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.3650 - accuracy: 0.8532\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.3390 - accuracy: 0.8569\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.3254 - accuracy: 0.8670\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3322 - accuracy: 0.8767\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 713us/step - loss: 0.3247 - accuracy: 0.8717\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.3533 - accuracy: 0.8601\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 719us/step - loss: 0.3321 - accuracy: 0.8633\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.3323 - accuracy: 0.8657\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.3443 - accuracy: 0.8606\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.3310 - accuracy: 0.8596\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.3308 - accuracy: 0.8620\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.3163 - accuracy: 0.8749\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.3285 - accuracy: 0.8615\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.3399 - accuracy: 0.8670\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3206 - accuracy: 0.8638\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3254 - accuracy: 0.8647\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.3232 - accuracy: 0.8735\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.3117 - accuracy: 0.8726\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.3178 - accuracy: 0.8717\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.3196 - accuracy: 0.8693\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.3118 - accuracy: 0.8800\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.3142 - accuracy: 0.8666\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.3116 - accuracy: 0.8712\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3141 - accuracy: 0.8744\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3055 - accuracy: 0.8813\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.3073 - accuracy: 0.8777\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3034 - accuracy: 0.8809\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.2900 - accuracy: 0.8827\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2922 - accuracy: 0.8790\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2942 - accuracy: 0.8804\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.3057 - accuracy: 0.8753\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2927 - accuracy: 0.8823\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 707us/step - loss: 0.2991 - accuracy: 0.8777\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2949 - accuracy: 0.8809\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2826 - accuracy: 0.8873\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2935 - accuracy: 0.8864\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.3074 - accuracy: 0.8735\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2856 - accuracy: 0.8901\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.2775 - accuracy: 0.8901\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2933 - accuracy: 0.8841\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 929us/step - loss: 0.2854 - accuracy: 0.8929\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 951us/step - loss: 0.2891 - accuracy: 0.8809\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 983us/step - loss: 0.2871 - accuracy: 0.8850\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.2842 - accuracy: 0.8897\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2754 - accuracy: 0.8892\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2763 - accuracy: 0.8869\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.2888 - accuracy: 0.8878\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2781 - accuracy: 0.8929\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2548 - accuracy: 0.9081\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2691 - accuracy: 0.8910\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2673 - accuracy: 0.8961\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2841 - accuracy: 0.8837\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 719us/step - loss: 0.2794 - accuracy: 0.8873\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.2619 - accuracy: 0.9017\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.2714 - accuracy: 0.8920\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.2869 - accuracy: 0.8860\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2597 - accuracy: 0.8934\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2555 - accuracy: 0.9030\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.2671 - accuracy: 0.8924\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2700 - accuracy: 0.8906\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.2624 - accuracy: 0.8938\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.2595 - accuracy: 0.8957\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 736us/step - loss: 0.2608 - accuracy: 0.8934\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2754 - accuracy: 0.8910\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2502 - accuracy: 0.8994\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2473 - accuracy: 0.8984\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.2512 - accuracy: 0.8938\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.9035\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2549 - accuracy: 0.9007\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.2498 - accuracy: 0.9012\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2321 - accuracy: 0.9100\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.2533 - accuracy: 0.8943\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2456 - accuracy: 0.8998\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.9090\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.2522 - accuracy: 0.9003\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2559 - accuracy: 0.9026\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2597 - accuracy: 0.9026\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2497 - accuracy: 0.8970\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2444 - accuracy: 0.9035\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2352 - accuracy: 0.9044\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2602 - accuracy: 0.8934\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2420 - accuracy: 0.9086\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2470 - accuracy: 0.8961\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.2414 - accuracy: 0.9072\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2457 - accuracy: 0.9063\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2371 - accuracy: 0.9035\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.2388 - accuracy: 0.9090\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.2466 - accuracy: 0.8989\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.2406 - accuracy: 0.9095\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 994us/step - loss: 0.2284 - accuracy: 0.9109\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.2257 - accuracy: 0.9169\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 943us/step - loss: 0.2258 - accuracy: 0.9077\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.2302 - accuracy: 0.9155\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.2493 - accuracy: 0.9026\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.2289 - accuracy: 0.9118\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.2146 - accuracy: 0.9160\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.2276 - accuracy: 0.9127\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2118 - accuracy: 0.9155\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2276 - accuracy: 0.9192\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.2289 - accuracy: 0.9086\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 721us/step - loss: 0.2292 - accuracy: 0.9095\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.2311 - accuracy: 0.9095\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.2139 - accuracy: 0.9151\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2329 - accuracy: 0.9123\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2244 - accuracy: 0.9104\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2215 - accuracy: 0.9132\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2117 - accuracy: 0.9220\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2155 - accuracy: 0.9178\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.2425 - accuracy: 0.9067\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.2222 - accuracy: 0.9095\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.2177 - accuracy: 0.9164\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2156 - accuracy: 0.9164\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.2123 - accuracy: 0.9169\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2169 - accuracy: 0.9192\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9021\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.2130 - accuracy: 0.9224\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2144 - accuracy: 0.9146\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2255 - accuracy: 0.9067\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2105 - accuracy: 0.9201\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2196 - accuracy: 0.9141\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2139 - accuracy: 0.9114\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.2095 - accuracy: 0.9127\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.2241 - accuracy: 0.9100\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 994us/step - loss: 0.2065 - accuracy: 0.9211\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.2079 - accuracy: 0.9271\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9187\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9257\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9192\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2241 - accuracy: 0.9109\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2146 - accuracy: 0.9104\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2247 - accuracy: 0.9118\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2060 - accuracy: 0.9183\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.1889 - accuracy: 0.9298\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2028 - accuracy: 0.9137\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1996 - accuracy: 0.9220\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2150 - accuracy: 0.9104\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2095 - accuracy: 0.9197\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2151 - accuracy: 0.9178\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.2023 - accuracy: 0.9183\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2051 - accuracy: 0.9211\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.9192\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 994us/step - loss: 0.2073 - accuracy: 0.9169\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.2047 - accuracy: 0.9211\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9257\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.2124 - accuracy: 0.9132\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.1945 - accuracy: 0.9243\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1869 - accuracy: 0.9280\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2108 - accuracy: 0.9215\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.2090 - accuracy: 0.9183\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9238\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2057 - accuracy: 0.9178\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9257\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9280\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.2033 - accuracy: 0.9229\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9317\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.1930 - accuracy: 0.9275\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.1972 - accuracy: 0.9192\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 988us/step - loss: 0.1834 - accuracy: 0.9307\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.1999 - accuracy: 0.9243\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.1881 - accuracy: 0.9271\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9178\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.1925 - accuracy: 0.9211\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 971us/step - loss: 0.2000 - accuracy: 0.9183\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.1746 - accuracy: 0.9344\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.1912 - accuracy: 0.9220\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1779 - accuracy: 0.9326\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 971us/step - loss: 0.1900 - accuracy: 0.9266\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.1943 - accuracy: 0.9178\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 975us/step - loss: 0.1810 - accuracy: 0.9340\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 993us/step - loss: 0.1939 - accuracy: 0.9247\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.1885 - accuracy: 0.9266\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.1831 - accuracy: 0.9280\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.1866 - accuracy: 0.9312\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.1934 - accuracy: 0.9289\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1966 - accuracy: 0.9127\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.1756 - accuracy: 0.9331\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2002 - accuracy: 0.9247\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 732us/step - loss: 0.1929 - accuracy: 0.9229\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.1884 - accuracy: 0.9321\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 707us/step - loss: 0.1740 - accuracy: 0.9340\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1840 - accuracy: 0.9312\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1775 - accuracy: 0.9266\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.1797 - accuracy: 0.9289\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1835 - accuracy: 0.9252\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.1713 - accuracy: 0.9358\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.1827 - accuracy: 0.9234\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1717 - accuracy: 0.9367\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1694 - accuracy: 0.9344\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.1797 - accuracy: 0.9358\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.1769 - accuracy: 0.9340\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1810 - accuracy: 0.9211\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 696us/step - loss: 0.1743 - accuracy: 0.9326\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.1683 - accuracy: 0.9340\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.1800 - accuracy: 0.9317\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.1817 - accuracy: 0.9289\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.1865 - accuracy: 0.9275\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.1818 - accuracy: 0.9284\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 713us/step - loss: 0.1801 - accuracy: 0.9358\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1725 - accuracy: 0.9280\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9358\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.1817 - accuracy: 0.9284\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1667 - accuracy: 0.9354\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1806 - accuracy: 0.9317\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.1661 - accuracy: 0.9386\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1890 - accuracy: 0.9224\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.1812 - accuracy: 0.9294\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.1809 - accuracy: 0.9326\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 713us/step - loss: 0.1648 - accuracy: 0.9372\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1668 - accuracy: 0.9381\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1555 - accuracy: 0.9414\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1739 - accuracy: 0.9344\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1796 - accuracy: 0.9349\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.1706 - accuracy: 0.9303\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1740 - accuracy: 0.9307\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1504 - accuracy: 0.9423\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1771 - accuracy: 0.9284\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1537 - accuracy: 0.9418\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1731 - accuracy: 0.9358\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1561 - accuracy: 0.9414\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1788 - accuracy: 0.9275\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1617 - accuracy: 0.9404\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1703 - accuracy: 0.9335\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1685 - accuracy: 0.9367\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 698us/step - loss: 0.1551 - accuracy: 0.9391\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.1618 - accuracy: 0.9354\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1637 - accuracy: 0.9363\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 715us/step - loss: 0.1591 - accuracy: 0.9404\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.1637 - accuracy: 0.9409\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 718us/step - loss: 0.1675 - accuracy: 0.9395\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 712us/step - loss: 0.1640 - accuracy: 0.9395\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.1754 - accuracy: 0.9326\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 723us/step - loss: 0.1718 - accuracy: 0.9367\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1553 - accuracy: 0.9474\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.1756 - accuracy: 0.9243\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1721 - accuracy: 0.9303\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.1524 - accuracy: 0.9404\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.1507 - accuracy: 0.9391\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1507 - accuracy: 0.9367\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.1641 - accuracy: 0.9363\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.1617 - accuracy: 0.9400\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1636 - accuracy: 0.9367\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.1667 - accuracy: 0.9377\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1582 - accuracy: 0.9349\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 715us/step - loss: 0.1602 - accuracy: 0.9372\n",
      "Epoch 317/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1288 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 287.\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1739 - accuracy: 0.9317\n",
      "Epoch 317: early stopping\n",
      "8/8 [==============================] - 0s 696us/step - loss: 0.9241 - accuracy: 0.6761\n",
      "8/8 [==============================] - 0s 571us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "Final Test Results - Loss: 0.9241028428077698, Accuracy: 0.6761133670806885, Precision: 0.700454717696097, Recall: 0.678617216117216, F1 Score: 0.6795405982905983\n",
      "Confusion Matrix:\n",
      " [[110   2  48]\n",
      " [  8  27   0]\n",
      " [ 22   0  30]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "106A    14\n",
      "042A    14\n",
      "028A    13\n",
      "111A    13\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "036A    11\n",
      "025A    11\n",
      "005A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "051B     9\n",
      "022A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "010A     8\n",
      "095A     8\n",
      "094A     8\n",
      "013B     8\n",
      "031A     7\n",
      "117A     7\n",
      "053A     6\n",
      "108A     6\n",
      "008A     6\n",
      "109A     6\n",
      "007A     6\n",
      "037A     6\n",
      "070A     5\n",
      "021A     5\n",
      "044A     5\n",
      "023B     5\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "062A     4\n",
      "009A     4\n",
      "104A     4\n",
      "058A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "056A     3\n",
      "113A     3\n",
      "025B     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "054A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "032A     2\n",
      "069A     2\n",
      "049A     1\n",
      "088A     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "004A     1\n",
      "092A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 748, 1: 675, 2: 615})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1278 - accuracy: 0.4833\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9478 - accuracy: 0.5662\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8554 - accuracy: 0.6099\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.8061 - accuracy: 0.6300\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.7542 - accuracy: 0.6634\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 882us/step - loss: 0.7571 - accuracy: 0.6654\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.7479 - accuracy: 0.6771\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.7099 - accuracy: 0.6860\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 787us/step - loss: 0.6958 - accuracy: 0.6943\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.6905 - accuracy: 0.6928\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.6781 - accuracy: 0.7026\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.6527 - accuracy: 0.7080\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.6528 - accuracy: 0.7149\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.6409 - accuracy: 0.7218\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.6399 - accuracy: 0.7159\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.6150 - accuracy: 0.7233\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.6235 - accuracy: 0.7208\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.6120 - accuracy: 0.7296\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.6023 - accuracy: 0.7390\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.5882 - accuracy: 0.7370\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.5728 - accuracy: 0.7507\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.5690 - accuracy: 0.7586\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.5598 - accuracy: 0.7507\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.5746 - accuracy: 0.7439\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.5645 - accuracy: 0.7601\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.5638 - accuracy: 0.7532\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.5699 - accuracy: 0.7532\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.5468 - accuracy: 0.7625\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.5343 - accuracy: 0.7640\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.5485 - accuracy: 0.7704\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.5241 - accuracy: 0.7630\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 787us/step - loss: 0.5132 - accuracy: 0.7709\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.5385 - accuracy: 0.7674\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.5159 - accuracy: 0.7674\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.5107 - accuracy: 0.7748\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.5262 - accuracy: 0.7689\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.4869 - accuracy: 0.7880\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.5151 - accuracy: 0.7787\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.4981 - accuracy: 0.7870\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.5197 - accuracy: 0.7748\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.4885 - accuracy: 0.7861\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.4959 - accuracy: 0.7763\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.4942 - accuracy: 0.7826\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.4996 - accuracy: 0.7851\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4819 - accuracy: 0.7895\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.4826 - accuracy: 0.7949\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.4767 - accuracy: 0.7944\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.4859 - accuracy: 0.7792\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.4732 - accuracy: 0.7998\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4531 - accuracy: 0.8027\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.4628 - accuracy: 0.7954\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.4563 - accuracy: 0.8086\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.4598 - accuracy: 0.8067\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.4472 - accuracy: 0.8096\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.4507 - accuracy: 0.8052\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.4629 - accuracy: 0.7993\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 0.4491 - accuracy: 0.8003\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.4528 - accuracy: 0.7998\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.4300 - accuracy: 0.8204\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.4410 - accuracy: 0.8096\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.4534 - accuracy: 0.8023\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.4371 - accuracy: 0.7993\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.4331 - accuracy: 0.8106\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.4449 - accuracy: 0.8018\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.4442 - accuracy: 0.8023\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.4269 - accuracy: 0.8165\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.4309 - accuracy: 0.8165\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.4318 - accuracy: 0.8180\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.4364 - accuracy: 0.8170\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.4202 - accuracy: 0.8263\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.4153 - accuracy: 0.8180\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.4176 - accuracy: 0.8243\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.4186 - accuracy: 0.8184\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.3913 - accuracy: 0.8351\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.4221 - accuracy: 0.8273\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.4043 - accuracy: 0.8253\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.4072 - accuracy: 0.8229\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.4117 - accuracy: 0.8219\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.4109 - accuracy: 0.8263\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.4089 - accuracy: 0.8337\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.4144 - accuracy: 0.8278\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.4126 - accuracy: 0.8253\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.4214 - accuracy: 0.8253\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.4165 - accuracy: 0.8214\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.4127 - accuracy: 0.8219\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.4012 - accuracy: 0.8307\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.3991 - accuracy: 0.8278\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.3926 - accuracy: 0.8400\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.3838 - accuracy: 0.8376\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3729 - accuracy: 0.8386\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.3996 - accuracy: 0.8248\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.3982 - accuracy: 0.8302\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.3783 - accuracy: 0.8391\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.3867 - accuracy: 0.8366\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.3912 - accuracy: 0.8268\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.3743 - accuracy: 0.8445\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.3811 - accuracy: 0.8361\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.3840 - accuracy: 0.8292\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.3809 - accuracy: 0.8440\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.3588 - accuracy: 0.8523\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.3620 - accuracy: 0.8518\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.3670 - accuracy: 0.8459\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.3626 - accuracy: 0.8484\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.3696 - accuracy: 0.8415\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.3694 - accuracy: 0.8469\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.3589 - accuracy: 0.8533\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.3565 - accuracy: 0.8499\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 787us/step - loss: 0.3422 - accuracy: 0.8587\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.3619 - accuracy: 0.8499\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.3640 - accuracy: 0.8435\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.3681 - accuracy: 0.8435\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.3611 - accuracy: 0.8454\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.3585 - accuracy: 0.8513\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.3505 - accuracy: 0.8562\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.3490 - accuracy: 0.8528\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.3441 - accuracy: 0.8543\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.3610 - accuracy: 0.8557\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.3292 - accuracy: 0.8695\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.3299 - accuracy: 0.8660\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.3556 - accuracy: 0.8415\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.3548 - accuracy: 0.8587\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.3423 - accuracy: 0.8577\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.3523 - accuracy: 0.8508\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.3487 - accuracy: 0.8474\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.3375 - accuracy: 0.8597\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.3368 - accuracy: 0.8616\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.3311 - accuracy: 0.8665\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.3566 - accuracy: 0.8474\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.3427 - accuracy: 0.8533\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.3333 - accuracy: 0.8656\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.3277 - accuracy: 0.8665\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.3364 - accuracy: 0.8543\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.3380 - accuracy: 0.8602\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.3283 - accuracy: 0.8621\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.3177 - accuracy: 0.8734\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.3427 - accuracy: 0.8562\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.3388 - accuracy: 0.8557\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.3111 - accuracy: 0.8690\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.3412 - accuracy: 0.8543\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.3318 - accuracy: 0.8616\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.3222 - accuracy: 0.8739\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.3167 - accuracy: 0.8646\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.3202 - accuracy: 0.8646\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.3291 - accuracy: 0.8651\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.3190 - accuracy: 0.8714\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.3211 - accuracy: 0.8651\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.3261 - accuracy: 0.8665\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.3189 - accuracy: 0.8670\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.3012 - accuracy: 0.8808\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.3192 - accuracy: 0.8729\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.3184 - accuracy: 0.8675\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.3181 - accuracy: 0.8705\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.3137 - accuracy: 0.8710\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.3226 - accuracy: 0.8714\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.3038 - accuracy: 0.8710\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.3143 - accuracy: 0.8651\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.3063 - accuracy: 0.8798\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.3056 - accuracy: 0.8749\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.3102 - accuracy: 0.8763\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.3037 - accuracy: 0.8700\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.2970 - accuracy: 0.8867\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2989 - accuracy: 0.8778\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.3153 - accuracy: 0.8660\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.3169 - accuracy: 0.8724\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.3096 - accuracy: 0.8724\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8705\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.8739\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2991 - accuracy: 0.8813\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.3066 - accuracy: 0.8710\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.2993 - accuracy: 0.8729\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.2971 - accuracy: 0.8759\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.2915 - accuracy: 0.8763\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.3095 - accuracy: 0.8714\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2997 - accuracy: 0.8763\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2930 - accuracy: 0.8827\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 911us/step - loss: 0.2973 - accuracy: 0.8719\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.3103 - accuracy: 0.8739\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.3047 - accuracy: 0.8729\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.3060 - accuracy: 0.8754\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.2863 - accuracy: 0.8852\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.2933 - accuracy: 0.8857\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.2880 - accuracy: 0.8773\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.2939 - accuracy: 0.8822\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.3005 - accuracy: 0.8719\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2901 - accuracy: 0.8739\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.2987 - accuracy: 0.8724\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2737 - accuracy: 0.8852\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.2958 - accuracy: 0.8803\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.2999 - accuracy: 0.8759\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.2895 - accuracy: 0.8813\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.2794 - accuracy: 0.8808\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.2811 - accuracy: 0.8827\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2837 - accuracy: 0.8852\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.2855 - accuracy: 0.8921\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2763 - accuracy: 0.8862\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2859 - accuracy: 0.8817\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2862 - accuracy: 0.8867\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2777 - accuracy: 0.8896\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2946 - accuracy: 0.8793\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2751 - accuracy: 0.8891\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.2793 - accuracy: 0.8876\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.2854 - accuracy: 0.8847\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.2608 - accuracy: 0.8925\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.2845 - accuracy: 0.8906\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2583 - accuracy: 0.8881\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.2680 - accuracy: 0.8945\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.2813 - accuracy: 0.8901\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2709 - accuracy: 0.8916\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2755 - accuracy: 0.8896\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2736 - accuracy: 0.8970\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.2682 - accuracy: 0.8965\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2695 - accuracy: 0.8837\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2545 - accuracy: 0.9048\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.2656 - accuracy: 0.8862\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2783 - accuracy: 0.8955\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.2599 - accuracy: 0.8970\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2745 - accuracy: 0.8896\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2654 - accuracy: 0.8921\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.2781 - accuracy: 0.8930\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2742 - accuracy: 0.8871\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2623 - accuracy: 0.8906\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2719 - accuracy: 0.8940\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2603 - accuracy: 0.8984\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2646 - accuracy: 0.8935\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.2645 - accuracy: 0.8940\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.2548 - accuracy: 0.9024\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2598 - accuracy: 0.8886\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.2600 - accuracy: 0.9004\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2652 - accuracy: 0.8891\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.2636 - accuracy: 0.9009\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2673 - accuracy: 0.8906\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2589 - accuracy: 0.9014\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.2613 - accuracy: 0.8965\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2566 - accuracy: 0.8906\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2587 - accuracy: 0.8940\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2565 - accuracy: 0.8979\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2674 - accuracy: 0.8837\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.2505 - accuracy: 0.8984\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2383 - accuracy: 0.9038\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2503 - accuracy: 0.8955\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.2456 - accuracy: 0.8950\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2612 - accuracy: 0.8906\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2387 - accuracy: 0.9073\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2652 - accuracy: 0.8916\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.8979\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2534 - accuracy: 0.8994\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2646 - accuracy: 0.8935\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.2526 - accuracy: 0.8930\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.2408 - accuracy: 0.9058\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.2570 - accuracy: 0.8940\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.2541 - accuracy: 0.8921\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.2448 - accuracy: 0.9004\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2440 - accuracy: 0.9019\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2427 - accuracy: 0.9038\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.2328 - accuracy: 0.9028\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2474 - accuracy: 0.9024\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.2575 - accuracy: 0.8921\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.2359 - accuracy: 0.9043\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.2523 - accuracy: 0.9048\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.2455 - accuracy: 0.9024\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.2513 - accuracy: 0.8994\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2360 - accuracy: 0.9068\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.2577 - accuracy: 0.8974\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2317 - accuracy: 0.9112\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2401 - accuracy: 0.9136\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2266 - accuracy: 0.9161\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2494 - accuracy: 0.9009\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.2469 - accuracy: 0.8989\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2442 - accuracy: 0.8989\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2415 - accuracy: 0.9033\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.2436 - accuracy: 0.8984\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.2411 - accuracy: 0.8970\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.2364 - accuracy: 0.9063\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2334 - accuracy: 0.9087\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.2417 - accuracy: 0.9014\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2262 - accuracy: 0.9156\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2304 - accuracy: 0.9161\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.2367 - accuracy: 0.9097\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.2193 - accuracy: 0.9151\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2380 - accuracy: 0.9014\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.2479 - accuracy: 0.9028\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2344 - accuracy: 0.9033\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2285 - accuracy: 0.9136\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2214 - accuracy: 0.9092\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.2191 - accuracy: 0.9097\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.2346 - accuracy: 0.9063\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2210 - accuracy: 0.9215\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2096 - accuracy: 0.9215\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2445 - accuracy: 0.9033\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2424 - accuracy: 0.9068\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2287 - accuracy: 0.9068\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.2403 - accuracy: 0.9009\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.2231 - accuracy: 0.9087\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2244 - accuracy: 0.9127\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.2285 - accuracy: 0.9082\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.2448 - accuracy: 0.9024\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2311 - accuracy: 0.9092\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2230 - accuracy: 0.9112\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.2218 - accuracy: 0.9146\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.2330 - accuracy: 0.9014\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2233 - accuracy: 0.9097\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2156 - accuracy: 0.9151\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.2104 - accuracy: 0.9151\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2188 - accuracy: 0.9136\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.2327 - accuracy: 0.9063\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2179 - accuracy: 0.9078\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2105 - accuracy: 0.9176\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.2180 - accuracy: 0.9156\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.2189 - accuracy: 0.9127\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.2343 - accuracy: 0.9009\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2340 - accuracy: 0.9127\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.2183 - accuracy: 0.9122\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2250 - accuracy: 0.9181\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.2302 - accuracy: 0.9107\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2375 - accuracy: 0.9033\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.2053 - accuracy: 0.9195\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2170 - accuracy: 0.9122\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9195\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2259 - accuracy: 0.9107\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9132\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 802us/step - loss: 0.2116 - accuracy: 0.9195\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.2124 - accuracy: 0.9176\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.2251 - accuracy: 0.9171\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.2166 - accuracy: 0.9156\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2099 - accuracy: 0.9190\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.2179 - accuracy: 0.9063\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.1945 - accuracy: 0.9205\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.2147 - accuracy: 0.9132\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2237 - accuracy: 0.9063\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2089 - accuracy: 0.9161\n",
      "Epoch 331/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.1988 - accuracy: 0.9289\n",
      "Epoch 332/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2137 - accuracy: 0.9132\n",
      "Epoch 333/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.2161 - accuracy: 0.9122\n",
      "Epoch 334/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.2137 - accuracy: 0.9156\n",
      "Epoch 335/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.2093 - accuracy: 0.9190\n",
      "Epoch 336/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.2165 - accuracy: 0.9132\n",
      "Epoch 337/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.2120 - accuracy: 0.9205\n",
      "Epoch 338/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.1995 - accuracy: 0.9205\n",
      "Epoch 339/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.2216 - accuracy: 0.9073\n",
      "Epoch 340/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.2124 - accuracy: 0.9161\n",
      "Epoch 341/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.1995 - accuracy: 0.9200\n",
      "Epoch 342/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2083 - accuracy: 0.9195\n",
      "Epoch 343/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2070 - accuracy: 0.9205\n",
      "Epoch 344/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.2047 - accuracy: 0.9293\n",
      "Epoch 345/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2057 - accuracy: 0.9176\n",
      "Epoch 346/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.1946 - accuracy: 0.9230\n",
      "Epoch 347/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.2197 - accuracy: 0.9181\n",
      "Epoch 348/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1948 - accuracy: 0.9205\n",
      "Epoch 349/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.2027 - accuracy: 0.9166\n",
      "Epoch 350/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.2120 - accuracy: 0.9166\n",
      "Epoch 351/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.2111 - accuracy: 0.9127\n",
      "Epoch 352/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.2002 - accuracy: 0.9156\n",
      "Epoch 353/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1978 - accuracy: 0.9215\n",
      "Epoch 354/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1873 - accuracy: 0.9259\n",
      "Epoch 355/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.2001 - accuracy: 0.9185\n",
      "Epoch 356/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.1964 - accuracy: 0.9190\n",
      "Epoch 357/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2032 - accuracy: 0.9146\n",
      "Epoch 358/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.1967 - accuracy: 0.9210\n",
      "Epoch 359/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.1966 - accuracy: 0.9210\n",
      "Epoch 360/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2032 - accuracy: 0.9171\n",
      "Epoch 361/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1967 - accuracy: 0.9166\n",
      "Epoch 362/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1987 - accuracy: 0.9244\n",
      "Epoch 363/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2031 - accuracy: 0.9215\n",
      "Epoch 364/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.1997 - accuracy: 0.9176\n",
      "Epoch 365/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2073 - accuracy: 0.9156\n",
      "Epoch 366/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2080 - accuracy: 0.9171\n",
      "Epoch 367/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1945 - accuracy: 0.9235\n",
      "Epoch 368/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.1849 - accuracy: 0.9289\n",
      "Epoch 369/1500\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.2142 - accuracy: 0.9205\n",
      "Epoch 370/1500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.2023 - accuracy: 0.9249\n",
      "Epoch 371/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.1697 - accuracy: 0.9377\n",
      "Epoch 372/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1889 - accuracy: 0.9249\n",
      "Epoch 373/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.2053 - accuracy: 0.9215\n",
      "Epoch 374/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.2068 - accuracy: 0.9117\n",
      "Epoch 375/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1855 - accuracy: 0.9230\n",
      "Epoch 376/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.1910 - accuracy: 0.9259\n",
      "Epoch 377/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2038 - accuracy: 0.9166\n",
      "Epoch 378/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.2089 - accuracy: 0.9176\n",
      "Epoch 379/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1962 - accuracy: 0.9161\n",
      "Epoch 380/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1909 - accuracy: 0.9274\n",
      "Epoch 381/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1801 - accuracy: 0.9308\n",
      "Epoch 382/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.1925 - accuracy: 0.9220\n",
      "Epoch 383/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1991 - accuracy: 0.9225\n",
      "Epoch 384/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2045 - accuracy: 0.9235\n",
      "Epoch 385/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.1785 - accuracy: 0.9318\n",
      "Epoch 386/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.1921 - accuracy: 0.9176\n",
      "Epoch 387/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.1985 - accuracy: 0.9269\n",
      "Epoch 388/1500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9279\n",
      "Epoch 389/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9259\n",
      "Epoch 390/1500\n",
      "32/32 [==============================] - 0s 818us/step - loss: 0.1984 - accuracy: 0.9244\n",
      "Epoch 391/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1855 - accuracy: 0.9264\n",
      "Epoch 392/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.1933 - accuracy: 0.9274\n",
      "Epoch 393/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.1945 - accuracy: 0.9284\n",
      "Epoch 394/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.1742 - accuracy: 0.9323\n",
      "Epoch 395/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.2018 - accuracy: 0.9215\n",
      "Epoch 396/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.1810 - accuracy: 0.9289\n",
      "Epoch 397/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.2065 - accuracy: 0.9200\n",
      "Epoch 398/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.2099 - accuracy: 0.9127\n",
      "Epoch 399/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.2016 - accuracy: 0.9107\n",
      "Epoch 400/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.1996 - accuracy: 0.9156\n",
      "Epoch 401/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2057 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 371.\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.1970 - accuracy: 0.9269\n",
      "Epoch 401: early stopping\n",
      "10/10 [==============================] - 0s 670us/step - loss: 0.6933 - accuracy: 0.7508\n",
      "10/10 [==============================] - 0s 597us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "After appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "Final Test Results - Loss: 0.6933193802833557, Accuracy: 0.7508196830749512, Precision: 0.7177212827382218, Recall: 0.7594496365524402, F1 Score: 0.7317914929855228\n",
      "Confusion Matrix:\n",
      " [[163   5  46]\n",
      " [  3  33   0]\n",
      " [ 22   0  33]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "071A    10\n",
      "016A    10\n",
      "072A     9\n",
      "022A     9\n",
      "045A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023A     6\n",
      "053A     6\n",
      "025C     5\n",
      "075A     5\n",
      "044A     5\n",
      "034A     5\n",
      "070A     5\n",
      "009A     4\n",
      "052A     4\n",
      "105A     4\n",
      "104A     4\n",
      "003A     4\n",
      "060A     3\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "025B     2\n",
      "073A     1\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "048A     1\n",
      "019B     1\n",
      "088A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'037A'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Test Set:\n",
      "{'037A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "782\n",
      "Length of y_train_val:\n",
      "782\n",
      "Length of groups_train_val:\n",
      "782\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     501\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     87\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1002, 2: 825, 1: 580})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.2465 - accuracy: 0.4794\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 1.0256 - accuracy: 0.5634\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.9252 - accuracy: 0.6136\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.8747 - accuracy: 0.6157\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.8422 - accuracy: 0.6518\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.8084 - accuracy: 0.6693\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.7838 - accuracy: 0.6772\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.7811 - accuracy: 0.6718\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.7443 - accuracy: 0.6830\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.7291 - accuracy: 0.6926\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.7081 - accuracy: 0.6988\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.6745 - accuracy: 0.7250\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.6942 - accuracy: 0.7046\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.6561 - accuracy: 0.7192\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.6592 - accuracy: 0.7200\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.6605 - accuracy: 0.7333\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.6162 - accuracy: 0.7416\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.6275 - accuracy: 0.7412\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.6133 - accuracy: 0.7399\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.6175 - accuracy: 0.7378\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.5966 - accuracy: 0.7611\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.6068 - accuracy: 0.7399\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.5880 - accuracy: 0.7486\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.5781 - accuracy: 0.7561\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.5797 - accuracy: 0.7520\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.5584 - accuracy: 0.7682\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.5563 - accuracy: 0.7653\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.5438 - accuracy: 0.7727\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.5653 - accuracy: 0.7624\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.5499 - accuracy: 0.7657\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.5482 - accuracy: 0.7661\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.5313 - accuracy: 0.7761\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.5365 - accuracy: 0.7781\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.5131 - accuracy: 0.7823\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.5226 - accuracy: 0.7881\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.4977 - accuracy: 0.7948\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.5165 - accuracy: 0.7798\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.4965 - accuracy: 0.7939\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.5000 - accuracy: 0.7877\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.4977 - accuracy: 0.7889\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.5088 - accuracy: 0.7794\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.4997 - accuracy: 0.7848\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.4992 - accuracy: 0.7869\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.4715 - accuracy: 0.8027\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.4776 - accuracy: 0.7989\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.4672 - accuracy: 0.8047\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.4787 - accuracy: 0.7985\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.4663 - accuracy: 0.8027\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.4760 - accuracy: 0.8027\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.4643 - accuracy: 0.8072\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.4619 - accuracy: 0.8060\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.4269 - accuracy: 0.8193\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.4435 - accuracy: 0.8184\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.4569 - accuracy: 0.8089\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.4468 - accuracy: 0.8081\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.4454 - accuracy: 0.8189\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.4442 - accuracy: 0.8160\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.4447 - accuracy: 0.8097\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.4277 - accuracy: 0.8164\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.4343 - accuracy: 0.8184\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.4250 - accuracy: 0.8193\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.8193\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.8151\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.4381 - accuracy: 0.8135\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.4134 - accuracy: 0.8297\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.4311 - accuracy: 0.8201\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4100 - accuracy: 0.8301\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.4111 - accuracy: 0.8238\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3988 - accuracy: 0.8284\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.4222 - accuracy: 0.8197\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.4051 - accuracy: 0.8317\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.4033 - accuracy: 0.8309\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.4025 - accuracy: 0.8288\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.3947 - accuracy: 0.8413\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.3966 - accuracy: 0.8309\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.4183 - accuracy: 0.8226\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.3933 - accuracy: 0.8384\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.4077 - accuracy: 0.8268\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.4020 - accuracy: 0.8338\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.3941 - accuracy: 0.8342\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.3882 - accuracy: 0.8359\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.3824 - accuracy: 0.8450\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3765 - accuracy: 0.8455\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3871 - accuracy: 0.8355\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.3707 - accuracy: 0.8434\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.3794 - accuracy: 0.8442\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3897 - accuracy: 0.8280\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.3785 - accuracy: 0.8529\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3789 - accuracy: 0.8405\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.3759 - accuracy: 0.8434\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3676 - accuracy: 0.8459\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.3755 - accuracy: 0.8488\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.3755 - accuracy: 0.8463\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3665 - accuracy: 0.8475\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3665 - accuracy: 0.8450\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3698 - accuracy: 0.8400\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.3673 - accuracy: 0.8513\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3469 - accuracy: 0.8662\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.3487 - accuracy: 0.8579\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.3514 - accuracy: 0.8517\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3623 - accuracy: 0.8446\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3490 - accuracy: 0.8538\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.3725 - accuracy: 0.8442\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3584 - accuracy: 0.8538\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.3693 - accuracy: 0.8446\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.3530 - accuracy: 0.8521\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.3500 - accuracy: 0.8504\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.3445 - accuracy: 0.8596\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3468 - accuracy: 0.8575\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.3540 - accuracy: 0.8533\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.3517 - accuracy: 0.8621\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3499 - accuracy: 0.8533\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3447 - accuracy: 0.8612\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.3526 - accuracy: 0.8500\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.3569 - accuracy: 0.8621\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.3510 - accuracy: 0.8525\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.3538 - accuracy: 0.8504\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.3417 - accuracy: 0.8587\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.3409 - accuracy: 0.8617\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.3233 - accuracy: 0.8691\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.3225 - accuracy: 0.8695\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.3345 - accuracy: 0.8646\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.3301 - accuracy: 0.8604\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.3305 - accuracy: 0.8687\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.3341 - accuracy: 0.8641\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.3336 - accuracy: 0.8729\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.3208 - accuracy: 0.8625\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3194 - accuracy: 0.8745\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3298 - accuracy: 0.8637\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3251 - accuracy: 0.8596\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3205 - accuracy: 0.8737\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3154 - accuracy: 0.8658\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3303 - accuracy: 0.8675\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.3310 - accuracy: 0.8621\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.3203 - accuracy: 0.8662\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3105 - accuracy: 0.8783\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.3269 - accuracy: 0.8662\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.3118 - accuracy: 0.8654\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3265 - accuracy: 0.8600\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.3198 - accuracy: 0.8666\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.3029 - accuracy: 0.8758\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.3117 - accuracy: 0.8695\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.3041 - accuracy: 0.8795\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.3097 - accuracy: 0.8712\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2982 - accuracy: 0.8774\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.3048 - accuracy: 0.8729\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.3017 - accuracy: 0.8783\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.3166 - accuracy: 0.8658\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.3085 - accuracy: 0.8749\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.3070 - accuracy: 0.8758\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2962 - accuracy: 0.8824\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2838 - accuracy: 0.8853\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3010 - accuracy: 0.8774\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3103 - accuracy: 0.8725\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3038 - accuracy: 0.8783\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2968 - accuracy: 0.8833\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2982 - accuracy: 0.8816\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2964 - accuracy: 0.8791\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2886 - accuracy: 0.8791\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2851 - accuracy: 0.8828\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.2931 - accuracy: 0.8799\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2924 - accuracy: 0.8824\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.2904 - accuracy: 0.8795\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.3043 - accuracy: 0.8758\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2801 - accuracy: 0.8916\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2897 - accuracy: 0.8862\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2849 - accuracy: 0.8874\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2890 - accuracy: 0.8866\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.2805 - accuracy: 0.8824\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2928 - accuracy: 0.8891\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2971 - accuracy: 0.8799\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2864 - accuracy: 0.8841\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2823 - accuracy: 0.8849\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2853 - accuracy: 0.8870\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.2981 - accuracy: 0.8737\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2803 - accuracy: 0.8837\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2649 - accuracy: 0.8970\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.2836 - accuracy: 0.8857\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2868 - accuracy: 0.8862\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.2728 - accuracy: 0.8920\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2769 - accuracy: 0.8916\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2790 - accuracy: 0.8857\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.2783 - accuracy: 0.8907\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2651 - accuracy: 0.8920\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2789 - accuracy: 0.8878\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2785 - accuracy: 0.8857\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.2711 - accuracy: 0.8986\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2914 - accuracy: 0.8803\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2742 - accuracy: 0.8924\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2779 - accuracy: 0.8920\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2702 - accuracy: 0.8907\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2735 - accuracy: 0.8903\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2694 - accuracy: 0.8970\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2692 - accuracy: 0.8912\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.2640 - accuracy: 0.8887\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2514 - accuracy: 0.8953\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2593 - accuracy: 0.8953\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2636 - accuracy: 0.8957\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2617 - accuracy: 0.8941\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2525 - accuracy: 0.9011\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2780 - accuracy: 0.8907\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2643 - accuracy: 0.8978\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.2612 - accuracy: 0.8966\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2548 - accuracy: 0.8999\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2751 - accuracy: 0.8891\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2660 - accuracy: 0.8957\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2629 - accuracy: 0.8895\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2460 - accuracy: 0.9032\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.2470 - accuracy: 0.8961\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2558 - accuracy: 0.9024\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2496 - accuracy: 0.8974\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2455 - accuracy: 0.8974\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.2489 - accuracy: 0.8999\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2497 - accuracy: 0.9007\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2539 - accuracy: 0.8978\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2490 - accuracy: 0.9015\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2550 - accuracy: 0.8957\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2552 - accuracy: 0.9061\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.2521 - accuracy: 0.9003\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.2458 - accuracy: 0.9007\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.2591 - accuracy: 0.8932\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2474 - accuracy: 0.8990\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2490 - accuracy: 0.9040\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2605 - accuracy: 0.8945\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2467 - accuracy: 0.8970\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2695 - accuracy: 0.8853\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2493 - accuracy: 0.8986\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2529 - accuracy: 0.9057\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.2426 - accuracy: 0.9011\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2330 - accuracy: 0.9090\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2447 - accuracy: 0.9069\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.2507 - accuracy: 0.9024\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2474 - accuracy: 0.9024\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2598 - accuracy: 0.8941\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2404 - accuracy: 0.9044\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2501 - accuracy: 0.9011\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2520 - accuracy: 0.8936\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2444 - accuracy: 0.9061\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2441 - accuracy: 0.9090\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2331 - accuracy: 0.9115\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2533 - accuracy: 0.9028\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2342 - accuracy: 0.9040\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2439 - accuracy: 0.8995\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2395 - accuracy: 0.9036\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2340 - accuracy: 0.9082\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2317 - accuracy: 0.9015\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2266 - accuracy: 0.9111\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2341 - accuracy: 0.9036\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2267 - accuracy: 0.9115\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.2301 - accuracy: 0.9082\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2296 - accuracy: 0.9123\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2277 - accuracy: 0.9103\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2356 - accuracy: 0.9107\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2304 - accuracy: 0.9098\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2179 - accuracy: 0.9157\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2306 - accuracy: 0.9111\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2278 - accuracy: 0.9082\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2322 - accuracy: 0.9040\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2270 - accuracy: 0.9094\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2254 - accuracy: 0.9144\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.2207 - accuracy: 0.9123\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2281 - accuracy: 0.9128\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2275 - accuracy: 0.9090\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.2306 - accuracy: 0.9082\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2196 - accuracy: 0.9128\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2311 - accuracy: 0.9065\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2373 - accuracy: 0.9082\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2350 - accuracy: 0.9098\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2240 - accuracy: 0.9157\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2082 - accuracy: 0.9173\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2260 - accuracy: 0.9161\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2303 - accuracy: 0.9136\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2203 - accuracy: 0.9069\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2168 - accuracy: 0.9128\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2227 - accuracy: 0.9111\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2205 - accuracy: 0.9107\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2185 - accuracy: 0.9107\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2157 - accuracy: 0.9123\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2232 - accuracy: 0.9161\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2338 - accuracy: 0.9053\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2088 - accuracy: 0.9123\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2082 - accuracy: 0.9256\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2054 - accuracy: 0.9252\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2043 - accuracy: 0.9269\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2174 - accuracy: 0.9144\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2111 - accuracy: 0.9148\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2114 - accuracy: 0.9157\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2090 - accuracy: 0.9198\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.2082 - accuracy: 0.9198\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2210 - accuracy: 0.9144\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2222 - accuracy: 0.9148\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2292 - accuracy: 0.9090\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2112 - accuracy: 0.9169\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2266 - accuracy: 0.9128\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2161 - accuracy: 0.9198\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2204 - accuracy: 0.9086\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2063 - accuracy: 0.9194\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2284 - accuracy: 0.9177\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2160 - accuracy: 0.9173\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.1923 - accuracy: 0.9248\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2156 - accuracy: 0.9128\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2191 - accuracy: 0.9136\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2174 - accuracy: 0.9169\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2262 - accuracy: 0.9148\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2033 - accuracy: 0.9177\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2024 - accuracy: 0.9252\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2128 - accuracy: 0.9198\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2107 - accuracy: 0.9177\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2029 - accuracy: 0.9215\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2196 - accuracy: 0.9144\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2069 - accuracy: 0.9194\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2120 - accuracy: 0.9177\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1967 - accuracy: 0.9244\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2148 - accuracy: 0.9161\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1997 - accuracy: 0.9215\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2089 - accuracy: 0.9190\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2112 - accuracy: 0.9111\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2148 - accuracy: 0.9148\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1896 - accuracy: 0.9244\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1910 - accuracy: 0.9273\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2085 - accuracy: 0.9169\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2029 - accuracy: 0.9231\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.1977 - accuracy: 0.9240\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1979 - accuracy: 0.9236\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.2128 - accuracy: 0.9132\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.1961 - accuracy: 0.9227\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.1969 - accuracy: 0.9306\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2113 - accuracy: 0.9182\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2072 - accuracy: 0.9177\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2030 - accuracy: 0.9182\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2014 - accuracy: 0.9248\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1899 - accuracy: 0.9265\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1912 - accuracy: 0.9265\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1909 - accuracy: 0.9260\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2053 - accuracy: 0.9202\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1979 - accuracy: 0.9223\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1936 - accuracy: 0.9231\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.1915 - accuracy: 0.9281\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.2143 - accuracy: 0.9144\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1984 - accuracy: 0.9256\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2004 - accuracy: 0.9190\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1987 - accuracy: 0.9198\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1974 - accuracy: 0.9223\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2016 - accuracy: 0.9169\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1957 - accuracy: 0.9194\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1908 - accuracy: 0.9248\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1841 - accuracy: 0.9285\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1976 - accuracy: 0.9202\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1940 - accuracy: 0.9219\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1681 - accuracy: 0.9356\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.1977 - accuracy: 0.9269\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1961 - accuracy: 0.9194\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.2040 - accuracy: 0.9269\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9227\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.1885 - accuracy: 0.9302\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.1848 - accuracy: 0.9327\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1998 - accuracy: 0.9198\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1969 - accuracy: 0.9219\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2025 - accuracy: 0.9273\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1968 - accuracy: 0.9302\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1918 - accuracy: 0.9256\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9277\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1830 - accuracy: 0.9331\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1983 - accuracy: 0.9219\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1803 - accuracy: 0.9281\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.1884 - accuracy: 0.9294\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.1749 - accuracy: 0.9281\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1813 - accuracy: 0.9231\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1903 - accuracy: 0.9211\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1935 - accuracy: 0.9206\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1861 - accuracy: 0.9248\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.1911 - accuracy: 0.9265\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1838 - accuracy: 0.9344\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.2006 - accuracy: 0.9240\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.1827 - accuracy: 0.9260\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1820 - accuracy: 0.9285\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1902 - accuracy: 0.9273\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1830 - accuracy: 0.9273\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1843 - accuracy: 0.9302\n",
      "Epoch 380/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.3055 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 350.\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.1849 - accuracy: 0.9256\n",
      "Epoch 380: early stopping\n",
      "5/5 [==============================] - 0s 995us/step - loss: 0.6412 - accuracy: 0.7484\n",
      "5/5 [==============================] - 0s 659us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "After appending - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n",
      "Final Test Results - Loss: 0.641211748123169, Accuracy: 0.7483870983123779, Precision: 0.6703355938097721, Recall: 0.743273584652895, F1 Score: 0.6782644157148914\n",
      "Confusion Matrix:\n",
      " [[61  8 18]\n",
      " [ 6 46  3]\n",
      " [ 4  0  9]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.696302635519683\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8158645629882812\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7166695445775986\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6893478314059556\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7298056400227385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.73 (80/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, kitten, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[kitten, senior, adult, senior, kitten, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, senior, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, adult...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                     [kitten, kitten, adult, adult]         adult            adult                   True\n",
       "63    057A  [adult, adult, adult, senior, senior, adult, s...        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, senior, adult, senior, ...        senior           senior                   True\n",
       "80    074A  [adult, adult, adult, kitten, adult, adult, ad...         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, adult, adult, kitten, a...         adult            adult                   True\n",
       "92    097A  [senior, senior, senior, adult, adult, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "20    019A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "2     002A  [adult, adult, senior, adult, senior, adult, a...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "109   117A  [senior, senior, adult, senior, adult, adult, ...        senior           senior                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "40    034A               [adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "42    036A  [adult, senior, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "43    037A  [senior, senior, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "59    053A    [kitten, senior, adult, senior, kitten, senior]        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "4     003A                    [senior, senior, adult, senior]        senior            adult                  False\n",
       "12    011A                                    [adult, senior]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "71    065A  [senior, adult, kitten, adult, adult, senior, ...        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "89    094A  [senior, senior, adult, adult, adult, senior, ...         adult           senior                  False\n",
       "69    063A  [senior, adult, senior, adult, senior, senior,...        senior            adult                  False\n",
       "74    068A  [senior, senior, senior, senior, senior, adult...        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "34    027A  [adult, senior, senior, adult, senior, senior,...        senior            adult                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "36    029A  [senior, adult, adult, adult, senior, senior, ...        senior            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "17    015A  [senior, senior, adult, adult, senior, senior,...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, senior,...        senior            adult                  False"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     55\n",
      "kitten    12\n",
      "senior    13\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             55  75.342466\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22             13  59.090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABmWElEQVR4nO3deXhM5///8eckEpFFEiEitthJ1b6kaO1rba1WdVVqq71VH60qWnSztJYqpdRWRWsvSkutSdVaKmJrCLGLyIYs8/sjv5xvRoKYhCTm9bgu12XOOXPO+0zmzLzmPve5j8lsNpsREREREbERdtldgIiIiIjIo6QALCIiIiI2RQFYRERERGyKArCIiIiI2BQFYBERERGxKQrAIiIiImJTFIBFRERExKYoAIuIiIiITVEAFhHJxRISErK7hCz3OO6TiOQsebK7AJGMiouLo1WrVsTExABQoUIFFi1alM1VSWacPHmSb775hoMHDxITE0OBAgVo2LAhw4YNu+tzatWqZfE4f/78/P7779jZWf6e/+KLL1i2bJnFtFGjRtGuXTurat2zZw99+vQBoEiRIqxZs8aq9TyI0aNHs3btWgB69uxJ7969LeZv3LiRZcuWMWvWrCzd7u3bt2nZsiVRUVEAvPnmm/Tv3/+uy7dt25YLFy4A0KNHD+N1elBRUVF89913eHh48NZbb1m1jqy2Zs0aPv74YwBq1KjBd999l631fPzxxxbvvcWLF1OuXLlsrCjjIiMj+fXXX9myZQvnzp0jIiKCPHnyUKhQISpXrkzbtm2pU6dOdpcpNkItwJJrbNq0yQi/ACEhIfz777/ZWJFkRnx8PH379mXbtm1ERkaSkJDApUuXuHjx4gOt58aNGwQHB6eZvnv37qwqNce5cuUKPXv2ZPjw4UbwzEqOjo40bdrUeLxp06a7Lnv48GGLGlq3bm3VNrds2cLzzz/P4sWL1QJ8FzExMfz+++8W05YvX55N1TyYHTt20LlzZyZNmsT+/fu5dOkS8fHxxMXFcebMGdatW0ffvn0ZPnw4t2/fzu5yxQaoBVhyjVWrVqWZtmLFCp544olsqEYy6+TJk1y9etV43Lp1azw8PKhSpcoDr2v37t0W74NLly5x+vTpLKkzhY+PD127dgXAzc0tS9d9Nw0aNMDLywuAatWqGdNDQ0PZv3//Q912q1atWLlyJQDnzp3j33//TfdY++OPP4z/+/v7U7JkSau2t3XrViIiIqx6rq3YtGkTcXFxFtPWr1/PoEGDcHJyyqaq7m/z5s3873//Mx47OztTt25dihQpwvXr1/nrr7+Mz4KNGzfi4uLChx9+mF3lio1QAJZcITQ0lIMHDwLJp7xv3LgBJH9YvvPOO7i4uGRneWKF1K353t7ejBkz5oHX4eTkxM2bN9m9ezfdunUzpqdu/c2XL1+a0GCNYsWKMWDAgEyv50E0a9aMZs2aPdJtpqhZsyaFCxc2WuQ3bdqUbgDevHmz8f9WrVo9svpsUepGgJTPwejoaDZu3Ej79u2zsbK7O3v2rNGFBKBOnTqMGzcOT09PY9rt27cZM2YM69evB2DlypW89tprVv+YEskIBWDJFVJ/8L/44osEBQXx77//Ehsby4YNG+jUqdNdn3v06FEWLFjAvn37uH79OgUKFKBMmTJ06dKFevXqpVk+OjqaRYsWsWXLFs6ePYuDgwO+vr60aNGCF198EWdnZ2PZe/XRvFef0ZR+rF5eXsyaNYvRo0cTHBxM/vz5+d///kfTpk25ffs2ixYtYtOmTYSFhXHr1i1cXFwoVaoUnTp14tlnn7W69u7du/PPP/8AMHjwYF577TWL9SxevJiJEycCya2QX3/99V1f3xQJCQmsWbOGdevW8d9//xEXF0fhwoWpX78+r7/+Ot7e3say7dq14/z588bjS5cuGa/J6tWr8fX1ve/2AKpUqcLu3bv5559/uHXrFnnz5gXg77//NpapWrUqQUFB6T7/ypUrfP/99wQGBnLp0iUSExPx8PDA39+fbt26WbRGZ6QP8MaNG1m9ejXHjx8nKioKLy8v6tSpw+uvv46fn5/FsjNnzjT67r7//vvcuHGDH3/8kbi4OPz9/Y33xZ3vr9TTAM6fP0+tWrUoUqQIH374odFX193dnd9++408ef7vYz4hIYFWrVpx/fp1AObPn4+/v3+6r43JZKJly5bMnz8fSA7AgwYNwmQyGcsEBwdz7tw5AOzt7WnRooUx7/r16yxbtozNmzcTHh6O2WymZMmSNG/enM6dO1u0WN7Zr3vWrFnMmjUrzTH1+++/s3TpUkJCQkhMTKR48eI0b96cV155JU0LaGxsLAsWLGDr1q2EhYVx+/ZtXF1dKVeuHB06dLC6q8aVK1eYMmUKO3bsID4+ngoVKtC1a1eefvppAJKSkmjXrp3xw+GLL76w6E4CMHHiRBYvXgwkf57dq897ipMnT3Lo0CHg/85GfPHFF0DymbB7BeCzZ88yY8YMgoKCiIuLo2LFivTs2RMnJyd69OgBJPfjHj16tMXzHuT1vpt58+YZP3aLFCnChAkTLD5DIbnLzYcffsi1a9fw9vamTJkyODg4GPMzcqykOHToEEuXLuXAgQNcuXIFNzc3KleuTOfOnQkICLDY7v2O6dSfUzNmzDDep6mPwa+++go3Nze+++47Dh8+jIODA3Xq1KFfv34UK1YsQ6+RZA8FYMnxEhIS+PXXX43H7dq1w8fHx+j/u2LFirsG4LVr1zJmzBgSExONaRcvXuTixYvs2rWL/v378+abbxrzLly4wNtvv01YWJgx7ebNm4SEhBASEsIff/zBjBkz0nyAW+vmzZv079+f8PBwAK5evUr58uVJSkriww8/ZMuWLRbLR0VF8c8///DPP/9w9uxZi3DwILW3b9/eCMAbN25ME4BT9/ls27btfffj+vXrDBkyxGilT3HmzBnOnDnD2rVrGT9+fJqgk1k1a9Zk9+7d3Lp1i/379xtfcHv27AGgRIkSFCxYMN3nRkRE0KtXL86cOWMx/erVq2zfvp1du3YxZcoU6tate986bt26xfDhw9m6davF9PPnz7Nq1SrWr1/PqFGjaNmyZbrPX758OceOHTMe+/j43Heb6alTpw4+Pj5cuHCByMhIgoKCaNCggTF/z549RvgtXbr0XcNvitatWxsB+OLFi/zzzz9UrVrVmJ+6+0Pt2rWN1zo4OJghQ4Zw6dIli/UFBwcTHBzM2rVrmTp1KoULF87wvqV3UePx48c5fvw4v//+O99++y3u7u5A8vu+R48eFq8pJF+EtWfPHvbs2cPZs2fp2bNnhrcPye+Nrl27WvRTP3DgAAcOHODdd9/llVdewc7OjrZt2/L9998DycdX6gBsNpstXreMXpSZuhGgbdu2tG7dmq+//ppbt25x6NAhTpw4QdmyZdM87+jRo7z99tvGBY0ABw8eZMCAATz33HN33d6DvN53k5SUZHGGoFOnTnf97HRycuKbb7655/rg3sfKnDlzmDFjBklJSca0a9eusW3bNrZt28bLL7/MkCFD7ruNB7Ft2zZWr15t8R2zadMm/vrrL2bMmEH58uWzdHuSdXQRnOR427dv59q1awBUr16dYsWK0aJFC/Llywckf8CndxHUqVOnGDdunPHBVK5cOV588UWLVoBp06YREhJiPP7www+NAOnq6krbtm3p0KGD0cXiyJEjfPvtt1m2bzExMYSHh/P000/z3HPPUbduXYoXL86OHTuM8Ovi4kKHDh3o0qWLxYfpjz/+iNlstqr2Fi1aGF9ER44c4ezZs8Z6Lly4YLQ05c+fn2eeeea++/Hxxx8b4TdPnjw0btyY5557zgg4UVFRvPfee8Z2OnXqZBEGXVxc6Nq1K127dsXV1TXDr1/NmjWN/6e0+p4+fdoIKKnn3+mHH34wwm/RokXp0qULzz//vBHiEhMT+emnnzJUx5QpU4zwazKZqFevHp06dTJO4d6+fZtRo0YZr+udjh07RsGCBencuTM1atS4a1CG5Bb59F67Tp06YWdnZxGoNm7caPHcB/1hU65cOcqUKZPu8yH97g9RUVEMHTrUCL8eHh60a9eOli1bGu+5U6dO8e677xoXu3Xt2tViO1WrVqVr165Gv+dff/3VCGMmk4lnnnmGTp06GWcVjh07xpdffmk8f926dUZI8vT0pH379rzyyisWIwzMmjXL4n2fESnvrQYNGvD8889bBPjJkycTGhoKJIfalJbyHTt2EBsbayx38OBB47XJyI8QSL5gdN26dcb+t23bFldXV4tgnd7FcElJSXz00UdG+M2bNy+tW7emTZs2ODs73/UCugd9ve8mPDycyMhI43HqfuzWutuxsnnzZqZPn26E34oVK/Liiy9So0YN47mLFy9m4cKFma4htRUrVuDg4EDr1q1p3bq1cRbqxo0bjBgxwuIzWnIWtQBLjpe65SPly93FxYVmzZoZp6yWL1+e5qKJxYsXEx8fD0CjRo34/PPPjdPBY8eOZeXKlbi4uLB7924qVKjAwYMHjRDn4uLCwoULjVNY7dq1o0ePHtjb2/Pvv/+SlJSUZtgtazVu3Jjx48dbTHN0dKRjx44cP36cPn368NRTTwHJLVvNmzcnLi6OmJgYrl+/jqen5wPX7uzsTLNmzVi9ejWQHJS6d+8OJJ/2TPnQbtGiBY6Ojves/+DBg2zfvh1IPg3+7bffUr16dSC5S0bfvn05cuQI0dHRzJ49m9GjR/Pmm2+yZ88efvvtNyA5aFvTv7Zy5coW/YDBsvtDzZo179r9oXjx4rRs2ZIzZ84wefJkChQoACS3eqa0DKac3r+XCxcuWLSUjRkzxgiDt2/fZtiwYWzfvp2EhASmTp1612G0pk6dmqHhrJo1a4aHh8ddX7v27dsze/ZszGYzW7duNbqGJCQk8OeffwLJf6c2bdrcd1uQ/HpMmzYNSH5vvPvuu9jZ2XHs2DHjB0TevHlp3LgxAMuWLTNGhfD19WXOnDnGj4rQ0FC6du1KTEwMISEhrF+/nnbt2jFgwACuXr3KyZMngeSW7NRnN+bNm2f8//333zfO+PTr148uXbpw6dIlNm3axIABA/Dx8bH4u/Xr14+OHTsaj7/55hsuXLhAqVKlLFrtMup///sfnTt3BpJDTvfu3QkNDSUxMZFVq1YxaNAgihUrRq1atfj777+5desW27ZtM94TqX9EpNeNKT1bt241Wu5TGgEAOnToYATj9evXM3DgQIuuCXv27OG///4Dkv/m3333ndGPOzQ0lFdffZVbt26l2d6Dvt53k/oiV8A4xlL89ddf9OvXL93nptclI0V6x0rKexSSf2APGzbM+IyeO3eu0bo8a9YsOnbs+EA/tO/F3t6e2bNnU7FiRQBeeOEFevTogdls5tSpU+zevTtDZ5Hk0VMLsORoly5dIjAwEEi+mCn1BUEdOnQw/r9x40aLVhb4v9PgAJ07d7boC9mvXz9WrlzJn3/+yeuvv55m+Weeecai/1a1atVYuHAh27ZtY86cOVkWfoF0W/sCAgIYMWIE8+bN46mnnuLWrVscOHCABQsWWLQopHx5WVP7na9fitTDLGWklTD18i1atDDCLyS3RKceP3br1q0WpyczK0+ePEY/3ZCQECIjIy0ugLtXl4sXXniBcePGsWDBAgoUKEBkZCQ7duyw6G6TXji40+bNm419qlatmsWFYI6OjhanXPfv328EmdRKly6dZWO5FilSxGjpjImJYefOnUDyhYEprXF169a9a9eQO7Vq1cpozbxy5Qr79u0DLLs/PPPMM8aZhtTvh+7du1tsx8/Pjy5duhiP7+zik54rV65w6tQpABwcHCzCbP78+WnYsCGQ3NqZ8uMnJYwAjB8/nvfee48lS5YY3QHGjBlD9+7dH/giK3d3d4vuVvnz5+f55583Hh8+fNj4f+rjK+XHSuouAfb29hkOwHd2f0hRo0YNihcvDiS3vN85RFrqLklPPfWUxUWMfn5+6f4Isub1vpuU1tAU1vzguFN6x0pISIjxY8zJyYmBAwdafEa/8cYbFClSBEg+Ju5X94No3LixxfutatWqRoMFkKZbmOQcagGWHG3NmjXGh6a9vT3vvfeexXyTyYTZbCYmJobffvvNok9b6v6HKR9+KTw9PS2uQr7f8mD5pZoRGT31ld62ILllcfny5QQFBRkXodwpJXhZU3vVqlXx8/MjNDSUEydO8N9//5EvXz7jS9zPz4/KlSvft/7UfY7T207qaVFRUURGRqZ57TMjpR9wyhfy3r17AShZsuR9Q97hw4dZtWoVe/fuTdMXGMhQWL/f/hcrVgwXFxdiYmIwm82cO3cODw8Pi2Xu9h6wVocOHfjrr7+A5BbHJk2aPHD3hxQ+Pj5Ur17dCL6bNm2iVq1aFt0fUgepB3k/ZKQLQuoxhuPj4+/ZmpbS2tmsWTPjx8ytW7f4888/jdbv/Pnz06hRI15//XVKlSp13+2nVrRoUezt7S2mpb64MXWLZ+PGjXFzcyMqKoqgoCCioqI4fvw4ly9fBjL+I+TChQvG3xKSR0jYsGGD8fjmzZvG/5cvX27xt03ZFpBu2E9v/615ve/mzj7eFy9etNimr6+vMbQgJHcXSTkLcDfpHSup33PFixdPMyqQvb095cqVMy5oS738vWTk+E/vdfXz82PXrl1A2lZwyTkUgCXHMpvNxil6SD6dfq+bG6xYseKuF3U8aMuDNS0VdwbelO4X95PeEG4pF6nExsZiMpmoVq0aNWrUoEqVKowdO9bii+1OD1J7hw4dmDx5MpDcCpz6ApWMhqTULevpufN1ST2KQFZI3c934cKFRivnvfr/QnIXmUmTJmE2m3FycqJhw4ZUq1YNHx8fPvjggwxv/377f6f09j+rh/Fr1KgR7u7uREZGsn37dm7cuGH0UXZzczNa8TKqVatWRgDevHkznTp1MsKPu7u7RYvXg74f7id1CLGzs7vnj6eUdZtMJj7++GOee+451q9fT2BgoHGh6Y0bN1i9ejXr169nxowZFhf13U96N+hIfbyl3ve8efPSqlUrli1bRnx8PFu2bLG4ViGjrb9r1qyxeA1SLl5Nzz///MPJkyeN/tSpX+uMnnmx5vW+G09PT4oWLWp0SdmzZ4/FNRjFixe36L6TuhvM3aR3rGTkGExda3rHYHqvT0ZuyJLeTTtSj2CR1Z93knUUgCXH2rt3b4b6YKY4cuQIISEhVKhQAUgeWzbll35oaKhFS82ZM2f45ZdfKF26NBUqVKBixYoWw3SldxOFb7/9Fjc3N8qUKUP16tVxcnKyOM2WuiUGSPdUd3pSf1immDRpktGlI3WfUkj/Q9ma2iH5S/ibb74hISHBGIAekr/4MtpHNHWLTOoLCtOblj9//vteOf6gnnjiCaMfcOpT0PcKwDdu3GDq1KmYzWYcHBxYunSpMfRayunfjLrf/p89e9YYBsrOzo6iRYumWSa990BmODo60rp1a3766Sdu3rzJ+PHjjbGzmzdvnubU9P00a9aM8ePHEx8fT0REhMUFUM2bN7cIIEWKFDEuugoJCUnTCpz6NSpRosR9t536ve3g4MD69estjrvExMQ0rbIp/Pz8GDp0KHny5OHChQscOHCAn3/+mQMHDhAfH8/s2bOZOnXqfWtIcfbsWW7evGnRzzb1mYM7W3Q7dOhg9A/fsGGDEe5cXV1p1KjRfbdnNpsf+JbbK1asMM6UFSpUKN06U5w4cSLNtMy83ulp1aqVMSJGyvi+d54BSZGRkJ7esZL6GAwLCyMmJsYiKCcmJlrsa0q3kdT7cefnd1JSknHM3Et6r2Hq1zr130ByFvUBlhwr5S5UAF26dDGGL7rzX+oru1Nf1Zw6AC1dutSiRXbp0qUsWrSIMWPGGB/OqZcPDAy0aIk4evQo33//PV9//TWDBw82fvXnz5/fWObO4JS6j+S9pNdCcPz4ceP/qb8sAgMDLe6WlfKFYU3tkHxRSsr4padPn+bIkSNA8kVIqb8I7yX1KBG//fYbBw4cMB7HxMRYDG3UqFGjLG8RcXBwSPfucfcKwKdPnzZeB3t7e4s7u6VcVAQZ+0JOvf/79++36GoQHx/PV199ZVFTej8AHvQ1Sf3FfbdWqtR9UFNuMAAP1v0hRf78+alfv77xOPXf+M6bX6R+PebMmcOVK1eMx6dPn2bJkiXG45QL5wCLkJV6n3x8fIwfDbdu3eKXX34x5sXFxdGxY0c6dOjAO++8Y4SRjz76iBYtWtCsWTPjM8HHx4dWrVrxwgsvGM9/0Ntup4wtnCI6OtriAsg7RzmoWLGi8YN89+7dxunwjP4I+euvv4yWa3d3d4KCgtL9DEx9E5l169YZfddT98cPDAw0jm9IHk0hdVeKFNa83vfSuXNn4zPs+vXrvPPOO2mGx7t9+zZz585NM2pJetI7VsqXL2+E4Js3bzJt2jSLFt8FCxYY3R9cXV2pXbs2YHlHxxs3bli8V7du3Zqhs3gpf5MUJ06cMLo/gOXfQHIWtQBLjhQVFWVxgcy97obVsmVLo2vEhg0bGDx4MPny5aNLly6sXbuWhIQEdu/ezcsvv0zt2rU5d+6cxQfUSy+9BCR/eVWpUsW4qUK3bt1o2LAhTk5OFqGmTZs2RvBNfTHGrl27+Oyzz6hQoQJbt241Lj6yRsGCBY0vvuHDh9OiRQuuXr3Ktm3bLJZL+aKzpvYUHTp0SHMx0oOEpJo1a1K9enX2799PYmIiffr04ZlnnsHd3Z3AwECjT6Gbm9sDj7uaUTVq1LDoHnO//r+p5928eZNu3bpRt25dgoODLU4xZ+QiuGLFitG6dWsjZA4fPpy1a9dSpEgR9uzZYwyN5eDgYHFBYGakbt26fPkyo0aNArC441a5cuXw9/e3CD0lSpSw6lbTkBx0U/rRpihatGia0PfCCy/wyy+/EBERwblz53j55Zdp0KABCQkJbN261Tiz4e/vbxGeU+/T6tWriY6Oply5cjz//PO88sorxkgpX3zxBdu3b6dEiRL89ddfRrBJSEgw+mOWLVvW+HtMnDiRwMBAihcvbowJm+JBuj+kmDlzJv/88w/FihVj165dxlmqvHnzpnszig4dOqQZMiyjx1fqi98aNWp011P9DRs2JG/evNy6dYsbN27w+++/8+yzz1KzZk1Kly7NqVOnSEpKolevXjRp0gSz2cyWLVvSPX0PPPDrfS9eXl6MGDGCYcOGkZiYyKFDh3juueeoV68eRYoUISIigsDAwDRnzB6kW5DJZOKtt95i7NixQPJIJIcPH6Zy5cqcPHnS6L4D0Lt3b2PdJUqUMF43s9nM4MGDee655wgPD8/wEIhms5kBAwbQqFEjnJyc2Lx5s/G5Ub58eYth2CRnUQuw5Ejr1683PkQKFSp0zy+qJk2aGKfFUi6Gg+QvwQ8++MBoLQsNDWXZsmUW4bdbt24WIwWMHTvWaP2IjY1l/fr1rFixgujoaCD5CuTBgwdbbDv1Ke1ffvmFTz/9lJ07d/Liiy9avf8pI1NAcsvEzz//zJYtW0hMTLQYvif1xRwPWnuKp556yuI0nYuLS4ZOz6aws7Pjs88+o1KlSkDyF+PmzZtZsWKFEX7z58/PxIkTs/xirxR3jvZwv/6/RYoUsfhRFRoaypIlS/jnn3/IkyePcYo7MjIyQ6dBP/jgA6Nvo9lsZufOnfz8889G+M2bNy9jxoxJ91bC1ihVqpRFS/Kvv/7K+vXr07QG3xnIrGn9TfH000+nCSXpjWBSsGBBvvzyS7y8vIDkG46sWbOG9evXG+G3bNmyTJgwwaIlO3WQvnr1KsuWLTOuoH/xxRcttrVr1y5++uknox+yq6srX3zxhfE58Nprr9G8eXMg+fT39u3b+fHHH9mwYYNRg5+fH3379n2g16B58+Z4eXkRGBjIsmXLjPBrZ2fH+++/n+6QYKnHhoXk0JWR4B0ZGWlxY5V7NQI4OztbtLyvWLHCqGvMmDHG3+3mzZusW7eO9evXk5SUZLxGYNmy+qCv9/00atSIb775xnhP3Lp1iy1btvDjjz+yfv16i/Dr5uZG7969eeeddzK07hQdO3bkzTffNPYjODiYZcuWWYTfV199lZdfftl47OjoaDSAQPLZss8++4x58+ZRuHBhi7OLd1OrVi3s7OzYtGkTa9asMbo7ubu7W3V7d3l0FIAlR0rd8tGkSZN7niJ2c3OzuKVxyoc/JLe+zJ071/jisre3J3/+/NStW5cJEyakGYPS19eXBQsW0L17d0qVKkXevHnJmzcvZcqUoVevXsybN88ieOTLl4/Zs2fTunVrPDw8cHJyonLlyowdOzbdsJlRL774Ip9//jn+/v44OzuTL18+KleuzJgxYyzWm7qbxYPWnsLe3t4imDVr1izDtzlNUbBgQebOncsHH3xAjRo1cHd3x9HRkeLFi/Pyyy+zZMmSh9oSktIPOMX9AjDAJ598Qt++ffHz88PR0RF3d3caNGjA7NmzjVPzZrPZGO3gzouDUnN2dmbq1KmMHTuWevXq4eXlhYODAz4+PnTo0IEff/zxngHmQTk4ODB+/Hj8/f1xcHAgf/781KpVK02LderWXpPJlOF+3enJmzcvTZo0sZh2t9sJV69enZ9++omePXtSvnx54z1cqVIlBg0axA8//JCmi02TJk3o3bs33t7e5MmTh8KFCxstjHZ2dowdO5YxY8ZQu3Zti/fX888/z6JFiyxGLLG3t2fcuHF8+eWXBAQEUKRIEfLkyYOLiwuVKlWiT58+zJ8//4FHI/H19WXRokW0a9fOON5r1KjBtGnT7npHNzc3N4uW0oz+DdavX2+00Lq7uxun7e8mdWA9cOCAEVYrVKjAvHnzaNy4Mfnz5ydfvnzUrVuXOXPmWATxlBsLwYO/3hlRq1YtfvnlF4YMGUKdOnUoUKAA9vb2uLi4UKJECVq1asXo0aNZt24dPXv2fOCLSwH69+/P7NmzadOmDUWKFMHBwQFPT0+eeeYZpk+fnm6oHjBgAIMHD6ZkyZI4OjpSpEgRXn/9debPn5+h6xWqV6/O999/T+3atXFycsLd3d24hXjqm7tIzmMy6zYlIjbtzJkzdOnSxfiynTlzZoYCpK354YcfjMH2y5QpY9GXNaf65JNPjJFUatasycyZM7O5Ituzb98+evXqBST/CFm1apVxweXDduHCBdavX4+Hhwfu7u5Ur17dIvR//PHHxkV2gwcPTnNLdEnf6NGjWbt2LQA9e/a0uGmL5B7qAyxig86fP8/SpUtJTExkw4YNRvgtU6aMwu8dNmzYwPjx4y1u6fqwunJkhZ9//plLly5x9OhRi+4+memSIw/m6NGjbNq0idjYWIsbq9SvX/+RhV9IPoOR+iLU4sWLU69ePezs7Dhx4oRxQwiTyUSDBg0eWV0iOUGODcAXL17kpZdeYsKECRb9+8LCwpg0aRL79+/H3t6eZs2aMWDAAIt+kbGxsUydOpXNmzcTGxtL9erVeffddy2GwRKxZSaTyeJqdkg+rT506NBsqijn+vfffy3CLyTf8S6nOnLkiMX42ZB8Z8GmTZtmU0W2Jy4uzuJ2wpDcb3bQoEGPtI4iRYrw3HPPGd3CwsLC0j1z8corr+j7UWxOjgzAFy5cYMCAAcbFOymioqLo06cPXl5ejB49moiICKZMmUJ4eLjFWI4ffvghhw8fZuDAgbi4uDBr1iz69OnD0qVL01wBL2KLChUqRPHixbl06RJOTk5UqFCB7t273/PWwbbM3d2d2NhYfH19eemllzLVl/ZhK1++PB4eHsTFxVGoUCGaNWtGjx49NCD/I+Tr64uPjw/Xrl3Dzc2NypUr06tXrwe+81xWGD58OFWrVuW3337j+PHjxgVn7u7uVKhQgY4dO6bp2y1iC3JUH+CkpCR+/fVXvv76ayD5KtgZM2YYX8pz587l+++/Z+3atca4gjt37mTQoEHMnj2batWq8c8//9C9e3cmT55sjFsZERFB+/btefPNN3nrrbeyY9dEREREJIfIUaNAHD9+nM8++4xnn33WYjzLFIGBgVSvXt3ixgABAQG4uLgYY64GBgaSL18+i9stenp6UqNGjUyNyyoiIiIij4ccFYB9fHxYsWIF7777brrDMIWGhqa5daa9vT2+vr7G7V9DQ0MpWrRomls1Fi9ePN1bxIqIiIiIbclRfYDd3d3vOe5edHR0uneHcXZ2NgafzsgyDyokJMR4bkYH/hYRERGRRys+Ph6TyXTf21DnqAB8P6kHor9TysD0GVnGGildpe9260gRERERyR1yVQB2dXU1bmOZWkxMjHFXIVdXV65du5buMqmHSnsQFSpU4NChQ5jNZsqWLWvVOkRERETk4Tpx4kSGRr3JVQG4ZMmShIWFWUxLTEwkPDzcuHVpyZIlCQoKIikpyaLFNywsLNPjHJpMJpydnTO1DhERERF5ODI65GOOugjufgICAti3bx8RERHGtKCgIGJjY41RHwICAoiJiSEwMNBYJiIigv3791uMDCEiIiIitilXBeAXXniBvHnz0q9fP7Zs2cLKlSv56KOPqFevHlWrVgWgRo0a1KxZk48++oiVK1eyZcsW+vbti5ubGy+88EI274GIiIiIZLdc1QXC09OTGTNmMGnSJEaMGIGLiwtNmzZl8ODBFsuNHz+er776ismTJ5OUlETVqlX57LPPdBc4EREREclZd4LLyQ4dOgTAk08+mc2ViIiIiEh6MprXclUXCBERERGRzFIAFhERERGbogAsIiIiIjZFAVhEREREbIoCsIiIiIjYFAVgEREREbEpCsAiIiIiYlMUgEVERETEpigAi4iIiIhNUQAWEREREZuiACwiIiIiNkUBWERERERsigKwiIiIiNgUBWARERERsSkKwCIiIiJiUxSARURERMSmKACLiIiIiE1RABYRERERm6IALCIiIiI2RQFYRERERGyKArCIiIiI2BQFYBERERGxKQrAIiIiImJTFIBFRERExKYoAIuIiIiITVEAFhERERGbogAsIiIiIjZFAVhEREREbIoCsIiIiIjYFAVgEREREbEpCsAiIiIiYlMUgEVERETEpigAi4iIiIhNUQAWEREREZuiACwiIiIiNiVPdhcgIiKZt2LFChYvXkx4eDg+Pj507tyZF198EZPJBEBYWBiTJk1i//792Nvb06xZMwYMGICrq+s913vkyBG+/vprgoODcXFxoV27dvTq1QsHB4dHsVsiIg+FArCISC63cuVKxo0bx0svvUTDhg3Zv38/48eP5/bt27z22mtERUXRp08fvLy8GD16NBEREUyZMoXw8HCmTp161/WePXuWvn37UqVKFT777DNCQ0OZPn06kZGRDB8+/BHuoYhI1lIAFhHJ5VavXk21atUYOnQoAHXq1OH06dMsXbqU1157jZ9//pnIyEgWLVqEh4cHAN7e3gwaNIgDBw5QrVq1dNc7b948XFxcmDhxIg4ODjRo0AAnJye+/PJLunfvjo+PzyPaQxGRrKU+wCIiudytW7dwcXGxmObu7k5kZCQAgYGBVK9e3Qi/AAEBAbi4uLBz5867rjcoKIj69etbdHdo2rQpSUlJBAYGZu1OiIg8QgrAIiK53Msvv0xQUBDr1q0jOjqawMBAfv31V9q0aQNAaGgoJUqUsHiOvb09vr6+nD59Ot113rx5k/Pnz6d5nqenJy4uLnd9nohIbqAuECIiuVzLli3Zu3cvI0eONKY99dRTDBkyBIDo6Og0LcQAzs7OxMTEpLvO6OhogHQvknNxcbnr80REcgO1AIuI5HJDhgzhjz/+YODAgcycOZOhQ4dy5MgRhg0bhtlsJikp6a7PtbNL/2vAbDbfc5spo0uIiORGagEWEcnFDh48yK5duxgxYgQdO3YEoGbNmhQtWpTBgwezY8cOXF1diY2NTfPcmJgYvL29011vSotxei29MTEx9x0+TUQkJ1MLsIhILnb+/HkAqlatajG9Ro0aAJw8eZKSJUsSFhZmMT8xMZHw8HD8/PzSXa+zszPe3t6cPXvWYvq1a9eIiYmhVKlSWbQHIiKPngKwiEgulhJg9+/fbzH94MGDABQrVoyAgAD27dtHRESEMT8oKIjY2FgCAgLuuu66deuyfft2bt++bUzbvHkz9vb21K5dOwv3QkTk0VIXCBGRXKxixYo0adKEr776ihs3blC5cmVOnTrFd999R6VKlWjUqBE1a9ZkyZIl9OvXj549exIZGcmUKVOoV6+eRcvxoUOH8PT0pFixYgB07dqVjRs3MnDgQF599VVOnz7N9OnTee655zQGsIjkaibz/a50ECD5iwHgySefzOZKREQsxcfH8/3337Nu3TouX76Mj48PjRo1omfPnjg7OwNw4sQJJk2axMGDB3FxcaFhw4YMHjzYYnSIWrVq0bZtW0aPHm1M279/P5MnT+bYsWN4eHjQpk0b+vTpQ548aj8RkZwno3lNATiDFIBFREREcraM5jX1ARYRERERm5Irz2GtWLGCxYsXEx4ejo+PD507d+bFF180xqUMCwtj0qRJ7N+/H3t7e5o1a8aAAQM0bE8Ot2fPHvr06XPX+b169aJXr1689dZbxgU+qc2fPx9/f/90n5uUlMSiRYtYvnw5ly5dokSJErzxxhu0bt06y+oXERGR3CHXBeCVK1cybtw4XnrpJRo2bMj+/fsZP348t2/f5rXXXiMqKoo+ffrg5eXF6NGjiYiIYMqUKYSHhzN16tTsLl/uoWLFisydOzfN9G+//ZZ///2Xli1bYjabOXHiBK+++irNmjWzWO5ewzLNmDGD+fPn06dPH/z9/dm5cycfffQRJpOJVq1aZfm+iIiISM6V6wLw6tWrqVatGkOHDgWgTp06nD59mqVLl/Laa6/x888/ExkZyaJFi/Dw8ADA29ubQYMGceDAAapVq5Z9xcs9ubq6pumzs3XrVnbv3s3nn39ujGUaExND/fr1M9wf++bNmyxevJiXX36ZN998E0h+3wQHB7NkyRIFYBERERuT6wLwrVu3KFiwoMU0d3d3IiMjAQgMDKR69epG+AUICAjAxcWFnTt3KgDnIjdv3mT8+PE0aNDAaO0NCQkBoHz58hlej4ODA3PmzMHT0zPN9Ojo6KwrWERERHKFXHcR3Msvv0xQUBDr1q0jOjqawMBAfv31V9q0aQNAaGgoJUqUsHiOvb09vr6+nD59OjtKFiv99NNPXL58mSFDhhjTjh07hrOzM5MnT6Zp06bUq1ePgQMHEhoaetf12NvbU65cOQoWLIjZbObq1av88MMP7N69mxdffPER7ImIiIjkJLmuBbhly5bs3buXkSNHGtOeeuopIyRFR0dbjGuZwtnZOd172j8Is9lMbGxsptYhGRMfH8+PP/5IkyZN8PLyMl734OBgYmNjyZcvH2PHjuXixYvMnTuXHj16MGfOnDRnB+70+++/88knnwDJ75tGjRrpbyoPJOViW8mZNLKniG0zm80Z+pzOdQF4yJAhHDhwgIEDB/LEE09w4sQJvvvuO4YNG8aECRNISkq663Pt7DLX4B0fH09wcHCm1iEZs3v3bq5du0bdunUtXvOUVt+ULhDFixfn7bffZvTo0cyYMYNOnTrdc72Ojo4MGTKEc+fOsXr1avr27cuQIUMUaiRDHBwc8H/iCfLY22d3KZKOhMREjvz7L/Hx8dldiohkI0dHx/suk6sC8MGDB9m1axcjRoygY8eOANSsWZOiRYsyePBgduzYgaura7otejExMXh7e2dq+w4ODpQtWzZT65CMWbhwIaVKlaJFixYW0ytVqpRm2UqVKuHn58f169fTnX+355cuXZpPP/2U27dvq2+4ZIjJZCKPvT0/BR3j0g2dOchJvPM70yWgPOXKlVMrsIgNO3HiRIaWy1UB+Pz58wAW964HqFGjBgAnT540RgpILTExkfDwcBo3bpyp7ZtMJuO2ovLwJCQk8Pfff9O1a1eL1zshIYENGzZQokQJqlSpYvGc27dv4+Xlle7fJyIigp07d1KvXj0KFChgTE9ZR1RUlP6u8kAu3YglPCJzXark4ciXL192lyAi2SijZ3Rz1UVwfn5+QPK96VNLuSlCsWLFCAgIYN++fURERBjzg4KCiI2NJSAg4JHVKtY7ceIEN2/eTPNDJ0+ePMyaNYvJkydbTD969Chnz56lVq1a6a7v1q1bjB49mlWrVllMDwoKAqBcuXJZWL2IiIjkdLmqBbhixYo0adKEr776ihs3blC5cmVOnTrFd999R6VKlWjUqBE1a9ZkyZIl9OvXj549exIZGcmUKVOoV69emkAlOVPK6YvSpUunmdezZ09Gjx7NyJEjadOmDRcuXGDGjBmUL1+etm3bAsmtwSEhIXh7e1O4cGF8fHxo3749s2fPJk+ePFSoUIH9+/czb948OnTokO52RERE5PGVqwIwwLhx4/j+++9Zvnw5M2fOxMfHh3bt2tGzZ0/y5MmDp6cnM2bMYNKkSYwYMQIXFxeaNm3K4MGDs7t0yaCrV68C4ObmlmZe27ZtyZs3L/Pnz+e9994jX758NGrUiP79+2P//y9MunLlCt26daNnz5707t0bgA8++ICiRYuyYsUKzp8/T+HChenduzevv/76o9sxERERyRFMZl0tkCGHDh0CyPDdx0Tk8TVl4wH1Ac5hfD1dGNiiWnaXISLZLKN5LVf1ARYRERERySwFYBERERGxKQrAIiIiImJTFIBFRERExKYoAIuIiIiITVEAFhERERGbogAsIiIiIjZFAdhGJWn45xxNfx8REZGHJ9fdCU6yhp3JxE9Bx7h0Iza7S5E7eOd3pktA+ewuQ0RE5LGlAGzDLt2I1d2sRERExOaoC4SIiIiI2BQFYBERERGxKQrAIiIiImJTFIBFRERExKYoAIuIiIiITVEAFhERERGbogAsIiIiIjZFAVhEREREbIoCsIiIiIjYFAVgEREREbEpCsAiIiIiYlMUgEVERETEpigAi4iIiIhNUQAWEREREZuiACwiIiIiNiVPZp589uxZLl68SEREBHny5MHDw4PSpUuTP3/+rKpPRERERCRLPXAAPnz4MCtWrCAoKIjLly+nu0yJEiV4+umnadeuHaVLl850kSIiIiIiWSXDAfjAgQNMmTKFw4cPA2A2m++67OnTpzlz5gyLFi2iWrVqDB48GH9//8xXKyIiIiKSSRkKwOPGjWP16tUkJSUB4Ofnx5NPPkm5cuUoVKgQLi4uANy4cYPLly9z/Phxjh49yqlTp9i/fz/dunWjTZs2jBo16uHtiYiIiIhIBmQoAK9cuRJvb2+ef/55mjVrRsmSJTO08qtXr/L777+zfPlyfv31VwVgEREREcl2GQrAX375JQ0bNsTO7sEGjfDy8uKll17ipZdeIigoyKoCRURERESyUoYCcOPGjTO9oYCAgEyvQ0REREQkszI1DBpAdHQ03377LTt27ODq1at4e3vTqlUrunXrhoODQ1bUKCIiIiKSZTIdgD/55BO2bNliPA4LC2P27NnExcUxaNCgzK5eRERERCRLZSoAx8fHs3XrVpo0acLrr7+Oh4cH0dHRrFq1it9++00BWERERERynAxd1TZu3DiuXLmSZvqtW7dISkqidOnSPPHEExQrVoyKFSvyxBNPcOvWrSwvVkREREQkszI8DNr69evp3Lkzb775pnGrY1dXV8qVK8f333/PokWLcHNzIzY2lpiYGBo2bPhQCxcRERERsUaGWoA//vhjvLy8WLBgAR06dGDu3LncvHnTmOfn50dcXByXLl0iOjqaKlWqMHTo0IdauIiIiEhm3Lp1i7p161KrVi2Lf08//bSxzJo1a+jcuTP16tWjQ4cOzJo1i4SEhAxvIyYmhvbt27NmzZqHsQtipQy1ALdp04YWLVqwfPly5syZw/Tp01myZAk9evTgueeeY8mSJZw/f55r167h7e2Nt7f3w65bREREJFNOnjxJYmIiY8aMoVixYsb0lPseLF68mIkTJ9K0aVMGDRpEREQEM2fO5NixY4wfP/6+679x4wZDhgwhPDz8oe2DWCfDF8HlyZOHzp070759e3788UcWLlzIl19+yaJFi+jduzetWrXC19f3YdYqIiIikmWOHTuGvb09TZs2xdHR0WJeYmIis2fPpm7dunzxxRfG9IoVK9KlSxeCgoLueY+DrVu3MmHCBGJjYx9a/WK9B7u1G+Dk5ET37t1ZtWoVr7/+OpcvX2bkyJG88sor7Ny582HUKCIiIpLlQkJC8PPzSxN+Aa5du0ZkZKRFdwiAsmXL4uHhcc/MExUVxdChQ6lRowZTp07N8rol8zLcAnz16lWCgoKMbg7169dnwIABvPzyy8yaNYvVq1fzzjvvUK1aNfr370+VKlUeZt0iIiIimZLSAtyvXz8OHjyIo6MjTZs2ZfDgwbi5uWFvb8/58+ctnnPjxg2ioqI4e/bsXdfr5OTE0qVL8fPzU/eHHCpDAXjPnj0MGTKEuLg4Y5qnpyczZ87Ez8+PDz74gNdff51vv/2WTZs20aNHDxo0aMCkSZMeWuEiIiIi1jKbzZw4cQKz2UzHjh156623OHLkCLNmzeK///7ju+++o0WLFixdupTSpUvTuHFjrl27xsSJE7G3tzcGA0iPg4MDfn5+j25n5IFlKABPmTKFPHnyUL9+fVxdXbl58yZHjhxh+vTpfPnllwAUK1aMcePG0bVrV7755ht27NjxUAsXERERsZbZbGbixIl4enpSpkwZAGrUqIGXlxcfffQRgYGBfPDBBzg4ODB27FjGjBlD3rx5efPNN4mJicHJySmb90AyI0MBODQ0lClTplCtWjVjWlRUFD169EizbPny5Zk8eTIHDhzIqhpFREREspSdnR21atVKM71BgwYAHD9+nPr16zNy5Ejee+89zp8/T5EiRXB2dmblypUUL178UZcsWShDAdjHx4cxY8ZQr149XF1diYuL48CBAxQpUuSuz0kdlkVERERyksuXL7Njxw6eeuopfHx8jOkpd7L18PBg+/btuLm5Ua1aNaOV+Nq1a1y6dImKFStmS92SNTI0CkT37t05e/YsP/30k3HXt2PHjvHmm28+5PJEREREsl5iYiLjxo3jl19+sZi+ceNG7O3tqV69Or/88guTJ0+2mL948WLs7OzSjA4huUuGWoBbtWpFqVKl2Lp1qzEKRIsWLSwGjRYRERHJLXx8fGjXrh0LFiwgb968VKlShQMHDjB37lw6d+5MyZIl6dKlC/3792fixIk0bNiQ3bt3M3fuXLp27WqRgQ4dOoSnp6dyUS6S4WHQKlSoQIUKFR5mLSIiIiKPzAcffEDRokVZt24dc+bMwdvbm969e/PGG28AEBAQwNixY5kzZw7Lly+nSJEivPfee3Tp0sViPd26daNt27aMHj06G/ZCrJGhADxkyBBeeukl6tSpY9VGjhw5wo8//sjYsWOtev6dDh06xLRp0/j3339xdnbmqaeeYtCgQRQoUACAsLAwJk2axP79+7G3t6dZs2YMGDAAV1fXLNm+iIiI5H6Ojo706NEj3Yv6U7Rq1YpWrVrdcz179uy56zxfX997zpfskaEAvH37drZv306xYsVo2rQpjRo1olKlSsa9su+UkJDAwYMH2b17N9u3b+fEiRMAWRKAg4OD6dOnD3Xq1GHChAlcvnyZadOmERYWxpw5c4iKiqJPnz54eXkxevRoIiIimDJlCuHh4bobi4iIiIhkLADPmjWLL774guPHjzNv3jzmzZuHg4MDpUqVolChQri4uGAymYiNjeXChQucOXPGuIrSbDZTsWJFhgwZkiUFT5kyhQoVKjBx4kQjgLu4uDBx4kTOnTvHxo0biYyMZNGiRXh4eADg7e3NoEGDOHDggEanEBEREbFxGQrAVatWZeHChfzxxx8sWLCA4OBgbt++TUhICMeOHbNY1mw2A2AymahTpw6dOnWiUaNGmEymTBd7/fp19u7dy+jRoy1an5s0aUKTJk0ACAwMpHr16kb4heQ+PC4uLuzcuVMBWERERMTGZfgiODs7O5o3b07z5s0JDw9n165dHDx4kMuXL3Pt2jUAChQoQLFixahWrRq1a9emcOHCWVrsiRMnSEpKwtPTkxEjRrBt2zbMZjONGzdm6NChuLm5ERoaSvPmzS2eZ29vj6+vL6dPn87U9s1mM7GxsZlaR05gMpnIly9fdpch9xEXF2f8oJScQcdOzqfjRsS2mc3mDDW6ZjgAp+br68sLL7zACy+8YM3TrRYREQHAJ598Qr169ZgwYQJnzpzhm2++4dy5c8yePZvo6GhcXFzSPNfZ2ZmYmJhMbT8+Pp7g4OBMrSMnyJcvH/7+/tldhtzHf//9R1xcXHaXIano2Mn5dNyIiKOj432XsSoAZ5f4+HgAKlasyEcffQRAnTp1cHNz48MPP+Svv/4iKSnprs+/20V7GeXg4EDZsmUztY6cICu6o8jDV6pUKbVk5TA6dnI+HTciti1l4IX7yVUB2NnZGSDN3Vfq1asHwNGjR3F1dU23m0JMTAze3t6Z2r7JZDJqEHnYdKpd5MHpuBGxbRltqMhck+gjVqJECQBu375tMT0hIQEAJycnSpYsSVhYmMX8xMREwsPD8fPzeyR1ioiIiKUktcznWLb4t8lVLcClSpXC19eXjRs38tJLLxkpf+vWrQBUq1aNqKgo5s+fT0REBJ6engAEBQURGxtLQEBAttUuIiJiy+xMJn4KOsalG7n/YvLHiXd+Z7oElM/uMh65XBWATSYTAwcO5IMPPmD48OF07NiR//77j+nTp9OkSRMqVqxI4cKFWbJkCf369aNnz55ERkYyZcoU6tWrR9WqVbN7F0RERGzWpRuxhEdk7oJ0kaxgVQA+fPgwlStXzupaMqRZs2bkzZuXWbNm8c4775A/f346derE22+/DYCnpyczZsxg0qRJjBgxAhcXF5o2bcrgwYOzpV4RERERyVmsCsDdunWjVKlSPPvss7Rp04ZChQpldV339PTTT6e5EC61smXLMn369EdYkYiIiIjkFlZfBBcaGso333xD27Zt6d+/P7/99ptx+2MRERERkZzKqhbgrl278scff3D27FnMZjO7d+9m9+7dODs707x5c5599lndclhEREREciSrAnD//v3p378/ISEh/P777/zxxx+EhYURExPDqlWrWLVqFb6+vrRt25a2bdvi4+OT1XWLiIiIiFglU+MAV6hQgX79+rF8+XIWLVpEhw4dMJvNmM1mwsPD+e677+jYsSPjx4+/5x3aREREREQelUwPgxYVFcUff/zBpk2b2Lt3LyaTyQjBkHwTimXLlpE/f3569+6d6YJFRERERDLDqgAcGxvLn3/+ycaNG9m9e7dxJzaz2YydnR1169alffv2mEwmpk6dSnh4OBs2bFAAFhEREZFsZ1UAbt68OfHx8QBGS6+vry/t2rVL0+fX29ubt956i0uXLmVBuSIiIiIimWNVAL59+zYAjo6ONGnShA4dOlCrVq10l/X19QXAzc3NyhJFRERERLKOVQG4UqVKtG/fnlatWuHq6nrPZfPly8c333xD0aJFrSpQRERERCQrWRWA58+fDyT3BY6Pj8fBwQGA06dPU7BgQVxcXIxlXVxcqFOnThaUKiIiIiKSeVYPg7Zq1Sratm3LoUOHjGkLFy6kdevWrF69OkuKExERERHJalYF4J07dzJ27Fiio6M5ceKEMT00NJS4uDjGjh3L7t27s6xIEREREZGsYlUAXrRoEQBFihShTJkyxvRXX32V4sWLYzabWbBgQdZUKCIiIiKShazqA3zy5ElMJhMjR46kZs2axvRGjRrh7u5Or169OH78eJYVKSIiIiKSVaxqAY6OjgbA09MzzbyU4c6ioqIyUZaIiIiIyMNhVQAuXLgwAMuXL7eYbjab+emnnyyWERERERHJSazqAtGoUSMWLFjA0qVLCQoKoly5ciQkJHDs2DHOnz+PyWSiYcOGWV2riIiIiEimWRWAu3fvzp9//klYWBhnzpzhzJkzxjyz2Uzx4sV56623sqxIEREREZGsYlUXCFdXV+bOnUvHjh1xdXXFbDZjNptxcXGhY8eOzJkz5753iBMRERERyQ5WtQADuLu78+GHHzJ8+HCuX7+O2WzG09MTk8mUlfWJiIiIiGQpq+8El8JkMuHp6UmBAgWM8JuUlMSuXbsyXZyIiIiISFazqgXYbDYzZ84ctm3bxo0bN0hKSjLmJSQkcP36dRISEvjrr7+yrFARERERkaxgVQBesmQJM2bMwGQyYTabLealTFNXCBERERHJiazqAvHrr78CkC9fPooXL47JZOKJJ56gVKlSRvgdNmxYlhYqIiIiIpIVrArAZ8+exWQy8cUXX/DZZ59hNpvp3bs3S5cu5ZVXXsFsNhMaGprFpYqIiIiIZJ5VAfjWrVsAlChRgvLly+Ps7Mzhw4cBeO655wDYuXNnFpUoIiIiIpJ1rArABQoUACAkJASTyUS5cuWMwHv27FkALl26lEUlioiIiIhkHasCcNWqVTGbzXz00UeEhYVRvXp1jhw5QufOnRk+fDjwfyFZRERERCQnsSoA9+jRg/z58xMfH0+hQoVo2bIlJpOJ0NBQ4uLiMJlMNGvWLKtrFRERERHJNKsCcKlSpViwYAE9e/bEycmJsmXLMmrUKAoXLkz+/Pnp0KEDvXv3zupaRUREREQyzapxgHfu3EmVKlXo0aOHMa1Nmza0adMmywoTEREREXkYrGoBHjlyJK1atWLbtm1ZXY+IiIiIyENlVQC+efMm8fHx+Pn5ZXE5IiIiIiIPl1UBuGnTpgBs2bIlS4sREREREXnYrOoDXL58eXbs2ME333zD8uXLKV26NK6uruTJ83+rM5lMjBw5MssKFRERERHJClYF4MmTJ2MymQA4f/4858+fT3c5BWARERERyWmsCsAAZrP5nvNTArKIiIiISE5iVQBevXp1VtchIiIiIvJIWBWAixQpktV1iIiIiIg8ElYF4H379mVouRo1alizehERERGRh8aqANy7d+/79vE1mUz89ddfVhUlIiIiIvKwPLSL4EREREREciKrAnDPnj0tHpvNZm7fvs2FCxfYsmULFStWpHv37llSoIiIiIhIVrIqAPfq1euu837//XeGDx9OVFSU1UWJiIiIiDwsVt0K+V6aNGkCwOLFi7N61SIiIiIimZblAfjvv//GbDZz8uTJrF61iIiIiEimWdUFok+fPmmmJSUlER0dzalTpwAoUKBA5ioTEREREXkIrArAe/fuveswaCmjQ7Rt29b6qkREREREHpIsHQbNwcGBQoUK0bJlS3r06JGpwjJq6NChHD16lDVr1hjTwsLCmDRpEvv378fe3p5mzZoxYMAAXF1dH0lNIiIiIpJzWRWA//7776yuwyrr1q1jy5YtFrdmjoqKok+fPnh5eTF69GgiIiKYMmUK4eHhTJ06NRurFREREZGcwOoW4PTEx8fj4OCQlau8q8uXLzNhwgQKFy5sMf3nn38mMjKSRYsW4eHhAYC3tzeDBg3iwIEDVKtW7ZHUJyIiIiI5k9WjQISEhNC3b1+OHj1qTJsyZQo9evTg+PHjWVLcvYwZM4a6detSu3Zti+mBgYFUr17dCL8AAQEBuLi4sHPnzodel4iIiIjkbFYF4FOnTtG7d2/27NljEXZDQ0M5ePAgvXr1IjQ0NKtqTGPlypUcPXqUYcOGpZkXGhpKiRIlLKbZ29vj6+vL6dOnH1pNIiIiIpI7WNUFYs6cOcTExODo6GgxGkSlSpXYt28fMTEx/PDDD4wePTqr6jScP3+er776ipEjR1q08qaIjo7GxcUlzXRnZ2diYmIytW2z2UxsbGym1pETmEwm8uXLl91lyH3ExcWle7GpZB8dOzmfjpucScdOzve4HDtms/muI5WlZlUAPnDgACaTiREjRtC6dWtjet++fSlbtiwffvgh+/fvt2bV92Q2m/nkk0+oV68eTZs2TXeZpKSkuz7fzi5z9/2Ij48nODg4U+vICfLly4e/v392lyH38d9//xEXF5fdZUgqOnZyPh03OZOOnZzvcTp2HB0d77uMVQH42rVrAFSuXDnNvAoVKgBw5coVa1Z9T0uXLuX48eP89NNPJCQkAP83HFtCQgJ2dna4urqm20obExODt7d3prbv4OBA2bJlM7WOnCAjv4wk+5UqVeqx+DX+ONGxk/PpuMmZdOzkfI/LsXPixIkMLWdVAHZ3d+fq1av8/fffFC9e3GLerl27AHBzc7Nm1ff0xx9/cP36dVq1apVmXkBAAD179qRkyZKEhYVZzEtMTCQ8PJzGjRtnavsmkwlnZ+dMrUMko3S6UOTB6bgRsc7jcuxk9MeWVQG4Vq1abNiwgYkTJxIcHEyFChVISEjgyJEjbNq0CZPJlGZ0hqwwfPjwNK27s2bNIjg4mEmTJlGoUCHs7OyYP38+EREReHp6AhAUFERsbCwBAQFZXpOIiIiI5C5WBeAePXqwbds24uLiWLVqlcU8s9lMvnz5eOutt7KkwNT8/PzSTHN3d8fBwcHoW/TCCy+wZMkS+vXrR8+ePYmMjGTKlCnUq1ePqlWrZnlNIiIiIpK7WHVVWMmSJZk6dSolSpTAbDZb/CtRogRTp05NN6w+Cp6ensyYMQMPDw9GjBjB9OnTadq0KZ999lm21CMiIiIiOYvVd4KrUqUKP//8MyEhIYSFhWE2mylevDgVKlR4pJ3d0xtqrWzZskyfPv2R1SAiIiIiuUemboUcGxtL6dKljZEfTp8+TWxsbLrj8IqIiIiI5ARWD4y7atUq2rZty6FDh4xpCxcupHXr1qxevTpLihMRERERyWpWBeCdO3cyduxYoqOjLcZbCw0NJS4ujrFjx7J79+4sK1JEREREJKtYFYAXLVoEQJEiRShTpowx/dVXX6V48eKYzWYWLFiQNRWKiIiIiGQhq/oAnzx5EpPJxMiRI6lZs6YxvVGjRri7u9OrVy+OHz+eZUWKiIiIiGQVq1qAo6OjAYwbTaSWcge4qKioTJQlIiIiIvJwWBWACxcuDMDy5cstppvNZn766SeLZUREREREchKrukA0atSIBQsWsHTpUoKCgihXrhwJCQkcO3aM8+fPYzKZaNiwYVbXKiIiIiKSaVYF4O7du/Pnn38SFhbGmTNnOHPmjDEv5YYYD+NWyCIiIiIimWVVFwhXV1fmzp1Lx44dcXV1NW6D7OLiQseOHZkzZw6urq5ZXauIiIiISKZZfSc4d3d3PvzwQ4YPH87169cxm814eno+0tsgi4iIiIg8KKvvBJfCZDLh6elJgQIFMJlMxMXFsWLFCt54442sqE9EREREJEtZ3QJ8p+DgYJYvX87GjRuJi4vLqtWKiIiIiGSpTAXg2NhY1q9fz8qVKwkJCTGmm81mdYUQERERkRzJqgD877//smLFCjZt2mS09prNZgDs7e1p2LAhnTp1yroqRURERESySIYDcExMDOvXr2fFihXGbY5TQm8Kk8nE2rVrKViwYNZWKSIiIiKSRTIUgD/55BN+//13bt68aRF6nZ2dadKkCT4+PsyePRtA4VdEREREcrQMBeA1a9ZgMpkwm83kyZOHgIAAWrduTcOGDcmbNy+BgYEPu04RERERkSzxQMOgmUwmvL29qVy5Mv7+/uTNm/dh1SUiIiIi8lBkqAW4WrVqHDhwAIDz588zc+ZMZs6cib+/P61atdJd30REREQk18hQAJ41axZnzpxh5cqVrFu3jqtXrwJw5MgRjhw5YrFsYmIi9vb2WV+piIiIiEgWyHAXiBIlSjBw4EB+/fVXxo8fT4MGDYx+wanH/W3VqhVff/01J0+efGhFi4iIiIhY64HHAba3t6dRo0Y0atSIK1eusHr1atasWcPZs2cBiIyM5Mcff2Tx4sX89ddfWV6wiIiIiEhmPNBFcHcqWLAg3bt3Z8WKFXz77be0atUKBwcHo1VYRERERCSnydStkFOrVasWtWrVYtiwYaxbt47Vq1dn1apFRERERLJMlgXgFK6urnTu3JnOnTtn9apFRERERDItU10gRERERERyGwVgEREREbEpCsAiIiIiYlMUgEVERETEpigAi4iIiIhNUQAWEREREZuiACwiIiIiNkUBWERERERsigKwiIiIiNgUBWARERERsSkKwCIiIiJiUxSARURERMSmKACLiIiIiE1RABYRERERm6IALCIiIiI2RQFYRERERGyKArCIiIiI2BQFYBERERGxKQrAIiIiImJTFIBFRERExKYoAIuIiIiITVEAFhERERGbogAsIiIiIjYlT3YX8KCSkpJYvnw5P//8M+fOnaNAgQI888wz9O7dG1dXVwDCwsKYNGkS+/fvx97enmbNmjFgwABjvoiIiIjYrlwXgOfPn8+3337L66+/Tu3atTlz5gwzZszg5MmTfPPNN0RHR9OnTx+8vLwYPXo0ERERTJkyhfDwcKZOnZrd5YuIiIhINstVATgpKYl58+bx/PPP079/fwDq1q2Lu7s7w4cPJzg4mL/++ovIyEgWLVqEh4cHAN7e3gwaNIgDBw5QrVq17NsBEREREcl2uaoPcExMDG3atKFly5YW0/38/AA4e/YsgYGBVK9e3Qi/AAEBAbi4uLBz585HWK2IiIiI5ES5qgXYzc2NoUOHppn+559/AlC6dGlCQ0Np3ry5xXx7e3t8fX05ffr0oyhTRERERHKwXBWA03P48GHmzZvH008/TdmyZYmOjsbFxSXNcs7OzsTExGRqW2azmdjY2EytIycwmUzky5cvu8uQ+4iLi8NsNmd3GZKKjp2cT8dNzqRjJ+d7XI4ds9mMyWS673K5OgAfOHCAd955B19fX0aNGgUk9xO+Gzu7zPX4iI+PJzg4OFPryAny5cuHv79/dpch9/Hff/8RFxeX3WVIKjp2cj4dNzmTjp2c73E6dhwdHe+7TK4NwBs3buTjjz+mRIkSTJ061ejz6+rqmm4rbUxMDN7e3pnapoODA2XLls3UOnKCjPwykuxXqlSpx+LX+ONEx07Op+MmZ9Kxk/M9LsfOiRMnMrRcrgzACxYsYMqUKdSsWZMJEyZYjO9bsmRJwsLCLJZPTEwkPDycxo0bZ2q7JpMJZ2fnTK1DJKN0ulDkwem4EbHO43LsZPTHVq4aBQLgl19+YfLkyTRr1oypU6emublFQEAA+/btIyIiwpgWFBREbGwsAQEBj7pcEREREclhclUL8JUrV5g0aRK+vr689NJLHD161GJ+sWLFeOGFF1iyZAn9+vWjZ8+eREZGMmXKFOrVq0fVqlWzqXIRERERySlyVQDeuXMnt27dIjw8nB49eqSZP2rUKNq1a8eMGTOYNGkSI0aMwMXFhaZNmzJ48OBHX7CIiIiI5Di5KgB36NCBDh063He5smXLMn369EdQkYiIiIjkNrmuD7CIiIiISGYoAIuIiIiITVEAFhERERGbogAsIiIiIjZFAVhEREREbIoCsIiIiIjYFAVgEREREbEpCsAiIiIiYlMUgEVERETEpigAi4iIiIhNUQAWEREREZuiACwiIiIiNkUBWERERERsigKwiIiIiNgUBWARERERsSkKwCIiIiJiUxSARURERMSmKACLiIiIiE1RABYRERERm6IALCIiIiI2RQFYRERERGyKArCIiIiI2BQFYBERERGxKQrAIiIiImJTFIBFRERExKYoAIuIiIiITVEAFhERERGbogAsIiIiIjZFAVhEREREbIoCsIiIiIjYFAVgEREREbEpCsAiIiIiYlMUgEVERETEpigAi4iIiIhNUQAWEREREZuiACwiIiIiNkUBWERERERsigKwiIiIiNgUBWARERERsSkKwCIiIiJiUxSARURERMSmKACLiIiIiE1RABYRERERm6IALCIiIiI2RQFYRERERGyKArCIiIiI2BQFYBERERGxKQrAIiIiImJTHusAHBQUxBtvvEH9+vVp3749CxYswGw2Z3dZIiIiIpKNHtsAfOjQIQYPHkzJkiUZP348rVq1YsqUKcybNy+7SxMRERGRbJQnuwt4WGbOnEmFChUYM2YMAPXq1SMhIYG5c+fSpUsXnJycsrlCEREREckOj2UL8O3bt9m7dy+NGze2mN60aVNiYmI4cOBA9hQmIiIiItnusQzA586dIz4+nhIlSlhML168OACnT5/OjrJEREREJAd4LLtAREdHA+Di4mIx3dnZGYCYmJgHWl9ISAi3b98G4J9//smCCrOfyWSiToEkEj3UFSSnsbdL4tChQ7pgM4fSsZMz6bjJ+XTs5EyP27ETHx+PyWS673KPZQBOSkq653w7uwdv+E55MTPyouYWLnkdsrsEuYfH6b32uNGxk3PpuMnZdOzkXI/LsWMymWw3ALu6ugIQGxtrMT2l5TdlfkZVqFAhawoTERERkWz3WPYBLlasGPb29oSFhVlMT3ns5+eXDVWJiIiISE7wWAbgvHnzUr16dbZs2WLRp2Xz5s24urpSuXLlbKxORERERLLTYxmAAd566y0OHz7M+++/z86dO/n2229ZsGAB3bp10xjAIiIiIjbMZH5cLvtLx5YtW5g5cyanT5/G29ubF198kddeey27yxIRERGRbPRYB2ARERERkTs9tl0gRERERETSowAsIiIiIjZFAVhEREREbIoCsIiIiIjYFAVgEREREbEpCsAiIiIiYlMUgMXmaSRAedyl9x7X+15EbJkCsORK4eHh1KpVizVr1lj9nKioKEaOHMn+/fsfVpkiD0W7du0YPXp0uvNmzpxJrVq1jMcHDhxg0KBBFsvMnj2bBQsWPMwSRWyKNd9Jkr0UgMVmhYSEsG7dOpKSkrK7FJEs07FjR+bOnWs8XrlyJf/995/FMjNmzCAuLu5Rlyby2CpYsCBz586lQYMG2V2KZFCe7C5ARESyTuHChSlcuHB2lyFiUxwdHXnyySezuwx5AGoBlmx38+ZNpk2bxnPPPcdTTz1Fw4YN6du3LyEhIcYymzdv5uWXX6Z+/fq8+uqrHDt2zGIda9asoVatWoSHh1tMv9up4j179tCnTx8A+vTpQ69evbJ+x0QekVWrVlG7dm1mz55t0QVi9OjRrF27lvPnzxunZ1PmzZo1y6KrxIkTJxg8eDANGzakYcOGvPfee5w9e9aYv2fPHmrVqsXu3bvp168f9evXp2XLlkyZMoXExMRHu8MiDyA4OJi3336bhg0b8swzz9C3b18OHTpkzN+/fz+9evWifv36NGnShFGjRhEREWHMX7NmDXXr1uXw4cN069aNevXq0bZtW4tuROl1gThz5gz/+9//aNmyJQ0aNKB3794cOHAgzXMWLlxIp06dqF+/PqtXr364L4YYFIAl240aNYrVq1fz5ptvMm3aNN555x1OnTrFiBEjMJvNbNu2jWHDhlG2bFkmTJhA8+bN+eijjzK1zYoVKzJs2DAAhg0bxvvvv58VuyLyyG3cuJFx48bRo0cPevToYTGvR48e1K9fHy8vL+P0bEr3iA4dOhj/P336NG+99RbXrl1j9OjRfPTRR5w7d86YltpHH31E9erV+frrr2nZsiXz589n5cqVj2RfRR5UdHQ0AwYMwMPDgy+//JJPP/2UuLg4+vfvT3R0NPv27ePtt9/GycmJzz//nHfffZe9e/fSu3dvbt68aawnKSmJ999/nxYtWjB58mSqVavG5MmTCQwMTHe7p06d4vXXX+f8+fMMHTqUsWPHYjKZ6NOnD3v37rVYdtasWXTt2pVPPvmEunXrPtTXQ/6PukBItoqPjyc2NpahQ4fSvHlzAGrWrEl0dDRff/01V69eZfbs2TzxxBOMGTMGgKeeegqAadOmWb1dV1dXSpUqBUCpUqUoXbp0JvdE5NHbvn07I0eO5M0336R3795p5hcrVgxPT0+L07Oenp4AeHt7G9NmzZqFk5MT06dPx9XVFYDatWvToUMHFixYYHERXceOHY2gXbt2bbZu3cqOHTvo1KnTQ91XEWv8999/XL9+nS5dulC1alUA/Pz8WL58OTExMUybNo2SJUvy1VdfYW9vD8CTTz5J586dWb16NZ07dwaSR03p0aMHHTt2BKBq1aps2bKF7du3G99Jqc2aNQsHBwdmzJiBi4sLAA0aNOCll15i8uTJzJ8/31i2WbNmtG/f/mG+DJIOtQBLtnJwcGDq1Kk0b96cS5cusWfPHn755Rd27NgBJAfk4OBgnn76aYvnpYRlEVsVHBzM+++/j7e3t9Gdx1p///03NWrUwMnJiYSEBBISEnBxcaF69er89ddfFsve2c/R29tbF9RJjlWmTBk8PT155513+PTTT9myZQteXl4MHDgQd3d3Dh8+TIMGDTCbzcZ7v2jRovj5+aV571epUsX4v6OjIx4eHnd97+/du5enn37aCL8AefLkoUWLFgQHBxMbG2tML1++fBbvtWSEWoAl2wUGBjJx4kRCQ0NxcXGhXLlyODs7A3Dp0iXMZjMeHh4WzylYsGA2VCqSc5w8eZIGDRqwY8cOli5dSpcuXaxe1/Xr19m0aRObNm1KMy+lxTiFk5OTxWOTyaSRVCTHcnZ2ZtasWXz//fds2rSJ5cuXkzdvXp599lm6detGUlIS8+bNY968eWmemzdvXovHd7737ezs7jqedmRkJF5eXmmme3l5YTabiYmJsahRHj0FYMlWZ8+e5b333qNhw4Z8/fXXFC1aFJPJxLJly9i1axfu7u7Y2dml6YcYGRlp8dhkMgGk+SJO/Stb5HFSr149vv76az744AOmT59Oo0aN8PHxsWpdbm5u1KlTh9deey3NvJTTwiK5lZ+fH2PGjCExMZF///2XdevW8fPPP+Pt7Y3JZOKVV16hZcuWaZ53Z+B9EO7u7ly9ejXN9JRp7u7uXLlyxer1S+apC4Rkq+DgYG7dusWbb75JsWLFjCC7a9cuIPmUUZUqVdi8ebPFL+1t27ZZrCflNNPFixeNaaGhoWmCcmr6YpfcrECBAgAMGTIEOzs7Pv/883SXs7NL+zF/57QaNWrw33//Ub58efz9/fH396dSpUosWrSIP//8M8trF3lUfv/9d5o1a8aVK1ewt7enSpUqvP/++7i5uXH16lUqVqxIaGio8b739/endOnSzJw5M83Fag+iRo0abN++3aKlNzExkd9++w1/f38cHR2zYvckExSAJVtVrFgRe3t7pk6dSlBQENu3b2fo0KFGH+CbN2/Sr18/Tp06xdChQ9m1axeLFy9m5syZFuupVasWefPm5euvv2bnzp1s3LiRIUOG4O7uftdtu7m5AbBz5840w6qJ5BYFCxakX79+7Nixgw0bNqSZ7+bmxrVr19i5c6fR4uTm5sbBgwfZt28fZrOZnj17EhYWxjvvvMOff/5JYGAg//vf/9i4cSPlypV71LskkmWqVatGUlIS7733Hn/++Sd///0348aNIzo6mqZNm9KvXz+CgoIYMWIEO3bsYNu2bQwcOJC///6bihUrWr3dnj17cuvWLfr06cPvv//O1q1bGTBgAOfOnaNfv35ZuIdiLQVgyVbFixdn3LhxXLx4kSFDhvDpp58CybdzNZlM7N+/n+rVqzNlyhQuXbrE0KFDWb58OSNHjrRYj5ubG+PHjycxMZH33nuPGTNm0LNnT/z9/e+67dKlS9OyZUuWLl3KiBEjHup+ijxMnTp14oknnmDixIlpznq0a9eOIkWKMGTIENauXQtAt27dCA4OZuDAgVy8eJFy5coxe/ZsTCYTo0aNYtiwYVy5coUJEybQpEmT7NglkSxRsGBBpk6diqurK2PGjGHw4MGEhITw5ZdfUqtWLQICApg6dSoXL15k2LBhjBw5Ent7e6ZPn56pG1uUKVOG2bNn4+npySeffGJ8Z82cOVNDneUQJvPdenCLiIiIiDyG1AIsIiIiIjZFAVhEREREbIoCsIiIiIjYFAVgEREREbEpCsAiIiIiYlMUgEVERETEpigAi4iIiIhNyZPdBYiIPA569uzJ/v37geSbT4waNSqbK0rrxIkT/PLLL+zevZsrV65w+/ZtPD09qVSpEu3bt6dhw4bZXaKIyCOhG2GIiGTS6dOn6dSpk/HYycmJDRs24Orqmo1VWfrhhx+YMWMGCQkJd12mdevWfPzxx9jZ6eSgiDze9CknIpJJq1atsnh88+ZN1q1bl03VpLV06VKmTZtGQkIChQsXZvjw4SxbtoyffvqJwYMH4+LiAsD69ev58ccfs7laEZGHTy3AIiKZkJCQwLPPPsvVq1fx9fXl4sWLJCYmUr58+RwRJq9cuUK7du2Ij4+ncOHCzJ8/Hy8vL4tldu7cyaBBgwAoVKgQ69atw2QyZUe5IiKPhPoAi4hkwo4dO7h69SoA7du35/Dhw+zYsYNjx45x+PBhKleunOY54eHhTJs2jaCgIOLj46levTrvvvsun376Kfv27aNGjRp89913xvKhoaHMnDmTv//+m9jYWIoUKULr1q15/fXXyZs37z3rW7t2LfHx8QD06NEjTfgFqF+/PoMHD8bX1xd/f38j/K5Zs4aPP/4YgEmTJjFv3jyOHDmCp6cnCxYswMvLi/j4eH766Sc2bNhAWFgYAGXKlKFjx460b9/eIkj36tWLffv2AbBnzx5j+p49e+jTpw+Q3Je6d+/eFsuXL1+eL774gsmTJ/P3339jMpl46qmnGDBgAL6+vvfcfxGR9CgAi4hkQuruDy1btqR48eLs2LEDgOXLl6cJwOfPn6dr165EREQY03bt2sWRI0fS7TP877//0rdvX2JiYoxpp0+fZsaMGezevZvp06eTJ8/dP8pTAidAQEDAXZd77bXX7rGXMGrUKKKiogDw8vLCy8uL2NhYevXqxdGjRy2WPXToEIcOHWLnzp189tln2Nvb33Pd9xMREUG3bt24fv26MW3Tpk3s27ePefPm4ePjk6n1i4jtUR9gERErXb58mV27dgHg7+9P8eLFadiwodGndtOmTURHR1s8Z9q0aUb4bd26NYsXL+bbb7+lQIECnD171mJZs9nMJ598QkxMDB4eHowfP55ffvmFoUOHYmdnx759+1iyZMk9a7x48aLx/0KFClnMu3LlChcvXkzz7/bt22nWEx8fz6RJk/jxxx959913Afj666+N8NuiRQsWLlzInDlzqFu3LgCbN29mwYIF934RM+Dy5cvkz5+fadOmsXjxYlq3bg3A1atXmTp1aqbXLyK2RwFYRMRKa9asITExEYBWrVoBySNANG7cGIC4uDg2bNhgLJ+UlGS0DhcuXJhRo0ZRrlw5ateuzbhx49Ks//jx45w8eRKAtm3b4u/vj5OTE40aNaJGjRoA/Prrr/esMfWIDneOAPHGG2/w7LPPpvn3zz//pFlPs2bNeOaZZyhfvjzVq1cnJibG2HaZMmUYM2YMFStWpEqVKkyYMMHoanG/gJ5RH330EQEBAZQrV45Ro0ZRpEgRALZv3278DUREMkoBWETECmazmdWrVxuPXV1d2bVrF7t27bI4Jb9ixQrj/xEREUZXBn9/f4uuC+XKlTNajlOcOXPG+P/ChQstQmpKH9qTJ0+m22KbonDhwsb/w8PDH3Q3DWXKlElT261btwCoVauWRTeHfPnyUaVKFSC59TZ11wVrmEwmi64kefLkwd/fH4DY2NhMr19EbI/6AIuIWGHv3r0WXRY++eSTdJcLCQnh33//5YknnsDBwcGYnpEBeDLSdzYxMZEbN25QsGDBdOfXqVPHaHXesWMHpUuXNualHqpt9OjRrF279q7bubN/8v1qu9/+JSYmGutICdL3WldCQsJdXz+NWCEiD0otwCIiVrhz7N97SWkFzp8/P25ubgAEBwdbdEk4evSoxYVuAMWLFzf+37dvX/bs2WP8W7hwIRs2bGDPnj13Db+Q3DfXyckJgHnz5t21FfjObd/pzgvtihYtiqOjI5A8ikNSUpIxLy4ujkOHDgHJLdAeHh4AxvJ3bu/ChQv33DYk/+BIkZiYSEhICJAczFPWLyKSUQrAIiIPKCoqis2bNwPg7u5OYGCgRTjds2cPGzZsMFo4N27caAS+li1bAskXp3388cecOHGCoKAgPvzwwzTbKVOmDOXLlweSu0D89ttvnD17lnXr1tG1a1datWrF0KFD71lrwYIFeeeddwCIjIykW7duLFu2jNDQUEJDQ9mwYQO9e/dmy5YtD/QauLi40LRpUyC5G8bIkSM5evQohw4d4n//+58xNFznzp2N56S+CG/x4sUkJSUREhLCvHnz7ru9zz//nO3bt3PixAk+//xzzp07B0CjRo105zoReWDqAiEi8oDWr19vnLZv06aNxan5FAULFqRhw4Zs3ryZ2NhYNmzYQKdOnejevTtbtmzh6tWrrF+/nvXr1wPg4+NDvnz5iIuLM07pm0wmhgwZwsCBA7lx40aakOzu7m6MmXsvnTp1Ij4+nsmTJ3P16lW++OKLdJezt7enQ4cORv/a+xk6dCjHjh3j5MmTbNiwweKCP4AmTZpYDK/WsmVL1qxZA8CsWbOYPXs2ZrOZJ5988r79k81msxHkUxQqVIj+/ftnqFYRkdT0s1lE5AGl7v7QoUOHuy7XqVMn4/8p3SC8vb35/vvvady4MS4uLri4uNCkSRNmz55tdBFI3VWgZs2a/PDDDzRv3hwvLy8cHBwoXLgw7dq144cffqBs2bIZqrlLly4sW7aMbt26UaFCBdzd3XFwcKBgwYLUqVOH/v37s2bNGoYPH46zs3OG1pk/f34WLFjAoEGDqFSpEs7Ozjg5OVG5cmVGjBjBF198YdFXOCAggDFjxlCmTBkcHR0pUqQIPXv25KuvvrrvtlJes3z58uHq6kqLFi2YO3fuPbt/iIjcjW6FLCLyCAUFBeHo6Ii3tzc+Pj5G39qkpCSefvppbt26RYsWLfj000+zudLsd7c7x4mIZJa6QIiIPEJLlixh+/btAHTs2JGuXbty+/Zt1q5da3SryGgXBBERsY4CsIjII/TSSy+xc+dOkpKSWLlyJStXrrSYX7hwYdq3b589xYmI2Aj1ARYReYQCAgKYPn06Tz/9NF5eXtjb2+Po6EixYsXo1KkTP/zwA/nz58/uMkVEHmvqAywiIiIiNkUtwCIiIiJiUxSARURERMSmKACLiIiIiE1RABYRERERm6IALCIiIiI2RQFYRERERGyKArCIiIiI2BQFYBERERGxKQrAIiIiImJT/h8DcJOnNGXVQQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            404  72.792793\n",
      "1           kitten          136            115  84.558824\n",
      "2           senior          178            105  58.988764\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      164     77.00\n",
      "1          M    337      218     64.69\n",
      "2          X    319      242     75.86\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 2: 715, 1: 490})\n",
      "Epoch 1/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.1975 - accuracy: 0.4847\n",
      "Epoch 2/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0106 - accuracy: 0.5565\n",
      "Epoch 3/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.9483 - accuracy: 0.5779\n",
      "Epoch 4/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.9018 - accuracy: 0.6133\n",
      "Epoch 5/1500\n",
      "33/33 [==============================] - 0s 896us/step - loss: 0.8735 - accuracy: 0.6303\n",
      "Epoch 6/1500\n",
      "33/33 [==============================] - 0s 960us/step - loss: 0.8049 - accuracy: 0.6497\n",
      "Epoch 7/1500\n",
      "33/33 [==============================] - 0s 953us/step - loss: 0.8175 - accuracy: 0.6477\n",
      "Epoch 8/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7937 - accuracy: 0.6618\n",
      "Epoch 9/1500\n",
      "33/33 [==============================] - 0s 945us/step - loss: 0.7864 - accuracy: 0.6550\n",
      "Epoch 10/1500\n",
      "33/33 [==============================] - 0s 945us/step - loss: 0.7552 - accuracy: 0.6739\n",
      "Epoch 11/1500\n",
      "33/33 [==============================] - 0s 925us/step - loss: 0.7704 - accuracy: 0.6705\n",
      "Epoch 12/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7301 - accuracy: 0.6759\n",
      "Epoch 13/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7150 - accuracy: 0.6914\n",
      "Epoch 14/1500\n",
      "33/33 [==============================] - 0s 997us/step - loss: 0.6888 - accuracy: 0.6875\n",
      "Epoch 15/1500\n",
      "33/33 [==============================] - 0s 972us/step - loss: 0.6798 - accuracy: 0.7094\n",
      "Epoch 16/1500\n",
      "33/33 [==============================] - 0s 929us/step - loss: 0.6530 - accuracy: 0.7268\n",
      "Epoch 17/1500\n",
      "33/33 [==============================] - 0s 886us/step - loss: 0.6588 - accuracy: 0.7123\n",
      "Epoch 18/1500\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.6432 - accuracy: 0.7244\n",
      "Epoch 19/1500\n",
      "33/33 [==============================] - 0s 903us/step - loss: 0.6300 - accuracy: 0.7356\n",
      "Epoch 20/1500\n",
      "33/33 [==============================] - 0s 950us/step - loss: 0.6576 - accuracy: 0.7152\n",
      "Epoch 21/1500\n",
      "33/33 [==============================] - 0s 965us/step - loss: 0.6261 - accuracy: 0.7322\n",
      "Epoch 22/1500\n",
      "33/33 [==============================] - 0s 955us/step - loss: 0.6185 - accuracy: 0.7322\n",
      "Epoch 23/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5943 - accuracy: 0.7453\n",
      "Epoch 24/1500\n",
      "33/33 [==============================] - 0s 960us/step - loss: 0.6027 - accuracy: 0.7506\n",
      "Epoch 25/1500\n",
      "33/33 [==============================] - 0s 881us/step - loss: 0.5963 - accuracy: 0.7428\n",
      "Epoch 26/1500\n",
      "33/33 [==============================] - 0s 907us/step - loss: 0.5686 - accuracy: 0.7438\n",
      "Epoch 27/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.5647 - accuracy: 0.7618\n",
      "Epoch 28/1500\n",
      "33/33 [==============================] - 0s 823us/step - loss: 0.5741 - accuracy: 0.7462\n",
      "Epoch 29/1500\n",
      "33/33 [==============================] - 0s 956us/step - loss: 0.5696 - accuracy: 0.7555\n",
      "Epoch 30/1500\n",
      "33/33 [==============================] - 0s 974us/step - loss: 0.5625 - accuracy: 0.7656\n",
      "Epoch 31/1500\n",
      "33/33 [==============================] - 0s 979us/step - loss: 0.5501 - accuracy: 0.7637\n",
      "Epoch 32/1500\n",
      "33/33 [==============================] - 0s 920us/step - loss: 0.5593 - accuracy: 0.7535\n",
      "Epoch 33/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5329 - accuracy: 0.7783\n",
      "Epoch 34/1500\n",
      "33/33 [==============================] - 0s 999us/step - loss: 0.5525 - accuracy: 0.7608\n",
      "Epoch 35/1500\n",
      "33/33 [==============================] - 0s 989us/step - loss: 0.5530 - accuracy: 0.7671\n",
      "Epoch 36/1500\n",
      "33/33 [==============================] - 0s 905us/step - loss: 0.5479 - accuracy: 0.7608\n",
      "Epoch 37/1500\n",
      "33/33 [==============================] - 0s 880us/step - loss: 0.5278 - accuracy: 0.7763\n",
      "Epoch 38/1500\n",
      "33/33 [==============================] - 0s 906us/step - loss: 0.5233 - accuracy: 0.7802\n",
      "Epoch 39/1500\n",
      "33/33 [==============================] - 0s 819us/step - loss: 0.5281 - accuracy: 0.7758\n",
      "Epoch 40/1500\n",
      "33/33 [==============================] - 0s 831us/step - loss: 0.5238 - accuracy: 0.7802\n",
      "Epoch 41/1500\n",
      "33/33 [==============================] - 0s 945us/step - loss: 0.5033 - accuracy: 0.7885\n",
      "Epoch 42/1500\n",
      "33/33 [==============================] - 0s 858us/step - loss: 0.5111 - accuracy: 0.7749\n",
      "Epoch 43/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.4995 - accuracy: 0.7817\n",
      "Epoch 44/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.4926 - accuracy: 0.7899\n",
      "Epoch 45/1500\n",
      "33/33 [==============================] - 0s 817us/step - loss: 0.4998 - accuracy: 0.7817\n",
      "Epoch 46/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.4999 - accuracy: 0.7846\n",
      "Epoch 47/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.5131 - accuracy: 0.7754\n",
      "Epoch 48/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.4719 - accuracy: 0.7991\n",
      "Epoch 49/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.4911 - accuracy: 0.7938\n",
      "Epoch 50/1500\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.4903 - accuracy: 0.7948\n",
      "Epoch 51/1500\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.4808 - accuracy: 0.7855\n",
      "Epoch 52/1500\n",
      "33/33 [==============================] - 0s 845us/step - loss: 0.5009 - accuracy: 0.7923\n",
      "Epoch 53/1500\n",
      "33/33 [==============================] - 0s 896us/step - loss: 0.4856 - accuracy: 0.7982\n",
      "Epoch 54/1500\n",
      "33/33 [==============================] - 0s 854us/step - loss: 0.4606 - accuracy: 0.8049\n",
      "Epoch 55/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.4879 - accuracy: 0.7855\n",
      "Epoch 56/1500\n",
      "33/33 [==============================] - 0s 862us/step - loss: 0.4824 - accuracy: 0.7812\n",
      "Epoch 57/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.4667 - accuracy: 0.8117\n",
      "Epoch 58/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.4648 - accuracy: 0.8006\n",
      "Epoch 59/1500\n",
      "33/33 [==============================] - 0s 737us/step - loss: 0.4466 - accuracy: 0.8103\n",
      "Epoch 60/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.4593 - accuracy: 0.8045\n",
      "Epoch 61/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.4756 - accuracy: 0.7962\n",
      "Epoch 62/1500\n",
      "33/33 [==============================] - 0s 788us/step - loss: 0.4668 - accuracy: 0.7962\n",
      "Epoch 63/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.4555 - accuracy: 0.8049\n",
      "Epoch 64/1500\n",
      "33/33 [==============================] - 0s 818us/step - loss: 0.4476 - accuracy: 0.8093\n",
      "Epoch 65/1500\n",
      "33/33 [==============================] - 0s 856us/step - loss: 0.4517 - accuracy: 0.8093\n",
      "Epoch 66/1500\n",
      "33/33 [==============================] - 0s 821us/step - loss: 0.4630 - accuracy: 0.8210\n",
      "Epoch 67/1500\n",
      "33/33 [==============================] - 0s 807us/step - loss: 0.4456 - accuracy: 0.8258\n",
      "Epoch 68/1500\n",
      "33/33 [==============================] - 0s 828us/step - loss: 0.4384 - accuracy: 0.8132\n",
      "Epoch 69/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.4461 - accuracy: 0.8088\n",
      "Epoch 70/1500\n",
      "33/33 [==============================] - 0s 807us/step - loss: 0.4478 - accuracy: 0.8103\n",
      "Epoch 71/1500\n",
      "33/33 [==============================] - 0s 819us/step - loss: 0.4334 - accuracy: 0.8127\n",
      "Epoch 72/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.4413 - accuracy: 0.8156\n",
      "Epoch 73/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.4429 - accuracy: 0.8166\n",
      "Epoch 74/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.4228 - accuracy: 0.8195\n",
      "Epoch 75/1500\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.4441 - accuracy: 0.8156\n",
      "Epoch 76/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.4237 - accuracy: 0.8200\n",
      "Epoch 77/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.4250 - accuracy: 0.8171\n",
      "Epoch 78/1500\n",
      "33/33 [==============================] - 0s 729us/step - loss: 0.4364 - accuracy: 0.8113\n",
      "Epoch 79/1500\n",
      "33/33 [==============================] - 0s 731us/step - loss: 0.4346 - accuracy: 0.8171\n",
      "Epoch 80/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.4277 - accuracy: 0.8108\n",
      "Epoch 81/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.4149 - accuracy: 0.8282\n",
      "Epoch 82/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.4185 - accuracy: 0.8273\n",
      "Epoch 83/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.4145 - accuracy: 0.8244\n",
      "Epoch 84/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.4182 - accuracy: 0.8171\n",
      "Epoch 85/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.4157 - accuracy: 0.8195\n",
      "Epoch 86/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.4129 - accuracy: 0.8248\n",
      "Epoch 87/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.4126 - accuracy: 0.8287\n",
      "Epoch 88/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.4089 - accuracy: 0.8292\n",
      "Epoch 89/1500\n",
      "33/33 [==============================] - 0s 823us/step - loss: 0.4023 - accuracy: 0.8268\n",
      "Epoch 90/1500\n",
      "33/33 [==============================] - 0s 802us/step - loss: 0.3958 - accuracy: 0.8297\n",
      "Epoch 91/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.4236 - accuracy: 0.8234\n",
      "Epoch 92/1500\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8292\n",
      "Epoch 93/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4064 - accuracy: 0.8365\n",
      "Epoch 94/1500\n",
      "33/33 [==============================] - 0s 903us/step - loss: 0.4027 - accuracy: 0.8375\n",
      "Epoch 95/1500\n",
      "33/33 [==============================] - 0s 809us/step - loss: 0.3917 - accuracy: 0.8428\n",
      "Epoch 96/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.3906 - accuracy: 0.8394\n",
      "Epoch 97/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.3997 - accuracy: 0.8360\n",
      "Epoch 98/1500\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.3928 - accuracy: 0.8452\n",
      "Epoch 99/1500\n",
      "33/33 [==============================] - 0s 863us/step - loss: 0.3731 - accuracy: 0.8452\n",
      "Epoch 100/1500\n",
      "33/33 [==============================] - 0s 911us/step - loss: 0.4007 - accuracy: 0.8282\n",
      "Epoch 101/1500\n",
      "33/33 [==============================] - 0s 825us/step - loss: 0.3942 - accuracy: 0.8370\n",
      "Epoch 102/1500\n",
      "33/33 [==============================] - 0s 867us/step - loss: 0.4024 - accuracy: 0.8341\n",
      "Epoch 103/1500\n",
      "33/33 [==============================] - 0s 970us/step - loss: 0.3690 - accuracy: 0.8462\n",
      "Epoch 104/1500\n",
      "33/33 [==============================] - 0s 931us/step - loss: 0.3830 - accuracy: 0.8428\n",
      "Epoch 105/1500\n",
      "33/33 [==============================] - 0s 985us/step - loss: 0.3898 - accuracy: 0.8365\n",
      "Epoch 106/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3785 - accuracy: 0.8394\n",
      "Epoch 107/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.3772 - accuracy: 0.8418\n",
      "Epoch 108/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.3645 - accuracy: 0.8467\n",
      "Epoch 109/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.3751 - accuracy: 0.8438\n",
      "Epoch 110/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.3675 - accuracy: 0.8520\n",
      "Epoch 111/1500\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.3887 - accuracy: 0.8321\n",
      "Epoch 112/1500\n",
      "33/33 [==============================] - 0s 726us/step - loss: 0.3621 - accuracy: 0.8476\n",
      "Epoch 113/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.3879 - accuracy: 0.8350\n",
      "Epoch 114/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.3752 - accuracy: 0.8462\n",
      "Epoch 115/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.3566 - accuracy: 0.8496\n",
      "Epoch 116/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.3666 - accuracy: 0.8525\n",
      "Epoch 117/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.3793 - accuracy: 0.8544\n",
      "Epoch 118/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.3500 - accuracy: 0.8506\n",
      "Epoch 119/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.3672 - accuracy: 0.8457\n",
      "Epoch 120/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.3623 - accuracy: 0.8467\n",
      "Epoch 121/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.3559 - accuracy: 0.8544\n",
      "Epoch 122/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.3475 - accuracy: 0.8607\n",
      "Epoch 123/1500\n",
      "33/33 [==============================] - 0s 723us/step - loss: 0.3523 - accuracy: 0.8496\n",
      "Epoch 124/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.3562 - accuracy: 0.8554\n",
      "Epoch 125/1500\n",
      "33/33 [==============================] - 0s 722us/step - loss: 0.3599 - accuracy: 0.8438\n",
      "Epoch 126/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.3507 - accuracy: 0.8549\n",
      "Epoch 127/1500\n",
      "33/33 [==============================] - 0s 746us/step - loss: 0.3483 - accuracy: 0.8569\n",
      "Epoch 128/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.3439 - accuracy: 0.8632\n",
      "Epoch 129/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.3468 - accuracy: 0.8549\n",
      "Epoch 130/1500\n",
      "33/33 [==============================] - 0s 735us/step - loss: 0.3579 - accuracy: 0.8462\n",
      "Epoch 131/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.3535 - accuracy: 0.8559\n",
      "Epoch 132/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.3640 - accuracy: 0.8535\n",
      "Epoch 133/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.3644 - accuracy: 0.8452\n",
      "Epoch 134/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.3461 - accuracy: 0.8607\n",
      "Epoch 135/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.3432 - accuracy: 0.8622\n",
      "Epoch 136/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.3451 - accuracy: 0.8549\n",
      "Epoch 137/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.3402 - accuracy: 0.8564\n",
      "Epoch 138/1500\n",
      "33/33 [==============================] - 0s 817us/step - loss: 0.3443 - accuracy: 0.8617\n",
      "Epoch 139/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.3332 - accuracy: 0.8583\n",
      "Epoch 140/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.3480 - accuracy: 0.8574\n",
      "Epoch 141/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3177 - accuracy: 0.8738\n",
      "Epoch 142/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.3305 - accuracy: 0.8666\n",
      "Epoch 143/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.3218 - accuracy: 0.8758\n",
      "Epoch 144/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.3218 - accuracy: 0.8700\n",
      "Epoch 145/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.3263 - accuracy: 0.8641\n",
      "Epoch 146/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.3319 - accuracy: 0.8612\n",
      "Epoch 147/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.3314 - accuracy: 0.8700\n",
      "Epoch 148/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.3221 - accuracy: 0.8603\n",
      "Epoch 149/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.3203 - accuracy: 0.8680\n",
      "Epoch 150/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.3117 - accuracy: 0.8738\n",
      "Epoch 151/1500\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.3296 - accuracy: 0.8651\n",
      "Epoch 152/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.3470 - accuracy: 0.8506\n",
      "Epoch 153/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.3156 - accuracy: 0.8743\n",
      "Epoch 154/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.3473 - accuracy: 0.8646\n",
      "Epoch 155/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.3180 - accuracy: 0.8661\n",
      "Epoch 156/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.3097 - accuracy: 0.8782\n",
      "Epoch 157/1500\n",
      "33/33 [==============================] - 0s 794us/step - loss: 0.3069 - accuracy: 0.8768\n",
      "Epoch 158/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.3029 - accuracy: 0.8777\n",
      "Epoch 159/1500\n",
      "33/33 [==============================] - 0s 807us/step - loss: 0.3245 - accuracy: 0.8748\n",
      "Epoch 160/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.3325 - accuracy: 0.8680\n",
      "Epoch 161/1500\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.3209 - accuracy: 0.8743\n",
      "Epoch 162/1500\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.3193 - accuracy: 0.8768\n",
      "Epoch 163/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.3048 - accuracy: 0.8797\n",
      "Epoch 164/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.3194 - accuracy: 0.8671\n",
      "Epoch 165/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.3180 - accuracy: 0.8753\n",
      "Epoch 166/1500\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.3141 - accuracy: 0.8772\n",
      "Epoch 167/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.3077 - accuracy: 0.8768\n",
      "Epoch 168/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.3028 - accuracy: 0.8768\n",
      "Epoch 169/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.3047 - accuracy: 0.8831\n",
      "Epoch 170/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.3104 - accuracy: 0.8734\n",
      "Epoch 171/1500\n",
      "33/33 [==============================] - 0s 809us/step - loss: 0.3017 - accuracy: 0.8855\n",
      "Epoch 172/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.3078 - accuracy: 0.8782\n",
      "Epoch 173/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.2935 - accuracy: 0.8869\n",
      "Epoch 174/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.3023 - accuracy: 0.8816\n",
      "Epoch 175/1500\n",
      "33/33 [==============================] - 0s 818us/step - loss: 0.2883 - accuracy: 0.8952\n",
      "Epoch 176/1500\n",
      "33/33 [==============================] - 0s 878us/step - loss: 0.3083 - accuracy: 0.8724\n",
      "Epoch 177/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.3114 - accuracy: 0.8680\n",
      "Epoch 178/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.3068 - accuracy: 0.8787\n",
      "Epoch 179/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.3022 - accuracy: 0.8753\n",
      "Epoch 180/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.2995 - accuracy: 0.8923\n",
      "Epoch 181/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2928 - accuracy: 0.8860\n",
      "Epoch 182/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.3041 - accuracy: 0.8748\n",
      "Epoch 183/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.2877 - accuracy: 0.8889\n",
      "Epoch 184/1500\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.2882 - accuracy: 0.8879\n",
      "Epoch 185/1500\n",
      "33/33 [==============================] - 0s 813us/step - loss: 0.2864 - accuracy: 0.8836\n",
      "Epoch 186/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.2873 - accuracy: 0.8869\n",
      "Epoch 187/1500\n",
      "33/33 [==============================] - 0s 787us/step - loss: 0.2883 - accuracy: 0.8845\n",
      "Epoch 188/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.2851 - accuracy: 0.8811\n",
      "Epoch 189/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.2891 - accuracy: 0.8860\n",
      "Epoch 190/1500\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2745 - accuracy: 0.8981\n",
      "Epoch 191/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2937 - accuracy: 0.8845\n",
      "Epoch 192/1500\n",
      "33/33 [==============================] - 0s 893us/step - loss: 0.3024 - accuracy: 0.8797\n",
      "Epoch 193/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.2874 - accuracy: 0.8826\n",
      "Epoch 194/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3019 - accuracy: 0.8792\n",
      "Epoch 195/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2860 - accuracy: 0.8879\n",
      "Epoch 196/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.2977 - accuracy: 0.8836\n",
      "Epoch 197/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.2834 - accuracy: 0.8908\n",
      "Epoch 198/1500\n",
      "33/33 [==============================] - 0s 747us/step - loss: 0.2824 - accuracy: 0.8855\n",
      "Epoch 199/1500\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.2713 - accuracy: 0.8967\n",
      "Epoch 200/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.2780 - accuracy: 0.8923\n",
      "Epoch 201/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2948 - accuracy: 0.8729\n",
      "Epoch 202/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.2884 - accuracy: 0.8802\n",
      "Epoch 203/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.2856 - accuracy: 0.8923\n",
      "Epoch 204/1500\n",
      "33/33 [==============================] - 0s 746us/step - loss: 0.2748 - accuracy: 0.8976\n",
      "Epoch 205/1500\n",
      "33/33 [==============================] - 0s 726us/step - loss: 0.2849 - accuracy: 0.8850\n",
      "Epoch 206/1500\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.2923 - accuracy: 0.8782\n",
      "Epoch 207/1500\n",
      "33/33 [==============================] - 0s 739us/step - loss: 0.2702 - accuracy: 0.8952\n",
      "Epoch 208/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.2729 - accuracy: 0.8962\n",
      "Epoch 209/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.2658 - accuracy: 0.8947\n",
      "Epoch 210/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.2714 - accuracy: 0.8923\n",
      "Epoch 211/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.2792 - accuracy: 0.8942\n",
      "Epoch 212/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.2671 - accuracy: 0.8913\n",
      "Epoch 213/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.2858 - accuracy: 0.8903\n",
      "Epoch 214/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.2873 - accuracy: 0.8874\n",
      "Epoch 215/1500\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.2764 - accuracy: 0.8831\n",
      "Epoch 216/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.2738 - accuracy: 0.8952\n",
      "Epoch 217/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.2702 - accuracy: 0.8976\n",
      "Epoch 218/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.2756 - accuracy: 0.8928\n",
      "Epoch 219/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.2839 - accuracy: 0.8908\n",
      "Epoch 220/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.2666 - accuracy: 0.8928\n",
      "Epoch 221/1500\n",
      "33/33 [==============================] - 0s 730us/step - loss: 0.2653 - accuracy: 0.8865\n",
      "Epoch 222/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.2751 - accuracy: 0.8942\n",
      "Epoch 223/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.2660 - accuracy: 0.8971\n",
      "Epoch 224/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.2860 - accuracy: 0.8816\n",
      "Epoch 225/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.2533 - accuracy: 0.8967\n",
      "Epoch 226/1500\n",
      "33/33 [==============================] - 0s 794us/step - loss: 0.2512 - accuracy: 0.8913\n",
      "Epoch 227/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.2745 - accuracy: 0.8894\n",
      "Epoch 228/1500\n",
      "33/33 [==============================] - 0s 733us/step - loss: 0.2718 - accuracy: 0.8986\n",
      "Epoch 229/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.2625 - accuracy: 0.8937\n",
      "Epoch 230/1500\n",
      "33/33 [==============================] - 0s 735us/step - loss: 0.2682 - accuracy: 0.8962\n",
      "Epoch 231/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.2758 - accuracy: 0.8971\n",
      "Epoch 232/1500\n",
      "33/33 [==============================] - 0s 734us/step - loss: 0.2509 - accuracy: 0.9020\n",
      "Epoch 233/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.2643 - accuracy: 0.8991\n",
      "Epoch 234/1500\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.2721 - accuracy: 0.8957\n",
      "Epoch 235/1500\n",
      "33/33 [==============================] - 0s 729us/step - loss: 0.2603 - accuracy: 0.9054\n",
      "Epoch 236/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.2472 - accuracy: 0.8996\n",
      "Epoch 237/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.2443 - accuracy: 0.9044\n",
      "Epoch 238/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.2513 - accuracy: 0.9044\n",
      "Epoch 239/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.2722 - accuracy: 0.8928\n",
      "Epoch 240/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.2526 - accuracy: 0.8991\n",
      "Epoch 241/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.2585 - accuracy: 0.8971\n",
      "Epoch 242/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.2560 - accuracy: 0.8952\n",
      "Epoch 243/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2484 - accuracy: 0.9044\n",
      "Epoch 244/1500\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.2655 - accuracy: 0.8942\n",
      "Epoch 245/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.2508 - accuracy: 0.9010\n",
      "Epoch 246/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.2588 - accuracy: 0.8996\n",
      "Epoch 247/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2610 - accuracy: 0.9049\n",
      "Epoch 248/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2528 - accuracy: 0.8981\n",
      "Epoch 249/1500\n",
      "33/33 [==============================] - 0s 792us/step - loss: 0.2429 - accuracy: 0.9034\n",
      "Epoch 250/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.2611 - accuracy: 0.8899\n",
      "Epoch 251/1500\n",
      "33/33 [==============================] - 0s 758us/step - loss: 0.2658 - accuracy: 0.9054\n",
      "Epoch 252/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.2474 - accuracy: 0.9054\n",
      "Epoch 253/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.2435 - accuracy: 0.9059\n",
      "Epoch 254/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.2578 - accuracy: 0.8947\n",
      "Epoch 255/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.2541 - accuracy: 0.9015\n",
      "Epoch 256/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.2517 - accuracy: 0.9020\n",
      "Epoch 257/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.2566 - accuracy: 0.9039\n",
      "Epoch 258/1500\n",
      "33/33 [==============================] - 0s 731us/step - loss: 0.2525 - accuracy: 0.8957\n",
      "Epoch 259/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.2400 - accuracy: 0.9049\n",
      "Epoch 260/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.2438 - accuracy: 0.8996\n",
      "Epoch 261/1500\n",
      "33/33 [==============================] - 0s 739us/step - loss: 0.2525 - accuracy: 0.8937\n",
      "Epoch 262/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.2471 - accuracy: 0.8996\n",
      "Epoch 263/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2494 - accuracy: 0.9107\n",
      "Epoch 264/1500\n",
      "33/33 [==============================] - 0s 792us/step - loss: 0.2366 - accuracy: 0.9107\n",
      "Epoch 265/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.2633 - accuracy: 0.8918\n",
      "Epoch 266/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.2480 - accuracy: 0.9059\n",
      "Epoch 267/1500\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.2395 - accuracy: 0.9073\n",
      "Epoch 268/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9078\n",
      "Epoch 269/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2288 - accuracy: 0.9161\n",
      "Epoch 270/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.2309 - accuracy: 0.9088\n",
      "Epoch 271/1500\n",
      "33/33 [==============================] - 0s 746us/step - loss: 0.2417 - accuracy: 0.9112\n",
      "Epoch 272/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.2395 - accuracy: 0.9015\n",
      "Epoch 273/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.2373 - accuracy: 0.9146\n",
      "Epoch 274/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.2417 - accuracy: 0.9025\n",
      "Epoch 275/1500\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.2395 - accuracy: 0.9068\n",
      "Epoch 276/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.2397 - accuracy: 0.9025\n",
      "Epoch 277/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.2509 - accuracy: 0.9010\n",
      "Epoch 278/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.2308 - accuracy: 0.9020\n",
      "Epoch 279/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.2270 - accuracy: 0.9098\n",
      "Epoch 280/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.2251 - accuracy: 0.9199\n",
      "Epoch 281/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.2313 - accuracy: 0.9127\n",
      "Epoch 282/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.2384 - accuracy: 0.9015\n",
      "Epoch 283/1500\n",
      "33/33 [==============================] - 0s 733us/step - loss: 0.2344 - accuracy: 0.9141\n",
      "Epoch 284/1500\n",
      "33/33 [==============================] - 0s 787us/step - loss: 0.2431 - accuracy: 0.8991\n",
      "Epoch 285/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2513 - accuracy: 0.9034\n",
      "Epoch 286/1500\n",
      "33/33 [==============================] - 0s 736us/step - loss: 0.2294 - accuracy: 0.9034\n",
      "Epoch 287/1500\n",
      "33/33 [==============================] - 0s 801us/step - loss: 0.2313 - accuracy: 0.9098\n",
      "Epoch 288/1500\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.2418 - accuracy: 0.9044\n",
      "Epoch 289/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.2251 - accuracy: 0.9199\n",
      "Epoch 290/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.2267 - accuracy: 0.9151\n",
      "Epoch 291/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.2228 - accuracy: 0.9175\n",
      "Epoch 292/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.2114 - accuracy: 0.9219\n",
      "Epoch 293/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.2257 - accuracy: 0.9107\n",
      "Epoch 294/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.2332 - accuracy: 0.9083\n",
      "Epoch 295/1500\n",
      "33/33 [==============================] - 0s 746us/step - loss: 0.2247 - accuracy: 0.9238\n",
      "Epoch 296/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.2302 - accuracy: 0.9131\n",
      "Epoch 297/1500\n",
      "33/33 [==============================] - 0s 736us/step - loss: 0.2351 - accuracy: 0.9083\n",
      "Epoch 298/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.2191 - accuracy: 0.9199\n",
      "Epoch 299/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.2274 - accuracy: 0.9127\n",
      "Epoch 300/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.2358 - accuracy: 0.9122\n",
      "Epoch 301/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.2244 - accuracy: 0.9151\n",
      "Epoch 302/1500\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.2190 - accuracy: 0.9165\n",
      "Epoch 303/1500\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.2310 - accuracy: 0.9088\n",
      "Epoch 304/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.2259 - accuracy: 0.9131\n",
      "Epoch 305/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.2321 - accuracy: 0.9122\n",
      "Epoch 306/1500\n",
      "33/33 [==============================] - 0s 739us/step - loss: 0.2155 - accuracy: 0.9209\n",
      "Epoch 307/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2290 - accuracy: 0.9122\n",
      "Epoch 308/1500\n",
      "33/33 [==============================] - 0s 795us/step - loss: 0.2190 - accuracy: 0.9209\n",
      "Epoch 309/1500\n",
      "33/33 [==============================] - 0s 758us/step - loss: 0.2140 - accuracy: 0.9170\n",
      "Epoch 310/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.2134 - accuracy: 0.9219\n",
      "Epoch 311/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.2244 - accuracy: 0.9165\n",
      "Epoch 312/1500\n",
      "33/33 [==============================] - 0s 792us/step - loss: 0.2100 - accuracy: 0.9224\n",
      "Epoch 313/1500\n",
      "33/33 [==============================] - 0s 795us/step - loss: 0.2233 - accuracy: 0.9102\n",
      "Epoch 314/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.2139 - accuracy: 0.9170\n",
      "Epoch 315/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.2245 - accuracy: 0.9146\n",
      "Epoch 316/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.2314 - accuracy: 0.9127\n",
      "Epoch 317/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.2108 - accuracy: 0.9190\n",
      "Epoch 318/1500\n",
      "33/33 [==============================] - 0s 802us/step - loss: 0.2044 - accuracy: 0.9219\n",
      "Epoch 319/1500\n",
      "33/33 [==============================] - 0s 728us/step - loss: 0.2226 - accuracy: 0.9049\n",
      "Epoch 320/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.2144 - accuracy: 0.9165\n",
      "Epoch 321/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.2173 - accuracy: 0.9146\n",
      "Epoch 322/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.2086 - accuracy: 0.9214\n",
      "Epoch 323/1500\n",
      "33/33 [==============================] - 0s 720us/step - loss: 0.2129 - accuracy: 0.9219\n",
      "Epoch 324/1500\n",
      "33/33 [==============================] - 0s 718us/step - loss: 0.2070 - accuracy: 0.9224\n",
      "Epoch 325/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.2126 - accuracy: 0.9175\n",
      "Epoch 326/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.2083 - accuracy: 0.9253\n",
      "Epoch 327/1500\n",
      "33/33 [==============================] - 0s 736us/step - loss: 0.2142 - accuracy: 0.9098\n",
      "Epoch 328/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.2113 - accuracy: 0.9219\n",
      "Epoch 329/1500\n",
      "33/33 [==============================] - 0s 723us/step - loss: 0.2174 - accuracy: 0.9127\n",
      "Epoch 330/1500\n",
      "33/33 [==============================] - 0s 758us/step - loss: 0.2255 - accuracy: 0.9175\n",
      "Epoch 331/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.2048 - accuracy: 0.9243\n",
      "Epoch 332/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.2165 - accuracy: 0.9141\n",
      "Epoch 333/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.2236 - accuracy: 0.9117\n",
      "Epoch 334/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1993 - accuracy: 0.9204\n",
      "Epoch 335/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.1956 - accuracy: 0.9180\n",
      "Epoch 336/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.2118 - accuracy: 0.9131\n",
      "Epoch 337/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.2347 - accuracy: 0.9083\n",
      "Epoch 338/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.2124 - accuracy: 0.9175\n",
      "Epoch 339/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.2084 - accuracy: 0.9122\n",
      "Epoch 340/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.2102 - accuracy: 0.9258\n",
      "Epoch 341/1500\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2015 - accuracy: 0.9243\n",
      "Epoch 342/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9204\n",
      "Epoch 343/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.2097 - accuracy: 0.9156\n",
      "Epoch 344/1500\n",
      "33/33 [==============================] - 0s 714us/step - loss: 0.2094 - accuracy: 0.9180\n",
      "Epoch 345/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.2130 - accuracy: 0.9122\n",
      "Epoch 346/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2092 - accuracy: 0.9180\n",
      "Epoch 347/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.2118 - accuracy: 0.9195\n",
      "Epoch 348/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.2029 - accuracy: 0.9243\n",
      "Epoch 349/1500\n",
      "33/33 [==============================] - 0s 747us/step - loss: 0.2016 - accuracy: 0.9165\n",
      "Epoch 350/1500\n",
      "33/33 [==============================] - 0s 726us/step - loss: 0.2265 - accuracy: 0.9093\n",
      "Epoch 351/1500\n",
      "33/33 [==============================] - 0s 737us/step - loss: 0.2145 - accuracy: 0.9156\n",
      "Epoch 352/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.2049 - accuracy: 0.9224\n",
      "Epoch 353/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.2009 - accuracy: 0.9253\n",
      "Epoch 354/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1986 - accuracy: 0.9277\n",
      "Epoch 355/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.2090 - accuracy: 0.9229\n",
      "Epoch 356/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.2047 - accuracy: 0.9243\n",
      "Epoch 357/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.2045 - accuracy: 0.9229\n",
      "Epoch 358/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1955 - accuracy: 0.9214\n",
      "Epoch 359/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.2095 - accuracy: 0.9253\n",
      "Epoch 360/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.1925 - accuracy: 0.9267\n",
      "Epoch 361/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.1914 - accuracy: 0.9248\n",
      "Epoch 362/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.2162 - accuracy: 0.9112\n",
      "Epoch 363/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1965 - accuracy: 0.9253\n",
      "Epoch 364/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.2086 - accuracy: 0.9233\n",
      "Epoch 365/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.1946 - accuracy: 0.9238\n",
      "Epoch 366/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.2103 - accuracy: 0.9131\n",
      "Epoch 367/1500\n",
      "33/33 [==============================] - 0s 725us/step - loss: 0.2023 - accuracy: 0.9229\n",
      "Epoch 368/1500\n",
      "33/33 [==============================] - 0s 733us/step - loss: 0.1939 - accuracy: 0.9287\n",
      "Epoch 369/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.1972 - accuracy: 0.9267\n",
      "Epoch 370/1500\n",
      "33/33 [==============================] - 0s 723us/step - loss: 0.1919 - accuracy: 0.9311\n",
      "Epoch 371/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1894 - accuracy: 0.9282\n",
      "Epoch 372/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.2099 - accuracy: 0.9214\n",
      "Epoch 373/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.2129 - accuracy: 0.9180\n",
      "Epoch 374/1500\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.1872 - accuracy: 0.9262\n",
      "Epoch 375/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.2047 - accuracy: 0.9238\n",
      "Epoch 376/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.2092 - accuracy: 0.9170\n",
      "Epoch 377/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.1916 - accuracy: 0.9272\n",
      "Epoch 378/1500\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.1888 - accuracy: 0.9296\n",
      "Epoch 379/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1833 - accuracy: 0.9287\n",
      "Epoch 380/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.1949 - accuracy: 0.9282\n",
      "Epoch 381/1500\n",
      "33/33 [==============================] - 0s 747us/step - loss: 0.1953 - accuracy: 0.9233\n",
      "Epoch 382/1500\n",
      "33/33 [==============================] - 0s 728us/step - loss: 0.2042 - accuracy: 0.9262\n",
      "Epoch 383/1500\n",
      "33/33 [==============================] - 0s 723us/step - loss: 0.1898 - accuracy: 0.9233\n",
      "Epoch 384/1500\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.1975 - accuracy: 0.9262\n",
      "Epoch 385/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.1937 - accuracy: 0.9214\n",
      "Epoch 386/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1994 - accuracy: 0.9219\n",
      "Epoch 387/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.1957 - accuracy: 0.9180\n",
      "Epoch 388/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.1909 - accuracy: 0.9238\n",
      "Epoch 389/1500\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.1952 - accuracy: 0.9199\n",
      "Epoch 390/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.1710 - accuracy: 0.9355\n",
      "Epoch 391/1500\n",
      "33/33 [==============================] - 0s 728us/step - loss: 0.2040 - accuracy: 0.9146\n",
      "Epoch 392/1500\n",
      "33/33 [==============================] - 0s 747us/step - loss: 0.1867 - accuracy: 0.9292\n",
      "Epoch 393/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.1937 - accuracy: 0.9219\n",
      "Epoch 394/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.1957 - accuracy: 0.9272\n",
      "Epoch 395/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.1884 - accuracy: 0.9296\n",
      "Epoch 396/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2179 - accuracy: 0.9165\n",
      "Epoch 397/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.1979 - accuracy: 0.9243\n",
      "Epoch 398/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.1920 - accuracy: 0.9258\n",
      "Epoch 399/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1927 - accuracy: 0.9219\n",
      "Epoch 400/1500\n",
      "33/33 [==============================] - 0s 731us/step - loss: 0.1863 - accuracy: 0.9311\n",
      "Epoch 401/1500\n",
      "33/33 [==============================] - 0s 728us/step - loss: 0.1919 - accuracy: 0.9272\n",
      "Epoch 402/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.2025 - accuracy: 0.9238\n",
      "Epoch 403/1500\n",
      "33/33 [==============================] - 0s 732us/step - loss: 0.1892 - accuracy: 0.9306\n",
      "Epoch 404/1500\n",
      "33/33 [==============================] - 0s 733us/step - loss: 0.1792 - accuracy: 0.9335\n",
      "Epoch 405/1500\n",
      "33/33 [==============================] - 0s 727us/step - loss: 0.1875 - accuracy: 0.9272\n",
      "Epoch 406/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.2020 - accuracy: 0.9238\n",
      "Epoch 407/1500\n",
      "33/33 [==============================] - 0s 721us/step - loss: 0.1836 - accuracy: 0.9272\n",
      "Epoch 408/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.1944 - accuracy: 0.9272\n",
      "Epoch 409/1500\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1889 - accuracy: 0.9248\n",
      "Epoch 410/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9277\n",
      "Epoch 411/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.1727 - accuracy: 0.9389\n",
      "Epoch 412/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.1817 - accuracy: 0.9355\n",
      "Epoch 413/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.1816 - accuracy: 0.9326\n",
      "Epoch 414/1500\n",
      "33/33 [==============================] - 0s 724us/step - loss: 0.1881 - accuracy: 0.9219\n",
      "Epoch 415/1500\n",
      "33/33 [==============================] - 0s 728us/step - loss: 0.1889 - accuracy: 0.9296\n",
      "Epoch 416/1500\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.1718 - accuracy: 0.9306\n",
      "Epoch 417/1500\n",
      "33/33 [==============================] - 0s 719us/step - loss: 0.1845 - accuracy: 0.9296\n",
      "Epoch 418/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.1642 - accuracy: 0.9379\n",
      "Epoch 419/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.2009 - accuracy: 0.9233\n",
      "Epoch 420/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.2144 - accuracy: 0.9151\n",
      "Epoch 421/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.1862 - accuracy: 0.9296\n",
      "Epoch 422/1500\n",
      "33/33 [==============================] - 0s 700us/step - loss: 0.1735 - accuracy: 0.9374\n",
      "Epoch 423/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1877 - accuracy: 0.9253\n",
      "Epoch 424/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.1801 - accuracy: 0.9272\n",
      "Epoch 425/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.1852 - accuracy: 0.9335\n",
      "Epoch 426/1500\n",
      "33/33 [==============================] - 0s 716us/step - loss: 0.2003 - accuracy: 0.9229\n",
      "Epoch 427/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.1853 - accuracy: 0.9267\n",
      "Epoch 428/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1868 - accuracy: 0.9272\n",
      "Epoch 429/1500\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.1870 - accuracy: 0.9306\n",
      "Epoch 430/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.1788 - accuracy: 0.9345\n",
      "Epoch 431/1500\n",
      "33/33 [==============================] - 0s 729us/step - loss: 0.1783 - accuracy: 0.9360\n",
      "Epoch 432/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.1785 - accuracy: 0.9326\n",
      "Epoch 433/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.1722 - accuracy: 0.9345\n",
      "Epoch 434/1500\n",
      "33/33 [==============================] - 0s 726us/step - loss: 0.1959 - accuracy: 0.9267\n",
      "Epoch 435/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.1668 - accuracy: 0.9369\n",
      "Epoch 436/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.1583 - accuracy: 0.9418\n",
      "Epoch 437/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.1865 - accuracy: 0.9311\n",
      "Epoch 438/1500\n",
      "33/33 [==============================] - 0s 713us/step - loss: 0.1817 - accuracy: 0.9262\n",
      "Epoch 439/1500\n",
      "33/33 [==============================] - 0s 729us/step - loss: 0.1757 - accuracy: 0.9330\n",
      "Epoch 440/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.1697 - accuracy: 0.9384\n",
      "Epoch 441/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.1780 - accuracy: 0.9360\n",
      "Epoch 442/1500\n",
      "33/33 [==============================] - 0s 692us/step - loss: 0.1670 - accuracy: 0.9350\n",
      "Epoch 443/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.1860 - accuracy: 0.9287\n",
      "Epoch 444/1500\n",
      "33/33 [==============================] - 0s 727us/step - loss: 0.1574 - accuracy: 0.9447\n",
      "Epoch 445/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.1734 - accuracy: 0.9374\n",
      "Epoch 446/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.1791 - accuracy: 0.9301\n",
      "Epoch 447/1500\n",
      "33/33 [==============================] - 0s 725us/step - loss: 0.1646 - accuracy: 0.9360\n",
      "Epoch 448/1500\n",
      "33/33 [==============================] - 0s 732us/step - loss: 0.1764 - accuracy: 0.9340\n",
      "Epoch 449/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1754 - accuracy: 0.9282\n",
      "Epoch 450/1500\n",
      "33/33 [==============================] - 0s 721us/step - loss: 0.1746 - accuracy: 0.9360\n",
      "Epoch 451/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.1754 - accuracy: 0.9335\n",
      "Epoch 452/1500\n",
      "33/33 [==============================] - 0s 739us/step - loss: 0.1767 - accuracy: 0.9350\n",
      "Epoch 453/1500\n",
      "33/33 [==============================] - 0s 711us/step - loss: 0.1775 - accuracy: 0.9311\n",
      "Epoch 454/1500\n",
      "33/33 [==============================] - 0s 718us/step - loss: 0.1662 - accuracy: 0.9369\n",
      "Epoch 455/1500\n",
      "33/33 [==============================] - 0s 706us/step - loss: 0.1929 - accuracy: 0.9209\n",
      "Epoch 456/1500\n",
      "33/33 [==============================] - 0s 726us/step - loss: 0.1720 - accuracy: 0.9364\n",
      "Epoch 457/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.1743 - accuracy: 0.9398\n",
      "Epoch 458/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.1862 - accuracy: 0.9248\n",
      "Epoch 459/1500\n",
      "33/33 [==============================] - 0s 725us/step - loss: 0.1688 - accuracy: 0.9350\n",
      "Epoch 460/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.1675 - accuracy: 0.9350\n",
      "Epoch 461/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.1780 - accuracy: 0.9292\n",
      "Epoch 462/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.1759 - accuracy: 0.9340\n",
      "Epoch 463/1500\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.1758 - accuracy: 0.9398\n",
      "Epoch 464/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1641 - accuracy: 0.9408\n",
      "Epoch 465/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.1729 - accuracy: 0.9379\n",
      "Epoch 466/1500\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.1937 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 436.\n",
      "33/33 [==============================] - 0s 821us/step - loss: 0.1677 - accuracy: 0.9384\n",
      "Epoch 466: early stopping\n",
      "9/9 [==============================] - 0s 720us/step - loss: 0.8001 - accuracy: 0.6940\n",
      "9/9 [==============================] - 0s 540us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.8000537157058716, Accuracy: 0.6940298676490784, Precision: 0.7049983830544102, Recall: 0.6777764187866927, F1 Score: 0.6517832265254946\n",
      "Confusion Matrix:\n",
      " [[123   2  35]\n",
      " [ 37  36   0]\n",
      " [  8   0  27]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '041A' '042A'\n",
      " '044A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '040A'\n",
      " '043A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "741\n",
      "Length of y_train_val:\n",
      "741\n",
      "Length of groups_train_val:\n",
      "741\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     26\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 866, 2: 815, 1: 725})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 987us/step - loss: 1.1522 - accuracy: 0.5008\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 945us/step - loss: 0.9116 - accuracy: 0.6114\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.8209 - accuracy: 0.6613\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.7532 - accuracy: 0.6866\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.7123 - accuracy: 0.6974\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.7082 - accuracy: 0.7116\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.6859 - accuracy: 0.7195\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.6607 - accuracy: 0.7311\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.6133 - accuracy: 0.7515\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.6226 - accuracy: 0.7423\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.6090 - accuracy: 0.7569\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.6117 - accuracy: 0.7485\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.5771 - accuracy: 0.7639\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.5834 - accuracy: 0.7643\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.5706 - accuracy: 0.7714\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.5525 - accuracy: 0.7697\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.5501 - accuracy: 0.7843\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.5420 - accuracy: 0.7739\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.5247 - accuracy: 0.7839\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.5085 - accuracy: 0.7847\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.5150 - accuracy: 0.7926\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.5008 - accuracy: 0.7959\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.5071 - accuracy: 0.7922\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.4839 - accuracy: 0.8051\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.5061 - accuracy: 0.7934\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.4769 - accuracy: 0.8059\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.4903 - accuracy: 0.8034\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.4733 - accuracy: 0.8009\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.4688 - accuracy: 0.8126\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.4548 - accuracy: 0.8184\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.4551 - accuracy: 0.8234\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.4719 - accuracy: 0.8113\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.4613 - accuracy: 0.8130\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.4624 - accuracy: 0.8055\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.4527 - accuracy: 0.8063\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.4446 - accuracy: 0.8196\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.4365 - accuracy: 0.8263\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.4298 - accuracy: 0.8250\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.4408 - accuracy: 0.8213\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.4271 - accuracy: 0.8263\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.4160 - accuracy: 0.8275\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.4157 - accuracy: 0.8329\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.4435 - accuracy: 0.8246\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.4414 - accuracy: 0.8188\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.4239 - accuracy: 0.8279\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.4152 - accuracy: 0.8304\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.4037 - accuracy: 0.8342\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.4227 - accuracy: 0.8242\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.4032 - accuracy: 0.8392\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.3936 - accuracy: 0.8392\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.3946 - accuracy: 0.8375\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3932 - accuracy: 0.8433\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8250\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.3736 - accuracy: 0.8479\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.3976 - accuracy: 0.8337\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3989 - accuracy: 0.8421\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.3877 - accuracy: 0.8404\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3802 - accuracy: 0.8412\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.3868 - accuracy: 0.8462\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.3806 - accuracy: 0.8483\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.3698 - accuracy: 0.8525\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3804 - accuracy: 0.8512\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3727 - accuracy: 0.8483\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.3815 - accuracy: 0.8462\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3746 - accuracy: 0.8533\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.3839 - accuracy: 0.8408\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.3653 - accuracy: 0.8491\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.3779 - accuracy: 0.8491\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3659 - accuracy: 0.8421\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.3678 - accuracy: 0.8500\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.3550 - accuracy: 0.8537\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3654 - accuracy: 0.8487\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.3546 - accuracy: 0.8554\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.3496 - accuracy: 0.8545\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.3604 - accuracy: 0.8549\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3596 - accuracy: 0.8541\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.3529 - accuracy: 0.8533\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3597 - accuracy: 0.8549\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.3435 - accuracy: 0.8645\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3523 - accuracy: 0.8475\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.3386 - accuracy: 0.8637\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.3393 - accuracy: 0.8587\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3332 - accuracy: 0.8699\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.3434 - accuracy: 0.8587\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.3430 - accuracy: 0.8637\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.3357 - accuracy: 0.8641\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.3274 - accuracy: 0.8658\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3360 - accuracy: 0.8562\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.3338 - accuracy: 0.8608\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3344 - accuracy: 0.8633\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.3317 - accuracy: 0.8583\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.3310 - accuracy: 0.8624\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.3406 - accuracy: 0.8633\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.3310 - accuracy: 0.8662\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.3286 - accuracy: 0.8628\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3274 - accuracy: 0.8695\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3324 - accuracy: 0.8603\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.3134 - accuracy: 0.8687\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.3250 - accuracy: 0.8678\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.3105 - accuracy: 0.8691\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3057 - accuracy: 0.8749\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3126 - accuracy: 0.8791\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.3085 - accuracy: 0.8736\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.3263 - accuracy: 0.8562\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3064 - accuracy: 0.8753\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3080 - accuracy: 0.8766\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.3162 - accuracy: 0.8641\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.3139 - accuracy: 0.8749\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.3187 - accuracy: 0.8720\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3028 - accuracy: 0.8745\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3032 - accuracy: 0.8741\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.3074 - accuracy: 0.8732\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2977 - accuracy: 0.8791\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3027 - accuracy: 0.8795\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3153 - accuracy: 0.8712\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.2969 - accuracy: 0.8807\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2962 - accuracy: 0.8849\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.3211 - accuracy: 0.8633\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2932 - accuracy: 0.8820\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3068 - accuracy: 0.8707\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2955 - accuracy: 0.8753\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2995 - accuracy: 0.8778\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2893 - accuracy: 0.8857\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.2941 - accuracy: 0.8803\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2932 - accuracy: 0.8820\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2897 - accuracy: 0.8786\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.2929 - accuracy: 0.8815\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2792 - accuracy: 0.8840\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.2792 - accuracy: 0.8899\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.2912 - accuracy: 0.8807\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2895 - accuracy: 0.8840\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2797 - accuracy: 0.8894\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2737 - accuracy: 0.8853\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2727 - accuracy: 0.8957\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2823 - accuracy: 0.8865\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2730 - accuracy: 0.8982\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.2741 - accuracy: 0.8890\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2873 - accuracy: 0.8807\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2700 - accuracy: 0.8911\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2772 - accuracy: 0.8903\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.2810 - accuracy: 0.8882\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2821 - accuracy: 0.8886\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2687 - accuracy: 0.8911\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2718 - accuracy: 0.8919\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.2840 - accuracy: 0.8890\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.2812 - accuracy: 0.8936\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2773 - accuracy: 0.8953\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.2849 - accuracy: 0.8874\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2671 - accuracy: 0.8940\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2775 - accuracy: 0.8857\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2666 - accuracy: 0.8936\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2731 - accuracy: 0.8886\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2565 - accuracy: 0.8982\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.2751 - accuracy: 0.8807\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2560 - accuracy: 0.8990\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2587 - accuracy: 0.8944\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2680 - accuracy: 0.8924\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.2584 - accuracy: 0.8965\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2516 - accuracy: 0.9002\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2643 - accuracy: 0.8973\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2669 - accuracy: 0.8940\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2526 - accuracy: 0.9019\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.2655 - accuracy: 0.8944\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.2653 - accuracy: 0.8948\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.2527 - accuracy: 0.8998\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2631 - accuracy: 0.8915\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2638 - accuracy: 0.8928\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2415 - accuracy: 0.9036\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.2611 - accuracy: 0.8978\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.2513 - accuracy: 0.9015\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2593 - accuracy: 0.9002\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.2734 - accuracy: 0.8878\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.2494 - accuracy: 0.9032\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2513 - accuracy: 0.8982\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2424 - accuracy: 0.9040\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.2442 - accuracy: 0.9048\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.2411 - accuracy: 0.9032\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2336 - accuracy: 0.9052\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2453 - accuracy: 0.9052\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2463 - accuracy: 0.9057\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2506 - accuracy: 0.8965\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2484 - accuracy: 0.8969\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2486 - accuracy: 0.8986\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2305 - accuracy: 0.9081\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2460 - accuracy: 0.8982\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.2380 - accuracy: 0.9052\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2430 - accuracy: 0.9011\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2371 - accuracy: 0.9065\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.2445 - accuracy: 0.8969\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2442 - accuracy: 0.9015\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2380 - accuracy: 0.9073\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2358 - accuracy: 0.9115\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2320 - accuracy: 0.9077\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2397 - accuracy: 0.8998\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2349 - accuracy: 0.9057\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2339 - accuracy: 0.9036\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2362 - accuracy: 0.9102\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9081\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.2251 - accuracy: 0.9127\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2352 - accuracy: 0.9123\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.2385 - accuracy: 0.9102\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.2433 - accuracy: 0.8961\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2256 - accuracy: 0.9173\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2149 - accuracy: 0.9156\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2291 - accuracy: 0.9090\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2268 - accuracy: 0.9065\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2155 - accuracy: 0.9198\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.2223 - accuracy: 0.9115\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2247 - accuracy: 0.9123\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2266 - accuracy: 0.9123\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2266 - accuracy: 0.9123\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2163 - accuracy: 0.9127\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2205 - accuracy: 0.9140\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.2367 - accuracy: 0.9106\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.2309 - accuracy: 0.9102\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2380 - accuracy: 0.9048\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2189 - accuracy: 0.9194\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2195 - accuracy: 0.9094\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2177 - accuracy: 0.9057\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.2124 - accuracy: 0.9231\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.2208 - accuracy: 0.9127\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2320 - accuracy: 0.9032\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2334 - accuracy: 0.9098\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2365 - accuracy: 0.9057\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2089 - accuracy: 0.9177\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2166 - accuracy: 0.9094\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1992 - accuracy: 0.9210\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.2143 - accuracy: 0.9148\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2136 - accuracy: 0.9194\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2207 - accuracy: 0.9123\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2154 - accuracy: 0.9198\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2135 - accuracy: 0.9165\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2019 - accuracy: 0.9252\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2099 - accuracy: 0.9177\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2280 - accuracy: 0.9057\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2028 - accuracy: 0.9194\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2127 - accuracy: 0.9181\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2088 - accuracy: 0.9177\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2024 - accuracy: 0.9210\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2078 - accuracy: 0.9165\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2188 - accuracy: 0.9111\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2056 - accuracy: 0.9214\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2054 - accuracy: 0.9165\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.2118 - accuracy: 0.9156\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2049 - accuracy: 0.9219\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2004 - accuracy: 0.9223\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.1993 - accuracy: 0.9206\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.2042 - accuracy: 0.9198\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2152 - accuracy: 0.9185\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2010 - accuracy: 0.9223\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.1966 - accuracy: 0.9298\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2040 - accuracy: 0.9206\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1982 - accuracy: 0.9235\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1942 - accuracy: 0.9285\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2027 - accuracy: 0.9185\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1834 - accuracy: 0.9281\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1888 - accuracy: 0.9235\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.1945 - accuracy: 0.9244\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2071 - accuracy: 0.9140\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1848 - accuracy: 0.9256\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1975 - accuracy: 0.9235\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.1936 - accuracy: 0.9227\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.1960 - accuracy: 0.9219\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.1804 - accuracy: 0.9331\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2040 - accuracy: 0.9223\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1850 - accuracy: 0.9210\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1907 - accuracy: 0.9210\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.1995 - accuracy: 0.9231\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.1877 - accuracy: 0.9335\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1876 - accuracy: 0.9289\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.1908 - accuracy: 0.9252\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.1977 - accuracy: 0.9223\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.1996 - accuracy: 0.9256\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.1809 - accuracy: 0.9306\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1954 - accuracy: 0.9244\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2034 - accuracy: 0.9235\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1788 - accuracy: 0.9318\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.1926 - accuracy: 0.9235\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2081 - accuracy: 0.9102\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1923 - accuracy: 0.9331\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1786 - accuracy: 0.9293\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.1855 - accuracy: 0.9273\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1826 - accuracy: 0.9273\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1979 - accuracy: 0.9223\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1947 - accuracy: 0.9231\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.1879 - accuracy: 0.9281\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2017 - accuracy: 0.9198\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1888 - accuracy: 0.9331\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1777 - accuracy: 0.9314\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1921 - accuracy: 0.9235\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1953 - accuracy: 0.9273\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.1866 - accuracy: 0.9260\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1772 - accuracy: 0.9343\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1734 - accuracy: 0.9356\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1671 - accuracy: 0.9339\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1997 - accuracy: 0.9202\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1844 - accuracy: 0.9281\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1934 - accuracy: 0.9244\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1920 - accuracy: 0.9223\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1756 - accuracy: 0.9347\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1885 - accuracy: 0.9281\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1769 - accuracy: 0.9327\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.1753 - accuracy: 0.9306\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.1858 - accuracy: 0.9293\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1706 - accuracy: 0.9335\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1881 - accuracy: 0.9327\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.9281\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1710 - accuracy: 0.9302\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1781 - accuracy: 0.9310\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.1804 - accuracy: 0.9289\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1780 - accuracy: 0.9323\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1876 - accuracy: 0.9302\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1777 - accuracy: 0.9298\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.1685 - accuracy: 0.9347\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1702 - accuracy: 0.9381\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1791 - accuracy: 0.9293\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1840 - accuracy: 0.9331\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1867 - accuracy: 0.9306\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1908 - accuracy: 0.9318\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1877 - accuracy: 0.9289\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1744 - accuracy: 0.9318\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1720 - accuracy: 0.9293\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.1659 - accuracy: 0.9385\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1810 - accuracy: 0.9318\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1797 - accuracy: 0.9256\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.1589 - accuracy: 0.9381\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1667 - accuracy: 0.9356\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.1685 - accuracy: 0.9335\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1901 - accuracy: 0.9293\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.1632 - accuracy: 0.9352\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1841 - accuracy: 0.9318\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1717 - accuracy: 0.9372\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1765 - accuracy: 0.9323\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.1705 - accuracy: 0.9327\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.1766 - accuracy: 0.9281\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1618 - accuracy: 0.9364\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.1730 - accuracy: 0.9302\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1759 - accuracy: 0.9306\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1800 - accuracy: 0.9277\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1858 - accuracy: 0.9260\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1669 - accuracy: 0.9397\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1647 - accuracy: 0.9381\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1605 - accuracy: 0.9460\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1603 - accuracy: 0.9352\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1599 - accuracy: 0.9356\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1605 - accuracy: 0.9389\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1731 - accuracy: 0.9314\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1735 - accuracy: 0.9323\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1588 - accuracy: 0.9401\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1680 - accuracy: 0.9323\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.1617 - accuracy: 0.9381\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1724 - accuracy: 0.9335\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1807 - accuracy: 0.9327\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.1554 - accuracy: 0.9410\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1544 - accuracy: 0.9447\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1483 - accuracy: 0.9426\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.1779 - accuracy: 0.9377\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.1518 - accuracy: 0.9418\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1552 - accuracy: 0.9393\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.1628 - accuracy: 0.9360\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.1554 - accuracy: 0.9422\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.1641 - accuracy: 0.9360\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.1625 - accuracy: 0.9360\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.1641 - accuracy: 0.9406\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1541 - accuracy: 0.9435\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1717 - accuracy: 0.9318\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.1724 - accuracy: 0.9335\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1631 - accuracy: 0.9356\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1713 - accuracy: 0.9347\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1703 - accuracy: 0.9368\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1550 - accuracy: 0.9381\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1807 - accuracy: 0.9239\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1545 - accuracy: 0.9410\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1681 - accuracy: 0.9381\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1572 - accuracy: 0.9352\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1598 - accuracy: 0.9381\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.1662 - accuracy: 0.9323\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1569 - accuracy: 0.9406\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1524 - accuracy: 0.9364\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1599 - accuracy: 0.9381\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1644 - accuracy: 0.9368\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.1544 - accuracy: 0.9418\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1668 - accuracy: 0.9360\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1658 - accuracy: 0.9414\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1548 - accuracy: 0.9393\n",
      "Epoch 386/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1958 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 356.\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1475 - accuracy: 0.9435\n",
      "Epoch 386: early stopping\n",
      "7/7 [==============================] - 0s 732us/step - loss: 0.7272 - accuracy: 0.7347\n",
      "7/7 [==============================] - 0s 647us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.66 (19/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "Final Test Results - Loss: 0.7272123098373413, Accuracy: 0.7346938848495483, Precision: 0.5910178471154081, Recall: 0.7250620347394542, F1 Score: 0.6229269685988182\n",
      "Confusion Matrix:\n",
      " [[113  14  28]\n",
      " [  4  22   0]\n",
      " [  6   0   9]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 874, 1: 820, 2: 605})\n",
      "Epoch 1/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 1.2038 - accuracy: 0.4706\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.9101 - accuracy: 0.6185\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.8069 - accuracy: 0.6529\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.8014 - accuracy: 0.6581\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 714us/step - loss: 0.7452 - accuracy: 0.6942\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 708us/step - loss: 0.7143 - accuracy: 0.7025\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.6828 - accuracy: 0.7073\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.6642 - accuracy: 0.7190\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 710us/step - loss: 0.6548 - accuracy: 0.7308\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.6444 - accuracy: 0.7264\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.6363 - accuracy: 0.7316\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.6194 - accuracy: 0.7347\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.6213 - accuracy: 0.7255\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.5831 - accuracy: 0.7573\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.6040 - accuracy: 0.7512\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.5609 - accuracy: 0.7625\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.5745 - accuracy: 0.7586\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.5651 - accuracy: 0.7608\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.5616 - accuracy: 0.7682\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.5265 - accuracy: 0.7773\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.5163 - accuracy: 0.7799\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.5109 - accuracy: 0.7851\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.5218 - accuracy: 0.7773\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.5122 - accuracy: 0.7899\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.5042 - accuracy: 0.7964\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.5012 - accuracy: 0.7969\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.5027 - accuracy: 0.7847\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.5073 - accuracy: 0.7890\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.4762 - accuracy: 0.7951\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 965us/step - loss: 0.4750 - accuracy: 0.8077\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 947us/step - loss: 0.4827 - accuracy: 0.8047\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 970us/step - loss: 0.4817 - accuracy: 0.7973\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 967us/step - loss: 0.4741 - accuracy: 0.7916\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.4505 - accuracy: 0.8099\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 0.4685 - accuracy: 0.7995\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 0.4516 - accuracy: 0.8212\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - 0s 715us/step - loss: 0.4588 - accuracy: 0.8043\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.4560 - accuracy: 0.8130\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.4621 - accuracy: 0.8069\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 0.4472 - accuracy: 0.8060\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.4375 - accuracy: 0.8156\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.4419 - accuracy: 0.8073\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.4531 - accuracy: 0.8121\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.4391 - accuracy: 0.8173\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.4158 - accuracy: 0.8182\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.4257 - accuracy: 0.8264\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.4162 - accuracy: 0.8238\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.4133 - accuracy: 0.8182\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.4041 - accuracy: 0.8278\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.4330 - accuracy: 0.8177\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.4119 - accuracy: 0.8278\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 901us/step - loss: 0.4307 - accuracy: 0.8199\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.4176 - accuracy: 0.8282\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 770us/step - loss: 0.4018 - accuracy: 0.8234\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 714us/step - loss: 0.4026 - accuracy: 0.8286\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.3955 - accuracy: 0.8351\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.4122 - accuracy: 0.8334\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 0.3976 - accuracy: 0.8365\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.3918 - accuracy: 0.8373\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.3878 - accuracy: 0.8378\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.3971 - accuracy: 0.8334\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.3840 - accuracy: 0.8412\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 0.3814 - accuracy: 0.8404\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.3980 - accuracy: 0.8225\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 663us/step - loss: 0.3851 - accuracy: 0.8399\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.3831 - accuracy: 0.8317\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.3848 - accuracy: 0.8447\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.3859 - accuracy: 0.8347\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.3683 - accuracy: 0.8434\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.3610 - accuracy: 0.8443\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.3751 - accuracy: 0.8408\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.3919 - accuracy: 0.8456\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.3755 - accuracy: 0.8404\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 894us/step - loss: 0.3599 - accuracy: 0.8495\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 720us/step - loss: 0.3780 - accuracy: 0.8373\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 742us/step - loss: 0.3637 - accuracy: 0.8456\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.3737 - accuracy: 0.8486\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.3638 - accuracy: 0.8482\n",
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 0.3688 - accuracy: 0.8465\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 0.3616 - accuracy: 0.8512\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.3605 - accuracy: 0.8452\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.3519 - accuracy: 0.8525\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 728us/step - loss: 0.3378 - accuracy: 0.8608\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 682us/step - loss: 0.3359 - accuracy: 0.8573\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.3601 - accuracy: 0.8599\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 893us/step - loss: 0.3390 - accuracy: 0.8565\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 910us/step - loss: 0.3298 - accuracy: 0.8560\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 887us/step - loss: 0.3583 - accuracy: 0.8612\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.3310 - accuracy: 0.8673\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.3475 - accuracy: 0.8608\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.3318 - accuracy: 0.8591\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.3492 - accuracy: 0.8499\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 707us/step - loss: 0.3401 - accuracy: 0.8565\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 0.3249 - accuracy: 0.8708\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 709us/step - loss: 0.3387 - accuracy: 0.8612\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 708us/step - loss: 0.3407 - accuracy: 0.8499\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 742us/step - loss: 0.3124 - accuracy: 0.8699\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.3359 - accuracy: 0.8578\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.3342 - accuracy: 0.8565\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 710us/step - loss: 0.3226 - accuracy: 0.8678\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.3341 - accuracy: 0.8617\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.3192 - accuracy: 0.8726\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.3318 - accuracy: 0.8599\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 890us/step - loss: 0.3197 - accuracy: 0.8734\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.3161 - accuracy: 0.8699\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.3059 - accuracy: 0.8786\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.3104 - accuracy: 0.8699\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.3194 - accuracy: 0.8743\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.3144 - accuracy: 0.8699\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.3104 - accuracy: 0.8752\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.3224 - accuracy: 0.8647\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.3112 - accuracy: 0.8752\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 0.3012 - accuracy: 0.8760\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.3119 - accuracy: 0.8773\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 706us/step - loss: 0.3138 - accuracy: 0.8712\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.3201 - accuracy: 0.8612\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 699us/step - loss: 0.3066 - accuracy: 0.8704\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.3136 - accuracy: 0.8691\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 715us/step - loss: 0.3064 - accuracy: 0.8717\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.3090 - accuracy: 0.8743\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.3001 - accuracy: 0.8782\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 694us/step - loss: 0.3118 - accuracy: 0.8721\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.3019 - accuracy: 0.8795\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2937 - accuracy: 0.8834\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.2892 - accuracy: 0.8834\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2965 - accuracy: 0.8808\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.2855 - accuracy: 0.8839\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 703us/step - loss: 0.2859 - accuracy: 0.8895\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.2921 - accuracy: 0.8839\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.2841 - accuracy: 0.8830\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.2932 - accuracy: 0.8743\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 718us/step - loss: 0.2789 - accuracy: 0.8895\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 713us/step - loss: 0.2997 - accuracy: 0.8843\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.2936 - accuracy: 0.8856\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 728us/step - loss: 0.2926 - accuracy: 0.8821\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.2744 - accuracy: 0.8886\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 714us/step - loss: 0.2972 - accuracy: 0.8752\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.2925 - accuracy: 0.8821\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.2853 - accuracy: 0.8839\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 0.2925 - accuracy: 0.8860\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 744us/step - loss: 0.2948 - accuracy: 0.8747\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.2856 - accuracy: 0.8860\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.2832 - accuracy: 0.8830\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 708us/step - loss: 0.2830 - accuracy: 0.8869\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.2672 - accuracy: 0.8865\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2801 - accuracy: 0.8908\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 717us/step - loss: 0.2672 - accuracy: 0.8934\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2686 - accuracy: 0.8873\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.2733 - accuracy: 0.8943\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.2736 - accuracy: 0.8956\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.2765 - accuracy: 0.8878\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 721us/step - loss: 0.2783 - accuracy: 0.8860\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2698 - accuracy: 0.8917\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 720us/step - loss: 0.2706 - accuracy: 0.8826\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.2731 - accuracy: 0.8873\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.2701 - accuracy: 0.8952\n",
      "Epoch 157/1500\n",
      "36/36 [==============================] - 0s 770us/step - loss: 0.2580 - accuracy: 0.9030\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 0.2680 - accuracy: 0.8926\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.2636 - accuracy: 0.8930\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 744us/step - loss: 0.2643 - accuracy: 0.8973\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.2579 - accuracy: 0.8930\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 720us/step - loss: 0.2675 - accuracy: 0.8926\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.2455 - accuracy: 0.9060\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 733us/step - loss: 0.2860 - accuracy: 0.8878\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.2760 - accuracy: 0.8926\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2591 - accuracy: 0.9030\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.9043\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.2531 - accuracy: 0.9004\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.2529 - accuracy: 0.8969\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 0.2636 - accuracy: 0.8917\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 711us/step - loss: 0.2626 - accuracy: 0.9008\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.2636 - accuracy: 0.8930\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.2657 - accuracy: 0.8969\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 711us/step - loss: 0.2525 - accuracy: 0.9039\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.2601 - accuracy: 0.9004\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 706us/step - loss: 0.2491 - accuracy: 0.8995\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 718us/step - loss: 0.2632 - accuracy: 0.8934\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - 0s 714us/step - loss: 0.2473 - accuracy: 0.9074\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.2529 - accuracy: 0.8978\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.2516 - accuracy: 0.8995\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.2459 - accuracy: 0.8987\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.2420 - accuracy: 0.9047\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.2474 - accuracy: 0.9043\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.2512 - accuracy: 0.9052\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.2470 - accuracy: 0.9069\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.2539 - accuracy: 0.8982\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.2606 - accuracy: 0.8934\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 721us/step - loss: 0.2491 - accuracy: 0.8987\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.2605 - accuracy: 0.9034\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.2410 - accuracy: 0.9060\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.2510 - accuracy: 0.9047\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.2544 - accuracy: 0.8908\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.2423 - accuracy: 0.9074\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.2484 - accuracy: 0.9052\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.2419 - accuracy: 0.9069\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.2329 - accuracy: 0.9056\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.2589 - accuracy: 0.8917\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.2353 - accuracy: 0.9100\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 713us/step - loss: 0.2426 - accuracy: 0.9087\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.2340 - accuracy: 0.9052\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.2407 - accuracy: 0.9108\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - 0s 717us/step - loss: 0.2455 - accuracy: 0.9052\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.2350 - accuracy: 0.9082\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.2329 - accuracy: 0.9108\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.2455 - accuracy: 0.9056\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 747us/step - loss: 0.2292 - accuracy: 0.9108\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.2351 - accuracy: 0.9074\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2327 - accuracy: 0.9078\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.2346 - accuracy: 0.9069\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.2310 - accuracy: 0.9178\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 752us/step - loss: 0.2217 - accuracy: 0.9134\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 0.2375 - accuracy: 0.9043\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.2314 - accuracy: 0.9091\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.2282 - accuracy: 0.9126\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 711us/step - loss: 0.2453 - accuracy: 0.9078\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.2353 - accuracy: 0.9082\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.2175 - accuracy: 0.9204\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.2260 - accuracy: 0.9147\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.2359 - accuracy: 0.9074\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.2204 - accuracy: 0.9156\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.2340 - accuracy: 0.9126\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.2270 - accuracy: 0.9113\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 728us/step - loss: 0.2180 - accuracy: 0.9134\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.2248 - accuracy: 0.9169\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.2296 - accuracy: 0.9100\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.2173 - accuracy: 0.9182\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.2232 - accuracy: 0.9143\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.2070 - accuracy: 0.9195\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.2370 - accuracy: 0.9043\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 717us/step - loss: 0.2164 - accuracy: 0.9134\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.2243 - accuracy: 0.9147\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.2125 - accuracy: 0.9182\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2187 - accuracy: 0.9191\n",
      "Epoch 234/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.2279 - accuracy: 0.9030\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.1994 - accuracy: 0.9187\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.2194 - accuracy: 0.9156\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.2143 - accuracy: 0.9200\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.2270 - accuracy: 0.9178\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.2246 - accuracy: 0.9187\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2138 - accuracy: 0.9165\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.2251 - accuracy: 0.9121\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.2414 - accuracy: 0.9043\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.2301 - accuracy: 0.9121\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.2074 - accuracy: 0.9208\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.2163 - accuracy: 0.9165\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.2176 - accuracy: 0.9161\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 927us/step - loss: 0.2195 - accuracy: 0.9156\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 0.2093 - accuracy: 0.9200\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.2159 - accuracy: 0.9095\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.2106 - accuracy: 0.9104\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.2205 - accuracy: 0.9147\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.2231 - accuracy: 0.9121\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 747us/step - loss: 0.2147 - accuracy: 0.9169\n",
      "Epoch 254/1500\n",
      "36/36 [==============================] - 0s 800us/step - loss: 0.2036 - accuracy: 0.9243\n",
      "Epoch 255/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.2070 - accuracy: 0.9178\n",
      "Epoch 256/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.2097 - accuracy: 0.9178\n",
      "Epoch 257/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.2161 - accuracy: 0.9139\n",
      "Epoch 258/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.2223 - accuracy: 0.9091\n",
      "Epoch 259/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.1958 - accuracy: 0.9230\n",
      "Epoch 260/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.2240 - accuracy: 0.9065\n",
      "Epoch 261/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.2259 - accuracy: 0.9069\n",
      "Epoch 262/1500\n",
      "36/36 [==============================] - 0s 924us/step - loss: 0.2164 - accuracy: 0.9204\n",
      "Epoch 263/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.1994 - accuracy: 0.9221\n",
      "Epoch 264/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.2138 - accuracy: 0.9217\n",
      "Epoch 265/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.2123 - accuracy: 0.9161\n",
      "Epoch 266/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.1915 - accuracy: 0.9295\n",
      "Epoch 267/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9313\n",
      "Epoch 268/1500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9230\n",
      "Epoch 269/1500\n",
      "36/36 [==============================] - 0s 924us/step - loss: 0.2105 - accuracy: 0.9169\n",
      "Epoch 270/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2082 - accuracy: 0.9226\n",
      "Epoch 271/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.2054 - accuracy: 0.9217\n",
      "Epoch 272/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.2072 - accuracy: 0.9187\n",
      "Epoch 273/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.1989 - accuracy: 0.9217\n",
      "Epoch 274/1500\n",
      "36/36 [==============================] - 0s 747us/step - loss: 0.1997 - accuracy: 0.9287\n",
      "Epoch 275/1500\n",
      "36/36 [==============================] - 0s 770us/step - loss: 0.2058 - accuracy: 0.9234\n",
      "Epoch 276/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.2012 - accuracy: 0.9200\n",
      "Epoch 277/1500\n",
      "36/36 [==============================] - 0s 744us/step - loss: 0.2183 - accuracy: 0.9104\n",
      "Epoch 278/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.1958 - accuracy: 0.9287\n",
      "Epoch 279/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 0.2083 - accuracy: 0.9156\n",
      "Epoch 280/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.2024 - accuracy: 0.9187\n",
      "Epoch 281/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.1986 - accuracy: 0.9204\n",
      "Epoch 282/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.2051 - accuracy: 0.9174\n",
      "Epoch 283/1500\n",
      "36/36 [==============================] - 0s 721us/step - loss: 0.2056 - accuracy: 0.9208\n",
      "Epoch 284/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.1967 - accuracy: 0.9247\n",
      "Epoch 285/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.1927 - accuracy: 0.9226\n",
      "Epoch 286/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.1931 - accuracy: 0.9217\n",
      "Epoch 287/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.1823 - accuracy: 0.9278\n",
      "Epoch 288/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.2022 - accuracy: 0.9252\n",
      "Epoch 289/1500\n",
      "36/36 [==============================] - 0s 770us/step - loss: 0.1998 - accuracy: 0.9204\n",
      "Epoch 290/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.1818 - accuracy: 0.9308\n",
      "Epoch 291/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.1885 - accuracy: 0.9291\n",
      "Epoch 292/1500\n",
      "36/36 [==============================] - 0s 742us/step - loss: 0.1908 - accuracy: 0.9261\n",
      "Epoch 293/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.1757 - accuracy: 0.9317\n",
      "Epoch 294/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.2076 - accuracy: 0.9182\n",
      "Epoch 295/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2017 - accuracy: 0.9178\n",
      "Epoch 296/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 0.2110 - accuracy: 0.9182\n",
      "Epoch 297/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.1825 - accuracy: 0.9295\n",
      "Epoch 298/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.2040 - accuracy: 0.9226\n",
      "Epoch 299/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 0.1937 - accuracy: 0.9174\n",
      "Epoch 300/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.1809 - accuracy: 0.9374\n",
      "Epoch 301/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.1833 - accuracy: 0.9247\n",
      "Epoch 302/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.1870 - accuracy: 0.9282\n",
      "Epoch 303/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.1995 - accuracy: 0.9234\n",
      "Epoch 304/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 0.1986 - accuracy: 0.9291\n",
      "Epoch 305/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.1984 - accuracy: 0.9252\n",
      "Epoch 306/1500\n",
      "36/36 [==============================] - 0s 806us/step - loss: 0.1906 - accuracy: 0.9295\n",
      "Epoch 307/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.2186 - accuracy: 0.9187\n",
      "Epoch 308/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 0.1787 - accuracy: 0.9265\n",
      "Epoch 309/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 0.1829 - accuracy: 0.9230\n",
      "Epoch 310/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.1853 - accuracy: 0.9330\n",
      "Epoch 311/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.1918 - accuracy: 0.9252\n",
      "Epoch 312/1500\n",
      "36/36 [==============================] - 0s 715us/step - loss: 0.1763 - accuracy: 0.9308\n",
      "Epoch 313/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.1915 - accuracy: 0.9274\n",
      "Epoch 314/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.1711 - accuracy: 0.9326\n",
      "Epoch 315/1500\n",
      "36/36 [==============================] - 0s 755us/step - loss: 0.1760 - accuracy: 0.9330\n",
      "Epoch 316/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.1907 - accuracy: 0.9339\n",
      "Epoch 317/1500\n",
      "36/36 [==============================] - 0s 709us/step - loss: 0.1844 - accuracy: 0.9274\n",
      "Epoch 318/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.1788 - accuracy: 0.9304\n",
      "Epoch 319/1500\n",
      "36/36 [==============================] - 0s 717us/step - loss: 0.1833 - accuracy: 0.9247\n",
      "Epoch 320/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.1900 - accuracy: 0.9278\n",
      "Epoch 321/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.1909 - accuracy: 0.9300\n",
      "Epoch 322/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 0.1886 - accuracy: 0.9252\n",
      "Epoch 323/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.1866 - accuracy: 0.9334\n",
      "Epoch 324/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.1778 - accuracy: 0.9317\n",
      "Epoch 325/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.1850 - accuracy: 0.9313\n",
      "Epoch 326/1500\n",
      "36/36 [==============================] - 0s 742us/step - loss: 0.1821 - accuracy: 0.9326\n",
      "Epoch 327/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.2029 - accuracy: 0.9230\n",
      "Epoch 328/1500\n",
      "36/36 [==============================] - 0s 733us/step - loss: 0.1875 - accuracy: 0.9226\n",
      "Epoch 329/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.1909 - accuracy: 0.9300\n",
      "Epoch 330/1500\n",
      "36/36 [==============================] - 0s 715us/step - loss: 0.1867 - accuracy: 0.9261\n",
      "Epoch 331/1500\n",
      "36/36 [==============================] - 0s 710us/step - loss: 0.1749 - accuracy: 0.9300\n",
      "Epoch 332/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.1756 - accuracy: 0.9352\n",
      "Epoch 333/1500\n",
      "36/36 [==============================] - 0s 733us/step - loss: 0.1777 - accuracy: 0.9361\n",
      "Epoch 334/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.1814 - accuracy: 0.9295\n",
      "Epoch 335/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.1806 - accuracy: 0.9343\n",
      "Epoch 336/1500\n",
      "36/36 [==============================] - 0s 718us/step - loss: 0.1758 - accuracy: 0.9361\n",
      "Epoch 337/1500\n",
      "36/36 [==============================] - 0s 752us/step - loss: 0.1662 - accuracy: 0.9361\n",
      "Epoch 338/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 0.1738 - accuracy: 0.9330\n",
      "Epoch 339/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.1847 - accuracy: 0.9278\n",
      "Epoch 340/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.1793 - accuracy: 0.9287\n",
      "Epoch 341/1500\n",
      "36/36 [==============================] - 0s 715us/step - loss: 0.1845 - accuracy: 0.9313\n",
      "Epoch 342/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.1750 - accuracy: 0.9282\n",
      "Epoch 343/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.9374\n",
      "Epoch 344/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9426\n",
      "Epoch 345/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.1758 - accuracy: 0.9391\n",
      "Epoch 346/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.1744 - accuracy: 0.9321\n",
      "Epoch 347/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 0.1682 - accuracy: 0.9343\n",
      "Epoch 348/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 0.1665 - accuracy: 0.9408\n",
      "Epoch 349/1500\n",
      "36/36 [==============================] - 0s 770us/step - loss: 0.1723 - accuracy: 0.9321\n",
      "Epoch 350/1500\n",
      "36/36 [==============================] - 0s 727us/step - loss: 0.1912 - accuracy: 0.9278\n",
      "Epoch 351/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.1765 - accuracy: 0.9352\n",
      "Epoch 352/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.1725 - accuracy: 0.9304\n",
      "Epoch 353/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.1587 - accuracy: 0.9417\n",
      "Epoch 354/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.1709 - accuracy: 0.9300\n",
      "Epoch 355/1500\n",
      "36/36 [==============================] - 0s 755us/step - loss: 0.1787 - accuracy: 0.9274\n",
      "Epoch 356/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.1739 - accuracy: 0.9295\n",
      "Epoch 357/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.1657 - accuracy: 0.9378\n",
      "Epoch 358/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.1606 - accuracy: 0.9387\n",
      "Epoch 359/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.1696 - accuracy: 0.9361\n",
      "Epoch 360/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.1858 - accuracy: 0.9221\n",
      "Epoch 361/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.1720 - accuracy: 0.9326\n",
      "Epoch 362/1500\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.1623 - accuracy: 0.9404\n",
      "Epoch 363/1500\n",
      "36/36 [==============================] - 0s 726us/step - loss: 0.1736 - accuracy: 0.9365\n",
      "Epoch 364/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.1643 - accuracy: 0.9356\n",
      "Epoch 365/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.1768 - accuracy: 0.9304\n",
      "Epoch 366/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.1556 - accuracy: 0.9439\n",
      "Epoch 367/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.1707 - accuracy: 0.9326\n",
      "Epoch 368/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.1715 - accuracy: 0.9330\n",
      "Epoch 369/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.1715 - accuracy: 0.9317\n",
      "Epoch 370/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.1719 - accuracy: 0.9334\n",
      "Epoch 371/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.1692 - accuracy: 0.9348\n",
      "Epoch 372/1500\n",
      "36/36 [==============================] - 0s 744us/step - loss: 0.1659 - accuracy: 0.9365\n",
      "Epoch 373/1500\n",
      "36/36 [==============================] - 0s 755us/step - loss: 0.1666 - accuracy: 0.9369\n",
      "Epoch 374/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.1642 - accuracy: 0.9426\n",
      "Epoch 375/1500\n",
      "36/36 [==============================] - 0s 766us/step - loss: 0.1741 - accuracy: 0.9317\n",
      "Epoch 376/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.1659 - accuracy: 0.9361\n",
      "Epoch 377/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.1790 - accuracy: 0.9287\n",
      "Epoch 378/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.1743 - accuracy: 0.9321\n",
      "Epoch 379/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.1689 - accuracy: 0.9317\n",
      "Epoch 380/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.1685 - accuracy: 0.9356\n",
      "Epoch 381/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 0.1687 - accuracy: 0.9330\n",
      "Epoch 382/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.1585 - accuracy: 0.9456\n",
      "Epoch 383/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.1638 - accuracy: 0.9382\n",
      "Epoch 384/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.1764 - accuracy: 0.9334\n",
      "Epoch 385/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.1676 - accuracy: 0.9374\n",
      "Epoch 386/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.1579 - accuracy: 0.9395\n",
      "Epoch 387/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.1719 - accuracy: 0.9330\n",
      "Epoch 388/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.1568 - accuracy: 0.9395\n",
      "Epoch 389/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.1561 - accuracy: 0.9465\n",
      "Epoch 390/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.1625 - accuracy: 0.9378\n",
      "Epoch 391/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.1604 - accuracy: 0.9413\n",
      "Epoch 392/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.1656 - accuracy: 0.9382\n",
      "Epoch 393/1500\n",
      "36/36 [==============================] - 0s 755us/step - loss: 0.1623 - accuracy: 0.9361\n",
      "Epoch 394/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.1667 - accuracy: 0.9317\n",
      "Epoch 395/1500\n",
      "36/36 [==============================] - 0s 733us/step - loss: 0.1658 - accuracy: 0.9382\n",
      "Epoch 396/1500\n",
      " 1/36 [..............................] - ETA: 0s - loss: 0.1035 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 366.\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1661 - accuracy: 0.9343\n",
      "Epoch 396: early stopping\n",
      "7/7 [==============================] - 0s 829us/step - loss: 0.8249 - accuracy: 0.7302\n",
      "7/7 [==============================] - 0s 634us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "After appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "Final Test Results - Loss: 0.8248865604400635, Accuracy: 0.7302325367927551, Precision: 0.6908133623819898, Recall: 0.7209801545807123, F1 Score: 0.7045716769400979\n",
      "Confusion Matrix:\n",
      " [[123   2  26]\n",
      " [  1   6   0]\n",
      " [ 29   0  28]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'022A'}\n",
      "Moved to Test Set:\n",
      "{'022A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '022A' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "762\n",
      "Length of y_train_val:\n",
      "762\n",
      "Length of groups_train_val:\n",
      "762\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     496\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     92\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 992, 1: 795, 2: 535})\n",
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 939us/step - loss: 1.2172 - accuracy: 0.4556\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 923us/step - loss: 0.9380 - accuracy: 0.5857\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.8506 - accuracy: 0.6236\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.8040 - accuracy: 0.6568\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.7555 - accuracy: 0.6800\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.7342 - accuracy: 0.6869\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.7223 - accuracy: 0.6891\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.6863 - accuracy: 0.7054\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.6854 - accuracy: 0.7115\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.6611 - accuracy: 0.7227\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.6451 - accuracy: 0.7283\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.6193 - accuracy: 0.7364\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 896us/step - loss: 0.6207 - accuracy: 0.7360\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 936us/step - loss: 0.6174 - accuracy: 0.7399\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.5787 - accuracy: 0.7593\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.5803 - accuracy: 0.7649\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.5671 - accuracy: 0.7674\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.5442 - accuracy: 0.7657\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 726us/step - loss: 0.5473 - accuracy: 0.7709\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.5362 - accuracy: 0.7752\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.5368 - accuracy: 0.7717\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.5164 - accuracy: 0.7873\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.5242 - accuracy: 0.7903\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 805us/step - loss: 0.5029 - accuracy: 0.7941\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.5016 - accuracy: 0.7954\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.4921 - accuracy: 0.7941\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.5017 - accuracy: 0.7907\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.4951 - accuracy: 0.7997\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.5014 - accuracy: 0.7937\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.4882 - accuracy: 0.7963\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 875us/step - loss: 0.4822 - accuracy: 0.8062\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.4749 - accuracy: 0.8066\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.4734 - accuracy: 0.8015\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.4613 - accuracy: 0.8049\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.4523 - accuracy: 0.8127\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 791us/step - loss: 0.4533 - accuracy: 0.8109\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.4365 - accuracy: 0.8148\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.4536 - accuracy: 0.8114\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.4288 - accuracy: 0.8252\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.4466 - accuracy: 0.8140\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.4346 - accuracy: 0.8217\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.4223 - accuracy: 0.8282\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 716us/step - loss: 0.4338 - accuracy: 0.8221\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 726us/step - loss: 0.4242 - accuracy: 0.8269\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.4346 - accuracy: 0.8114\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.4148 - accuracy: 0.8260\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 703us/step - loss: 0.4376 - accuracy: 0.8152\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.4158 - accuracy: 0.8264\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.4237 - accuracy: 0.8221\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 723us/step - loss: 0.4250 - accuracy: 0.8200\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.3953 - accuracy: 0.8329\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 701us/step - loss: 0.4214 - accuracy: 0.8264\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.3966 - accuracy: 0.8389\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.4114 - accuracy: 0.8256\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.4134 - accuracy: 0.8303\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.4018 - accuracy: 0.8432\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.3970 - accuracy: 0.8407\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.3959 - accuracy: 0.8432\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.3844 - accuracy: 0.8385\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.3883 - accuracy: 0.8376\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 737us/step - loss: 0.3930 - accuracy: 0.8303\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 724us/step - loss: 0.3859 - accuracy: 0.8381\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.3994 - accuracy: 0.8411\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.3880 - accuracy: 0.8299\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.3597 - accuracy: 0.8458\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.3699 - accuracy: 0.8540\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.3786 - accuracy: 0.8441\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.3737 - accuracy: 0.8458\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.3753 - accuracy: 0.8506\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.3633 - accuracy: 0.8523\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.3753 - accuracy: 0.8407\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.3803 - accuracy: 0.8424\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.3711 - accuracy: 0.8497\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3579 - accuracy: 0.8549\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.3592 - accuracy: 0.8553\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.3794 - accuracy: 0.8501\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.3656 - accuracy: 0.8471\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.3614 - accuracy: 0.8432\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.3618 - accuracy: 0.8454\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 730us/step - loss: 0.3654 - accuracy: 0.8428\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.3572 - accuracy: 0.8402\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.3425 - accuracy: 0.8669\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 719us/step - loss: 0.3535 - accuracy: 0.8544\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 706us/step - loss: 0.3492 - accuracy: 0.8618\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.3486 - accuracy: 0.8557\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.3389 - accuracy: 0.8553\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.3434 - accuracy: 0.8544\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.3407 - accuracy: 0.8579\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.3520 - accuracy: 0.8536\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.3538 - accuracy: 0.8553\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.3437 - accuracy: 0.8600\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.3545 - accuracy: 0.8549\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.3265 - accuracy: 0.8674\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.3453 - accuracy: 0.8566\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 872us/step - loss: 0.3316 - accuracy: 0.8691\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.3345 - accuracy: 0.8691\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.3371 - accuracy: 0.8605\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.3369 - accuracy: 0.8661\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.3176 - accuracy: 0.8747\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 721us/step - loss: 0.3354 - accuracy: 0.8592\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.3287 - accuracy: 0.8712\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.3249 - accuracy: 0.8686\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.3281 - accuracy: 0.8592\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3152 - accuracy: 0.8786\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.3149 - accuracy: 0.8747\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3240 - accuracy: 0.8712\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.3357 - accuracy: 0.8570\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.3246 - accuracy: 0.8635\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3220 - accuracy: 0.8712\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.3064 - accuracy: 0.8846\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.3231 - accuracy: 0.8643\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.3212 - accuracy: 0.8682\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.3044 - accuracy: 0.8725\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.3104 - accuracy: 0.8674\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.3083 - accuracy: 0.8682\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.3333 - accuracy: 0.8648\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.3121 - accuracy: 0.8742\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.3088 - accuracy: 0.8730\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3140 - accuracy: 0.8717\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.3207 - accuracy: 0.8708\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.3055 - accuracy: 0.8738\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.3190 - accuracy: 0.8721\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.3102 - accuracy: 0.8717\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.3107 - accuracy: 0.8730\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2984 - accuracy: 0.8786\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.3094 - accuracy: 0.8747\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2878 - accuracy: 0.8833\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2959 - accuracy: 0.8794\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.3051 - accuracy: 0.8768\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2972 - accuracy: 0.8768\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.3029 - accuracy: 0.8751\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.2851 - accuracy: 0.8923\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.3033 - accuracy: 0.8829\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.2984 - accuracy: 0.8811\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.2980 - accuracy: 0.8798\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 899us/step - loss: 0.3004 - accuracy: 0.8725\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2882 - accuracy: 0.8889\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.2959 - accuracy: 0.8768\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.2826 - accuracy: 0.8854\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.3070 - accuracy: 0.8786\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2878 - accuracy: 0.8833\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.2845 - accuracy: 0.8876\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 741us/step - loss: 0.2900 - accuracy: 0.8833\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.2897 - accuracy: 0.8803\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.2753 - accuracy: 0.8932\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.2956 - accuracy: 0.8798\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.2865 - accuracy: 0.8798\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.2877 - accuracy: 0.8854\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2879 - accuracy: 0.8824\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2835 - accuracy: 0.8859\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.2879 - accuracy: 0.8747\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2844 - accuracy: 0.8863\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.2591 - accuracy: 0.8928\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.2753 - accuracy: 0.8798\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.2834 - accuracy: 0.8902\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2764 - accuracy: 0.8910\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2696 - accuracy: 0.8906\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 723us/step - loss: 0.2858 - accuracy: 0.8820\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.2676 - accuracy: 0.8880\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.2682 - accuracy: 0.8910\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2754 - accuracy: 0.8923\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.2828 - accuracy: 0.8880\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 723us/step - loss: 0.2666 - accuracy: 0.8872\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.2735 - accuracy: 0.8833\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 725us/step - loss: 0.2700 - accuracy: 0.8867\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2826 - accuracy: 0.8807\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2766 - accuracy: 0.8863\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2668 - accuracy: 0.8893\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2615 - accuracy: 0.8971\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.2596 - accuracy: 0.8949\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.2574 - accuracy: 0.8945\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2667 - accuracy: 0.8915\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.2647 - accuracy: 0.8919\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2531 - accuracy: 0.9001\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 719us/step - loss: 0.2609 - accuracy: 0.8971\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 721us/step - loss: 0.2688 - accuracy: 0.8915\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.2676 - accuracy: 0.8962\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.2470 - accuracy: 0.9048\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.2599 - accuracy: 0.9018\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.2539 - accuracy: 0.8992\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 721us/step - loss: 0.2607 - accuracy: 0.8962\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.2837 - accuracy: 0.8837\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.2674 - accuracy: 0.8915\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2582 - accuracy: 0.8949\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.2418 - accuracy: 0.9048\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.2570 - accuracy: 0.8984\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.2416 - accuracy: 0.9078\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.2429 - accuracy: 0.9057\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2443 - accuracy: 0.9053\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 906us/step - loss: 0.2543 - accuracy: 0.8966\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 909us/step - loss: 0.2632 - accuracy: 0.8953\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.2435 - accuracy: 0.9027\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.9061\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.9065\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.2420 - accuracy: 0.9053\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.2484 - accuracy: 0.9035\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.2506 - accuracy: 0.9001\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.2384 - accuracy: 0.9070\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2410 - accuracy: 0.9014\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2598 - accuracy: 0.8928\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.2459 - accuracy: 0.9078\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.2411 - accuracy: 0.9104\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2342 - accuracy: 0.9070\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.2508 - accuracy: 0.9053\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.2390 - accuracy: 0.9031\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2451 - accuracy: 0.9053\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2435 - accuracy: 0.8966\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 728us/step - loss: 0.2432 - accuracy: 0.9096\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 719us/step - loss: 0.2349 - accuracy: 0.9074\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 730us/step - loss: 0.2418 - accuracy: 0.9061\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2382 - accuracy: 0.9031\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2388 - accuracy: 0.9100\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.2322 - accuracy: 0.9109\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2386 - accuracy: 0.9018\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.2438 - accuracy: 0.9065\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.2405 - accuracy: 0.9057\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.2331 - accuracy: 0.9100\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.2282 - accuracy: 0.9096\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.2415 - accuracy: 0.9035\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 706us/step - loss: 0.2368 - accuracy: 0.9109\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2321 - accuracy: 0.9070\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.2449 - accuracy: 0.9040\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.2253 - accuracy: 0.9083\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.2183 - accuracy: 0.9156\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.2247 - accuracy: 0.9126\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2349 - accuracy: 0.9044\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2303 - accuracy: 0.9061\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2318 - accuracy: 0.9027\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.2448 - accuracy: 0.9009\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 906us/step - loss: 0.2261 - accuracy: 0.9121\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 871us/step - loss: 0.2242 - accuracy: 0.9104\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2212 - accuracy: 0.9113\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.2269 - accuracy: 0.9121\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2334 - accuracy: 0.9018\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.2351 - accuracy: 0.9057\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.2243 - accuracy: 0.9130\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.2276 - accuracy: 0.9130\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2195 - accuracy: 0.9078\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.2328 - accuracy: 0.9057\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.2258 - accuracy: 0.9040\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.2167 - accuracy: 0.9186\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2044 - accuracy: 0.9242\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.2304 - accuracy: 0.9083\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.2243 - accuracy: 0.9096\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.2173 - accuracy: 0.9195\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.2230 - accuracy: 0.9134\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.2228 - accuracy: 0.9121\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.2277 - accuracy: 0.9091\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.2127 - accuracy: 0.9130\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.2073 - accuracy: 0.9212\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2359 - accuracy: 0.9061\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.2059 - accuracy: 0.9203\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 727us/step - loss: 0.2166 - accuracy: 0.9190\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.2251 - accuracy: 0.9121\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.2052 - accuracy: 0.9199\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2130 - accuracy: 0.9186\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2235 - accuracy: 0.9074\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.2170 - accuracy: 0.9104\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2297 - accuracy: 0.9121\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.1995 - accuracy: 0.9294\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.2215 - accuracy: 0.9117\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.1989 - accuracy: 0.9195\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.2085 - accuracy: 0.9186\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.1964 - accuracy: 0.9186\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 805us/step - loss: 0.2269 - accuracy: 0.9096\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.2115 - accuracy: 0.9121\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2173 - accuracy: 0.9182\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.2038 - accuracy: 0.9212\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.2138 - accuracy: 0.9169\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2064 - accuracy: 0.9182\n",
      "Epoch 271/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.2154 - accuracy: 0.9177\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 0s 716us/step - loss: 0.2031 - accuracy: 0.9165\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.2111 - accuracy: 0.9199\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.1907 - accuracy: 0.9311\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1947 - accuracy: 0.9233\n",
      "Epoch 276/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.1869 - accuracy: 0.9302\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.2041 - accuracy: 0.9195\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.2040 - accuracy: 0.9208\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2159 - accuracy: 0.9165\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2041 - accuracy: 0.9195\n",
      "Epoch 281/1500\n",
      "37/37 [==============================] - 0s 737us/step - loss: 0.2020 - accuracy: 0.9229\n",
      "Epoch 282/1500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9173\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9251\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.2096 - accuracy: 0.9152\n",
      "Epoch 285/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.1807 - accuracy: 0.9272\n",
      "Epoch 286/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.2016 - accuracy: 0.9182\n",
      "Epoch 287/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.1873 - accuracy: 0.9251\n",
      "Epoch 288/1500\n",
      "37/37 [==============================] - 0s 730us/step - loss: 0.1893 - accuracy: 0.9268\n",
      "Epoch 289/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.2094 - accuracy: 0.9160\n",
      "Epoch 290/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.2109 - accuracy: 0.9186\n",
      "Epoch 291/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.2076 - accuracy: 0.9130\n",
      "Epoch 292/1500\n",
      "37/37 [==============================] - 0s 741us/step - loss: 0.2035 - accuracy: 0.9160\n",
      "Epoch 293/1500\n",
      "37/37 [==============================] - 0s 672us/step - loss: 0.1896 - accuracy: 0.9276\n",
      "Epoch 294/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2118 - accuracy: 0.9134\n",
      "Epoch 295/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1915 - accuracy: 0.9294\n",
      "Epoch 296/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.1883 - accuracy: 0.9255\n",
      "Epoch 297/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.1969 - accuracy: 0.9186\n",
      "Epoch 298/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.1886 - accuracy: 0.9311\n",
      "Epoch 299/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.2058 - accuracy: 0.9186\n",
      "Epoch 300/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.1944 - accuracy: 0.9203\n",
      "Epoch 301/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.1956 - accuracy: 0.9268\n",
      "Epoch 302/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.1853 - accuracy: 0.9307\n",
      "Epoch 303/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.2103 - accuracy: 0.9229\n",
      "Epoch 304/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.1917 - accuracy: 0.9195\n",
      "Epoch 305/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.1954 - accuracy: 0.9259\n",
      "Epoch 306/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.1964 - accuracy: 0.9190\n",
      "Epoch 307/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.1953 - accuracy: 0.9242\n",
      "Epoch 308/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.2109 - accuracy: 0.9251\n",
      "Epoch 309/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1880 - accuracy: 0.9285\n",
      "Epoch 310/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.1961 - accuracy: 0.9233\n",
      "Epoch 311/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.1898 - accuracy: 0.9268\n",
      "Epoch 312/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.1935 - accuracy: 0.9216\n",
      "Epoch 313/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.2049 - accuracy: 0.9186\n",
      "Epoch 314/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.1892 - accuracy: 0.9238\n",
      "Epoch 315/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1198 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 285.\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.1954 - accuracy: 0.9177\n",
      "Epoch 315: early stopping\n",
      "6/6 [==============================] - 0s 841us/step - loss: 1.0719 - accuracy: 0.6686\n",
      "6/6 [==============================] - 0s 642us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "After appending - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n",
      "Final Test Results - Loss: 1.0719001293182373, Accuracy: 0.668571412563324, Precision: 0.7424649240569638, Recall: 0.7131897666190379, F1 Score: 0.6794987566163293\n",
      "Confusion Matrix:\n",
      " [[84  3  5]\n",
      " [ 1 11  0]\n",
      " [49  0 22]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.664695157170185\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8560131788253784\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7068819254636765\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6823236291521929\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7092520936814742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (82/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[kitten, adult, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "77    071A  [senior, senior, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "76    070A              [adult, senior, adult, senior, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [adult, senior, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [kitten, adult, kitten, kitten, adult, adult, ...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, senior, senior, senior...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C              [senior, senior, adult, adult, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, senior, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "4     003A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [kitten, adult, adult, kitten, adult, adult, k...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, senior, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "70    064A                            [kitten, adult, kitten]        kitten            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, senior,...        senior            adult                  False\n",
       "69    063A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "102   108A         [adult, adult, adult, adult, adult, adult]         adult           senior                  False\n",
       "101   106A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, adult, adu...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "12    011A                                    [senior, adult]         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [senior, adult, senior, adult, senior, adult, ...         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "18    016A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "109   117A  [adult, adult, adult, adult, adult, adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    11\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          558            443  79.390681\n",
      "1           kitten          118             75  63.559322\n",
      "2           senior          178             86  48.314607\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    222      162     72.97\n",
      "1          M    337      254     75.37\n",
      "2          X    295      188     63.73\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 846, 2: 765, 1: 715})\n",
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 987us/step - loss: 1.0592 - accuracy: 0.5232\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.8470 - accuracy: 0.6376\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.7910 - accuracy: 0.6660\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.7356 - accuracy: 0.6969\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.7017 - accuracy: 0.7059\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.6909 - accuracy: 0.7128\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.6357 - accuracy: 0.7347\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.6500 - accuracy: 0.7313\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 727us/step - loss: 0.6366 - accuracy: 0.7399\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.6106 - accuracy: 0.7498\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.6023 - accuracy: 0.7468\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.5833 - accuracy: 0.7661\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.5649 - accuracy: 0.7721\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.5703 - accuracy: 0.7678\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.5593 - accuracy: 0.7614\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.5527 - accuracy: 0.7752\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.5415 - accuracy: 0.7842\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.5422 - accuracy: 0.7777\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.5248 - accuracy: 0.7872\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.5279 - accuracy: 0.7833\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.5177 - accuracy: 0.7868\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.5077 - accuracy: 0.7911\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.4970 - accuracy: 0.7958\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.5080 - accuracy: 0.7936\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.5004 - accuracy: 0.7911\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 729us/step - loss: 0.4832 - accuracy: 0.8005\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.4838 - accuracy: 0.7975\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.4683 - accuracy: 0.8061\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 719us/step - loss: 0.4768 - accuracy: 0.8078\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.4591 - accuracy: 0.8126\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.4566 - accuracy: 0.8199\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.4590 - accuracy: 0.8147\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.4458 - accuracy: 0.8147\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.4486 - accuracy: 0.8134\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.4427 - accuracy: 0.8199\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 722us/step - loss: 0.4303 - accuracy: 0.8315\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.4284 - accuracy: 0.8190\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.4206 - accuracy: 0.8319\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.4351 - accuracy: 0.8224\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 707us/step - loss: 0.4303 - accuracy: 0.8246\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 710us/step - loss: 0.4306 - accuracy: 0.8207\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 724us/step - loss: 0.4322 - accuracy: 0.8242\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.4121 - accuracy: 0.8323\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.4042 - accuracy: 0.8358\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 728us/step - loss: 0.4087 - accuracy: 0.8332\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 726us/step - loss: 0.4061 - accuracy: 0.8306\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 722us/step - loss: 0.4065 - accuracy: 0.8358\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 695us/step - loss: 0.4051 - accuracy: 0.8366\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.3944 - accuracy: 0.8396\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.3992 - accuracy: 0.8465\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.3924 - accuracy: 0.8371\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.3937 - accuracy: 0.8383\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.3942 - accuracy: 0.8439\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.3837 - accuracy: 0.8431\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 690us/step - loss: 0.3934 - accuracy: 0.8465\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.3812 - accuracy: 0.8461\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.3798 - accuracy: 0.8521\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 737us/step - loss: 0.3750 - accuracy: 0.8538\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 727us/step - loss: 0.3688 - accuracy: 0.8603\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 726us/step - loss: 0.3784 - accuracy: 0.8444\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 712us/step - loss: 0.3842 - accuracy: 0.8409\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.3765 - accuracy: 0.8594\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.3625 - accuracy: 0.8508\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.3676 - accuracy: 0.8538\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.3618 - accuracy: 0.8482\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.3583 - accuracy: 0.8637\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.3673 - accuracy: 0.8547\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.3719 - accuracy: 0.8517\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.3516 - accuracy: 0.8598\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.3543 - accuracy: 0.8624\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.3462 - accuracy: 0.8629\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.3673 - accuracy: 0.8555\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.3391 - accuracy: 0.8598\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 714us/step - loss: 0.3507 - accuracy: 0.8594\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.3443 - accuracy: 0.8689\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 725us/step - loss: 0.3410 - accuracy: 0.8659\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 724us/step - loss: 0.3396 - accuracy: 0.8573\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.3401 - accuracy: 0.8586\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.8689\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8676\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.3257 - accuracy: 0.8702\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.3336 - accuracy: 0.8663\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.3399 - accuracy: 0.8603\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.3343 - accuracy: 0.8719\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 705us/step - loss: 0.3424 - accuracy: 0.8633\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.3440 - accuracy: 0.8676\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.3403 - accuracy: 0.8573\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.3341 - accuracy: 0.8715\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 725us/step - loss: 0.3258 - accuracy: 0.8680\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 718us/step - loss: 0.3310 - accuracy: 0.8706\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.3251 - accuracy: 0.8706\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 709us/step - loss: 0.3399 - accuracy: 0.8676\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 730us/step - loss: 0.3307 - accuracy: 0.8684\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.3099 - accuracy: 0.8783\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.3071 - accuracy: 0.8788\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.3042 - accuracy: 0.8844\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.3160 - accuracy: 0.8813\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.3071 - accuracy: 0.8813\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.3293 - accuracy: 0.8796\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 726us/step - loss: 0.3069 - accuracy: 0.8796\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 725us/step - loss: 0.3089 - accuracy: 0.8818\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 724us/step - loss: 0.3128 - accuracy: 0.8723\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.2992 - accuracy: 0.8848\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 727us/step - loss: 0.3017 - accuracy: 0.8831\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.3129 - accuracy: 0.8740\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.3079 - accuracy: 0.8762\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 723us/step - loss: 0.2980 - accuracy: 0.8882\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 730us/step - loss: 0.2944 - accuracy: 0.8766\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.3068 - accuracy: 0.8805\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.3046 - accuracy: 0.8740\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.3024 - accuracy: 0.8852\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 708us/step - loss: 0.3112 - accuracy: 0.8805\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.2960 - accuracy: 0.8813\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.2891 - accuracy: 0.8904\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.2920 - accuracy: 0.8813\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2772 - accuracy: 0.8899\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 730us/step - loss: 0.2889 - accuracy: 0.8908\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.3014 - accuracy: 0.8852\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.2995 - accuracy: 0.8796\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.2797 - accuracy: 0.8917\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 728us/step - loss: 0.2852 - accuracy: 0.8912\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 741us/step - loss: 0.3053 - accuracy: 0.8775\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2906 - accuracy: 0.8783\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 722us/step - loss: 0.2908 - accuracy: 0.8831\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2746 - accuracy: 0.8955\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 707us/step - loss: 0.2862 - accuracy: 0.8908\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.3125 - accuracy: 0.8775\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 707us/step - loss: 0.2750 - accuracy: 0.8947\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.2970 - accuracy: 0.8831\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.2833 - accuracy: 0.8917\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.2899 - accuracy: 0.8899\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.2898 - accuracy: 0.8861\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2845 - accuracy: 0.8899\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 891us/step - loss: 0.2850 - accuracy: 0.8921\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.2947 - accuracy: 0.8891\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2723 - accuracy: 0.8960\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.2727 - accuracy: 0.8869\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.2792 - accuracy: 0.8891\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2788 - accuracy: 0.8878\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.2809 - accuracy: 0.8891\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2733 - accuracy: 0.8955\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.2679 - accuracy: 0.8925\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.2781 - accuracy: 0.8856\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.2624 - accuracy: 0.9037\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2733 - accuracy: 0.8921\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.2619 - accuracy: 0.8951\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2686 - accuracy: 0.8887\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2620 - accuracy: 0.8998\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.2553 - accuracy: 0.9003\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.2551 - accuracy: 0.9015\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.2667 - accuracy: 0.8960\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.2753 - accuracy: 0.8929\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2542 - accuracy: 0.9015\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2776 - accuracy: 0.8938\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 728us/step - loss: 0.2738 - accuracy: 0.8951\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.2683 - accuracy: 0.8904\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2459 - accuracy: 0.9058\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.2611 - accuracy: 0.9007\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2431 - accuracy: 0.9119\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.2725 - accuracy: 0.8938\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2472 - accuracy: 0.9003\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.2548 - accuracy: 0.8985\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.2613 - accuracy: 0.9007\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2534 - accuracy: 0.9015\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2441 - accuracy: 0.8998\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2560 - accuracy: 0.8985\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2571 - accuracy: 0.9007\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.2541 - accuracy: 0.8968\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.2522 - accuracy: 0.9058\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.2478 - accuracy: 0.9003\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 737us/step - loss: 0.2490 - accuracy: 0.9037\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.2483 - accuracy: 0.9024\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2408 - accuracy: 0.9046\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 729us/step - loss: 0.2533 - accuracy: 0.8990\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.2451 - accuracy: 0.9024\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.2518 - accuracy: 0.9015\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2313 - accuracy: 0.9101\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2522 - accuracy: 0.9033\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.2265 - accuracy: 0.9123\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.2498 - accuracy: 0.9080\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 716us/step - loss: 0.2397 - accuracy: 0.9015\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2489 - accuracy: 0.9063\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 715us/step - loss: 0.2466 - accuracy: 0.9011\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 718us/step - loss: 0.2444 - accuracy: 0.9024\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2377 - accuracy: 0.9119\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2344 - accuracy: 0.9046\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2368 - accuracy: 0.9136\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.2320 - accuracy: 0.9132\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.2339 - accuracy: 0.9132\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.2541 - accuracy: 0.9020\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2481 - accuracy: 0.9041\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2261 - accuracy: 0.9136\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.2361 - accuracy: 0.9093\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.2315 - accuracy: 0.9136\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2368 - accuracy: 0.9080\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.2370 - accuracy: 0.9127\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 728us/step - loss: 0.2315 - accuracy: 0.9140\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 712us/step - loss: 0.2392 - accuracy: 0.9080\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 730us/step - loss: 0.2378 - accuracy: 0.9024\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.2265 - accuracy: 0.9101\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.2285 - accuracy: 0.9136\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2186 - accuracy: 0.9166\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.2281 - accuracy: 0.9123\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.2328 - accuracy: 0.9149\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.2193 - accuracy: 0.9200\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.2309 - accuracy: 0.9114\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.2447 - accuracy: 0.9089\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.2286 - accuracy: 0.9132\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 732us/step - loss: 0.2304 - accuracy: 0.9140\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.2353 - accuracy: 0.9119\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.2246 - accuracy: 0.9106\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.2346 - accuracy: 0.9127\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 727us/step - loss: 0.2237 - accuracy: 0.9080\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.2349 - accuracy: 0.9106\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.2219 - accuracy: 0.9123\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 712us/step - loss: 0.2224 - accuracy: 0.9123\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.2288 - accuracy: 0.9166\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.2152 - accuracy: 0.9170\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2224 - accuracy: 0.9132\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.2111 - accuracy: 0.9179\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.2113 - accuracy: 0.9213\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.2015 - accuracy: 0.9256\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2159 - accuracy: 0.9144\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2251 - accuracy: 0.9067\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 713us/step - loss: 0.2301 - accuracy: 0.9110\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2056 - accuracy: 0.9153\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 911us/step - loss: 0.2138 - accuracy: 0.9170\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.2222 - accuracy: 0.9179\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9230\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 942us/step - loss: 0.2120 - accuracy: 0.9213\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.2254 - accuracy: 0.9157\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 902us/step - loss: 0.2234 - accuracy: 0.9170\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 949us/step - loss: 0.2186 - accuracy: 0.9106\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2271 - accuracy: 0.9114\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1985 - accuracy: 0.9213\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.2150 - accuracy: 0.9187\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.1907 - accuracy: 0.9248\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.2226 - accuracy: 0.9157\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2004 - accuracy: 0.9256\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.2026 - accuracy: 0.9222\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2098 - accuracy: 0.9157\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.2102 - accuracy: 0.9187\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2024 - accuracy: 0.9252\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2217 - accuracy: 0.9114\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.2085 - accuracy: 0.9196\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.2089 - accuracy: 0.9209\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.2048 - accuracy: 0.9282\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.2062 - accuracy: 0.9209\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 724us/step - loss: 0.2081 - accuracy: 0.9226\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 708us/step - loss: 0.2013 - accuracy: 0.9175\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.2039 - accuracy: 0.9213\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2040 - accuracy: 0.9175\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.2029 - accuracy: 0.9218\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 715us/step - loss: 0.2015 - accuracy: 0.9286\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.2067 - accuracy: 0.9248\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 714us/step - loss: 0.1995 - accuracy: 0.9213\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 716us/step - loss: 0.1976 - accuracy: 0.9239\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 712us/step - loss: 0.2099 - accuracy: 0.9213\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 717us/step - loss: 0.1918 - accuracy: 0.9230\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2010 - accuracy: 0.9187\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 882us/step - loss: 0.1952 - accuracy: 0.9295\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.1905 - accuracy: 0.9256\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1938 - accuracy: 0.9291\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 733us/step - loss: 0.1935 - accuracy: 0.9273\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.1980 - accuracy: 0.9295\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 706us/step - loss: 0.1987 - accuracy: 0.9243\n",
      "Epoch 267/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.2668 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 237.\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.1908 - accuracy: 0.9312\n",
      "Epoch 267: early stopping\n",
      "7/7 [==============================] - 0s 704us/step - loss: 1.0688 - accuracy: 0.6193\n",
      "7/7 [==============================] - 0s 571us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.56 (14/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 1.0687636137008667, Accuracy: 0.6192660331726074, Precision: 0.600576459309336, Recall: 0.6669119769119769, F1 Score: 0.5951130881422818\n",
      "Confusion Matrix:\n",
      " [[97  8 60]\n",
      " [ 3 25  0]\n",
      " [12  0 13]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'020A'}\n",
      "Moved to Test Set:\n",
      "{'020A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '018A' '019A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '011A' '014B' '015A' '016A' '019B' '020A' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "711\n",
      "Length of y_train_val:\n",
      "711\n",
      "Length of groups_train_val:\n",
      "711\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     446\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     142\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 892, 1: 680, 2: 645})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "35/35 [==============================] - 0s 907us/step - loss: 1.1205 - accuracy: 0.5029\n",
      "Epoch 2/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.8971 - accuracy: 0.6058\n",
      "Epoch 3/1500\n",
      "35/35 [==============================] - 0s 787us/step - loss: 0.8471 - accuracy: 0.6333\n",
      "Epoch 4/1500\n",
      "35/35 [==============================] - 0s 723us/step - loss: 0.7840 - accuracy: 0.6585\n",
      "Epoch 5/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.7712 - accuracy: 0.6707\n",
      "Epoch 6/1500\n",
      "35/35 [==============================] - 0s 717us/step - loss: 0.7227 - accuracy: 0.6996\n",
      "Epoch 7/1500\n",
      "35/35 [==============================] - 0s 705us/step - loss: 0.7143 - accuracy: 0.6973\n",
      "Epoch 8/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.6934 - accuracy: 0.7064\n",
      "Epoch 9/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.6761 - accuracy: 0.7118\n",
      "Epoch 10/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.6679 - accuracy: 0.7230\n",
      "Epoch 11/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.6543 - accuracy: 0.7127\n",
      "Epoch 12/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.6207 - accuracy: 0.7339\n",
      "Epoch 13/1500\n",
      "35/35 [==============================] - 0s 714us/step - loss: 0.6069 - accuracy: 0.7415\n",
      "Epoch 14/1500\n",
      "35/35 [==============================] - 0s 708us/step - loss: 0.5942 - accuracy: 0.7515\n",
      "Epoch 15/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.5738 - accuracy: 0.7704\n",
      "Epoch 16/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.5971 - accuracy: 0.7429\n",
      "Epoch 17/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.5670 - accuracy: 0.7564\n",
      "Epoch 18/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.5441 - accuracy: 0.7736\n",
      "Epoch 19/1500\n",
      "35/35 [==============================] - 0s 721us/step - loss: 0.5492 - accuracy: 0.7709\n",
      "Epoch 20/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.5578 - accuracy: 0.7722\n",
      "Epoch 21/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.5101 - accuracy: 0.7903\n",
      "Epoch 22/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.5218 - accuracy: 0.7772\n",
      "Epoch 23/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.5179 - accuracy: 0.7848\n",
      "Epoch 24/1500\n",
      "35/35 [==============================] - 0s 719us/step - loss: 0.5173 - accuracy: 0.7767\n",
      "Epoch 25/1500\n",
      "35/35 [==============================] - 0s 709us/step - loss: 0.5197 - accuracy: 0.7885\n",
      "Epoch 26/1500\n",
      "35/35 [==============================] - 0s 712us/step - loss: 0.5000 - accuracy: 0.7871\n",
      "Epoch 27/1500\n",
      "35/35 [==============================] - 0s 697us/step - loss: 0.5035 - accuracy: 0.7907\n",
      "Epoch 28/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.4822 - accuracy: 0.7961\n",
      "Epoch 29/1500\n",
      "35/35 [==============================] - 0s 706us/step - loss: 0.4994 - accuracy: 0.7880\n",
      "Epoch 30/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.4678 - accuracy: 0.8006\n",
      "Epoch 31/1500\n",
      "35/35 [==============================] - 0s 711us/step - loss: 0.4732 - accuracy: 0.8006\n",
      "Epoch 32/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.4635 - accuracy: 0.8006\n",
      "Epoch 33/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.4687 - accuracy: 0.8029\n",
      "Epoch 34/1500\n",
      "35/35 [==============================] - 0s 725us/step - loss: 0.4869 - accuracy: 0.8033\n",
      "Epoch 35/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.4586 - accuracy: 0.8042\n",
      "Epoch 36/1500\n",
      "35/35 [==============================] - 0s 728us/step - loss: 0.4574 - accuracy: 0.8124\n",
      "Epoch 37/1500\n",
      "35/35 [==============================] - 0s 719us/step - loss: 0.4593 - accuracy: 0.8133\n",
      "Epoch 38/1500\n",
      "35/35 [==============================] - 0s 725us/step - loss: 0.4351 - accuracy: 0.8223\n",
      "Epoch 39/1500\n",
      "35/35 [==============================] - 0s 721us/step - loss: 0.4495 - accuracy: 0.8187\n",
      "Epoch 40/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.4385 - accuracy: 0.8124\n",
      "Epoch 41/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.4152 - accuracy: 0.8327\n",
      "Epoch 42/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.4475 - accuracy: 0.8142\n",
      "Epoch 43/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.4298 - accuracy: 0.8187\n",
      "Epoch 44/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.4437 - accuracy: 0.8119\n",
      "Epoch 45/1500\n",
      "35/35 [==============================] - 0s 741us/step - loss: 0.4136 - accuracy: 0.8281\n",
      "Epoch 46/1500\n",
      "35/35 [==============================] - 0s 706us/step - loss: 0.4127 - accuracy: 0.8354\n",
      "Epoch 47/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.4296 - accuracy: 0.8205\n",
      "Epoch 48/1500\n",
      "35/35 [==============================] - 0s 728us/step - loss: 0.4099 - accuracy: 0.8354\n",
      "Epoch 49/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.3971 - accuracy: 0.8372\n",
      "Epoch 50/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.3934 - accuracy: 0.8363\n",
      "Epoch 51/1500\n",
      "35/35 [==============================] - 0s 709us/step - loss: 0.4229 - accuracy: 0.8182\n",
      "Epoch 52/1500\n",
      "35/35 [==============================] - 0s 705us/step - loss: 0.4048 - accuracy: 0.8295\n",
      "Epoch 53/1500\n",
      "35/35 [==============================] - 0s 708us/step - loss: 0.4050 - accuracy: 0.8376\n",
      "Epoch 54/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.3959 - accuracy: 0.8327\n",
      "Epoch 55/1500\n",
      "35/35 [==============================] - 0s 702us/step - loss: 0.3951 - accuracy: 0.8295\n",
      "Epoch 56/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.3788 - accuracy: 0.8421\n",
      "Epoch 57/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.3882 - accuracy: 0.8327\n",
      "Epoch 58/1500\n",
      "35/35 [==============================] - 0s 709us/step - loss: 0.3789 - accuracy: 0.8435\n",
      "Epoch 59/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.3959 - accuracy: 0.8309\n",
      "Epoch 60/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.3745 - accuracy: 0.8466\n",
      "Epoch 61/1500\n",
      "35/35 [==============================] - 0s 721us/step - loss: 0.3859 - accuracy: 0.8439\n",
      "Epoch 62/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.3620 - accuracy: 0.8543\n",
      "Epoch 63/1500\n",
      "35/35 [==============================] - 0s 714us/step - loss: 0.3740 - accuracy: 0.8430\n",
      "Epoch 64/1500\n",
      "35/35 [==============================] - 0s 710us/step - loss: 0.3627 - accuracy: 0.8462\n",
      "Epoch 65/1500\n",
      "35/35 [==============================] - 0s 717us/step - loss: 0.3648 - accuracy: 0.8453\n",
      "Epoch 66/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.3588 - accuracy: 0.8575\n",
      "Epoch 67/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.3598 - accuracy: 0.8548\n",
      "Epoch 68/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.3569 - accuracy: 0.8426\n",
      "Epoch 69/1500\n",
      "35/35 [==============================] - 0s 719us/step - loss: 0.3610 - accuracy: 0.8525\n",
      "Epoch 70/1500\n",
      "35/35 [==============================] - 0s 726us/step - loss: 0.3436 - accuracy: 0.8588\n",
      "Epoch 71/1500\n",
      "35/35 [==============================] - 0s 676us/step - loss: 0.3448 - accuracy: 0.8557\n",
      "Epoch 72/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.3542 - accuracy: 0.8439\n",
      "Epoch 73/1500\n",
      "35/35 [==============================] - 0s 723us/step - loss: 0.3485 - accuracy: 0.8593\n",
      "Epoch 74/1500\n",
      "35/35 [==============================] - 0s 714us/step - loss: 0.3486 - accuracy: 0.8579\n",
      "Epoch 75/1500\n",
      "35/35 [==============================] - 0s 712us/step - loss: 0.3486 - accuracy: 0.8588\n",
      "Epoch 76/1500\n",
      "35/35 [==============================] - 0s 728us/step - loss: 0.3456 - accuracy: 0.8647\n",
      "Epoch 77/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.3521 - accuracy: 0.8629\n",
      "Epoch 78/1500\n",
      "35/35 [==============================] - 0s 712us/step - loss: 0.3497 - accuracy: 0.8647\n",
      "Epoch 79/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.3341 - accuracy: 0.8593\n",
      "Epoch 80/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.3409 - accuracy: 0.8597\n",
      "Epoch 81/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.3247 - accuracy: 0.8719\n",
      "Epoch 82/1500\n",
      "35/35 [==============================] - 0s 696us/step - loss: 0.3480 - accuracy: 0.8602\n",
      "Epoch 83/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.3257 - accuracy: 0.8683\n",
      "Epoch 84/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.3451 - accuracy: 0.8552\n",
      "Epoch 85/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.3242 - accuracy: 0.8737\n",
      "Epoch 86/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.3200 - accuracy: 0.8724\n",
      "Epoch 87/1500\n",
      "35/35 [==============================] - 0s 695us/step - loss: 0.3206 - accuracy: 0.8678\n",
      "Epoch 88/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.3400 - accuracy: 0.8647\n",
      "Epoch 89/1500\n",
      "35/35 [==============================] - 0s 717us/step - loss: 0.3210 - accuracy: 0.8696\n",
      "Epoch 90/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.3254 - accuracy: 0.8651\n",
      "Epoch 91/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.3179 - accuracy: 0.8724\n",
      "Epoch 92/1500\n",
      "35/35 [==============================] - 0s 713us/step - loss: 0.3224 - accuracy: 0.8660\n",
      "Epoch 93/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.3154 - accuracy: 0.8692\n",
      "Epoch 94/1500\n",
      "35/35 [==============================] - 0s 717us/step - loss: 0.3160 - accuracy: 0.8787\n",
      "Epoch 95/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.3331 - accuracy: 0.8584\n",
      "Epoch 96/1500\n",
      "35/35 [==============================] - 0s 752us/step - loss: 0.3127 - accuracy: 0.8751\n",
      "Epoch 97/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.3030 - accuracy: 0.8755\n",
      "Epoch 98/1500\n",
      "35/35 [==============================] - 0s 709us/step - loss: 0.3239 - accuracy: 0.8656\n",
      "Epoch 99/1500\n",
      "35/35 [==============================] - 0s 719us/step - loss: 0.3204 - accuracy: 0.8719\n",
      "Epoch 100/1500\n",
      "35/35 [==============================] - 0s 799us/step - loss: 0.3034 - accuracy: 0.8805\n",
      "Epoch 101/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.3139 - accuracy: 0.8656\n",
      "Epoch 102/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.3233 - accuracy: 0.8687\n",
      "Epoch 103/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.3151 - accuracy: 0.8782\n",
      "Epoch 104/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.3021 - accuracy: 0.8805\n",
      "Epoch 105/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.2982 - accuracy: 0.8836\n",
      "Epoch 106/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.3124 - accuracy: 0.8696\n",
      "Epoch 107/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2944 - accuracy: 0.8868\n",
      "Epoch 108/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.3190 - accuracy: 0.8678\n",
      "Epoch 109/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.3077 - accuracy: 0.8778\n",
      "Epoch 110/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2971 - accuracy: 0.8787\n",
      "Epoch 111/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.2967 - accuracy: 0.8823\n",
      "Epoch 112/1500\n",
      "35/35 [==============================] - 0s 727us/step - loss: 0.3014 - accuracy: 0.8746\n",
      "Epoch 113/1500\n",
      "35/35 [==============================] - 0s 714us/step - loss: 0.2944 - accuracy: 0.8841\n",
      "Epoch 114/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.2939 - accuracy: 0.8814\n",
      "Epoch 115/1500\n",
      "35/35 [==============================] - 0s 714us/step - loss: 0.2796 - accuracy: 0.8850\n",
      "Epoch 116/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.2941 - accuracy: 0.8769\n",
      "Epoch 117/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.2946 - accuracy: 0.8841\n",
      "Epoch 118/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.2966 - accuracy: 0.8773\n",
      "Epoch 119/1500\n",
      "35/35 [==============================] - 0s 711us/step - loss: 0.2887 - accuracy: 0.8800\n",
      "Epoch 120/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.2829 - accuracy: 0.8832\n",
      "Epoch 121/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.2784 - accuracy: 0.8926\n",
      "Epoch 122/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.2733 - accuracy: 0.8949\n",
      "Epoch 123/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.2682 - accuracy: 0.8958\n",
      "Epoch 124/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.2791 - accuracy: 0.8841\n",
      "Epoch 125/1500\n",
      "35/35 [==============================] - 0s 716us/step - loss: 0.2755 - accuracy: 0.8931\n",
      "Epoch 126/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.2857 - accuracy: 0.8814\n",
      "Epoch 127/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2655 - accuracy: 0.8967\n",
      "Epoch 128/1500\n",
      "35/35 [==============================] - 0s 718us/step - loss: 0.2815 - accuracy: 0.8872\n",
      "Epoch 129/1500\n",
      "35/35 [==============================] - 0s 725us/step - loss: 0.2821 - accuracy: 0.8881\n",
      "Epoch 130/1500\n",
      "35/35 [==============================] - 0s 703us/step - loss: 0.2747 - accuracy: 0.8945\n",
      "Epoch 131/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.2800 - accuracy: 0.8859\n",
      "Epoch 132/1500\n",
      "35/35 [==============================] - 0s 711us/step - loss: 0.2691 - accuracy: 0.8958\n",
      "Epoch 133/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.2790 - accuracy: 0.8850\n",
      "Epoch 134/1500\n",
      "35/35 [==============================] - 0s 670us/step - loss: 0.2709 - accuracy: 0.8922\n",
      "Epoch 135/1500\n",
      "35/35 [==============================] - 0s 711us/step - loss: 0.2721 - accuracy: 0.8895\n",
      "Epoch 136/1500\n",
      "35/35 [==============================] - 0s 712us/step - loss: 0.2730 - accuracy: 0.8854\n",
      "Epoch 137/1500\n",
      "35/35 [==============================] - 0s 718us/step - loss: 0.2743 - accuracy: 0.8994\n",
      "Epoch 138/1500\n",
      "35/35 [==============================] - 0s 716us/step - loss: 0.2801 - accuracy: 0.8904\n",
      "Epoch 139/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.2553 - accuracy: 0.9035\n",
      "Epoch 140/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.2650 - accuracy: 0.8940\n",
      "Epoch 141/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.2671 - accuracy: 0.8913\n",
      "Epoch 142/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.2625 - accuracy: 0.8949\n",
      "Epoch 143/1500\n",
      "35/35 [==============================] - 0s 808us/step - loss: 0.2630 - accuracy: 0.8990\n",
      "Epoch 144/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.2602 - accuracy: 0.8868\n",
      "Epoch 145/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.2724 - accuracy: 0.8886\n",
      "Epoch 146/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2519 - accuracy: 0.9012\n",
      "Epoch 147/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.2596 - accuracy: 0.8881\n",
      "Epoch 148/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.2728 - accuracy: 0.8908\n",
      "Epoch 149/1500\n",
      "35/35 [==============================] - 0s 804us/step - loss: 0.2481 - accuracy: 0.8994\n",
      "Epoch 150/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.2496 - accuracy: 0.9048\n",
      "Epoch 151/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.2475 - accuracy: 0.9035\n",
      "Epoch 152/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.2382 - accuracy: 0.9039\n",
      "Epoch 153/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.2516 - accuracy: 0.9030\n",
      "Epoch 154/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.2531 - accuracy: 0.8963\n",
      "Epoch 155/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.2605 - accuracy: 0.8899\n",
      "Epoch 156/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2594 - accuracy: 0.9003\n",
      "Epoch 157/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.2592 - accuracy: 0.9003\n",
      "Epoch 158/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.2555 - accuracy: 0.9021\n",
      "Epoch 159/1500\n",
      "35/35 [==============================] - 0s 808us/step - loss: 0.2506 - accuracy: 0.9062\n",
      "Epoch 160/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.2458 - accuracy: 0.9057\n",
      "Epoch 161/1500\n",
      "35/35 [==============================] - 0s 852us/step - loss: 0.2568 - accuracy: 0.8976\n",
      "Epoch 162/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.2705 - accuracy: 0.8931\n",
      "Epoch 163/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.2497 - accuracy: 0.9080\n",
      "Epoch 164/1500\n",
      "35/35 [==============================] - 0s 728us/step - loss: 0.2414 - accuracy: 0.8990\n",
      "Epoch 165/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.2539 - accuracy: 0.8913\n",
      "Epoch 166/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.2256 - accuracy: 0.9134\n",
      "Epoch 167/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.2531 - accuracy: 0.9008\n",
      "Epoch 168/1500\n",
      "35/35 [==============================] - 0s 703us/step - loss: 0.2499 - accuracy: 0.9008\n",
      "Epoch 169/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.2414 - accuracy: 0.9012\n",
      "Epoch 170/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.2400 - accuracy: 0.8976\n",
      "Epoch 171/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.2359 - accuracy: 0.9030\n",
      "Epoch 172/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.2390 - accuracy: 0.9098\n",
      "Epoch 173/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.2466 - accuracy: 0.8981\n",
      "Epoch 174/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.2506 - accuracy: 0.8985\n",
      "Epoch 175/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.2482 - accuracy: 0.9030\n",
      "Epoch 176/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2327 - accuracy: 0.9116\n",
      "Epoch 177/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2384 - accuracy: 0.9017\n",
      "Epoch 178/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.2287 - accuracy: 0.9111\n",
      "Epoch 179/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.2396 - accuracy: 0.9102\n",
      "Epoch 180/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.2332 - accuracy: 0.9107\n",
      "Epoch 181/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.2310 - accuracy: 0.9071\n",
      "Epoch 182/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.2147 - accuracy: 0.9129\n",
      "Epoch 183/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2157 - accuracy: 0.9157\n",
      "Epoch 184/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.2292 - accuracy: 0.9098\n",
      "Epoch 185/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.2240 - accuracy: 0.9107\n",
      "Epoch 186/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.2235 - accuracy: 0.9098\n",
      "Epoch 187/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2091 - accuracy: 0.9161\n",
      "Epoch 188/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.2240 - accuracy: 0.9134\n",
      "Epoch 189/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.2189 - accuracy: 0.9157\n",
      "Epoch 190/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.2224 - accuracy: 0.9111\n",
      "Epoch 191/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.2368 - accuracy: 0.9044\n",
      "Epoch 192/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.2213 - accuracy: 0.9147\n",
      "Epoch 193/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2304 - accuracy: 0.9111\n",
      "Epoch 194/1500\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9080\n",
      "Epoch 195/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.2246 - accuracy: 0.9080\n",
      "Epoch 196/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.2150 - accuracy: 0.9129\n",
      "Epoch 197/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.2295 - accuracy: 0.9062\n",
      "Epoch 198/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.2280 - accuracy: 0.9107\n",
      "Epoch 199/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.2142 - accuracy: 0.9147\n",
      "Epoch 200/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.2306 - accuracy: 0.9120\n",
      "Epoch 201/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.2165 - accuracy: 0.9166\n",
      "Epoch 202/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.2255 - accuracy: 0.9089\n",
      "Epoch 203/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.2202 - accuracy: 0.9134\n",
      "Epoch 204/1500\n",
      "35/35 [==============================] - 0s 712us/step - loss: 0.2090 - accuracy: 0.9111\n",
      "Epoch 205/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.2304 - accuracy: 0.9166\n",
      "Epoch 206/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.2214 - accuracy: 0.9129\n",
      "Epoch 207/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2277 - accuracy: 0.9152\n",
      "Epoch 208/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.2074 - accuracy: 0.9220\n",
      "Epoch 209/1500\n",
      "35/35 [==============================] - 0s 713us/step - loss: 0.2177 - accuracy: 0.9184\n",
      "Epoch 210/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.2124 - accuracy: 0.9170\n",
      "Epoch 211/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.2013 - accuracy: 0.9220\n",
      "Epoch 212/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.2051 - accuracy: 0.9310\n",
      "Epoch 213/1500\n",
      "35/35 [==============================] - 0s 726us/step - loss: 0.2111 - accuracy: 0.9143\n",
      "Epoch 214/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.2124 - accuracy: 0.9134\n",
      "Epoch 215/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.2226 - accuracy: 0.9071\n",
      "Epoch 216/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.2185 - accuracy: 0.9170\n",
      "Epoch 217/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.2181 - accuracy: 0.9184\n",
      "Epoch 218/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.2043 - accuracy: 0.9224\n",
      "Epoch 219/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.2115 - accuracy: 0.9170\n",
      "Epoch 220/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.2040 - accuracy: 0.9274\n",
      "Epoch 221/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.2200 - accuracy: 0.9084\n",
      "Epoch 222/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.2057 - accuracy: 0.9202\n",
      "Epoch 223/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1962 - accuracy: 0.9242\n",
      "Epoch 224/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.2111 - accuracy: 0.9206\n",
      "Epoch 225/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2080 - accuracy: 0.9242\n",
      "Epoch 226/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.2065 - accuracy: 0.9166\n",
      "Epoch 227/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.2084 - accuracy: 0.9157\n",
      "Epoch 228/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.2125 - accuracy: 0.9157\n",
      "Epoch 229/1500\n",
      "35/35 [==============================] - 0s 728us/step - loss: 0.2049 - accuracy: 0.9184\n",
      "Epoch 230/1500\n",
      "35/35 [==============================] - 0s 741us/step - loss: 0.2120 - accuracy: 0.9170\n",
      "Epoch 231/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.2172 - accuracy: 0.9193\n",
      "Epoch 232/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1981 - accuracy: 0.9238\n",
      "Epoch 233/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.2030 - accuracy: 0.9260\n",
      "Epoch 234/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.2048 - accuracy: 0.9224\n",
      "Epoch 235/1500\n",
      "35/35 [==============================] - 0s 723us/step - loss: 0.1964 - accuracy: 0.9305\n",
      "Epoch 236/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1936 - accuracy: 0.9260\n",
      "Epoch 237/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.1962 - accuracy: 0.9242\n",
      "Epoch 238/1500\n",
      "35/35 [==============================] - 0s 741us/step - loss: 0.2017 - accuracy: 0.9229\n",
      "Epoch 239/1500\n",
      "35/35 [==============================] - 0s 713us/step - loss: 0.2088 - accuracy: 0.9161\n",
      "Epoch 240/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.2074 - accuracy: 0.9197\n",
      "Epoch 241/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.1818 - accuracy: 0.9310\n",
      "Epoch 242/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2160 - accuracy: 0.9184\n",
      "Epoch 243/1500\n",
      "35/35 [==============================] - 0s 726us/step - loss: 0.2019 - accuracy: 0.9319\n",
      "Epoch 244/1500\n",
      "35/35 [==============================] - 0s 741us/step - loss: 0.1933 - accuracy: 0.9215\n",
      "Epoch 245/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.2043 - accuracy: 0.9188\n",
      "Epoch 246/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.1957 - accuracy: 0.9256\n",
      "Epoch 247/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1940 - accuracy: 0.9233\n",
      "Epoch 248/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1819 - accuracy: 0.9269\n",
      "Epoch 249/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2033 - accuracy: 0.9188\n",
      "Epoch 250/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.1916 - accuracy: 0.9269\n",
      "Epoch 251/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.1917 - accuracy: 0.9251\n",
      "Epoch 252/1500\n",
      "35/35 [==============================] - 0s 791us/step - loss: 0.1828 - accuracy: 0.9314\n",
      "Epoch 253/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1921 - accuracy: 0.9202\n",
      "Epoch 254/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.1951 - accuracy: 0.9265\n",
      "Epoch 255/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.1938 - accuracy: 0.9229\n",
      "Epoch 256/1500\n",
      "35/35 [==============================] - 0s 787us/step - loss: 0.1941 - accuracy: 0.9247\n",
      "Epoch 257/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1980 - accuracy: 0.9224\n",
      "Epoch 258/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1872 - accuracy: 0.9274\n",
      "Epoch 259/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1845 - accuracy: 0.9256\n",
      "Epoch 260/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1831 - accuracy: 0.9296\n",
      "Epoch 261/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.1880 - accuracy: 0.9310\n",
      "Epoch 262/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.1913 - accuracy: 0.9251\n",
      "Epoch 263/1500\n",
      "35/35 [==============================] - 0s 760us/step - loss: 0.1772 - accuracy: 0.9364\n",
      "Epoch 264/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.1897 - accuracy: 0.9238\n",
      "Epoch 265/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1848 - accuracy: 0.9346\n",
      "Epoch 266/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.1875 - accuracy: 0.9292\n",
      "Epoch 267/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1816 - accuracy: 0.9292\n",
      "Epoch 268/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1742 - accuracy: 0.9314\n",
      "Epoch 269/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1708 - accuracy: 0.9296\n",
      "Epoch 270/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.1732 - accuracy: 0.9296\n",
      "Epoch 271/1500\n",
      "35/35 [==============================] - 0s 726us/step - loss: 0.1821 - accuracy: 0.9269\n",
      "Epoch 272/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.1937 - accuracy: 0.9224\n",
      "Epoch 273/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1757 - accuracy: 0.9378\n",
      "Epoch 274/1500\n",
      "35/35 [==============================] - 0s 725us/step - loss: 0.1876 - accuracy: 0.9287\n",
      "Epoch 275/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1794 - accuracy: 0.9332\n",
      "Epoch 276/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.1680 - accuracy: 0.9355\n",
      "Epoch 277/1500\n",
      "35/35 [==============================] - 0s 741us/step - loss: 0.1772 - accuracy: 0.9359\n",
      "Epoch 278/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.1940 - accuracy: 0.9197\n",
      "Epoch 279/1500\n",
      "35/35 [==============================] - 0s 710us/step - loss: 0.1731 - accuracy: 0.9319\n",
      "Epoch 280/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.1928 - accuracy: 0.9269\n",
      "Epoch 281/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.1825 - accuracy: 0.9193\n",
      "Epoch 282/1500\n",
      "35/35 [==============================] - 0s 727us/step - loss: 0.1758 - accuracy: 0.9323\n",
      "Epoch 283/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.1849 - accuracy: 0.9251\n",
      "Epoch 284/1500\n",
      "35/35 [==============================] - 0s 726us/step - loss: 0.1820 - accuracy: 0.9369\n",
      "Epoch 285/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.1864 - accuracy: 0.9238\n",
      "Epoch 286/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.1782 - accuracy: 0.9251\n",
      "Epoch 287/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.1863 - accuracy: 0.9233\n",
      "Epoch 288/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1704 - accuracy: 0.9373\n",
      "Epoch 289/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1855 - accuracy: 0.9296\n",
      "Epoch 290/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1779 - accuracy: 0.9341\n",
      "Epoch 291/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.1936 - accuracy: 0.9287\n",
      "Epoch 292/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1801 - accuracy: 0.9265\n",
      "Epoch 293/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9301\n",
      "Epoch 294/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.1759 - accuracy: 0.9314\n",
      "Epoch 295/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.1762 - accuracy: 0.9364\n",
      "Epoch 296/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.1790 - accuracy: 0.9328\n",
      "Epoch 297/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.1764 - accuracy: 0.9310\n",
      "Epoch 298/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1695 - accuracy: 0.9355\n",
      "Epoch 299/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1667 - accuracy: 0.9341\n",
      "Epoch 300/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1787 - accuracy: 0.9296\n",
      "Epoch 301/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.1664 - accuracy: 0.9319\n",
      "Epoch 302/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1646 - accuracy: 0.9314\n",
      "Epoch 303/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1811 - accuracy: 0.9296\n",
      "Epoch 304/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.1621 - accuracy: 0.9369\n",
      "Epoch 305/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.1859 - accuracy: 0.9305\n",
      "Epoch 306/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.1724 - accuracy: 0.9382\n",
      "Epoch 307/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1710 - accuracy: 0.9346\n",
      "Epoch 308/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1684 - accuracy: 0.9359\n",
      "Epoch 309/1500\n",
      "35/35 [==============================] - 0s 709us/step - loss: 0.1687 - accuracy: 0.9400\n",
      "Epoch 310/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.1643 - accuracy: 0.9414\n",
      "Epoch 311/1500\n",
      "35/35 [==============================] - 0s 741us/step - loss: 0.1698 - accuracy: 0.9355\n",
      "Epoch 312/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.1731 - accuracy: 0.9296\n",
      "Epoch 313/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1595 - accuracy: 0.9400\n",
      "Epoch 314/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1604 - accuracy: 0.9400\n",
      "Epoch 315/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.1668 - accuracy: 0.9396\n",
      "Epoch 316/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.1602 - accuracy: 0.9378\n",
      "Epoch 317/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.1804 - accuracy: 0.9278\n",
      "Epoch 318/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1675 - accuracy: 0.9341\n",
      "Epoch 319/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1584 - accuracy: 0.9409\n",
      "Epoch 320/1500\n",
      "35/35 [==============================] - 0s 727us/step - loss: 0.1492 - accuracy: 0.9441\n",
      "Epoch 321/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.1737 - accuracy: 0.9301\n",
      "Epoch 322/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.1612 - accuracy: 0.9346\n",
      "Epoch 323/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.1572 - accuracy: 0.9423\n",
      "Epoch 324/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.1597 - accuracy: 0.9369\n",
      "Epoch 325/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1563 - accuracy: 0.9405\n",
      "Epoch 326/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.1715 - accuracy: 0.9332\n",
      "Epoch 327/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1599 - accuracy: 0.9396\n",
      "Epoch 328/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1515 - accuracy: 0.9405\n",
      "Epoch 329/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.1661 - accuracy: 0.9337\n",
      "Epoch 330/1500\n",
      "35/35 [==============================] - 0s 727us/step - loss: 0.1679 - accuracy: 0.9346\n",
      "Epoch 331/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1480 - accuracy: 0.9450\n",
      "Epoch 332/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.1597 - accuracy: 0.9359\n",
      "Epoch 333/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.1599 - accuracy: 0.9382\n",
      "Epoch 334/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1620 - accuracy: 0.9382\n",
      "Epoch 335/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1508 - accuracy: 0.9387\n",
      "Epoch 336/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.1619 - accuracy: 0.9359\n",
      "Epoch 337/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1559 - accuracy: 0.9414\n",
      "Epoch 338/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.1630 - accuracy: 0.9378\n",
      "Epoch 339/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.1622 - accuracy: 0.9364\n",
      "Epoch 340/1500\n",
      "35/35 [==============================] - 0s 796us/step - loss: 0.1645 - accuracy: 0.9369\n",
      "Epoch 341/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1480 - accuracy: 0.9472\n",
      "Epoch 342/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1635 - accuracy: 0.9350\n",
      "Epoch 343/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.1556 - accuracy: 0.9445\n",
      "Epoch 344/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1656 - accuracy: 0.9382\n",
      "Epoch 345/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1716 - accuracy: 0.9332\n",
      "Epoch 346/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.1402 - accuracy: 0.9513\n",
      "Epoch 347/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.1474 - accuracy: 0.9450\n",
      "Epoch 348/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.1528 - accuracy: 0.9391\n",
      "Epoch 349/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.1487 - accuracy: 0.9486\n",
      "Epoch 350/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.1612 - accuracy: 0.9445\n",
      "Epoch 351/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1502 - accuracy: 0.9414\n",
      "Epoch 352/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1609 - accuracy: 0.9359\n",
      "Epoch 353/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1454 - accuracy: 0.9423\n",
      "Epoch 354/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.1589 - accuracy: 0.9332\n",
      "Epoch 355/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1590 - accuracy: 0.9423\n",
      "Epoch 356/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1566 - accuracy: 0.9405\n",
      "Epoch 357/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1522 - accuracy: 0.9391\n",
      "Epoch 358/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.1666 - accuracy: 0.9369\n",
      "Epoch 359/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.1578 - accuracy: 0.9441\n",
      "Epoch 360/1500\n",
      "35/35 [==============================] - 0s 752us/step - loss: 0.1528 - accuracy: 0.9355\n",
      "Epoch 361/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1404 - accuracy: 0.9445\n",
      "Epoch 362/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.1450 - accuracy: 0.9432\n",
      "Epoch 363/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1576 - accuracy: 0.9454\n",
      "Epoch 364/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.1408 - accuracy: 0.9445\n",
      "Epoch 365/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.1559 - accuracy: 0.9373\n",
      "Epoch 366/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9400\n",
      "Epoch 367/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9391\n",
      "Epoch 368/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.1600 - accuracy: 0.9387\n",
      "Epoch 369/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1360 - accuracy: 0.9504\n",
      "Epoch 370/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.1447 - accuracy: 0.9441\n",
      "Epoch 371/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.1513 - accuracy: 0.9369\n",
      "Epoch 372/1500\n",
      "35/35 [==============================] - 0s 718us/step - loss: 0.1513 - accuracy: 0.9481\n",
      "Epoch 373/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1492 - accuracy: 0.9436\n",
      "Epoch 374/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.1417 - accuracy: 0.9504\n",
      "Epoch 375/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.1487 - accuracy: 0.9409\n",
      "Epoch 376/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.1377 - accuracy: 0.9513\n",
      "Epoch 377/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.1418 - accuracy: 0.9427\n",
      "Epoch 378/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.1620 - accuracy: 0.9396\n",
      "Epoch 379/1500\n",
      "35/35 [==============================] - 0s 760us/step - loss: 0.1457 - accuracy: 0.9414\n",
      "Epoch 380/1500\n",
      "35/35 [==============================] - 0s 752us/step - loss: 0.1500 - accuracy: 0.9459\n",
      "Epoch 381/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.1339 - accuracy: 0.9490\n",
      "Epoch 382/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.1563 - accuracy: 0.9441\n",
      "Epoch 383/1500\n",
      "35/35 [==============================] - 0s 711us/step - loss: 0.1458 - accuracy: 0.9436\n",
      "Epoch 384/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1491 - accuracy: 0.9396\n",
      "Epoch 385/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.1360 - accuracy: 0.9522\n",
      "Epoch 386/1500\n",
      "35/35 [==============================] - 0s 722us/step - loss: 0.1496 - accuracy: 0.9400\n",
      "Epoch 387/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.1396 - accuracy: 0.9522\n",
      "Epoch 388/1500\n",
      "35/35 [==============================] - 0s 721us/step - loss: 0.1435 - accuracy: 0.9454\n",
      "Epoch 389/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.1338 - accuracy: 0.9463\n",
      "Epoch 390/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1558 - accuracy: 0.9400\n",
      "Epoch 391/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1516 - accuracy: 0.9400\n",
      "Epoch 392/1500\n",
      "35/35 [==============================] - 0s 803us/step - loss: 0.1543 - accuracy: 0.9436\n",
      "Epoch 393/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.1383 - accuracy: 0.9468\n",
      "Epoch 394/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.1376 - accuracy: 0.9450\n",
      "Epoch 395/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1345 - accuracy: 0.9477\n",
      "Epoch 396/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.1554 - accuracy: 0.9409\n",
      "Epoch 397/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.1416 - accuracy: 0.9468\n",
      "Epoch 398/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1394 - accuracy: 0.9418\n",
      "Epoch 399/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1425 - accuracy: 0.9432\n",
      "Epoch 400/1500\n",
      "35/35 [==============================] - 0s 726us/step - loss: 0.1345 - accuracy: 0.9486\n",
      "Epoch 401/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.1435 - accuracy: 0.9418\n",
      "Epoch 402/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1402 - accuracy: 0.9427\n",
      "Epoch 403/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1324 - accuracy: 0.9486\n",
      "Epoch 404/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1556 - accuracy: 0.9432\n",
      "Epoch 405/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.1441 - accuracy: 0.9468\n",
      "Epoch 406/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1362 - accuracy: 0.9472\n",
      "Epoch 407/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.1328 - accuracy: 0.9472\n",
      "Epoch 408/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1427 - accuracy: 0.9445\n",
      "Epoch 409/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.1352 - accuracy: 0.9454\n",
      "Epoch 410/1500\n",
      "35/35 [==============================] - 0s 760us/step - loss: 0.1391 - accuracy: 0.9508\n",
      "Epoch 411/1500\n",
      "35/35 [==============================] - 0s 725us/step - loss: 0.1381 - accuracy: 0.9499\n",
      "Epoch 412/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.1272 - accuracy: 0.9567\n",
      "Epoch 413/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.1571 - accuracy: 0.9373\n",
      "Epoch 414/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1307 - accuracy: 0.9526\n",
      "Epoch 415/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1364 - accuracy: 0.9499\n",
      "Epoch 416/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.1456 - accuracy: 0.9445\n",
      "Epoch 417/1500\n",
      "35/35 [==============================] - 0s 717us/step - loss: 0.1367 - accuracy: 0.9504\n",
      "Epoch 418/1500\n",
      "35/35 [==============================] - 0s 723us/step - loss: 0.1415 - accuracy: 0.9495\n",
      "Epoch 419/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.1511 - accuracy: 0.9418\n",
      "Epoch 420/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.1320 - accuracy: 0.9553\n",
      "Epoch 421/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.1274 - accuracy: 0.9581\n",
      "Epoch 422/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.1395 - accuracy: 0.9441\n",
      "Epoch 423/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.1173 - accuracy: 0.9522\n",
      "Epoch 424/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1381 - accuracy: 0.9490\n",
      "Epoch 425/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1403 - accuracy: 0.9486\n",
      "Epoch 426/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.1329 - accuracy: 0.9468\n",
      "Epoch 427/1500\n",
      "35/35 [==============================] - 0s 760us/step - loss: 0.1424 - accuracy: 0.9468\n",
      "Epoch 428/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9513\n",
      "Epoch 429/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9436\n",
      "Epoch 430/1500\n",
      "35/35 [==============================] - 0s 804us/step - loss: 0.1361 - accuracy: 0.9499\n",
      "Epoch 431/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.1344 - accuracy: 0.9540\n",
      "Epoch 432/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1382 - accuracy: 0.9472\n",
      "Epoch 433/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.1524 - accuracy: 0.9400\n",
      "Epoch 434/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1405 - accuracy: 0.9432\n",
      "Epoch 435/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.1415 - accuracy: 0.9418\n",
      "Epoch 436/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.1418 - accuracy: 0.9495\n",
      "Epoch 437/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.1396 - accuracy: 0.9445\n",
      "Epoch 438/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1358 - accuracy: 0.9468\n",
      "Epoch 439/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.1403 - accuracy: 0.9450\n",
      "Epoch 440/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.1412 - accuracy: 0.9477\n",
      "Epoch 441/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.1335 - accuracy: 0.9504\n",
      "Epoch 442/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1258 - accuracy: 0.9526\n",
      "Epoch 443/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.1232 - accuracy: 0.9531\n",
      "Epoch 444/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.1237 - accuracy: 0.9499\n",
      "Epoch 445/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1205 - accuracy: 0.9549\n",
      "Epoch 446/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1297 - accuracy: 0.9508\n",
      "Epoch 447/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1328 - accuracy: 0.9517\n",
      "Epoch 448/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1400 - accuracy: 0.9481\n",
      "Epoch 449/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.1391 - accuracy: 0.9459\n",
      "Epoch 450/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1231 - accuracy: 0.9522\n",
      "Epoch 451/1500\n",
      "35/35 [==============================] - 0s 703us/step - loss: 0.1384 - accuracy: 0.9450\n",
      "Epoch 452/1500\n",
      "35/35 [==============================] - 0s 741us/step - loss: 0.1230 - accuracy: 0.9526\n",
      "Epoch 453/1500\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.1338 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 423.\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.1379 - accuracy: 0.9531\n",
      "Epoch 453: early stopping\n",
      "8/8 [==============================] - 0s 785us/step - loss: 1.1572 - accuracy: 0.7080\n",
      "8/8 [==============================] - 0s 536us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.61 (17/28)\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "Final Test Results - Loss: 1.1572109460830688, Accuracy: 0.7079645991325378, Precision: 0.6467948717948718, Recall: 0.7032480597872951, F1 Score: 0.6680347831593201\n",
      "Confusion Matrix:\n",
      " [[106  13  23]\n",
      " [  0  32   3]\n",
      " [ 24   3  22]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '045A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '044A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    150\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 940, 1: 750, 2: 680})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 1.1030 - accuracy: 0.4827\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.8562 - accuracy: 0.6291\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.7721 - accuracy: 0.6772\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.7595 - accuracy: 0.6671\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.7410 - accuracy: 0.6667\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.7311 - accuracy: 0.6722\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.6979 - accuracy: 0.6920\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.6748 - accuracy: 0.7131\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 679us/step - loss: 0.6749 - accuracy: 0.7059\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.6568 - accuracy: 0.7093\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.6608 - accuracy: 0.7051\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.6437 - accuracy: 0.7236\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.6308 - accuracy: 0.7232\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.6084 - accuracy: 0.7211\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.6047 - accuracy: 0.7350\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 677us/step - loss: 0.6033 - accuracy: 0.7384\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.6054 - accuracy: 0.7287\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.5923 - accuracy: 0.7426\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.5722 - accuracy: 0.7561\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.5765 - accuracy: 0.7540\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.5711 - accuracy: 0.7498\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.5496 - accuracy: 0.7612\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.5506 - accuracy: 0.7527\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.5494 - accuracy: 0.7574\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.5489 - accuracy: 0.7599\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.5480 - accuracy: 0.7629\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.5264 - accuracy: 0.7675\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.5336 - accuracy: 0.7608\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.5147 - accuracy: 0.7768\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 687us/step - loss: 0.5268 - accuracy: 0.7662\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.5262 - accuracy: 0.7692\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.5110 - accuracy: 0.7835\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.4941 - accuracy: 0.7814\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.5288 - accuracy: 0.7696\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.5041 - accuracy: 0.7730\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.4947 - accuracy: 0.7941\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.4833 - accuracy: 0.7928\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.4945 - accuracy: 0.7806\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.5060 - accuracy: 0.7759\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.4738 - accuracy: 0.7941\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.4917 - accuracy: 0.7848\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 661us/step - loss: 0.4766 - accuracy: 0.7932\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.4849 - accuracy: 0.7873\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.4665 - accuracy: 0.7979\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.4856 - accuracy: 0.7903\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.4671 - accuracy: 0.8013\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.4840 - accuracy: 0.7903\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.4804 - accuracy: 0.7911\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.4727 - accuracy: 0.7890\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.4505 - accuracy: 0.8059\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.4629 - accuracy: 0.7958\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.4457 - accuracy: 0.8068\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.4561 - accuracy: 0.8055\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.4381 - accuracy: 0.7996\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.4591 - accuracy: 0.7928\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.4445 - accuracy: 0.8055\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.4557 - accuracy: 0.8038\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.4413 - accuracy: 0.8059\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.4567 - accuracy: 0.8008\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.4257 - accuracy: 0.8173\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.4284 - accuracy: 0.8089\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.4472 - accuracy: 0.7987\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.4279 - accuracy: 0.8105\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.4318 - accuracy: 0.8068\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.4283 - accuracy: 0.8122\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.4308 - accuracy: 0.8135\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.4159 - accuracy: 0.8215\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.4459 - accuracy: 0.8059\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.4281 - accuracy: 0.8101\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.4294 - accuracy: 0.8118\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.4246 - accuracy: 0.8139\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.4161 - accuracy: 0.8203\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.3944 - accuracy: 0.8245\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.4170 - accuracy: 0.8186\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.4151 - accuracy: 0.8101\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.4068 - accuracy: 0.8135\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.4125 - accuracy: 0.8169\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 656us/step - loss: 0.4103 - accuracy: 0.8219\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.4012 - accuracy: 0.8388\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.4123 - accuracy: 0.8177\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.3870 - accuracy: 0.8380\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.3977 - accuracy: 0.8312\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.3918 - accuracy: 0.8354\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.4112 - accuracy: 0.8101\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.3917 - accuracy: 0.8371\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.3937 - accuracy: 0.8354\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.3821 - accuracy: 0.8405\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.3979 - accuracy: 0.8257\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3904 - accuracy: 0.8295\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.3819 - accuracy: 0.8388\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.3865 - accuracy: 0.8304\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.3805 - accuracy: 0.8397\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 665us/step - loss: 0.3848 - accuracy: 0.8338\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.3944 - accuracy: 0.8325\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3987 - accuracy: 0.8295\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.3825 - accuracy: 0.8376\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3835 - accuracy: 0.8414\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.3724 - accuracy: 0.8422\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.3924 - accuracy: 0.8409\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.3801 - accuracy: 0.8363\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3815 - accuracy: 0.8321\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3819 - accuracy: 0.8371\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.3786 - accuracy: 0.8392\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.3657 - accuracy: 0.8418\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.3675 - accuracy: 0.8494\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.3722 - accuracy: 0.8422\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3670 - accuracy: 0.8519\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.3731 - accuracy: 0.8388\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.3692 - accuracy: 0.8435\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3596 - accuracy: 0.8376\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.3697 - accuracy: 0.8447\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.3654 - accuracy: 0.8460\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.3799 - accuracy: 0.8409\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.3648 - accuracy: 0.8460\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 664us/step - loss: 0.3414 - accuracy: 0.8561\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.3683 - accuracy: 0.8422\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3623 - accuracy: 0.8418\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.3514 - accuracy: 0.8473\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 668us/step - loss: 0.3602 - accuracy: 0.8506\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.3547 - accuracy: 0.8473\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.3562 - accuracy: 0.8489\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.3548 - accuracy: 0.8456\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3460 - accuracy: 0.8544\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.3464 - accuracy: 0.8519\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.3536 - accuracy: 0.8527\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.3535 - accuracy: 0.8515\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.3426 - accuracy: 0.8561\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.3359 - accuracy: 0.8595\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3554 - accuracy: 0.8540\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.3470 - accuracy: 0.8557\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3446 - accuracy: 0.8553\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 671us/step - loss: 0.3469 - accuracy: 0.8574\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3470 - accuracy: 0.8557\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.3446 - accuracy: 0.8447\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.3325 - accuracy: 0.8633\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.3334 - accuracy: 0.8650\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3391 - accuracy: 0.8527\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.3311 - accuracy: 0.8582\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.3274 - accuracy: 0.8650\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 668us/step - loss: 0.3427 - accuracy: 0.8591\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.3324 - accuracy: 0.8641\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.3415 - accuracy: 0.8447\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3419 - accuracy: 0.8511\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.3527 - accuracy: 0.8468\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.3480 - accuracy: 0.8481\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3374 - accuracy: 0.8511\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.3324 - accuracy: 0.8595\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3286 - accuracy: 0.8599\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.3345 - accuracy: 0.8549\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.3240 - accuracy: 0.8641\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.3477 - accuracy: 0.8477\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.3311 - accuracy: 0.8591\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3321 - accuracy: 0.8633\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3344 - accuracy: 0.8591\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.3214 - accuracy: 0.8679\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.3205 - accuracy: 0.8722\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3233 - accuracy: 0.8612\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.3307 - accuracy: 0.8646\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.3165 - accuracy: 0.8637\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.3213 - accuracy: 0.8679\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.3180 - accuracy: 0.8650\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.3400 - accuracy: 0.8553\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3288 - accuracy: 0.8603\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.3323 - accuracy: 0.8620\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3230 - accuracy: 0.8658\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.3427 - accuracy: 0.8662\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3093 - accuracy: 0.8722\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.3119 - accuracy: 0.8608\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.3143 - accuracy: 0.8709\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.3160 - accuracy: 0.8696\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3096 - accuracy: 0.8717\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.3134 - accuracy: 0.8692\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.3136 - accuracy: 0.8722\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.3168 - accuracy: 0.8709\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.3084 - accuracy: 0.8705\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.3183 - accuracy: 0.8667\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.3212 - accuracy: 0.8705\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.3039 - accuracy: 0.8684\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.3173 - accuracy: 0.8759\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2992 - accuracy: 0.8709\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.3090 - accuracy: 0.8709\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.3137 - accuracy: 0.8692\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3003 - accuracy: 0.8722\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.3155 - accuracy: 0.8679\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.3130 - accuracy: 0.8764\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3089 - accuracy: 0.8751\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2978 - accuracy: 0.8819\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.2996 - accuracy: 0.8747\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.3025 - accuracy: 0.8717\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.3013 - accuracy: 0.8772\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.3195 - accuracy: 0.8658\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.3189 - accuracy: 0.8696\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.3142 - accuracy: 0.8654\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2902 - accuracy: 0.8793\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2974 - accuracy: 0.8738\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.3026 - accuracy: 0.8759\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2983 - accuracy: 0.8793\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2916 - accuracy: 0.8814\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2991 - accuracy: 0.8768\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2949 - accuracy: 0.8810\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2964 - accuracy: 0.8738\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2975 - accuracy: 0.8823\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2993 - accuracy: 0.8734\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2721 - accuracy: 0.8869\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.3017 - accuracy: 0.8743\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2964 - accuracy: 0.8755\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.2887 - accuracy: 0.8802\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.2935 - accuracy: 0.8844\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2855 - accuracy: 0.8869\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2872 - accuracy: 0.8869\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2867 - accuracy: 0.8835\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2817 - accuracy: 0.8865\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2959 - accuracy: 0.8793\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2820 - accuracy: 0.8840\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.3043 - accuracy: 0.8671\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.2868 - accuracy: 0.8890\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2952 - accuracy: 0.8755\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2840 - accuracy: 0.8819\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.2867 - accuracy: 0.8844\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.2916 - accuracy: 0.8764\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2769 - accuracy: 0.8886\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2886 - accuracy: 0.8810\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.2730 - accuracy: 0.8882\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.2916 - accuracy: 0.8764\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2799 - accuracy: 0.8911\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.2852 - accuracy: 0.8865\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2821 - accuracy: 0.8911\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2833 - accuracy: 0.8903\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2969 - accuracy: 0.8789\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2767 - accuracy: 0.8827\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2833 - accuracy: 0.8954\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2657 - accuracy: 0.8869\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2717 - accuracy: 0.8911\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2733 - accuracy: 0.8886\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.2740 - accuracy: 0.8932\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2738 - accuracy: 0.8895\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2891 - accuracy: 0.8797\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.2763 - accuracy: 0.8895\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.2723 - accuracy: 0.8882\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2811 - accuracy: 0.8844\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2661 - accuracy: 0.8941\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.2527 - accuracy: 0.8987\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2696 - accuracy: 0.8911\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2635 - accuracy: 0.8949\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2801 - accuracy: 0.8776\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2675 - accuracy: 0.8916\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.2714 - accuracy: 0.8954\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2692 - accuracy: 0.8882\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2601 - accuracy: 0.8937\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2771 - accuracy: 0.8895\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2702 - accuracy: 0.8945\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2592 - accuracy: 0.9000\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2552 - accuracy: 0.9008\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2701 - accuracy: 0.8916\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2666 - accuracy: 0.8937\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2611 - accuracy: 0.8996\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2605 - accuracy: 0.8903\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2483 - accuracy: 0.8966\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.2570 - accuracy: 0.8941\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2620 - accuracy: 0.8958\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2649 - accuracy: 0.8932\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2660 - accuracy: 0.8928\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.2601 - accuracy: 0.8949\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 0.9013\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2589 - accuracy: 0.9051\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2733 - accuracy: 0.8937\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2514 - accuracy: 0.9004\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2518 - accuracy: 0.9034\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2516 - accuracy: 0.8983\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2650 - accuracy: 0.8920\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.2733 - accuracy: 0.8878\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.2638 - accuracy: 0.8941\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2527 - accuracy: 0.8966\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2477 - accuracy: 0.9000\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.2501 - accuracy: 0.9021\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2680 - accuracy: 0.8941\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2533 - accuracy: 0.8970\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2459 - accuracy: 0.9034\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2604 - accuracy: 0.8903\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2589 - accuracy: 0.8928\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2564 - accuracy: 0.8966\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.2724 - accuracy: 0.8865\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2538 - accuracy: 0.8975\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2627 - accuracy: 0.8903\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2608 - accuracy: 0.8928\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.2653 - accuracy: 0.8924\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2545 - accuracy: 0.8958\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2427 - accuracy: 0.9025\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2545 - accuracy: 0.8928\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.2533 - accuracy: 0.8992\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.2702 - accuracy: 0.8873\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2462 - accuracy: 0.9025\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2610 - accuracy: 0.8895\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.2616 - accuracy: 0.8954\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2477 - accuracy: 0.8979\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.2291 - accuracy: 0.9084\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2532 - accuracy: 0.8916\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2512 - accuracy: 0.8996\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.2560 - accuracy: 0.8987\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2273 - accuracy: 0.9105\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2317 - accuracy: 0.9063\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2319 - accuracy: 0.9131\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.2411 - accuracy: 0.9004\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.2365 - accuracy: 0.9080\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2517 - accuracy: 0.9021\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.2443 - accuracy: 0.9076\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2262 - accuracy: 0.9059\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.2450 - accuracy: 0.9080\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.2373 - accuracy: 0.8992\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2337 - accuracy: 0.9068\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2364 - accuracy: 0.9000\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2477 - accuracy: 0.9030\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2316 - accuracy: 0.9021\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2329 - accuracy: 0.9059\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2587 - accuracy: 0.8949\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2396 - accuracy: 0.9080\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2422 - accuracy: 0.9008\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2375 - accuracy: 0.9055\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2241 - accuracy: 0.9131\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2432 - accuracy: 0.8962\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2413 - accuracy: 0.9025\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2342 - accuracy: 0.9063\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2339 - accuracy: 0.8996\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2376 - accuracy: 0.8975\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2338 - accuracy: 0.9038\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2144 - accuracy: 0.9122\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2417 - accuracy: 0.9046\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2445 - accuracy: 0.9021\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2358 - accuracy: 0.9055\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.2284 - accuracy: 0.9084\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2484 - accuracy: 0.8987\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2325 - accuracy: 0.9017\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2365 - accuracy: 0.9013\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2409 - accuracy: 0.9038\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2391 - accuracy: 0.9025\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2359 - accuracy: 0.9038\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2342 - accuracy: 0.9105\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2402 - accuracy: 0.9063\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2228 - accuracy: 0.9135\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2392 - accuracy: 0.9076\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2300 - accuracy: 0.9089\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2388 - accuracy: 0.9017\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2208 - accuracy: 0.9148\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9072\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2469 - accuracy: 0.9025\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2286 - accuracy: 0.9021\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2252 - accuracy: 0.9097\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2374 - accuracy: 0.9080\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2340 - accuracy: 0.9046\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2332 - accuracy: 0.9080\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2253 - accuracy: 0.9080\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2191 - accuracy: 0.9076\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.2252 - accuracy: 0.9068\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2270 - accuracy: 0.9152\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2321 - accuracy: 0.9068\n",
      "Epoch 356/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.2124 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 326.\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2214 - accuracy: 0.9122\n",
      "Epoch 356: early stopping\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4855 - accuracy: 0.8122\n",
      "6/6 [==============================] - 0s 755us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.87 (26/30)\n",
      "Before appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "After appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "Final Test Results - Loss: 0.4855062663555145, Accuracy: 0.8121547102928162, Precision: 0.8037707390648566, Recall: 0.7308313155770784, F1 Score: 0.7613186191284663\n",
      "Confusion Matrix:\n",
      " [[107   3   8]\n",
      " [  7  14   0]\n",
      " [ 16   0  26]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 882, 1: 710, 2: 580})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 1s 1ms/step - loss: 1.3762 - accuracy: 0.3660\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 1.0456 - accuracy: 0.5212\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.9369 - accuracy: 0.5732\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.8936 - accuracy: 0.6165\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.8397 - accuracy: 0.6331\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.8188 - accuracy: 0.6436\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.7640 - accuracy: 0.6584\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.7358 - accuracy: 0.6878\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.7021 - accuracy: 0.7067\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.7072 - accuracy: 0.7104\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.6796 - accuracy: 0.6943\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.6619 - accuracy: 0.7196\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.6433 - accuracy: 0.7159\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.6404 - accuracy: 0.7238\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.6194 - accuracy: 0.7288\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.6327 - accuracy: 0.7192\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.6056 - accuracy: 0.7302\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.5963 - accuracy: 0.7486\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.5692 - accuracy: 0.7597\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.5666 - accuracy: 0.7597\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.5772 - accuracy: 0.7477\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.5603 - accuracy: 0.7523\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5439 - accuracy: 0.7680\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.5490 - accuracy: 0.7721\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.5298 - accuracy: 0.7735\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.5448 - accuracy: 0.7744\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.5167 - accuracy: 0.7827\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.5128 - accuracy: 0.7813\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.5165 - accuracy: 0.7878\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5043 - accuracy: 0.7753\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.5037 - accuracy: 0.7970\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.4920 - accuracy: 0.7901\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.5053 - accuracy: 0.7896\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.4748 - accuracy: 0.8039\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.4899 - accuracy: 0.7855\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.4683 - accuracy: 0.8117\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.4834 - accuracy: 0.7960\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4727 - accuracy: 0.7997\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.4613 - accuracy: 0.8043\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.4751 - accuracy: 0.8002\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4545 - accuracy: 0.8057\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.4600 - accuracy: 0.8029\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4430 - accuracy: 0.8149\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.4550 - accuracy: 0.8029\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.4508 - accuracy: 0.8149\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.4377 - accuracy: 0.8214\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.4467 - accuracy: 0.8094\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.4328 - accuracy: 0.8195\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.4352 - accuracy: 0.8241\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 959us/step - loss: 0.4439 - accuracy: 0.8149\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.4075 - accuracy: 0.8352\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.4235 - accuracy: 0.8292\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.4291 - accuracy: 0.8177\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.4246 - accuracy: 0.8158\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3908 - accuracy: 0.8490\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.4192 - accuracy: 0.8181\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.4165 - accuracy: 0.8255\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.4088 - accuracy: 0.8292\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.4080 - accuracy: 0.8292\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4033 - accuracy: 0.8292\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.4036 - accuracy: 0.8352\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.4116 - accuracy: 0.8209\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.4165 - accuracy: 0.8264\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.3969 - accuracy: 0.8310\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.3894 - accuracy: 0.8338\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.4048 - accuracy: 0.8356\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.3817 - accuracy: 0.8407\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.3832 - accuracy: 0.8421\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.3733 - accuracy: 0.8448\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.3836 - accuracy: 0.8370\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.3913 - accuracy: 0.8324\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3814 - accuracy: 0.8412\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.3867 - accuracy: 0.8333\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.3872 - accuracy: 0.8467\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.3788 - accuracy: 0.8462\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3841 - accuracy: 0.8425\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3851 - accuracy: 0.8338\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.3750 - accuracy: 0.8448\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.3755 - accuracy: 0.8462\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.3805 - accuracy: 0.8398\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.3734 - accuracy: 0.8439\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.3525 - accuracy: 0.8536\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3650 - accuracy: 0.8531\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.3566 - accuracy: 0.8564\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3564 - accuracy: 0.8476\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.3666 - accuracy: 0.8527\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.3620 - accuracy: 0.8467\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.3490 - accuracy: 0.8596\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3580 - accuracy: 0.8513\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.3458 - accuracy: 0.8545\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.3573 - accuracy: 0.8536\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3464 - accuracy: 0.8582\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.3544 - accuracy: 0.8517\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.3616 - accuracy: 0.8448\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.3441 - accuracy: 0.8591\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.3384 - accuracy: 0.8619\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3421 - accuracy: 0.8582\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.3263 - accuracy: 0.8637\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.3342 - accuracy: 0.8582\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.3356 - accuracy: 0.8559\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.3417 - accuracy: 0.8582\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.3362 - accuracy: 0.8554\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.3210 - accuracy: 0.8725\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.3480 - accuracy: 0.8504\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3529 - accuracy: 0.8587\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.3443 - accuracy: 0.8513\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3352 - accuracy: 0.8660\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.3388 - accuracy: 0.8605\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3399 - accuracy: 0.8623\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3361 - accuracy: 0.8587\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.3237 - accuracy: 0.8633\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.3430 - accuracy: 0.8536\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3234 - accuracy: 0.8619\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3191 - accuracy: 0.8725\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.3098 - accuracy: 0.8738\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.3171 - accuracy: 0.8674\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.3145 - accuracy: 0.8674\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.3201 - accuracy: 0.8651\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.3224 - accuracy: 0.8683\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.3309 - accuracy: 0.8665\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.3199 - accuracy: 0.8679\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.3137 - accuracy: 0.8752\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3101 - accuracy: 0.8725\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.2972 - accuracy: 0.8798\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.3078 - accuracy: 0.8835\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.3073 - accuracy: 0.8688\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.3154 - accuracy: 0.8679\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2839 - accuracy: 0.8890\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3084 - accuracy: 0.8669\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.3049 - accuracy: 0.8752\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2992 - accuracy: 0.8803\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3147 - accuracy: 0.8762\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.3145 - accuracy: 0.8725\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3010 - accuracy: 0.8808\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.3043 - accuracy: 0.8798\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.3089 - accuracy: 0.8715\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2870 - accuracy: 0.8812\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2935 - accuracy: 0.8849\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2988 - accuracy: 0.8803\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2808 - accuracy: 0.8844\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.3043 - accuracy: 0.8780\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.3010 - accuracy: 0.8766\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2974 - accuracy: 0.8715\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.2949 - accuracy: 0.8794\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.3024 - accuracy: 0.8817\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.3040 - accuracy: 0.8692\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2984 - accuracy: 0.8798\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2931 - accuracy: 0.8858\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2889 - accuracy: 0.8812\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2773 - accuracy: 0.8946\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.3046 - accuracy: 0.8697\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2802 - accuracy: 0.8872\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2904 - accuracy: 0.8738\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2745 - accuracy: 0.8895\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2751 - accuracy: 0.8923\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.2810 - accuracy: 0.8844\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2841 - accuracy: 0.8886\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2899 - accuracy: 0.8771\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.2768 - accuracy: 0.8826\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2965 - accuracy: 0.8789\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.2982 - accuracy: 0.8831\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2663 - accuracy: 0.8946\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2595 - accuracy: 0.8867\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2784 - accuracy: 0.8858\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2690 - accuracy: 0.8863\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2778 - accuracy: 0.8872\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.2851 - accuracy: 0.8821\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2677 - accuracy: 0.8877\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2806 - accuracy: 0.8872\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2853 - accuracy: 0.8844\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2778 - accuracy: 0.8872\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2772 - accuracy: 0.8923\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2554 - accuracy: 0.8955\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2587 - accuracy: 0.8992\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2657 - accuracy: 0.8918\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2651 - accuracy: 0.8927\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2620 - accuracy: 0.9019\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2681 - accuracy: 0.8858\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2770 - accuracy: 0.8821\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2491 - accuracy: 0.9061\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2609 - accuracy: 0.8950\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2578 - accuracy: 0.9006\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2644 - accuracy: 0.8936\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2567 - accuracy: 0.8992\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2636 - accuracy: 0.8918\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.2629 - accuracy: 0.8918\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2747 - accuracy: 0.8946\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2685 - accuracy: 0.8923\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.2555 - accuracy: 0.9056\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2578 - accuracy: 0.8964\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2523 - accuracy: 0.8992\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2508 - accuracy: 0.8955\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2448 - accuracy: 0.8992\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2645 - accuracy: 0.8941\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2553 - accuracy: 0.8941\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2690 - accuracy: 0.8904\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2562 - accuracy: 0.8946\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2374 - accuracy: 0.9052\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.2452 - accuracy: 0.9075\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2543 - accuracy: 0.8950\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.2412 - accuracy: 0.9052\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2347 - accuracy: 0.9056\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2541 - accuracy: 0.8987\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2476 - accuracy: 0.9006\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2571 - accuracy: 0.8941\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2503 - accuracy: 0.8996\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.9047\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.2584 - accuracy: 0.8913\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2392 - accuracy: 0.9029\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2302 - accuracy: 0.9130\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2421 - accuracy: 0.9024\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2324 - accuracy: 0.9093\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2403 - accuracy: 0.8996\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2452 - accuracy: 0.9061\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2248 - accuracy: 0.9134\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.2363 - accuracy: 0.9107\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2217 - accuracy: 0.9153\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2427 - accuracy: 0.9029\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2355 - accuracy: 0.9019\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2305 - accuracy: 0.9088\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2287 - accuracy: 0.9130\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2279 - accuracy: 0.9061\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2461 - accuracy: 0.8959\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2424 - accuracy: 0.9075\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2397 - accuracy: 0.9052\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2373 - accuracy: 0.9107\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2355 - accuracy: 0.9070\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.2298 - accuracy: 0.9098\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2209 - accuracy: 0.9167\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2216 - accuracy: 0.9116\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2223 - accuracy: 0.9107\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2258 - accuracy: 0.9130\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2194 - accuracy: 0.9102\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2336 - accuracy: 0.9075\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2151 - accuracy: 0.9153\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2152 - accuracy: 0.9176\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2191 - accuracy: 0.9153\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2340 - accuracy: 0.9047\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2410 - accuracy: 0.8932\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2234 - accuracy: 0.9157\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2148 - accuracy: 0.9180\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2370 - accuracy: 0.9088\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2227 - accuracy: 0.9107\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2063 - accuracy: 0.9176\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2037 - accuracy: 0.9254\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2080 - accuracy: 0.9162\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2133 - accuracy: 0.9180\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2354 - accuracy: 0.9107\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2277 - accuracy: 0.9084\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2194 - accuracy: 0.9185\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2327 - accuracy: 0.9144\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2096 - accuracy: 0.9134\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2253 - accuracy: 0.9121\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2329 - accuracy: 0.9084\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2027 - accuracy: 0.9199\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2059 - accuracy: 0.9217\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.1963 - accuracy: 0.9263\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2167 - accuracy: 0.9125\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2144 - accuracy: 0.9125\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2139 - accuracy: 0.9185\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2129 - accuracy: 0.9116\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1986 - accuracy: 0.9222\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2058 - accuracy: 0.9203\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2194 - accuracy: 0.9157\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2094 - accuracy: 0.9199\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2172 - accuracy: 0.9047\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2118 - accuracy: 0.9102\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2132 - accuracy: 0.9098\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2184 - accuracy: 0.9144\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2256 - accuracy: 0.9061\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2119 - accuracy: 0.9180\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2110 - accuracy: 0.9213\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2022 - accuracy: 0.9153\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2074 - accuracy: 0.9208\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2176 - accuracy: 0.9061\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.2143 - accuracy: 0.9144\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1951 - accuracy: 0.9236\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2013 - accuracy: 0.9236\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.2097 - accuracy: 0.9148\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2047 - accuracy: 0.9222\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2202 - accuracy: 0.9088\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9240\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9250\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2244 - accuracy: 0.9111\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1980 - accuracy: 0.9171\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2075 - accuracy: 0.9162\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.1943 - accuracy: 0.9273\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2174 - accuracy: 0.9157\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2083 - accuracy: 0.9157\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.1994 - accuracy: 0.9199\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1895 - accuracy: 0.9328\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1860 - accuracy: 0.9286\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2143 - accuracy: 0.9130\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.2032 - accuracy: 0.9157\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1963 - accuracy: 0.9176\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2037 - accuracy: 0.9222\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1899 - accuracy: 0.9263\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.1912 - accuracy: 0.9190\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2001 - accuracy: 0.9254\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1963 - accuracy: 0.9208\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1971 - accuracy: 0.9194\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2010 - accuracy: 0.9245\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.1903 - accuracy: 0.9236\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1961 - accuracy: 0.9240\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.1902 - accuracy: 0.9286\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.1893 - accuracy: 0.9231\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1870 - accuracy: 0.9194\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.1934 - accuracy: 0.9236\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2004 - accuracy: 0.9250\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.1853 - accuracy: 0.9305\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1869 - accuracy: 0.9273\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1820 - accuracy: 0.9309\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1800 - accuracy: 0.9323\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.1900 - accuracy: 0.9282\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1952 - accuracy: 0.9259\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.1861 - accuracy: 0.9236\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1877 - accuracy: 0.9286\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.1843 - accuracy: 0.9282\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.1854 - accuracy: 0.9259\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1890 - accuracy: 0.9240\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1955 - accuracy: 0.9208\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2021 - accuracy: 0.9227\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1916 - accuracy: 0.9222\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.1906 - accuracy: 0.9282\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1748 - accuracy: 0.9305\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1799 - accuracy: 0.9263\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.1824 - accuracy: 0.9273\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1964 - accuracy: 0.9190\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1816 - accuracy: 0.9319\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1849 - accuracy: 0.9259\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1839 - accuracy: 0.9277\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1986 - accuracy: 0.9273\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1817 - accuracy: 0.9296\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.1974 - accuracy: 0.9282\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.1787 - accuracy: 0.9309\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.1843 - accuracy: 0.9277\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1698 - accuracy: 0.9328\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1717 - accuracy: 0.9351\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.1757 - accuracy: 0.9355\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1828 - accuracy: 0.9286\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.1732 - accuracy: 0.9319\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 704us/step - loss: 0.1865 - accuracy: 0.9217\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 694us/step - loss: 0.1741 - accuracy: 0.9286\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.1605 - accuracy: 0.9355\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 703us/step - loss: 0.1753 - accuracy: 0.9378\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 690us/step - loss: 0.1753 - accuracy: 0.9277\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 693us/step - loss: 0.1728 - accuracy: 0.9383\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 699us/step - loss: 0.1814 - accuracy: 0.9231\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1700 - accuracy: 0.9392\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.1677 - accuracy: 0.9323\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1778 - accuracy: 0.9245\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1716 - accuracy: 0.9332\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.1771 - accuracy: 0.9273\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1914 - accuracy: 0.9259\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1841 - accuracy: 0.9323\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.1700 - accuracy: 0.9365\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.1976 - accuracy: 0.9199\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1716 - accuracy: 0.9309\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9355\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1573 - accuracy: 0.9383\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 706us/step - loss: 0.1606 - accuracy: 0.9346\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.1881 - accuracy: 0.9245\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.1739 - accuracy: 0.9355\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.1825 - accuracy: 0.9296\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.1695 - accuracy: 0.9291\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1547 - accuracy: 0.9424\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1691 - accuracy: 0.9291\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1682 - accuracy: 0.9342\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1764 - accuracy: 0.9291\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1783 - accuracy: 0.9309\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1603 - accuracy: 0.9392\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1634 - accuracy: 0.9351\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.1652 - accuracy: 0.9383\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1753 - accuracy: 0.9319\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 959us/step - loss: 0.1497 - accuracy: 0.9415\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1725 - accuracy: 0.9365\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.1604 - accuracy: 0.9300\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1608 - accuracy: 0.9378\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1698 - accuracy: 0.9319\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.1639 - accuracy: 0.9346\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1606 - accuracy: 0.9342\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.1799 - accuracy: 0.9227\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.1632 - accuracy: 0.9355\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.1645 - accuracy: 0.9314\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.1728 - accuracy: 0.9309\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.1626 - accuracy: 0.9411\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1701 - accuracy: 0.9351\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.1725 - accuracy: 0.9319\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1715 - accuracy: 0.9365\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.1635 - accuracy: 0.9305\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 697us/step - loss: 0.1706 - accuracy: 0.9355\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.1587 - accuracy: 0.9406\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.1610 - accuracy: 0.9360\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1731 - accuracy: 0.9337\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.1626 - accuracy: 0.9411\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 702us/step - loss: 0.1576 - accuracy: 0.9365\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.1674 - accuracy: 0.9300\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 698us/step - loss: 0.1624 - accuracy: 0.9369\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1626 - accuracy: 0.9369\n",
      "Epoch 400/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.1621 - accuracy: 0.9383\n",
      "Epoch 401/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.1533 - accuracy: 0.9397\n",
      "Epoch 402/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1627 - accuracy: 0.9351\n",
      "Epoch 403/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1477 - accuracy: 0.9429\n",
      "Epoch 404/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.1679 - accuracy: 0.9365\n",
      "Epoch 405/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1753 - accuracy: 0.9323\n",
      "Epoch 406/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.1614 - accuracy: 0.9401\n",
      "Epoch 407/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.1716 - accuracy: 0.9319\n",
      "Epoch 408/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1570 - accuracy: 0.9346\n",
      "Epoch 409/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.1608 - accuracy: 0.9392\n",
      "Epoch 410/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.1556 - accuracy: 0.9355\n",
      "Epoch 411/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1636 - accuracy: 0.9337\n",
      "Epoch 412/1500\n",
      "34/34 [==============================] - 0s 716us/step - loss: 0.1584 - accuracy: 0.9397\n",
      "Epoch 413/1500\n",
      "34/34 [==============================] - 0s 719us/step - loss: 0.1489 - accuracy: 0.9424\n",
      "Epoch 414/1500\n",
      "34/34 [==============================] - 0s 732us/step - loss: 0.1627 - accuracy: 0.9365\n",
      "Epoch 415/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.1572 - accuracy: 0.9369\n",
      "Epoch 416/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1397 - accuracy: 0.9452\n",
      "Epoch 417/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1844 - accuracy: 0.9305\n",
      "Epoch 418/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.1663 - accuracy: 0.9319\n",
      "Epoch 419/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1712 - accuracy: 0.9259\n",
      "Epoch 420/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.1640 - accuracy: 0.9346\n",
      "Epoch 421/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1563 - accuracy: 0.9429\n",
      "Epoch 422/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1642 - accuracy: 0.9365\n",
      "Epoch 423/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1503 - accuracy: 0.9434\n",
      "Epoch 424/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1557 - accuracy: 0.9397\n",
      "Epoch 425/1500\n",
      "34/34 [==============================] - 0s 707us/step - loss: 0.1585 - accuracy: 0.9420\n",
      "Epoch 426/1500\n",
      "34/34 [==============================] - 0s 719us/step - loss: 0.1539 - accuracy: 0.9388\n",
      "Epoch 427/1500\n",
      "34/34 [==============================] - 0s 713us/step - loss: 0.1470 - accuracy: 0.9457\n",
      "Epoch 428/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1526 - accuracy: 0.9397\n",
      "Epoch 429/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9466\n",
      "Epoch 430/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.1578 - accuracy: 0.9434\n",
      "Epoch 431/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1555 - accuracy: 0.9411\n",
      "Epoch 432/1500\n",
      "34/34 [==============================] - 0s 701us/step - loss: 0.1613 - accuracy: 0.9314\n",
      "Epoch 433/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.1442 - accuracy: 0.9443\n",
      "Epoch 434/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.1672 - accuracy: 0.9328\n",
      "Epoch 435/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.1500 - accuracy: 0.9424\n",
      "Epoch 436/1500\n",
      "34/34 [==============================] - 0s 686us/step - loss: 0.1680 - accuracy: 0.9323\n",
      "Epoch 437/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.1655 - accuracy: 0.9388\n",
      "Epoch 438/1500\n",
      "34/34 [==============================] - 0s 700us/step - loss: 0.1591 - accuracy: 0.9346\n",
      "Epoch 439/1500\n",
      "34/34 [==============================] - 0s 703us/step - loss: 0.1512 - accuracy: 0.9466\n",
      "Epoch 440/1500\n",
      "34/34 [==============================] - 0s 693us/step - loss: 0.1396 - accuracy: 0.9475\n",
      "Epoch 441/1500\n",
      "34/34 [==============================] - 0s 703us/step - loss: 0.1540 - accuracy: 0.9429\n",
      "Epoch 442/1500\n",
      "34/34 [==============================] - 0s 693us/step - loss: 0.1436 - accuracy: 0.9443\n",
      "Epoch 443/1500\n",
      "34/34 [==============================] - 0s 710us/step - loss: 0.1477 - accuracy: 0.9438\n",
      "Epoch 444/1500\n",
      "34/34 [==============================] - 0s 700us/step - loss: 0.1553 - accuracy: 0.9378\n",
      "Epoch 445/1500\n",
      "34/34 [==============================] - 0s 714us/step - loss: 0.1532 - accuracy: 0.9360\n",
      "Epoch 446/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.0815 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 416.\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.1477 - accuracy: 0.9411\n",
      "Epoch 446: early stopping\n",
      "8/8 [==============================] - 0s 740us/step - loss: 0.9239 - accuracy: 0.6807\n",
      "8/8 [==============================] - 0s 745us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (23/29)\n",
      "Before appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "After appending - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n",
      "Final Test Results - Loss: 0.9239308834075928, Accuracy: 0.680672287940979, Precision: 0.671296159733079, Recall: 0.6600896940162791, F1 Score: 0.6604788615001661\n",
      "Confusion Matrix:\n",
      " [[119   3  25]\n",
      " [  3  26   0]\n",
      " [ 45   0  17]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6712363379825587\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.9088529273867607\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7050144076347351\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6806095574755359\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6902702615731574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.71 (78/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[kitten, adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, kitten...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, kitten, adult, kitten, kitten, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, kitten, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, kitten, senior, adult, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, kitten, senior,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "70    064A                             [adult, adult, kitten]         adult            adult                   True\n",
       "68    062A                     [kitten, kitten, adult, adult]         adult            adult                   True\n",
       "59    053A       [adult, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, senior, adult, ...         adult            adult                   True\n",
       "92    097A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, adult, senior,...        senior           senior                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "31    026A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A               [adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "41    035A                     [kitten, adult, kitten, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, senior, senior, senior, adult, ...         adult            adult                   True\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "90    095A  [senior, adult, senior, senior, senior, kitten...        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, senior,...        senior            adult                  False\n",
       "50    044A  [adult, adult, kitten, adult, kitten, kitten, ...         adult           kitten                  False\n",
       "36    029A  [senior, adult, adult, senior, adult, senior, ...        senior            adult                  False\n",
       "57    051B  [senior, adult, kitten, adult, senior, adult, ...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, se...         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "65    059A  [senior, senior, senior, senior, adult, adult,...         adult           senior                  False\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [senior, adult]         adult           senior                  False\n",
       "69    063A  [senior, kitten, senior, adult, adult, senior,...        senior            adult                  False\n",
       "71    065A  [senior, adult, kitten, adult, kitten, senior,...        kitten            adult                  False\n",
       "74    068A  [adult, senior, adult, senior, senior, adult, ...        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "109   117A  [adult, senior, adult, adult, senior, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     57\n",
      "kitten    13\n",
      "senior     8\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             57  78.082192\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22              8  36.363636\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d5976f8e-1c1f-4a9a-9a9a-7ef0f17c054a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          572            429  75.000000\n",
      "1           kitten          113             97  85.840708\n",
      "2           senior          178             78  43.820225\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1bfe9994-06d5-484f-a14f-8e22006f1c29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      148     69.48\n",
      "1          M    360      259     71.94\n",
      "2          X    290      197     67.93\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e374ff1a-cc4a-46a2-9ad1-341c7253b595",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      148     69.48\n",
      "1          M    360      259     71.94\n",
      "2          X    290      197     67.93\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 936, 1: 815, 2: 795})\n",
      "Epoch 1/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.1656 - accuracy: 0.5177\n",
      "Epoch 2/1500\n",
      "40/40 [==============================] - 0s 967us/step - loss: 0.9169 - accuracy: 0.5939\n",
      "Epoch 3/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.8266 - accuracy: 0.6386\n",
      "Epoch 4/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.7804 - accuracy: 0.6614\n",
      "Epoch 5/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.7394 - accuracy: 0.6795\n",
      "Epoch 6/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.7334 - accuracy: 0.6803\n",
      "Epoch 7/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.7168 - accuracy: 0.6811\n",
      "Epoch 8/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.6762 - accuracy: 0.7003\n",
      "Epoch 9/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.6439 - accuracy: 0.7258\n",
      "Epoch 10/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.6340 - accuracy: 0.7168\n",
      "Epoch 11/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.6359 - accuracy: 0.7196\n",
      "Epoch 12/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.6064 - accuracy: 0.7412\n",
      "Epoch 13/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.5853 - accuracy: 0.7486\n",
      "Epoch 14/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.5876 - accuracy: 0.7537\n",
      "Epoch 15/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.7502\n",
      "Epoch 16/1500\n",
      "40/40 [==============================] - 0s 892us/step - loss: 0.5641 - accuracy: 0.7592\n",
      "Epoch 17/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.5592 - accuracy: 0.7604\n",
      "Epoch 18/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.5468 - accuracy: 0.7628\n",
      "Epoch 19/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.5453 - accuracy: 0.7643\n",
      "Epoch 20/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.5349 - accuracy: 0.7655\n",
      "Epoch 21/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.5223 - accuracy: 0.7789\n",
      "Epoch 22/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.5449 - accuracy: 0.7698\n",
      "Epoch 23/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.5221 - accuracy: 0.7690\n",
      "Epoch 24/1500\n",
      "40/40 [==============================] - 0s 846us/step - loss: 0.5204 - accuracy: 0.7800\n",
      "Epoch 25/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.5186 - accuracy: 0.7789\n",
      "Epoch 26/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.5005 - accuracy: 0.7863\n",
      "Epoch 27/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.5053 - accuracy: 0.7793\n",
      "Epoch 28/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.4859 - accuracy: 0.7871\n",
      "Epoch 29/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.4921 - accuracy: 0.7879\n",
      "Epoch 30/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.5017 - accuracy: 0.7883\n",
      "Epoch 31/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.4847 - accuracy: 0.7887\n",
      "Epoch 32/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.4889 - accuracy: 0.7895\n",
      "Epoch 33/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.4666 - accuracy: 0.8020\n",
      "Epoch 34/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.4617 - accuracy: 0.7977\n",
      "Epoch 35/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.4554 - accuracy: 0.8126\n",
      "Epoch 36/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.4510 - accuracy: 0.8064\n",
      "Epoch 37/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.4554 - accuracy: 0.8099\n",
      "Epoch 38/1500\n",
      "40/40 [==============================] - 0s 720us/step - loss: 0.4577 - accuracy: 0.8020\n",
      "Epoch 39/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.4535 - accuracy: 0.8044\n",
      "Epoch 40/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.4489 - accuracy: 0.8040\n",
      "Epoch 41/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.4456 - accuracy: 0.8091\n",
      "Epoch 42/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.4494 - accuracy: 0.8099\n",
      "Epoch 43/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.4338 - accuracy: 0.8162\n",
      "Epoch 44/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.4406 - accuracy: 0.8091\n",
      "Epoch 45/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.4404 - accuracy: 0.8103\n",
      "Epoch 46/1500\n",
      "40/40 [==============================] - 0s 729us/step - loss: 0.4345 - accuracy: 0.8236\n",
      "Epoch 47/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.4288 - accuracy: 0.8166\n",
      "Epoch 48/1500\n",
      "40/40 [==============================] - 0s 900us/step - loss: 0.4458 - accuracy: 0.8083\n",
      "Epoch 49/1500\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.4484 - accuracy: 0.8071\n",
      "Epoch 50/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.4386 - accuracy: 0.8048\n",
      "Epoch 51/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.4208 - accuracy: 0.8244\n",
      "Epoch 52/1500\n",
      "40/40 [==============================] - 0s 912us/step - loss: 0.4339 - accuracy: 0.8142\n",
      "Epoch 53/1500\n",
      "40/40 [==============================] - 0s 892us/step - loss: 0.4204 - accuracy: 0.8170\n",
      "Epoch 54/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.4173 - accuracy: 0.8201\n",
      "Epoch 55/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.4188 - accuracy: 0.8209\n",
      "Epoch 56/1500\n",
      "40/40 [==============================] - 0s 885us/step - loss: 0.4180 - accuracy: 0.8299\n",
      "Epoch 57/1500\n",
      "40/40 [==============================] - 0s 926us/step - loss: 0.4194 - accuracy: 0.8248\n",
      "Epoch 58/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.4018 - accuracy: 0.8272\n",
      "Epoch 59/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.4037 - accuracy: 0.8303\n",
      "Epoch 60/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.3985 - accuracy: 0.8288\n",
      "Epoch 61/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.3894 - accuracy: 0.8362\n",
      "Epoch 62/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.4042 - accuracy: 0.8264\n",
      "Epoch 63/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.4084 - accuracy: 0.8295\n",
      "Epoch 64/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.3902 - accuracy: 0.8288\n",
      "Epoch 65/1500\n",
      "40/40 [==============================] - 0s 897us/step - loss: 0.3812 - accuracy: 0.8378\n",
      "Epoch 66/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.3996 - accuracy: 0.8288\n",
      "Epoch 67/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.3925 - accuracy: 0.8386\n",
      "Epoch 68/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.3890 - accuracy: 0.8327\n",
      "Epoch 69/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.3772 - accuracy: 0.8397\n",
      "Epoch 70/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.3766 - accuracy: 0.8303\n",
      "Epoch 71/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3855 - accuracy: 0.8382\n",
      "Epoch 72/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.3834 - accuracy: 0.8437\n",
      "Epoch 73/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.3745 - accuracy: 0.8492\n",
      "Epoch 74/1500\n",
      "40/40 [==============================] - 0s 852us/step - loss: 0.3815 - accuracy: 0.8390\n",
      "Epoch 75/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.3909 - accuracy: 0.8394\n",
      "Epoch 76/1500\n",
      "40/40 [==============================] - 0s 858us/step - loss: 0.3750 - accuracy: 0.8441\n",
      "Epoch 77/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.3743 - accuracy: 0.8445\n",
      "Epoch 78/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.3650 - accuracy: 0.8456\n",
      "Epoch 79/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.3715 - accuracy: 0.8433\n",
      "Epoch 80/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.3828 - accuracy: 0.8417\n",
      "Epoch 81/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.3692 - accuracy: 0.8449\n",
      "Epoch 82/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.3743 - accuracy: 0.8445\n",
      "Epoch 83/1500\n",
      "40/40 [==============================] - 0s 837us/step - loss: 0.3738 - accuracy: 0.8484\n",
      "Epoch 84/1500\n",
      "40/40 [==============================] - 0s 868us/step - loss: 0.3557 - accuracy: 0.8511\n",
      "Epoch 85/1500\n",
      "40/40 [==============================] - 0s 885us/step - loss: 0.3546 - accuracy: 0.8500\n",
      "Epoch 86/1500\n",
      "40/40 [==============================] - 0s 859us/step - loss: 0.3644 - accuracy: 0.8409\n",
      "Epoch 87/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.3530 - accuracy: 0.8519\n",
      "Epoch 88/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.3605 - accuracy: 0.8449\n",
      "Epoch 89/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.3641 - accuracy: 0.8441\n",
      "Epoch 90/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.3637 - accuracy: 0.8433\n",
      "Epoch 91/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.3663 - accuracy: 0.8472\n",
      "Epoch 92/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.3742 - accuracy: 0.8417\n",
      "Epoch 93/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.3627 - accuracy: 0.8460\n",
      "Epoch 94/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.3386 - accuracy: 0.8598\n",
      "Epoch 95/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.3493 - accuracy: 0.8598\n",
      "Epoch 96/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.3403 - accuracy: 0.8594\n",
      "Epoch 97/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.3454 - accuracy: 0.8555\n",
      "Epoch 98/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.3542 - accuracy: 0.8574\n",
      "Epoch 99/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.3484 - accuracy: 0.8543\n",
      "Epoch 100/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.3508 - accuracy: 0.8519\n",
      "Epoch 101/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.3518 - accuracy: 0.8488\n",
      "Epoch 102/1500\n",
      "40/40 [==============================] - 0s 830us/step - loss: 0.3517 - accuracy: 0.8551\n",
      "Epoch 103/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.3508 - accuracy: 0.8539\n",
      "Epoch 104/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.3411 - accuracy: 0.8598\n",
      "Epoch 105/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.3356 - accuracy: 0.8617\n",
      "Epoch 106/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.3451 - accuracy: 0.8574\n",
      "Epoch 107/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.3422 - accuracy: 0.8574\n",
      "Epoch 108/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.3549 - accuracy: 0.8527\n",
      "Epoch 109/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.3301 - accuracy: 0.8629\n",
      "Epoch 110/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3268 - accuracy: 0.8606\n",
      "Epoch 111/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.3447 - accuracy: 0.8629\n",
      "Epoch 112/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3370 - accuracy: 0.8562\n",
      "Epoch 113/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.3318 - accuracy: 0.8680\n",
      "Epoch 114/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.3221 - accuracy: 0.8653\n",
      "Epoch 115/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.3324 - accuracy: 0.8570\n",
      "Epoch 116/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.3355 - accuracy: 0.8606\n",
      "Epoch 117/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3191 - accuracy: 0.8712\n",
      "Epoch 118/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.3177 - accuracy: 0.8708\n",
      "Epoch 119/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.3405 - accuracy: 0.8602\n",
      "Epoch 120/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.3305 - accuracy: 0.8582\n",
      "Epoch 121/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.3283 - accuracy: 0.8708\n",
      "Epoch 122/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.3350 - accuracy: 0.8641\n",
      "Epoch 123/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.3195 - accuracy: 0.8723\n",
      "Epoch 124/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.3158 - accuracy: 0.8727\n",
      "Epoch 125/1500\n",
      "40/40 [==============================] - 0s 717us/step - loss: 0.3316 - accuracy: 0.8614\n",
      "Epoch 126/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.3248 - accuracy: 0.8629\n",
      "Epoch 127/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.3155 - accuracy: 0.8637\n",
      "Epoch 128/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.3224 - accuracy: 0.8629\n",
      "Epoch 129/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.3134 - accuracy: 0.8743\n",
      "Epoch 130/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.3150 - accuracy: 0.8594\n",
      "Epoch 131/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.3121 - accuracy: 0.8649\n",
      "Epoch 132/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.3178 - accuracy: 0.8684\n",
      "Epoch 133/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.3184 - accuracy: 0.8680\n",
      "Epoch 134/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.3088 - accuracy: 0.8731\n",
      "Epoch 135/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.3145 - accuracy: 0.8747\n",
      "Epoch 136/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3110 - accuracy: 0.8731\n",
      "Epoch 137/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.3033 - accuracy: 0.8751\n",
      "Epoch 138/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.3109 - accuracy: 0.8735\n",
      "Epoch 139/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.3156 - accuracy: 0.8696\n",
      "Epoch 140/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3007 - accuracy: 0.8700\n",
      "Epoch 141/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2951 - accuracy: 0.8810\n",
      "Epoch 142/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.3090 - accuracy: 0.8712\n",
      "Epoch 143/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3069 - accuracy: 0.8665\n",
      "Epoch 144/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3090 - accuracy: 0.8735\n",
      "Epoch 145/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2970 - accuracy: 0.8818\n",
      "Epoch 146/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2936 - accuracy: 0.8731\n",
      "Epoch 147/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.3052 - accuracy: 0.8767\n",
      "Epoch 148/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.3008 - accuracy: 0.8771\n",
      "Epoch 149/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.3000 - accuracy: 0.8763\n",
      "Epoch 150/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.2963 - accuracy: 0.8802\n",
      "Epoch 151/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2903 - accuracy: 0.8794\n",
      "Epoch 152/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2964 - accuracy: 0.8818\n",
      "Epoch 153/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3025 - accuracy: 0.8763\n",
      "Epoch 154/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2969 - accuracy: 0.8814\n",
      "Epoch 155/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.3030 - accuracy: 0.8712\n",
      "Epoch 156/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.3052 - accuracy: 0.8751\n",
      "Epoch 157/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.2933 - accuracy: 0.8794\n",
      "Epoch 158/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.2862 - accuracy: 0.8849\n",
      "Epoch 159/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.2910 - accuracy: 0.8885\n",
      "Epoch 160/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.2992 - accuracy: 0.8763\n",
      "Epoch 161/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2985 - accuracy: 0.8700\n",
      "Epoch 162/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.3043 - accuracy: 0.8790\n",
      "Epoch 163/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.2927 - accuracy: 0.8786\n",
      "Epoch 164/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2912 - accuracy: 0.8853\n",
      "Epoch 165/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.2929 - accuracy: 0.8798\n",
      "Epoch 166/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2776 - accuracy: 0.8845\n",
      "Epoch 167/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.2822 - accuracy: 0.8912\n",
      "Epoch 168/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2892 - accuracy: 0.8763\n",
      "Epoch 169/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2880 - accuracy: 0.8759\n",
      "Epoch 170/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2792 - accuracy: 0.8857\n",
      "Epoch 171/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2724 - accuracy: 0.8873\n",
      "Epoch 172/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.2921 - accuracy: 0.8845\n",
      "Epoch 173/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2703 - accuracy: 0.8940\n",
      "Epoch 174/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2793 - accuracy: 0.8896\n",
      "Epoch 175/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2901 - accuracy: 0.8790\n",
      "Epoch 176/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2865 - accuracy: 0.8782\n",
      "Epoch 177/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.2773 - accuracy: 0.8853\n",
      "Epoch 178/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.2717 - accuracy: 0.8900\n",
      "Epoch 179/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.2832 - accuracy: 0.8806\n",
      "Epoch 180/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2753 - accuracy: 0.8881\n",
      "Epoch 181/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2831 - accuracy: 0.8837\n",
      "Epoch 182/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.2618 - accuracy: 0.8947\n",
      "Epoch 183/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.2704 - accuracy: 0.8912\n",
      "Epoch 184/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.2647 - accuracy: 0.8912\n",
      "Epoch 185/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2704 - accuracy: 0.8841\n",
      "Epoch 186/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2737 - accuracy: 0.8924\n",
      "Epoch 187/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.2761 - accuracy: 0.8857\n",
      "Epoch 188/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2676 - accuracy: 0.8865\n",
      "Epoch 189/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2639 - accuracy: 0.8943\n",
      "Epoch 190/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2632 - accuracy: 0.8932\n",
      "Epoch 191/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.2700 - accuracy: 0.8885\n",
      "Epoch 192/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2724 - accuracy: 0.8920\n",
      "Epoch 193/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2669 - accuracy: 0.8904\n",
      "Epoch 194/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.2592 - accuracy: 0.8924\n",
      "Epoch 195/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.2580 - accuracy: 0.9002\n",
      "Epoch 196/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.2604 - accuracy: 0.8943\n",
      "Epoch 197/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2575 - accuracy: 0.9022\n",
      "Epoch 198/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.2551 - accuracy: 0.8975\n",
      "Epoch 199/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2716 - accuracy: 0.8869\n",
      "Epoch 200/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.2618 - accuracy: 0.8940\n",
      "Epoch 201/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.2629 - accuracy: 0.8928\n",
      "Epoch 202/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.2593 - accuracy: 0.8967\n",
      "Epoch 203/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.2669 - accuracy: 0.8975\n",
      "Epoch 204/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2644 - accuracy: 0.8892\n",
      "Epoch 205/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2510 - accuracy: 0.8998\n",
      "Epoch 206/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2661 - accuracy: 0.8936\n",
      "Epoch 207/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.2571 - accuracy: 0.8951\n",
      "Epoch 208/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.2525 - accuracy: 0.9030\n",
      "Epoch 209/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2513 - accuracy: 0.8991\n",
      "Epoch 210/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.2468 - accuracy: 0.9026\n",
      "Epoch 211/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2632 - accuracy: 0.8928\n",
      "Epoch 212/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.2399 - accuracy: 0.9022\n",
      "Epoch 213/1500\n",
      "40/40 [==============================] - 0s 726us/step - loss: 0.2588 - accuracy: 0.8916\n",
      "Epoch 214/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.2607 - accuracy: 0.8987\n",
      "Epoch 215/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2411 - accuracy: 0.8975\n",
      "Epoch 216/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.2513 - accuracy: 0.9049\n",
      "Epoch 217/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.2457 - accuracy: 0.8987\n",
      "Epoch 218/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.2385 - accuracy: 0.9034\n",
      "Epoch 219/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.2533 - accuracy: 0.8940\n",
      "Epoch 220/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2436 - accuracy: 0.9006\n",
      "Epoch 221/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.2433 - accuracy: 0.9002\n",
      "Epoch 222/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2473 - accuracy: 0.9038\n",
      "Epoch 223/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.2556 - accuracy: 0.9030\n",
      "Epoch 224/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.2502 - accuracy: 0.8955\n",
      "Epoch 225/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2497 - accuracy: 0.8991\n",
      "Epoch 226/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2445 - accuracy: 0.9026\n",
      "Epoch 227/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.2406 - accuracy: 0.9049\n",
      "Epoch 228/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.2538 - accuracy: 0.8983\n",
      "Epoch 229/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9018\n",
      "Epoch 230/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9038\n",
      "Epoch 231/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.2550 - accuracy: 0.9010\n",
      "Epoch 232/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2364 - accuracy: 0.9081\n",
      "Epoch 233/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2363 - accuracy: 0.9042\n",
      "Epoch 234/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.2546 - accuracy: 0.8928\n",
      "Epoch 235/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2404 - accuracy: 0.8995\n",
      "Epoch 236/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.2335 - accuracy: 0.9030\n",
      "Epoch 237/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2363 - accuracy: 0.9046\n",
      "Epoch 238/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.2604 - accuracy: 0.8979\n",
      "Epoch 239/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2505 - accuracy: 0.8940\n",
      "Epoch 240/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.2392 - accuracy: 0.9053\n",
      "Epoch 241/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2308 - accuracy: 0.9042\n",
      "Epoch 242/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.2376 - accuracy: 0.9057\n",
      "Epoch 243/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2294 - accuracy: 0.9081\n",
      "Epoch 244/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2418 - accuracy: 0.9057\n",
      "Epoch 245/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2326 - accuracy: 0.9093\n",
      "Epoch 246/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2291 - accuracy: 0.9061\n",
      "Epoch 247/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2245 - accuracy: 0.9104\n",
      "Epoch 248/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2436 - accuracy: 0.8995\n",
      "Epoch 249/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2366 - accuracy: 0.9101\n",
      "Epoch 250/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2339 - accuracy: 0.9022\n",
      "Epoch 251/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.2448 - accuracy: 0.8967\n",
      "Epoch 252/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2322 - accuracy: 0.9061\n",
      "Epoch 253/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.2376 - accuracy: 0.9026\n",
      "Epoch 254/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.2494 - accuracy: 0.9026\n",
      "Epoch 255/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2278 - accuracy: 0.9081\n",
      "Epoch 256/1500\n",
      "40/40 [==============================] - 0s 719us/step - loss: 0.2286 - accuracy: 0.9116\n",
      "Epoch 257/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.2332 - accuracy: 0.9042\n",
      "Epoch 258/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2212 - accuracy: 0.9116\n",
      "Epoch 259/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2356 - accuracy: 0.8963\n",
      "Epoch 260/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.2231 - accuracy: 0.9069\n",
      "Epoch 261/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2215 - accuracy: 0.9167\n",
      "Epoch 262/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.2338 - accuracy: 0.9077\n",
      "Epoch 263/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2308 - accuracy: 0.9061\n",
      "Epoch 264/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2302 - accuracy: 0.9073\n",
      "Epoch 265/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.2290 - accuracy: 0.9136\n",
      "Epoch 266/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2149 - accuracy: 0.9140\n",
      "Epoch 267/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.2321 - accuracy: 0.9069\n",
      "Epoch 268/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2245 - accuracy: 0.9089\n",
      "Epoch 269/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.2302 - accuracy: 0.9046\n",
      "Epoch 270/1500\n",
      "40/40 [==============================] - 0s 696us/step - loss: 0.2355 - accuracy: 0.9065\n",
      "Epoch 271/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2249 - accuracy: 0.9128\n",
      "Epoch 272/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2131 - accuracy: 0.9175\n",
      "Epoch 273/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2214 - accuracy: 0.9104\n",
      "Epoch 274/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.2335 - accuracy: 0.9034\n",
      "Epoch 275/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.2249 - accuracy: 0.9093\n",
      "Epoch 276/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2188 - accuracy: 0.9081\n",
      "Epoch 277/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.2225 - accuracy: 0.9049\n",
      "Epoch 278/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.2263 - accuracy: 0.9104\n",
      "Epoch 279/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.2215 - accuracy: 0.9120\n",
      "Epoch 280/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.2124 - accuracy: 0.9218\n",
      "Epoch 281/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.2131 - accuracy: 0.9124\n",
      "Epoch 282/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2244 - accuracy: 0.9124\n",
      "Epoch 283/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2069 - accuracy: 0.9152\n",
      "Epoch 284/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.2173 - accuracy: 0.9132\n",
      "Epoch 285/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2186 - accuracy: 0.9104\n",
      "Epoch 286/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2319 - accuracy: 0.9042\n",
      "Epoch 287/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2232 - accuracy: 0.9140\n",
      "Epoch 288/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.2254 - accuracy: 0.9077\n",
      "Epoch 289/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2138 - accuracy: 0.9116\n",
      "Epoch 290/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.2135 - accuracy: 0.9183\n",
      "Epoch 291/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.2096 - accuracy: 0.9156\n",
      "Epoch 292/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.2123 - accuracy: 0.9167\n",
      "Epoch 293/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2079 - accuracy: 0.9183\n",
      "Epoch 294/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2265 - accuracy: 0.9124\n",
      "Epoch 295/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2190 - accuracy: 0.9152\n",
      "Epoch 296/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.2133 - accuracy: 0.9136\n",
      "Epoch 297/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2094 - accuracy: 0.9152\n",
      "Epoch 298/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2028 - accuracy: 0.9163\n",
      "Epoch 299/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9191\n",
      "Epoch 300/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2092 - accuracy: 0.9187\n",
      "Epoch 301/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.2092 - accuracy: 0.9132\n",
      "Epoch 302/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.2222 - accuracy: 0.9136\n",
      "Epoch 303/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.1971 - accuracy: 0.9226\n",
      "Epoch 304/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.2055 - accuracy: 0.9214\n",
      "Epoch 305/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.1944 - accuracy: 0.9273\n",
      "Epoch 306/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2126 - accuracy: 0.9144\n",
      "Epoch 307/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.2061 - accuracy: 0.9222\n",
      "Epoch 308/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.1998 - accuracy: 0.9234\n",
      "Epoch 309/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.2101 - accuracy: 0.9159\n",
      "Epoch 310/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2133 - accuracy: 0.9140\n",
      "Epoch 311/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.2129 - accuracy: 0.9175\n",
      "Epoch 312/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.2116 - accuracy: 0.9191\n",
      "Epoch 313/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.2048 - accuracy: 0.9183\n",
      "Epoch 314/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.2057 - accuracy: 0.9191\n",
      "Epoch 315/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2063 - accuracy: 0.9203\n",
      "Epoch 316/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1932 - accuracy: 0.9234\n",
      "Epoch 317/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.2065 - accuracy: 0.9187\n",
      "Epoch 318/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2020 - accuracy: 0.9234\n",
      "Epoch 319/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.2009 - accuracy: 0.9203\n",
      "Epoch 320/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.2116 - accuracy: 0.9156\n",
      "Epoch 321/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1998 - accuracy: 0.9195\n",
      "Epoch 322/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.1984 - accuracy: 0.9203\n",
      "Epoch 323/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.1932 - accuracy: 0.9250\n",
      "Epoch 324/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.2029 - accuracy: 0.9195\n",
      "Epoch 325/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2051 - accuracy: 0.9222\n",
      "Epoch 326/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.1955 - accuracy: 0.9266\n",
      "Epoch 327/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.2009 - accuracy: 0.9175\n",
      "Epoch 328/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2097 - accuracy: 0.9250\n",
      "Epoch 329/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.2030 - accuracy: 0.9222\n",
      "Epoch 330/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.2049 - accuracy: 0.9171\n",
      "Epoch 331/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.2080 - accuracy: 0.9108\n",
      "Epoch 332/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.2045 - accuracy: 0.9175\n",
      "Epoch 333/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.2006 - accuracy: 0.9183\n",
      "Epoch 334/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.2001 - accuracy: 0.9214\n",
      "Epoch 335/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.2082 - accuracy: 0.9156\n",
      "Epoch 336/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.2119 - accuracy: 0.9112\n",
      "Epoch 337/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.1903 - accuracy: 0.9250\n",
      "Epoch 338/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.2005 - accuracy: 0.9144\n",
      "Epoch 339/1500\n",
      "40/40 [==============================] - 0s 678us/step - loss: 0.2007 - accuracy: 0.9234\n",
      "Epoch 340/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.1919 - accuracy: 0.9246\n",
      "Epoch 341/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.2085 - accuracy: 0.9156\n",
      "Epoch 342/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.1926 - accuracy: 0.9222\n",
      "Epoch 343/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.1981 - accuracy: 0.9234\n",
      "Epoch 344/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.2033 - accuracy: 0.9199\n",
      "Epoch 345/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.1931 - accuracy: 0.9230\n",
      "Epoch 346/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.1938 - accuracy: 0.9262\n",
      "Epoch 347/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.1815 - accuracy: 0.9321\n",
      "Epoch 348/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.2151 - accuracy: 0.9120\n",
      "Epoch 349/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.1971 - accuracy: 0.9211\n",
      "Epoch 350/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.1896 - accuracy: 0.9238\n",
      "Epoch 351/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.1993 - accuracy: 0.9203\n",
      "Epoch 352/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.1955 - accuracy: 0.9313\n",
      "Epoch 353/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.1824 - accuracy: 0.9321\n",
      "Epoch 354/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.1932 - accuracy: 0.9230\n",
      "Epoch 355/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2024 - accuracy: 0.9230\n",
      "Epoch 356/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.1990 - accuracy: 0.9222\n",
      "Epoch 357/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2132 - accuracy: 0.9128\n",
      "Epoch 358/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.1846 - accuracy: 0.9324\n",
      "Epoch 359/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1930 - accuracy: 0.9207\n",
      "Epoch 360/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1726 - accuracy: 0.9356\n",
      "Epoch 361/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2020 - accuracy: 0.9199\n",
      "Epoch 362/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.1980 - accuracy: 0.9246\n",
      "Epoch 363/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.1944 - accuracy: 0.9281\n",
      "Epoch 364/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.1903 - accuracy: 0.9246\n",
      "Epoch 365/1500\n",
      "40/40 [==============================] - 0s 851us/step - loss: 0.1942 - accuracy: 0.9203\n",
      "Epoch 366/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.1971 - accuracy: 0.9214\n",
      "Epoch 367/1500\n",
      "40/40 [==============================] - 0s 830us/step - loss: 0.1999 - accuracy: 0.9281\n",
      "Epoch 368/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.1895 - accuracy: 0.9289\n",
      "Epoch 369/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.1943 - accuracy: 0.9262\n",
      "Epoch 370/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.1983 - accuracy: 0.9258\n",
      "Epoch 371/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.1774 - accuracy: 0.9281\n",
      "Epoch 372/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2111 - accuracy: 0.9140\n",
      "Epoch 373/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.1927 - accuracy: 0.9214\n",
      "Epoch 374/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.1962 - accuracy: 0.9238\n",
      "Epoch 375/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.1948 - accuracy: 0.9203\n",
      "Epoch 376/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.1882 - accuracy: 0.9262\n",
      "Epoch 377/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.1797 - accuracy: 0.9269\n",
      "Epoch 378/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.1944 - accuracy: 0.9246\n",
      "Epoch 379/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.1944 - accuracy: 0.9281\n",
      "Epoch 380/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.2005 - accuracy: 0.9226\n",
      "Epoch 381/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.1766 - accuracy: 0.9262\n",
      "Epoch 382/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.1886 - accuracy: 0.9293\n",
      "Epoch 383/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.1946 - accuracy: 0.9218\n",
      "Epoch 384/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.1933 - accuracy: 0.9281\n",
      "Epoch 385/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.1795 - accuracy: 0.9289\n",
      "Epoch 386/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.1748 - accuracy: 0.9360\n",
      "Epoch 387/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.1694 - accuracy: 0.9336\n",
      "Epoch 388/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.1734 - accuracy: 0.9356\n",
      "Epoch 389/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1867 - accuracy: 0.9250\n",
      "Epoch 390/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.1816 - accuracy: 0.9273\n",
      "Epoch 391/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.1840 - accuracy: 0.9309\n",
      "Epoch 392/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.1933 - accuracy: 0.9230\n",
      "Epoch 393/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.1740 - accuracy: 0.9309\n",
      "Epoch 394/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.1963 - accuracy: 0.9195\n",
      "Epoch 395/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1653 - accuracy: 0.9324\n",
      "Epoch 396/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.1779 - accuracy: 0.9321\n",
      "Epoch 397/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.1824 - accuracy: 0.9324\n",
      "Epoch 398/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.1822 - accuracy: 0.9250\n",
      "Epoch 399/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.1840 - accuracy: 0.9234\n",
      "Epoch 400/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1847 - accuracy: 0.9348\n",
      "Epoch 401/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.1766 - accuracy: 0.9321\n",
      "Epoch 402/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.1759 - accuracy: 0.9321\n",
      "Epoch 403/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.1696 - accuracy: 0.9332\n",
      "Epoch 404/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.1756 - accuracy: 0.9360\n",
      "Epoch 405/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.1770 - accuracy: 0.9364\n",
      "Epoch 406/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.1755 - accuracy: 0.9372\n",
      "Epoch 407/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.1970 - accuracy: 0.9281\n",
      "Epoch 408/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.1705 - accuracy: 0.9332\n",
      "Epoch 409/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.1853 - accuracy: 0.9269\n",
      "Epoch 410/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.1769 - accuracy: 0.9360\n",
      "Epoch 411/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.1825 - accuracy: 0.9250\n",
      "Epoch 412/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.1787 - accuracy: 0.9301\n",
      "Epoch 413/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1776 - accuracy: 0.9234\n",
      "Epoch 414/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1986 - accuracy: 0.9199\n",
      "Epoch 415/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.1684 - accuracy: 0.9305\n",
      "Epoch 416/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.1809 - accuracy: 0.9305\n",
      "Epoch 417/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.1719 - accuracy: 0.9301\n",
      "Epoch 418/1500\n",
      "40/40 [==============================] - 0s 722us/step - loss: 0.1745 - accuracy: 0.9321\n",
      "Epoch 419/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.1849 - accuracy: 0.9262\n",
      "Epoch 420/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.1826 - accuracy: 0.9281\n",
      "Epoch 421/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.1817 - accuracy: 0.9332\n",
      "Epoch 422/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9301\n",
      "Epoch 423/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9317\n",
      "Epoch 424/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1565 - accuracy: 0.9375\n",
      "Epoch 425/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.1852 - accuracy: 0.9277\n",
      "Epoch 426/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1657 - accuracy: 0.9293\n",
      "Epoch 427/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1676 - accuracy: 0.9364\n",
      "Epoch 428/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.1736 - accuracy: 0.9289\n",
      "Epoch 429/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.1803 - accuracy: 0.9317\n",
      "Epoch 430/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.1679 - accuracy: 0.9356\n",
      "Epoch 431/1500\n",
      "40/40 [==============================] - 0s 706us/step - loss: 0.1803 - accuracy: 0.9340\n",
      "Epoch 432/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.1755 - accuracy: 0.9277\n",
      "Epoch 433/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.1839 - accuracy: 0.9262\n",
      "Epoch 434/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.1664 - accuracy: 0.9344\n",
      "Epoch 435/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.1785 - accuracy: 0.9281\n",
      "Epoch 436/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.1878 - accuracy: 0.9297\n",
      "Epoch 437/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.1904 - accuracy: 0.9203\n",
      "Epoch 438/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.1657 - accuracy: 0.9360\n",
      "Epoch 439/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.1595 - accuracy: 0.9368\n",
      "Epoch 440/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1688 - accuracy: 0.9356\n",
      "Epoch 441/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.1648 - accuracy: 0.9387\n",
      "Epoch 442/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1860 - accuracy: 0.9250\n",
      "Epoch 443/1500\n",
      "40/40 [==============================] - 0s 719us/step - loss: 0.1721 - accuracy: 0.9344\n",
      "Epoch 444/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.1806 - accuracy: 0.9266\n",
      "Epoch 445/1500\n",
      "40/40 [==============================] - 0s 719us/step - loss: 0.1775 - accuracy: 0.9266\n",
      "Epoch 446/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.1834 - accuracy: 0.9281\n",
      "Epoch 447/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.1739 - accuracy: 0.9352\n",
      "Epoch 448/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.1829 - accuracy: 0.9281\n",
      "Epoch 449/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.1700 - accuracy: 0.9344\n",
      "Epoch 450/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.1616 - accuracy: 0.9364\n",
      "Epoch 451/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.1544 - accuracy: 0.9407\n",
      "Epoch 452/1500\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.1677 - accuracy: 0.9317\n",
      "Epoch 453/1500\n",
      "40/40 [==============================] - 0s 856us/step - loss: 0.1648 - accuracy: 0.9336\n",
      "Epoch 454/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.1636 - accuracy: 0.9375\n",
      "Epoch 455/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.1691 - accuracy: 0.9332\n",
      "Epoch 456/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.1727 - accuracy: 0.9301\n",
      "Epoch 457/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.1744 - accuracy: 0.9360\n",
      "Epoch 458/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.1682 - accuracy: 0.9379\n",
      "Epoch 459/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.1644 - accuracy: 0.9391\n",
      "Epoch 460/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.1674 - accuracy: 0.9375\n",
      "Epoch 461/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.1674 - accuracy: 0.9321\n",
      "Epoch 462/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.1738 - accuracy: 0.9281\n",
      "Epoch 463/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.1639 - accuracy: 0.9364\n",
      "Epoch 464/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.1707 - accuracy: 0.9332\n",
      "Epoch 465/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.1729 - accuracy: 0.9305\n",
      "Epoch 466/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.1619 - accuracy: 0.9364\n",
      "Epoch 467/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.1749 - accuracy: 0.9309\n",
      "Epoch 468/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.1707 - accuracy: 0.9352\n",
      "Epoch 469/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.1813 - accuracy: 0.9281\n",
      "Epoch 470/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.1683 - accuracy: 0.9360\n",
      "Epoch 471/1500\n",
      "40/40 [==============================] - 0s 891us/step - loss: 0.1690 - accuracy: 0.9352\n",
      "Epoch 472/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.1839 - accuracy: 0.9313\n",
      "Epoch 473/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.1529 - accuracy: 0.9368\n",
      "Epoch 474/1500\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.1574 - accuracy: 0.9442\n",
      "Epoch 475/1500\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.1763 - accuracy: 0.9344\n",
      "Epoch 476/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.1810 - accuracy: 0.9246\n",
      "Epoch 477/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.1515 - accuracy: 0.9391\n",
      "Epoch 478/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.1549 - accuracy: 0.9411\n",
      "Epoch 479/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.1620 - accuracy: 0.9352\n",
      "Epoch 480/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.1633 - accuracy: 0.9372\n",
      "Epoch 481/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.1588 - accuracy: 0.9391\n",
      "Epoch 482/1500\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1768 - accuracy: 0.9250\n",
      "Epoch 483/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9324\n",
      "Epoch 484/1500\n",
      "40/40 [==============================] - 0s 847us/step - loss: 0.1641 - accuracy: 0.9364\n",
      "Epoch 485/1500\n",
      "40/40 [==============================] - 0s 885us/step - loss: 0.1626 - accuracy: 0.9364\n",
      "Epoch 486/1500\n",
      "40/40 [==============================] - 0s 907us/step - loss: 0.1698 - accuracy: 0.9336\n",
      "Epoch 487/1500\n",
      "40/40 [==============================] - 0s 973us/step - loss: 0.1665 - accuracy: 0.9344\n",
      "Epoch 488/1500\n",
      "40/40 [==============================] - 0s 852us/step - loss: 0.1647 - accuracy: 0.9364\n",
      "Epoch 489/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.1580 - accuracy: 0.9372\n",
      "Epoch 490/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.1544 - accuracy: 0.9419\n",
      "Epoch 491/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.1687 - accuracy: 0.9372\n",
      "Epoch 492/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.1527 - accuracy: 0.9407\n",
      "Epoch 493/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.1689 - accuracy: 0.9336\n",
      "Epoch 494/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.1608 - accuracy: 0.9415\n",
      "Epoch 495/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.1676 - accuracy: 0.9407\n",
      "Epoch 496/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.1517 - accuracy: 0.9407\n",
      "Epoch 497/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.1388 - accuracy: 0.9454\n",
      "Epoch 498/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1625 - accuracy: 0.9395\n",
      "Epoch 499/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.1604 - accuracy: 0.9368\n",
      "Epoch 500/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.1647 - accuracy: 0.9427\n",
      "Epoch 501/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.1606 - accuracy: 0.9356\n",
      "Epoch 502/1500\n",
      "40/40 [==============================] - 0s 851us/step - loss: 0.1674 - accuracy: 0.9379\n",
      "Epoch 503/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.1487 - accuracy: 0.9442\n",
      "Epoch 504/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.1573 - accuracy: 0.9387\n",
      "Epoch 505/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.1589 - accuracy: 0.9383\n",
      "Epoch 506/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.1577 - accuracy: 0.9387\n",
      "Epoch 507/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.1401 - accuracy: 0.9438\n",
      "Epoch 508/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.1560 - accuracy: 0.9423\n",
      "Epoch 509/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.1473 - accuracy: 0.9462\n",
      "Epoch 510/1500\n",
      "40/40 [==============================] - 0s 893us/step - loss: 0.1711 - accuracy: 0.9297\n",
      "Epoch 511/1500\n",
      "40/40 [==============================] - 0s 828us/step - loss: 0.1567 - accuracy: 0.9395\n",
      "Epoch 512/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.1660 - accuracy: 0.9368\n",
      "Epoch 513/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.1414 - accuracy: 0.9470\n",
      "Epoch 514/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.1667 - accuracy: 0.9391\n",
      "Epoch 515/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.1535 - accuracy: 0.9391\n",
      "Epoch 516/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.1470 - accuracy: 0.9497\n",
      "Epoch 517/1500\n",
      "40/40 [==============================] - 0s 868us/step - loss: 0.1528 - accuracy: 0.9391\n",
      "Epoch 518/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.1449 - accuracy: 0.9458\n",
      "Epoch 519/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.1618 - accuracy: 0.9375\n",
      "Epoch 520/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.1494 - accuracy: 0.9387\n",
      "Epoch 521/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.1655 - accuracy: 0.9340\n",
      "Epoch 522/1500\n",
      "40/40 [==============================] - 0s 829us/step - loss: 0.1562 - accuracy: 0.9383\n",
      "Epoch 523/1500\n",
      "40/40 [==============================] - 0s 863us/step - loss: 0.1463 - accuracy: 0.9442\n",
      "Epoch 524/1500\n",
      "40/40 [==============================] - 0s 924us/step - loss: 0.1511 - accuracy: 0.9434\n",
      "Epoch 525/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.1509 - accuracy: 0.9419\n",
      "Epoch 526/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.1700 - accuracy: 0.9328\n",
      "Epoch 527/1500\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.1322 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 497.\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.1637 - accuracy: 0.9328\n",
      "Epoch 527: early stopping\n",
      "5/5 [==============================] - 0s 902us/step - loss: 0.6577 - accuracy: 0.7891\n",
      "5/5 [==============================] - 0s 737us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.6576976180076599, Accuracy: 0.7891156673431396, Precision: 0.6598509691293197, Recall: 0.8548245614035087, F1 Score: 0.7182539682539683\n",
      "Confusion Matrix:\n",
      " [[93  5 22]\n",
      " [ 0  8  0]\n",
      " [ 4  0 15]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 842, 1: 770, 2: 770})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 1.1848 - accuracy: 0.4920\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.9670 - accuracy: 0.5743\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.8933 - accuracy: 0.6087\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.8191 - accuracy: 0.6373\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.7829 - accuracy: 0.6612\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.7475 - accuracy: 0.6772\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.7058 - accuracy: 0.6835\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.7297 - accuracy: 0.6860\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.6846 - accuracy: 0.7124\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.6643 - accuracy: 0.7170\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.6600 - accuracy: 0.7267\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.6517 - accuracy: 0.7250\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.6397 - accuracy: 0.7309\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.5911 - accuracy: 0.7523\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.6165 - accuracy: 0.7448\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.5772 - accuracy: 0.7498\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.5954 - accuracy: 0.7431\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.5792 - accuracy: 0.7531\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.5638 - accuracy: 0.7544\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.5542 - accuracy: 0.7523\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.5499 - accuracy: 0.7620\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.5422 - accuracy: 0.7720\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.5324 - accuracy: 0.7657\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.5353 - accuracy: 0.7741\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.5333 - accuracy: 0.7712\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.5138 - accuracy: 0.7809\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.5231 - accuracy: 0.7741\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.4931 - accuracy: 0.7893\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.5087 - accuracy: 0.7834\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.4902 - accuracy: 0.7884\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.4886 - accuracy: 0.7901\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.5016 - accuracy: 0.7947\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.4986 - accuracy: 0.7914\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.4822 - accuracy: 0.7939\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.4841 - accuracy: 0.7905\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.4792 - accuracy: 0.7914\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.4760 - accuracy: 0.7893\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.4843 - accuracy: 0.7918\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.4729 - accuracy: 0.7935\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.4554 - accuracy: 0.7976\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.4530 - accuracy: 0.8023\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.4531 - accuracy: 0.8069\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.4494 - accuracy: 0.8102\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.4601 - accuracy: 0.8031\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.4594 - accuracy: 0.8081\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.4361 - accuracy: 0.8123\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.4527 - accuracy: 0.8128\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.4426 - accuracy: 0.8052\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.4407 - accuracy: 0.8144\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.4451 - accuracy: 0.8077\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.4520 - accuracy: 0.8048\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.4173 - accuracy: 0.8203\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.4406 - accuracy: 0.8081\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.4270 - accuracy: 0.8195\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.4376 - accuracy: 0.8136\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.4370 - accuracy: 0.8165\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.4278 - accuracy: 0.8203\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.8182\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.8060\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.4109 - accuracy: 0.8287\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4156 - accuracy: 0.8182\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.4110 - accuracy: 0.8262\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.4068 - accuracy: 0.8258\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.4199 - accuracy: 0.8132\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.4064 - accuracy: 0.8203\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.4265 - accuracy: 0.8132\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.4244 - accuracy: 0.8254\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.3981 - accuracy: 0.8291\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.4019 - accuracy: 0.8262\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3986 - accuracy: 0.8371\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3981 - accuracy: 0.8266\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.4050 - accuracy: 0.8237\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3979 - accuracy: 0.8304\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.3885 - accuracy: 0.8296\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3976 - accuracy: 0.8258\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.4053 - accuracy: 0.8237\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.3854 - accuracy: 0.8342\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.3823 - accuracy: 0.8354\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.3947 - accuracy: 0.8207\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3675 - accuracy: 0.8417\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.3819 - accuracy: 0.8417\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.3833 - accuracy: 0.8312\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.3802 - accuracy: 0.8396\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.3822 - accuracy: 0.8367\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3761 - accuracy: 0.8346\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3773 - accuracy: 0.8371\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.3662 - accuracy: 0.8434\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.3859 - accuracy: 0.8321\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3717 - accuracy: 0.8455\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3749 - accuracy: 0.8434\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.3667 - accuracy: 0.8396\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.3593 - accuracy: 0.8463\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.3741 - accuracy: 0.8363\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3815 - accuracy: 0.8388\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.3526 - accuracy: 0.8480\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.3544 - accuracy: 0.8510\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.3517 - accuracy: 0.8526\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3572 - accuracy: 0.8493\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.3733 - accuracy: 0.8350\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3563 - accuracy: 0.8451\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.3463 - accuracy: 0.8501\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.3614 - accuracy: 0.8430\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.3445 - accuracy: 0.8526\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.3426 - accuracy: 0.8531\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.3566 - accuracy: 0.8484\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.3476 - accuracy: 0.8535\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.3370 - accuracy: 0.8623\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.3433 - accuracy: 0.8560\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.3473 - accuracy: 0.8497\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.3471 - accuracy: 0.8535\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.3379 - accuracy: 0.8652\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.3441 - accuracy: 0.8518\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3321 - accuracy: 0.8636\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.3416 - accuracy: 0.8543\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.3369 - accuracy: 0.8564\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.3423 - accuracy: 0.8581\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.3303 - accuracy: 0.8589\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3459 - accuracy: 0.8484\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.3528 - accuracy: 0.8472\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.3424 - accuracy: 0.8623\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.3446 - accuracy: 0.8568\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.3360 - accuracy: 0.8505\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.3225 - accuracy: 0.8657\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.3296 - accuracy: 0.8623\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.3193 - accuracy: 0.8606\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.3337 - accuracy: 0.8606\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3241 - accuracy: 0.8640\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3240 - accuracy: 0.8720\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3168 - accuracy: 0.8657\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.3180 - accuracy: 0.8678\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3173 - accuracy: 0.8657\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.3245 - accuracy: 0.8644\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.3120 - accuracy: 0.8753\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3168 - accuracy: 0.8707\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.3193 - accuracy: 0.8673\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.3147 - accuracy: 0.8682\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.3032 - accuracy: 0.8749\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.3079 - accuracy: 0.8678\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.3075 - accuracy: 0.8657\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.3163 - accuracy: 0.8615\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3135 - accuracy: 0.8699\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.3154 - accuracy: 0.8682\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.3151 - accuracy: 0.8640\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3048 - accuracy: 0.8741\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2983 - accuracy: 0.8774\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2993 - accuracy: 0.8741\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.3090 - accuracy: 0.8703\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.3024 - accuracy: 0.8799\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2918 - accuracy: 0.8825\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.3039 - accuracy: 0.8728\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2999 - accuracy: 0.8762\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.3049 - accuracy: 0.8715\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2998 - accuracy: 0.8791\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.3020 - accuracy: 0.8694\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2952 - accuracy: 0.8762\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2937 - accuracy: 0.8715\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.2970 - accuracy: 0.8711\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2876 - accuracy: 0.8812\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2846 - accuracy: 0.8820\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.2874 - accuracy: 0.8783\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2815 - accuracy: 0.8816\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2910 - accuracy: 0.8766\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2972 - accuracy: 0.8774\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2805 - accuracy: 0.8866\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2898 - accuracy: 0.8762\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2772 - accuracy: 0.8858\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2957 - accuracy: 0.8732\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2795 - accuracy: 0.8850\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8770\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.2782 - accuracy: 0.8862\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2937 - accuracy: 0.8753\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.2812 - accuracy: 0.8846\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.2659 - accuracy: 0.8908\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2920 - accuracy: 0.8787\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2847 - accuracy: 0.8774\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2806 - accuracy: 0.8812\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2909 - accuracy: 0.8816\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2803 - accuracy: 0.8816\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2768 - accuracy: 0.8820\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2748 - accuracy: 0.8854\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2809 - accuracy: 0.8787\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2815 - accuracy: 0.8833\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2817 - accuracy: 0.8816\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2735 - accuracy: 0.8812\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2753 - accuracy: 0.8921\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2655 - accuracy: 0.8887\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2705 - accuracy: 0.8900\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2581 - accuracy: 0.9009\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2790 - accuracy: 0.8812\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.2671 - accuracy: 0.8892\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2619 - accuracy: 0.8896\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2589 - accuracy: 0.8963\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.2627 - accuracy: 0.8929\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2644 - accuracy: 0.8946\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2567 - accuracy: 0.8946\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2665 - accuracy: 0.8896\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2667 - accuracy: 0.8908\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2612 - accuracy: 0.8833\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.2735 - accuracy: 0.8841\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2656 - accuracy: 0.8963\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2660 - accuracy: 0.8883\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2643 - accuracy: 0.8971\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2605 - accuracy: 0.8934\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2564 - accuracy: 0.8896\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2602 - accuracy: 0.8908\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2567 - accuracy: 0.8896\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2532 - accuracy: 0.8976\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2652 - accuracy: 0.8900\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2503 - accuracy: 0.8963\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.2614 - accuracy: 0.8934\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2564 - accuracy: 0.8938\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2562 - accuracy: 0.9093\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.2611 - accuracy: 0.8934\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.2543 - accuracy: 0.8963\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2564 - accuracy: 0.8946\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.2531 - accuracy: 0.8971\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2496 - accuracy: 0.8976\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2675 - accuracy: 0.8879\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2548 - accuracy: 0.8971\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2423 - accuracy: 0.9060\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2547 - accuracy: 0.8976\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2559 - accuracy: 0.8942\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2373 - accuracy: 0.9060\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.2557 - accuracy: 0.8971\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.2516 - accuracy: 0.9005\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2505 - accuracy: 0.8992\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2507 - accuracy: 0.8984\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2462 - accuracy: 0.9013\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.2481 - accuracy: 0.8955\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.2459 - accuracy: 0.9001\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2510 - accuracy: 0.8925\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2368 - accuracy: 0.9009\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2342 - accuracy: 0.9039\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2349 - accuracy: 0.9114\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2389 - accuracy: 0.9005\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2348 - accuracy: 0.9064\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2311 - accuracy: 0.9068\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2335 - accuracy: 0.9093\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2288 - accuracy: 0.9043\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2514 - accuracy: 0.8976\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2432 - accuracy: 0.9034\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2409 - accuracy: 0.9022\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.2401 - accuracy: 0.9072\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.2398 - accuracy: 0.9055\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2429 - accuracy: 0.9039\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2362 - accuracy: 0.9034\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2362 - accuracy: 0.9022\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.2272 - accuracy: 0.9051\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9051\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9089\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2277 - accuracy: 0.9076\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2347 - accuracy: 0.9060\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2150 - accuracy: 0.9139\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2357 - accuracy: 0.9026\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2313 - accuracy: 0.9060\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.2310 - accuracy: 0.9097\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2321 - accuracy: 0.9030\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.2321 - accuracy: 0.9102\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.2326 - accuracy: 0.9081\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2307 - accuracy: 0.9093\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2401 - accuracy: 0.9072\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2227 - accuracy: 0.9068\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2155 - accuracy: 0.9131\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2317 - accuracy: 0.9009\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2251 - accuracy: 0.9047\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2136 - accuracy: 0.9198\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.2288 - accuracy: 0.9093\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2106 - accuracy: 0.9131\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2312 - accuracy: 0.9076\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.2257 - accuracy: 0.9135\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.2197 - accuracy: 0.9127\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2198 - accuracy: 0.9085\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2185 - accuracy: 0.9173\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2296 - accuracy: 0.9047\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2196 - accuracy: 0.9102\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.2206 - accuracy: 0.9068\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.2181 - accuracy: 0.9114\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.2213 - accuracy: 0.9139\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2142 - accuracy: 0.9160\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2126 - accuracy: 0.9186\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2256 - accuracy: 0.9081\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2036 - accuracy: 0.9173\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2153 - accuracy: 0.9135\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.2173 - accuracy: 0.9169\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.2342 - accuracy: 0.9089\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.2213 - accuracy: 0.9156\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2154 - accuracy: 0.9110\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.2215 - accuracy: 0.9093\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.1970 - accuracy: 0.9198\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.2115 - accuracy: 0.9169\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2081 - accuracy: 0.9198\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.2222 - accuracy: 0.9089\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.2162 - accuracy: 0.9131\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2201 - accuracy: 0.9114\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2192 - accuracy: 0.9123\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.2192 - accuracy: 0.9089\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2155 - accuracy: 0.9156\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.2074 - accuracy: 0.9160\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 692us/step - loss: 0.2156 - accuracy: 0.9123\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2141 - accuracy: 0.9165\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2075 - accuracy: 0.9223\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.2054 - accuracy: 0.9194\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2144 - accuracy: 0.9135\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2098 - accuracy: 0.9198\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.1957 - accuracy: 0.9253\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2116 - accuracy: 0.9207\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.2150 - accuracy: 0.9118\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.1848 - accuracy: 0.9282\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.2043 - accuracy: 0.9156\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.2025 - accuracy: 0.9131\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2031 - accuracy: 0.9181\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2205 - accuracy: 0.9123\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2029 - accuracy: 0.9240\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2051 - accuracy: 0.9198\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.2149 - accuracy: 0.9177\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9177\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2134 - accuracy: 0.9123\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2012 - accuracy: 0.9232\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1871 - accuracy: 0.9253\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2006 - accuracy: 0.9186\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2056 - accuracy: 0.9181\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2102 - accuracy: 0.9152\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.1890 - accuracy: 0.9236\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2035 - accuracy: 0.9215\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2124 - accuracy: 0.9097\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.1867 - accuracy: 0.9265\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.1931 - accuracy: 0.9211\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.1978 - accuracy: 0.9215\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.2079 - accuracy: 0.9186\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.1861 - accuracy: 0.9207\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.2063 - accuracy: 0.9215\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.1997 - accuracy: 0.9219\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.1966 - accuracy: 0.9244\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.2067 - accuracy: 0.9152\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.1866 - accuracy: 0.9223\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.2018 - accuracy: 0.9144\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.1962 - accuracy: 0.9249\n",
      "Epoch 338/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.2638 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 308.\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1926 - accuracy: 0.9278\n",
      "Epoch 338: early stopping\n",
      "7/7 [==============================] - 0s 739us/step - loss: 0.6071 - accuracy: 0.7548\n",
      "7/7 [==============================] - 0s 636us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.6071423292160034, Accuracy: 0.754807710647583, Precision: 0.6226473867138829, Recall: 0.7735558295174357, F1 Score: 0.6681468811332961\n",
      "Confusion Matrix:\n",
      " [[126  10  31]\n",
      " [  1  16   0]\n",
      " [  9   0  15]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A', '000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'109A', '031A'}\n",
      "Moved to Test Set:\n",
      "{'109A', '031A'}\n",
      "Removed from Test Set\n",
      "{'046A', '000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A'\n",
      " '073A' '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A' '031A'\n",
      " '032A' '035A' '037A' '040A' '042A' '051B' '052A' '057A' '058A' '070A'\n",
      " '072A' '076A' '095A' '103A' '105A' '106A' '109A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "679\n",
      "Length of y_train_val:\n",
      "679\n",
      "Length of groups_train_val:\n",
      "679\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     449\n",
      "kitten    117\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     139\n",
      "senior     65\n",
      "kitten     54\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Counter({0: 898, 1: 585, 2: 565})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 1.1825 - accuracy: 0.4648\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.9610 - accuracy: 0.5640\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.8818 - accuracy: 0.6138\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.8155 - accuracy: 0.6396\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 702us/step - loss: 0.7722 - accuracy: 0.6602\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.7389 - accuracy: 0.6831\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.7273 - accuracy: 0.6953\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.6946 - accuracy: 0.6978\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.6657 - accuracy: 0.7251\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.6497 - accuracy: 0.7188\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.6361 - accuracy: 0.7275\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.6305 - accuracy: 0.7437\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.6087 - accuracy: 0.7393\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.6098 - accuracy: 0.7446\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.5893 - accuracy: 0.7612\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.5740 - accuracy: 0.7627\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.5635 - accuracy: 0.7598\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.5579 - accuracy: 0.7695\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.5374 - accuracy: 0.7759\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.5370 - accuracy: 0.7744\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.5451 - accuracy: 0.7798\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.5201 - accuracy: 0.7812\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.5311 - accuracy: 0.7847\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 697us/step - loss: 0.5224 - accuracy: 0.7788\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.4942 - accuracy: 0.7891\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.5214 - accuracy: 0.7817\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.5006 - accuracy: 0.7939\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.5018 - accuracy: 0.7969\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.5003 - accuracy: 0.7910\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.4757 - accuracy: 0.8101\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.4722 - accuracy: 0.8115\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.4723 - accuracy: 0.8057\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.4842 - accuracy: 0.7959\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.4648 - accuracy: 0.8066\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.4677 - accuracy: 0.8081\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.4589 - accuracy: 0.8125\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.4440 - accuracy: 0.8140\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.4473 - accuracy: 0.8198\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.4512 - accuracy: 0.8140\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 702us/step - loss: 0.4508 - accuracy: 0.8135\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.4407 - accuracy: 0.8223\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.4265 - accuracy: 0.8247\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.4497 - accuracy: 0.8149\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 702us/step - loss: 0.4314 - accuracy: 0.8110\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 696us/step - loss: 0.4397 - accuracy: 0.8188\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.4214 - accuracy: 0.8281\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.4443 - accuracy: 0.8135\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.4235 - accuracy: 0.8315\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.4247 - accuracy: 0.8291\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.4189 - accuracy: 0.8242\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.4337 - accuracy: 0.8223\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.4344 - accuracy: 0.8184\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.4134 - accuracy: 0.8340\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.4147 - accuracy: 0.8374\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.3985 - accuracy: 0.8291\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.4223 - accuracy: 0.8276\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.3802 - accuracy: 0.8525\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.4010 - accuracy: 0.8398\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.4010 - accuracy: 0.8398\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 684us/step - loss: 0.3891 - accuracy: 0.8423\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.3981 - accuracy: 0.8403\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.3850 - accuracy: 0.8374\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.3869 - accuracy: 0.8403\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.4048 - accuracy: 0.8340\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.3874 - accuracy: 0.8481\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.3832 - accuracy: 0.8491\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.3800 - accuracy: 0.8481\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.3667 - accuracy: 0.8506\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.3909 - accuracy: 0.8379\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.3695 - accuracy: 0.8579\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.3666 - accuracy: 0.8491\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.3781 - accuracy: 0.8413\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 668us/step - loss: 0.3768 - accuracy: 0.8501\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.3587 - accuracy: 0.8633\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.3657 - accuracy: 0.8560\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.3565 - accuracy: 0.8452\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.3634 - accuracy: 0.8521\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.3634 - accuracy: 0.8599\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.3604 - accuracy: 0.8550\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.3476 - accuracy: 0.8574\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.3466 - accuracy: 0.8623\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.3598 - accuracy: 0.8599\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.3578 - accuracy: 0.8530\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.3615 - accuracy: 0.8496\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.3396 - accuracy: 0.8677\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 682us/step - loss: 0.3528 - accuracy: 0.8657\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.3599 - accuracy: 0.8564\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.3434 - accuracy: 0.8638\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.3333 - accuracy: 0.8638\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.3356 - accuracy: 0.8628\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.3402 - accuracy: 0.8638\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.3450 - accuracy: 0.8657\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.3339 - accuracy: 0.8643\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.3375 - accuracy: 0.8682\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 694us/step - loss: 0.3564 - accuracy: 0.8540\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.3324 - accuracy: 0.8643\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.3502 - accuracy: 0.8511\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.3242 - accuracy: 0.8730\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.3283 - accuracy: 0.8667\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3392 - accuracy: 0.8613\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.3213 - accuracy: 0.8730\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.3226 - accuracy: 0.8740\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.3068 - accuracy: 0.8799\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.3191 - accuracy: 0.8755\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.3213 - accuracy: 0.8716\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.3117 - accuracy: 0.8838\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.3236 - accuracy: 0.8706\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.3055 - accuracy: 0.8784\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.3199 - accuracy: 0.8745\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.3090 - accuracy: 0.8857\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.3185 - accuracy: 0.8774\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.3176 - accuracy: 0.8760\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.3104 - accuracy: 0.8760\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.3221 - accuracy: 0.8716\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.3129 - accuracy: 0.8853\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.3023 - accuracy: 0.8823\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.3135 - accuracy: 0.8706\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.3131 - accuracy: 0.8760\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.2952 - accuracy: 0.8853\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.3102 - accuracy: 0.8789\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.3165 - accuracy: 0.8721\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.3216 - accuracy: 0.8706\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.2870 - accuracy: 0.8887\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 673us/step - loss: 0.3050 - accuracy: 0.8726\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.3126 - accuracy: 0.8740\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.3046 - accuracy: 0.8848\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.2938 - accuracy: 0.8779\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.2891 - accuracy: 0.8823\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.3002 - accuracy: 0.8838\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 678us/step - loss: 0.2987 - accuracy: 0.8818\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.3035 - accuracy: 0.8745\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.2898 - accuracy: 0.8828\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 686us/step - loss: 0.2907 - accuracy: 0.8877\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.2976 - accuracy: 0.8799\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.2802 - accuracy: 0.8887\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.2954 - accuracy: 0.8823\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.2760 - accuracy: 0.8975\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2918 - accuracy: 0.8794\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.2892 - accuracy: 0.8892\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2742 - accuracy: 0.8945\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2771 - accuracy: 0.8931\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2703 - accuracy: 0.8994\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.2802 - accuracy: 0.8926\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.2876 - accuracy: 0.8916\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2798 - accuracy: 0.8872\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.2824 - accuracy: 0.8862\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.2721 - accuracy: 0.8906\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.2709 - accuracy: 0.8926\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.2676 - accuracy: 0.8916\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.2793 - accuracy: 0.8896\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.2760 - accuracy: 0.8960\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.2593 - accuracy: 0.8945\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.2658 - accuracy: 0.8965\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.2636 - accuracy: 0.9004\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2558 - accuracy: 0.9043\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2615 - accuracy: 0.9009\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2718 - accuracy: 0.8916\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2570 - accuracy: 0.8965\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2704 - accuracy: 0.8887\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 684us/step - loss: 0.2579 - accuracy: 0.8994\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.2676 - accuracy: 0.9004\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.2635 - accuracy: 0.8931\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.2730 - accuracy: 0.8896\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.2671 - accuracy: 0.8970\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.2606 - accuracy: 0.8940\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2559 - accuracy: 0.8994\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.2687 - accuracy: 0.8955\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.2647 - accuracy: 0.8979\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 697us/step - loss: 0.2580 - accuracy: 0.8975\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2511 - accuracy: 0.9023\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.2574 - accuracy: 0.9014\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.2689 - accuracy: 0.8936\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.2577 - accuracy: 0.9009\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.2581 - accuracy: 0.8979\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.2502 - accuracy: 0.9053\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.2642 - accuracy: 0.8970\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2504 - accuracy: 0.9072\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2358 - accuracy: 0.9111\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.2427 - accuracy: 0.9033\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2544 - accuracy: 0.9028\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2546 - accuracy: 0.8994\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2381 - accuracy: 0.9097\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.2359 - accuracy: 0.9097\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.2580 - accuracy: 0.8970\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.2421 - accuracy: 0.9116\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.2517 - accuracy: 0.8994\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2390 - accuracy: 0.9072\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.2505 - accuracy: 0.9038\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 694us/step - loss: 0.2549 - accuracy: 0.8994\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.2590 - accuracy: 0.8994\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.2343 - accuracy: 0.9082\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2547 - accuracy: 0.8975\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.2429 - accuracy: 0.9053\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2415 - accuracy: 0.9023\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.2204 - accuracy: 0.9150\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.2529 - accuracy: 0.8975\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.2400 - accuracy: 0.9111\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.2304 - accuracy: 0.9106\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.2383 - accuracy: 0.9067\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.2274 - accuracy: 0.9121\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2226 - accuracy: 0.9126\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 677us/step - loss: 0.2300 - accuracy: 0.9106\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.2400 - accuracy: 0.9092\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.2291 - accuracy: 0.9136\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2297 - accuracy: 0.9067\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2388 - accuracy: 0.9077\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.2160 - accuracy: 0.9233\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2322 - accuracy: 0.9116\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.2266 - accuracy: 0.9150\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.2359 - accuracy: 0.9028\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.2136 - accuracy: 0.9155\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.2064 - accuracy: 0.9277\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.2329 - accuracy: 0.9097\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.2271 - accuracy: 0.9116\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.2195 - accuracy: 0.9131\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.2270 - accuracy: 0.9058\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.2410 - accuracy: 0.9048\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.2338 - accuracy: 0.9170\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2355 - accuracy: 0.9092\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 665us/step - loss: 0.2126 - accuracy: 0.9219\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.2299 - accuracy: 0.9072\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.2264 - accuracy: 0.9199\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 683us/step - loss: 0.2212 - accuracy: 0.9160\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.2234 - accuracy: 0.9189\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2197 - accuracy: 0.9155\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.2128 - accuracy: 0.9131\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2079 - accuracy: 0.9214\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.2128 - accuracy: 0.9155\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 656us/step - loss: 0.2185 - accuracy: 0.9141\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.2159 - accuracy: 0.9155\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2244 - accuracy: 0.9155\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.2161 - accuracy: 0.9204\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2173 - accuracy: 0.9126\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 672us/step - loss: 0.2064 - accuracy: 0.9150\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.2278 - accuracy: 0.9131\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.2222 - accuracy: 0.9082\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.2191 - accuracy: 0.9155\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2022 - accuracy: 0.9219\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2087 - accuracy: 0.9209\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.2159 - accuracy: 0.9185\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.2082 - accuracy: 0.9165\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.2036 - accuracy: 0.9243\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 664us/step - loss: 0.2115 - accuracy: 0.9146\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.2008 - accuracy: 0.9243\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.2192 - accuracy: 0.9121\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.2079 - accuracy: 0.9189\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 678us/step - loss: 0.2033 - accuracy: 0.9238\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1982 - accuracy: 0.9209\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.2170 - accuracy: 0.9214\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 679us/step - loss: 0.2050 - accuracy: 0.9204\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2028 - accuracy: 0.9214\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 696us/step - loss: 0.2136 - accuracy: 0.9180\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 702us/step - loss: 0.2122 - accuracy: 0.9155\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.2081 - accuracy: 0.9219\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1935 - accuracy: 0.9243\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1949 - accuracy: 0.9189\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.2023 - accuracy: 0.9243\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 684us/step - loss: 0.2061 - accuracy: 0.9248\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.2036 - accuracy: 0.9238\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.2096 - accuracy: 0.9131\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.2016 - accuracy: 0.9224\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1936 - accuracy: 0.9336\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 686us/step - loss: 0.1955 - accuracy: 0.9229\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.1971 - accuracy: 0.9214\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1864 - accuracy: 0.9292\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1981 - accuracy: 0.9248\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1940 - accuracy: 0.9219\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1834 - accuracy: 0.9253\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.1861 - accuracy: 0.9277\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1935 - accuracy: 0.9292\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.1857 - accuracy: 0.9346\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.2028 - accuracy: 0.9199\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9292\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9253\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.1967 - accuracy: 0.9287\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.1872 - accuracy: 0.9292\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 675us/step - loss: 0.1893 - accuracy: 0.9316\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 681us/step - loss: 0.1885 - accuracy: 0.9302\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1761 - accuracy: 0.9380\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.1872 - accuracy: 0.9370\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1714 - accuracy: 0.9355\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.1813 - accuracy: 0.9326\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1856 - accuracy: 0.9336\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.1959 - accuracy: 0.9214\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 664us/step - loss: 0.1973 - accuracy: 0.9199\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 675us/step - loss: 0.1801 - accuracy: 0.9292\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1660 - accuracy: 0.9375\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.1981 - accuracy: 0.9263\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.1718 - accuracy: 0.9346\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.1892 - accuracy: 0.9297\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 677us/step - loss: 0.1682 - accuracy: 0.9370\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1936 - accuracy: 0.9248\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 682us/step - loss: 0.1742 - accuracy: 0.9351\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 654us/step - loss: 0.1805 - accuracy: 0.9316\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1844 - accuracy: 0.9312\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 675us/step - loss: 0.1801 - accuracy: 0.9297\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1786 - accuracy: 0.9238\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.1909 - accuracy: 0.9219\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.1851 - accuracy: 0.9287\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 672us/step - loss: 0.1891 - accuracy: 0.9302\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1764 - accuracy: 0.9331\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.1909 - accuracy: 0.9287\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 687us/step - loss: 0.1785 - accuracy: 0.9268\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 684us/step - loss: 0.1892 - accuracy: 0.9302\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 691us/step - loss: 0.1766 - accuracy: 0.9321\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 678us/step - loss: 0.1849 - accuracy: 0.9243\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1646 - accuracy: 0.9351\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 697us/step - loss: 0.1834 - accuracy: 0.9302\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.1879 - accuracy: 0.9302\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.1654 - accuracy: 0.9380\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.1581 - accuracy: 0.9429\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 672us/step - loss: 0.1778 - accuracy: 0.9277\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.1797 - accuracy: 0.9238\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 679us/step - loss: 0.1859 - accuracy: 0.9248\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.1880 - accuracy: 0.9282\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 675us/step - loss: 0.1600 - accuracy: 0.9395\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 700us/step - loss: 0.1621 - accuracy: 0.9414\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.1605 - accuracy: 0.9453\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 684us/step - loss: 0.1768 - accuracy: 0.9336\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.1845 - accuracy: 0.9307\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.1638 - accuracy: 0.9336\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.1638 - accuracy: 0.9380\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 679us/step - loss: 0.1755 - accuracy: 0.9292\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 696us/step - loss: 0.1784 - accuracy: 0.9321\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.1628 - accuracy: 0.9395\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.1645 - accuracy: 0.9380\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.1664 - accuracy: 0.9380\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 680us/step - loss: 0.1762 - accuracy: 0.9365\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1714 - accuracy: 0.9302\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.1684 - accuracy: 0.9355\n",
      "Epoch 331/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.1799 - accuracy: 0.9287\n",
      "Epoch 332/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1517 - accuracy: 0.9375\n",
      "Epoch 333/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1677 - accuracy: 0.9419\n",
      "Epoch 334/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1698 - accuracy: 0.9380\n",
      "Epoch 335/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1665 - accuracy: 0.9360\n",
      "Epoch 336/1500\n",
      "32/32 [==============================] - 0s 665us/step - loss: 0.1763 - accuracy: 0.9316\n",
      "Epoch 337/1500\n",
      "32/32 [==============================] - 0s 694us/step - loss: 0.1777 - accuracy: 0.9307\n",
      "Epoch 338/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.1779 - accuracy: 0.9321\n",
      "Epoch 339/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.1684 - accuracy: 0.9404\n",
      "Epoch 340/1500\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.1661 - accuracy: 0.9409\n",
      "Epoch 341/1500\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.1742 - accuracy: 0.9375\n",
      "Epoch 342/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1682 - accuracy: 0.9380\n",
      "Epoch 343/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1637 - accuracy: 0.9409\n",
      "Epoch 344/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.1608 - accuracy: 0.9419\n",
      "Epoch 345/1500\n",
      "32/32 [==============================] - 0s 665us/step - loss: 0.1696 - accuracy: 0.9395\n",
      "Epoch 346/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.1681 - accuracy: 0.9375\n",
      "Epoch 347/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.1465 - accuracy: 0.9448\n",
      "Epoch 348/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.1573 - accuracy: 0.9443\n",
      "Epoch 349/1500\n",
      "32/32 [==============================] - 0s 698us/step - loss: 0.1701 - accuracy: 0.9390\n",
      "Epoch 350/1500\n",
      "32/32 [==============================] - 0s 696us/step - loss: 0.1669 - accuracy: 0.9409\n",
      "Epoch 351/1500\n",
      "32/32 [==============================] - 0s 660us/step - loss: 0.1524 - accuracy: 0.9390\n",
      "Epoch 352/1500\n",
      "32/32 [==============================] - 0s 683us/step - loss: 0.1876 - accuracy: 0.9224\n",
      "Epoch 353/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.1641 - accuracy: 0.9370\n",
      "Epoch 354/1500\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.1660 - accuracy: 0.9336\n",
      "Epoch 355/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9434\n",
      "Epoch 356/1500\n",
      "32/32 [==============================] - 0s 651us/step - loss: 0.1659 - accuracy: 0.9331\n",
      "Epoch 357/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1781 - accuracy: 0.9233\n",
      "Epoch 358/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.1711 - accuracy: 0.9365\n",
      "Epoch 359/1500\n",
      "32/32 [==============================] - 0s 675us/step - loss: 0.1610 - accuracy: 0.9399\n",
      "Epoch 360/1500\n",
      "32/32 [==============================] - 0s 659us/step - loss: 0.1628 - accuracy: 0.9390\n",
      "Epoch 361/1500\n",
      "32/32 [==============================] - 0s 697us/step - loss: 0.1495 - accuracy: 0.9414\n",
      "Epoch 362/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1526 - accuracy: 0.9419\n",
      "Epoch 363/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1651 - accuracy: 0.9380\n",
      "Epoch 364/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.1421 - accuracy: 0.9458\n",
      "Epoch 365/1500\n",
      "32/32 [==============================] - 0s 696us/step - loss: 0.1666 - accuracy: 0.9355\n",
      "Epoch 366/1500\n",
      "32/32 [==============================] - 0s 684us/step - loss: 0.1541 - accuracy: 0.9468\n",
      "Epoch 367/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.1646 - accuracy: 0.9375\n",
      "Epoch 368/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1533 - accuracy: 0.9399\n",
      "Epoch 369/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.1550 - accuracy: 0.9448\n",
      "Epoch 370/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1652 - accuracy: 0.9370\n",
      "Epoch 371/1500\n",
      "32/32 [==============================] - 0s 694us/step - loss: 0.1551 - accuracy: 0.9409\n",
      "Epoch 372/1500\n",
      "32/32 [==============================] - 0s 677us/step - loss: 0.1754 - accuracy: 0.9355\n",
      "Epoch 373/1500\n",
      "32/32 [==============================] - 0s 663us/step - loss: 0.1684 - accuracy: 0.9351\n",
      "Epoch 374/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1575 - accuracy: 0.9395\n",
      "Epoch 375/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1551 - accuracy: 0.9404\n",
      "Epoch 376/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.1441 - accuracy: 0.9453\n",
      "Epoch 377/1500\n",
      "32/32 [==============================] - 0s 680us/step - loss: 0.1348 - accuracy: 0.9478\n",
      "Epoch 378/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1710 - accuracy: 0.9287\n",
      "Epoch 379/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1376 - accuracy: 0.9458\n",
      "Epoch 380/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1648 - accuracy: 0.9375\n",
      "Epoch 381/1500\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.1589 - accuracy: 0.9453\n",
      "Epoch 382/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.1446 - accuracy: 0.9443\n",
      "Epoch 383/1500\n",
      "32/32 [==============================] - 0s 669us/step - loss: 0.1598 - accuracy: 0.9355\n",
      "Epoch 384/1500\n",
      "32/32 [==============================] - 0s 670us/step - loss: 0.1558 - accuracy: 0.9370\n",
      "Epoch 385/1500\n",
      "32/32 [==============================] - 0s 684us/step - loss: 0.1462 - accuracy: 0.9443\n",
      "Epoch 386/1500\n",
      "32/32 [==============================] - 0s 697us/step - loss: 0.1483 - accuracy: 0.9434\n",
      "Epoch 387/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1527 - accuracy: 0.9448\n",
      "Epoch 388/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.1579 - accuracy: 0.9380\n",
      "Epoch 389/1500\n",
      "32/32 [==============================] - 0s 697us/step - loss: 0.1427 - accuracy: 0.9478\n",
      "Epoch 390/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.1475 - accuracy: 0.9468\n",
      "Epoch 391/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1391 - accuracy: 0.9492\n",
      "Epoch 392/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.1616 - accuracy: 0.9395\n",
      "Epoch 393/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.1432 - accuracy: 0.9443\n",
      "Epoch 394/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.1497 - accuracy: 0.9468\n",
      "Epoch 395/1500\n",
      "32/32 [==============================] - 0s 694us/step - loss: 0.1387 - accuracy: 0.9448\n",
      "Epoch 396/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1403 - accuracy: 0.9473\n",
      "Epoch 397/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.1525 - accuracy: 0.9424\n",
      "Epoch 398/1500\n",
      "32/32 [==============================] - 0s 686us/step - loss: 0.1473 - accuracy: 0.9453\n",
      "Epoch 399/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.1813 - accuracy: 0.9331\n",
      "Epoch 400/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.1347 - accuracy: 0.9502\n",
      "Epoch 401/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.1506 - accuracy: 0.9409\n",
      "Epoch 402/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1371 - accuracy: 0.9473\n",
      "Epoch 403/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1568 - accuracy: 0.9380\n",
      "Epoch 404/1500\n",
      "32/32 [==============================] - 0s 680us/step - loss: 0.1505 - accuracy: 0.9473\n",
      "Epoch 405/1500\n",
      "32/32 [==============================] - 0s 650us/step - loss: 0.1542 - accuracy: 0.9395\n",
      "Epoch 406/1500\n",
      "32/32 [==============================] - 0s 668us/step - loss: 0.1554 - accuracy: 0.9399\n",
      "Epoch 407/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2292 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 377.\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.1391 - accuracy: 0.9458\n",
      "Epoch 407: early stopping\n",
      "9/9 [==============================] - 0s 728us/step - loss: 0.8852 - accuracy: 0.7171\n",
      "9/9 [==============================] - 0s 587us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 0.8852482438087463, Accuracy: 0.7170542478561401, Precision: 0.7351947931105293, Recall: 0.6901897285590332, F1 Score: 0.7086662055370461\n",
      "Confusion Matrix:\n",
      " [[110   3  26]\n",
      " [ 11  40   3]\n",
      " [ 30   0  35]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 916, 1: 680, 2: 540})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.1052 - accuracy: 0.5056\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9705 - accuracy: 0.5707\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8832 - accuracy: 0.6180\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 0.8560 - accuracy: 0.6367\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.8213 - accuracy: 0.6554\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.8024 - accuracy: 0.6587\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 985us/step - loss: 0.7437 - accuracy: 0.6901\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 984us/step - loss: 0.7285 - accuracy: 0.6901\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 951us/step - loss: 0.7357 - accuracy: 0.6859\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 973us/step - loss: 0.7072 - accuracy: 0.6985\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.6839 - accuracy: 0.7069\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.6775 - accuracy: 0.7121\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.6671 - accuracy: 0.7168\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.6550 - accuracy: 0.7280\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.6408 - accuracy: 0.7238\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.6422 - accuracy: 0.7214\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.6070 - accuracy: 0.7458\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.5833 - accuracy: 0.7514\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.5990 - accuracy: 0.7481\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.5957 - accuracy: 0.7453\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.5886 - accuracy: 0.7383\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5822 - accuracy: 0.7551\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.5666 - accuracy: 0.7542\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.5628 - accuracy: 0.7566\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.5456 - accuracy: 0.7715\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.5597 - accuracy: 0.7608\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5378 - accuracy: 0.7743\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5448 - accuracy: 0.7697\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.5397 - accuracy: 0.7664\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.5274 - accuracy: 0.7725\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.5176 - accuracy: 0.7790\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.5248 - accuracy: 0.7673\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.5016 - accuracy: 0.7870\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.5049 - accuracy: 0.7879\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.5016 - accuracy: 0.7860\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.5038 - accuracy: 0.7870\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.4818 - accuracy: 0.7945\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.4925 - accuracy: 0.7837\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.4863 - accuracy: 0.7832\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5022 - accuracy: 0.7865\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.4786 - accuracy: 0.7926\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.4794 - accuracy: 0.8001\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.4794 - accuracy: 0.7963\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.8123\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.4789 - accuracy: 0.7978\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.4682 - accuracy: 0.7931\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.4603 - accuracy: 0.8085\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.4636 - accuracy: 0.7959\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.4567 - accuracy: 0.8090\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.4476 - accuracy: 0.8081\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.4481 - accuracy: 0.8085\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.4487 - accuracy: 0.8127\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.4638 - accuracy: 0.7959\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.4414 - accuracy: 0.8034\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.4318 - accuracy: 0.8137\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.4470 - accuracy: 0.8169\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4338 - accuracy: 0.8198\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.4430 - accuracy: 0.8085\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4156 - accuracy: 0.8212\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.4179 - accuracy: 0.8165\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.4147 - accuracy: 0.8282\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.4198 - accuracy: 0.8249\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.4229 - accuracy: 0.8244\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.3997 - accuracy: 0.8221\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.4000 - accuracy: 0.8258\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.4139 - accuracy: 0.8240\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.3926 - accuracy: 0.8366\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4033 - accuracy: 0.8268\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.4075 - accuracy: 0.8324\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.4147 - accuracy: 0.8221\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.4038 - accuracy: 0.8310\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.4033 - accuracy: 0.8249\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.4088 - accuracy: 0.8329\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.4068 - accuracy: 0.8244\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.3919 - accuracy: 0.8357\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.3849 - accuracy: 0.8404\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3763 - accuracy: 0.8347\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.3825 - accuracy: 0.8418\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.3853 - accuracy: 0.8319\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.3752 - accuracy: 0.8408\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3877 - accuracy: 0.8399\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.3886 - accuracy: 0.8310\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.3777 - accuracy: 0.8390\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.3703 - accuracy: 0.8516\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.3656 - accuracy: 0.8525\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.3754 - accuracy: 0.8413\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.3895 - accuracy: 0.8329\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.3697 - accuracy: 0.8436\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.3713 - accuracy: 0.8436\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3655 - accuracy: 0.8436\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.3692 - accuracy: 0.8385\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 951us/step - loss: 0.3510 - accuracy: 0.8535\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.3655 - accuracy: 0.8446\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3597 - accuracy: 0.8507\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.3598 - accuracy: 0.8478\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.3535 - accuracy: 0.8507\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.3655 - accuracy: 0.8399\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.3563 - accuracy: 0.8530\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.3591 - accuracy: 0.8502\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.3395 - accuracy: 0.8563\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.3444 - accuracy: 0.8610\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.3346 - accuracy: 0.8535\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.3526 - accuracy: 0.8464\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 960us/step - loss: 0.3495 - accuracy: 0.8497\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.3626 - accuracy: 0.8469\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.3438 - accuracy: 0.8581\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3571 - accuracy: 0.8474\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.3480 - accuracy: 0.8464\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.3392 - accuracy: 0.8544\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.3397 - accuracy: 0.8553\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.3413 - accuracy: 0.8539\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.3447 - accuracy: 0.8507\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.3422 - accuracy: 0.8521\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.3359 - accuracy: 0.8600\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.3294 - accuracy: 0.8727\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3379 - accuracy: 0.8596\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.3507 - accuracy: 0.8464\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.3278 - accuracy: 0.8638\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3290 - accuracy: 0.8605\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.3336 - accuracy: 0.8656\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3425 - accuracy: 0.8502\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.3164 - accuracy: 0.8642\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.3257 - accuracy: 0.8624\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.3301 - accuracy: 0.8614\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.3225 - accuracy: 0.8619\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.3122 - accuracy: 0.8755\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.3248 - accuracy: 0.8647\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.3285 - accuracy: 0.8596\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.3285 - accuracy: 0.8652\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3258 - accuracy: 0.8684\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.3212 - accuracy: 0.8605\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3160 - accuracy: 0.8670\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.3167 - accuracy: 0.8666\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.3107 - accuracy: 0.8708\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.3226 - accuracy: 0.8591\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.3057 - accuracy: 0.8736\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.3088 - accuracy: 0.8750\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.3067 - accuracy: 0.8600\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.3041 - accuracy: 0.8741\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2998 - accuracy: 0.8801\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.3017 - accuracy: 0.8741\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.3091 - accuracy: 0.8689\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3060 - accuracy: 0.8722\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2998 - accuracy: 0.8759\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.3129 - accuracy: 0.8741\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.2899 - accuracy: 0.8778\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.3061 - accuracy: 0.8722\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.2882 - accuracy: 0.8806\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2975 - accuracy: 0.8736\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3112 - accuracy: 0.8670\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3108 - accuracy: 0.8736\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2910 - accuracy: 0.8806\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2966 - accuracy: 0.8769\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.2847 - accuracy: 0.8750\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3184 - accuracy: 0.8717\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3019 - accuracy: 0.8731\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 917us/step - loss: 0.3097 - accuracy: 0.8713\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.3023 - accuracy: 0.8801\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.2971 - accuracy: 0.8750\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.3006 - accuracy: 0.8801\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.2883 - accuracy: 0.8834\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.2779 - accuracy: 0.8881\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.3013 - accuracy: 0.8717\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3014 - accuracy: 0.8708\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.2934 - accuracy: 0.8769\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2874 - accuracy: 0.8783\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2809 - accuracy: 0.8839\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2792 - accuracy: 0.8834\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2637 - accuracy: 0.8895\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2968 - accuracy: 0.8806\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 981us/step - loss: 0.2837 - accuracy: 0.8933\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.2690 - accuracy: 0.8928\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 951us/step - loss: 0.2841 - accuracy: 0.8895\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 970us/step - loss: 0.2718 - accuracy: 0.8872\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.2856 - accuracy: 0.8830\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 940us/step - loss: 0.2795 - accuracy: 0.8806\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 987us/step - loss: 0.2582 - accuracy: 0.8956\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2677 - accuracy: 0.8923\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2844 - accuracy: 0.8848\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2771 - accuracy: 0.8867\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2674 - accuracy: 0.8923\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2712 - accuracy: 0.8848\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2727 - accuracy: 0.8848\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.2807 - accuracy: 0.8820\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.2700 - accuracy: 0.8876\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2758 - accuracy: 0.8942\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2681 - accuracy: 0.8867\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2710 - accuracy: 0.9007\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2494 - accuracy: 0.9026\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.2792 - accuracy: 0.8848\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.2753 - accuracy: 0.8806\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2624 - accuracy: 0.8933\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8872\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.2684 - accuracy: 0.8839\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.2663 - accuracy: 0.8909\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2626 - accuracy: 0.8895\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2733 - accuracy: 0.8764\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.2518 - accuracy: 0.8947\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2657 - accuracy: 0.8933\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2650 - accuracy: 0.8919\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2509 - accuracy: 0.8989\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.2767 - accuracy: 0.8844\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2651 - accuracy: 0.8965\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.2637 - accuracy: 0.8876\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2724 - accuracy: 0.8904\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2585 - accuracy: 0.8975\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2608 - accuracy: 0.8886\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2609 - accuracy: 0.8909\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.2550 - accuracy: 0.8970\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2545 - accuracy: 0.8970\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2628 - accuracy: 0.8975\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2547 - accuracy: 0.8923\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.2439 - accuracy: 0.9026\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2685 - accuracy: 0.8951\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2465 - accuracy: 0.9101\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.2583 - accuracy: 0.8989\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.2444 - accuracy: 0.9007\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2501 - accuracy: 0.9003\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2513 - accuracy: 0.9022\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.2417 - accuracy: 0.8984\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.2733 - accuracy: 0.8797\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2362 - accuracy: 0.9054\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.2421 - accuracy: 0.9007\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.2373 - accuracy: 0.9115\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2485 - accuracy: 0.8933\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2375 - accuracy: 0.9007\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.2570 - accuracy: 0.8970\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2404 - accuracy: 0.9031\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 705us/step - loss: 0.2572 - accuracy: 0.8942\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 722us/step - loss: 0.2390 - accuracy: 0.9050\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2333 - accuracy: 0.9082\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.2585 - accuracy: 0.8984\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2491 - accuracy: 0.8961\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.2418 - accuracy: 0.9031\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2507 - accuracy: 0.9026\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 718us/step - loss: 0.2443 - accuracy: 0.8970\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 707us/step - loss: 0.2506 - accuracy: 0.9012\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2362 - accuracy: 0.9087\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2316 - accuracy: 0.9120\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.2511 - accuracy: 0.8965\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2341 - accuracy: 0.8993\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2342 - accuracy: 0.9036\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2389 - accuracy: 0.9054\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2307 - accuracy: 0.9073\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2356 - accuracy: 0.9050\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2186 - accuracy: 0.9176\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.2336 - accuracy: 0.9059\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2414 - accuracy: 0.8984\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2362 - accuracy: 0.9096\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2247 - accuracy: 0.9092\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2385 - accuracy: 0.9007\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2282 - accuracy: 0.9092\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2278 - accuracy: 0.9110\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 736us/step - loss: 0.2409 - accuracy: 0.8975\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2405 - accuracy: 0.9007\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2442 - accuracy: 0.8998\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2350 - accuracy: 0.9040\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2309 - accuracy: 0.9040\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2213 - accuracy: 0.9120\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2186 - accuracy: 0.9120\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2180 - accuracy: 0.9153\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2338 - accuracy: 0.9078\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2211 - accuracy: 0.9087\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2343 - accuracy: 0.9036\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2341 - accuracy: 0.9092\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2295 - accuracy: 0.9064\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2189 - accuracy: 0.9125\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2270 - accuracy: 0.9129\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2131 - accuracy: 0.9171\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2209 - accuracy: 0.9181\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2185 - accuracy: 0.9073\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2136 - accuracy: 0.9143\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2102 - accuracy: 0.9171\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2317 - accuracy: 0.9092\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2068 - accuracy: 0.9190\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.2373 - accuracy: 0.9087\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 716us/step - loss: 0.2287 - accuracy: 0.9087\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2261 - accuracy: 0.9059\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 719us/step - loss: 0.2290 - accuracy: 0.9054\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2327 - accuracy: 0.9017\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2224 - accuracy: 0.9153\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2003 - accuracy: 0.9167\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2226 - accuracy: 0.9153\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.2038 - accuracy: 0.9190\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.2115 - accuracy: 0.9171\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.2241 - accuracy: 0.9082\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.2111 - accuracy: 0.9148\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2066 - accuracy: 0.9157\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1944 - accuracy: 0.9260\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.2064 - accuracy: 0.9167\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.2196 - accuracy: 0.9167\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 736us/step - loss: 0.2103 - accuracy: 0.9153\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2068 - accuracy: 0.9143\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.2198 - accuracy: 0.9134\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 732us/step - loss: 0.2206 - accuracy: 0.9139\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.2027 - accuracy: 0.9176\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 708us/step - loss: 0.2088 - accuracy: 0.9185\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2132 - accuracy: 0.9162\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.1939 - accuracy: 0.9218\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.1961 - accuracy: 0.9157\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2187 - accuracy: 0.9096\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2029 - accuracy: 0.9185\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2059 - accuracy: 0.9190\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.1896 - accuracy: 0.9232\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2073 - accuracy: 0.9176\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 714us/step - loss: 0.1930 - accuracy: 0.9223\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2186 - accuracy: 0.9054\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1954 - accuracy: 0.9251\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1995 - accuracy: 0.9209\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1887 - accuracy: 0.9251\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2024 - accuracy: 0.9223\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.2015 - accuracy: 0.9115\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.1998 - accuracy: 0.9246\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2051 - accuracy: 0.9185\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2054 - accuracy: 0.9204\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9120\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9171\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 935us/step - loss: 0.1942 - accuracy: 0.9251\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2015 - accuracy: 0.9181\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2198 - accuracy: 0.9110\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2003 - accuracy: 0.9213\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 732us/step - loss: 0.1948 - accuracy: 0.9204\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2078 - accuracy: 0.9171\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2079 - accuracy: 0.9190\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1874 - accuracy: 0.9298\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1999 - accuracy: 0.9284\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1904 - accuracy: 0.9298\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1846 - accuracy: 0.9274\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.1929 - accuracy: 0.9190\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.2033 - accuracy: 0.9162\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.1944 - accuracy: 0.9251\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1920 - accuracy: 0.9260\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1986 - accuracy: 0.9228\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1965 - accuracy: 0.9204\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1911 - accuracy: 0.9279\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1921 - accuracy: 0.9199\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1964 - accuracy: 0.9223\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.1804 - accuracy: 0.9274\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 994us/step - loss: 0.1815 - accuracy: 0.9288\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9209\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.1938 - accuracy: 0.9237\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1919 - accuracy: 0.9279\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1967 - accuracy: 0.9223\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1967 - accuracy: 0.9199\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.1996 - accuracy: 0.9143\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.1997 - accuracy: 0.9246\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.1975 - accuracy: 0.9270\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1780 - accuracy: 0.9316\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1888 - accuracy: 0.9279\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9298\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1817 - accuracy: 0.9316\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.1796 - accuracy: 0.9288\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1910 - accuracy: 0.9284\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.1937 - accuracy: 0.9228\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.1815 - accuracy: 0.9345\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1838 - accuracy: 0.9279\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.1982 - accuracy: 0.9162\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.1865 - accuracy: 0.9246\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.1966 - accuracy: 0.9232\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.1805 - accuracy: 0.9312\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.1786 - accuracy: 0.9321\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 701us/step - loss: 0.1817 - accuracy: 0.9260\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1646 - accuracy: 0.9368\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1724 - accuracy: 0.9326\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1859 - accuracy: 0.9284\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.1823 - accuracy: 0.9288\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1848 - accuracy: 0.9204\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 702us/step - loss: 0.1751 - accuracy: 0.9312\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 728us/step - loss: 0.1955 - accuracy: 0.9237\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.1815 - accuracy: 0.9256\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1858 - accuracy: 0.9251\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.1875 - accuracy: 0.9246\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1717 - accuracy: 0.9284\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.1850 - accuracy: 0.9316\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.1706 - accuracy: 0.9359\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1828 - accuracy: 0.9223\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.1773 - accuracy: 0.9321\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 728us/step - loss: 0.1828 - accuracy: 0.9274\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1866 - accuracy: 0.9260\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1844 - accuracy: 0.9260\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1823 - accuracy: 0.9218\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.1839 - accuracy: 0.9251\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.1768 - accuracy: 0.9288\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.1785 - accuracy: 0.9274\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1737 - accuracy: 0.9293\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1809 - accuracy: 0.9270\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1752 - accuracy: 0.9326\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.9321\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 0.1892 - accuracy: 0.9265\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.1849 - accuracy: 0.9228\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.1559 - accuracy: 0.9387\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1926 - accuracy: 0.9232\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1737 - accuracy: 0.9265\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1763 - accuracy: 0.9265\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1670 - accuracy: 0.9340\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 721us/step - loss: 0.1776 - accuracy: 0.9307\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1806 - accuracy: 0.9298\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1744 - accuracy: 0.9382\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.1601 - accuracy: 0.9331\n",
      "Epoch 400/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1850 - accuracy: 0.9307\n",
      "Epoch 401/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.1758 - accuracy: 0.9335\n",
      "Epoch 402/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1731 - accuracy: 0.9359\n",
      "Epoch 403/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1681 - accuracy: 0.9335\n",
      "Epoch 404/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.1641 - accuracy: 0.9377\n",
      "Epoch 405/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.1796 - accuracy: 0.9228\n",
      "Epoch 406/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.1690 - accuracy: 0.9373\n",
      "Epoch 407/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1668 - accuracy: 0.9354\n",
      "Epoch 408/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1675 - accuracy: 0.9349\n",
      "Epoch 409/1500\n",
      "34/34 [==============================] - 0s 721us/step - loss: 0.1932 - accuracy: 0.9190\n",
      "Epoch 410/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1599 - accuracy: 0.9415\n",
      "Epoch 411/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1785 - accuracy: 0.9316\n",
      "Epoch 412/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.1602 - accuracy: 0.9391\n",
      "Epoch 413/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.1786 - accuracy: 0.9284\n",
      "Epoch 414/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1735 - accuracy: 0.9326\n",
      "Epoch 415/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1724 - accuracy: 0.9312\n",
      "Epoch 416/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.1715 - accuracy: 0.9368\n",
      "Epoch 417/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1641 - accuracy: 0.9373\n",
      "Epoch 418/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1574 - accuracy: 0.9349\n",
      "Epoch 419/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.1653 - accuracy: 0.9345\n",
      "Epoch 420/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.1650 - accuracy: 0.9316\n",
      "Epoch 421/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.2710 - accuracy: 0.8906Restoring model weights from the end of the best epoch: 391.\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.1773 - accuracy: 0.9293\n",
      "Epoch 421: early stopping\n",
      "8/8 [==============================] - 0s 761us/step - loss: 0.6938 - accuracy: 0.7064\n",
      "8/8 [==============================] - 0s 659us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n",
      "Final Test Results - Loss: 0.6938115358352661, Accuracy: 0.7063829898834229, Precision: 0.7678311868302741, Recall: 0.638095238095238, F1 Score: 0.672679979862393\n",
      "Confusion Matrix:\n",
      " [[117   2  11]\n",
      " [ 13  22   0]\n",
      " [ 43   0  27]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6919367586966759\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.710974931716919\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7418401539325714\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6963810839460015\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7391663393938039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.77 (85/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, senior, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, adult, adult, adult, ad...         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, kitten, adult, adult, senior, ...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "63    057A  [senior, adult, adult, senior, adult, adult, s...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [adult, senior, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "60    054A                                   [senior, senior]        senior           senior                   True\n",
       "59    053A        [adult, adult, adult, adult, kitten, adult]         adult            adult                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A     [senior, adult, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, senior, adult, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, senior, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "18    016A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, kitten, adult]         adult            adult                   True\n",
       "4     003A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, kitten, kitten, adult, k...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "56    051A  [senior, senior, adult, adult, adult, senior, ...         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "103   109A  [kitten, adult, kitten, kitten, kitten, kitten...         adult           kitten                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, adult, ...        senior            adult                  False\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...        senior            adult                  False\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "69    063A  [senior, senior, senior, adult, senior, adult,...        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "40    034A             [adult, senior, senior, adult, senior]        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "17    015A  [adult, senior, senior, senior, senior, adult,...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "109   117A  [adult, adult, adult, adult, adult, senior, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    13\n",
      "senior    12\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             12  54.545455\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            446  80.215827\n",
      "1           kitten          114             86  75.438596\n",
      "2           senior          178             92  51.685393\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    220      165     75.00\n",
      "1          M    337      243     72.11\n",
      "2          X    291      216     74.23\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 944, 2: 825, 1: 735})\n",
      "Epoch 1/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.0694 - accuracy: 0.5236\n",
      "Epoch 2/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9372 - accuracy: 0.5823\n",
      "Epoch 3/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.8692 - accuracy: 0.6210\n",
      "Epoch 4/1500\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.8434 - accuracy: 0.6366\n",
      "Epoch 5/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.8073 - accuracy: 0.6506\n",
      "Epoch 6/1500\n",
      "40/40 [==============================] - 0s 883us/step - loss: 0.8042 - accuracy: 0.6617\n",
      "Epoch 7/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.7801 - accuracy: 0.6745\n",
      "Epoch 8/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.7525 - accuracy: 0.6805\n",
      "Epoch 9/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.7501 - accuracy: 0.6873\n",
      "Epoch 10/1500\n",
      "40/40 [==============================] - 0s 910us/step - loss: 0.7046 - accuracy: 0.7069\n",
      "Epoch 11/1500\n",
      "40/40 [==============================] - 0s 891us/step - loss: 0.6946 - accuracy: 0.7149\n",
      "Epoch 12/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.6659 - accuracy: 0.7169\n",
      "Epoch 13/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.6614 - accuracy: 0.7173\n",
      "Epoch 14/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.6409 - accuracy: 0.7256\n",
      "Epoch 15/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.6351 - accuracy: 0.7292\n",
      "Epoch 16/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.6204 - accuracy: 0.7420\n",
      "Epoch 17/1500\n",
      "40/40 [==============================] - 0s 880us/step - loss: 0.5994 - accuracy: 0.7512\n",
      "Epoch 18/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.6125 - accuracy: 0.7444\n",
      "Epoch 19/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.5753 - accuracy: 0.7524\n",
      "Epoch 20/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.6039 - accuracy: 0.7420\n",
      "Epoch 21/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.5690 - accuracy: 0.7560\n",
      "Epoch 22/1500\n",
      "40/40 [==============================] - 0s 870us/step - loss: 0.5653 - accuracy: 0.7620\n",
      "Epoch 23/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.5528 - accuracy: 0.7656\n",
      "Epoch 24/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.5528 - accuracy: 0.7656\n",
      "Epoch 25/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.5528 - accuracy: 0.7648\n",
      "Epoch 26/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.5401 - accuracy: 0.7720\n",
      "Epoch 27/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.5491 - accuracy: 0.7724\n",
      "Epoch 28/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.5199 - accuracy: 0.7871\n",
      "Epoch 29/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.5336 - accuracy: 0.7748\n",
      "Epoch 30/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.5191 - accuracy: 0.7756\n",
      "Epoch 31/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.5315 - accuracy: 0.7684\n",
      "Epoch 32/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.5128 - accuracy: 0.7867\n",
      "Epoch 33/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.5105 - accuracy: 0.7943\n",
      "Epoch 34/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.5092 - accuracy: 0.7800\n",
      "Epoch 35/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.5036 - accuracy: 0.7812\n",
      "Epoch 36/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.4973 - accuracy: 0.7979\n",
      "Epoch 37/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.4815 - accuracy: 0.7979\n",
      "Epoch 38/1500\n",
      "40/40 [==============================] - 0s 717us/step - loss: 0.4901 - accuracy: 0.7899\n",
      "Epoch 39/1500\n",
      "40/40 [==============================] - 0s 717us/step - loss: 0.4830 - accuracy: 0.7963\n",
      "Epoch 40/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.4852 - accuracy: 0.7991\n",
      "Epoch 41/1500\n",
      "40/40 [==============================] - 0s 888us/step - loss: 0.4813 - accuracy: 0.7995\n",
      "Epoch 42/1500\n",
      "40/40 [==============================] - 0s 880us/step - loss: 0.4718 - accuracy: 0.8079\n",
      "Epoch 43/1500\n",
      "40/40 [==============================] - 0s 894us/step - loss: 0.4822 - accuracy: 0.7899\n",
      "Epoch 44/1500\n",
      "40/40 [==============================] - 0s 885us/step - loss: 0.4691 - accuracy: 0.7979\n",
      "Epoch 45/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.4640 - accuracy: 0.8043\n",
      "Epoch 46/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.4679 - accuracy: 0.8047\n",
      "Epoch 47/1500\n",
      "40/40 [==============================] - 0s 719us/step - loss: 0.4674 - accuracy: 0.7983\n",
      "Epoch 48/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.4619 - accuracy: 0.8067\n",
      "Epoch 49/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.4491 - accuracy: 0.8131\n",
      "Epoch 50/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.4414 - accuracy: 0.8207\n",
      "Epoch 51/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.4501 - accuracy: 0.8091\n",
      "Epoch 52/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.4424 - accuracy: 0.8059\n",
      "Epoch 53/1500\n",
      "40/40 [==============================] - 0s 713us/step - loss: 0.4548 - accuracy: 0.8091\n",
      "Epoch 54/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.4484 - accuracy: 0.8127\n",
      "Epoch 55/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.4279 - accuracy: 0.8151\n",
      "Epoch 56/1500\n",
      "40/40 [==============================] - 0s 710us/step - loss: 0.4291 - accuracy: 0.8195\n",
      "Epoch 57/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.4324 - accuracy: 0.8203\n",
      "Epoch 58/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.4269 - accuracy: 0.8251\n",
      "Epoch 59/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.4239 - accuracy: 0.8183\n",
      "Epoch 60/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.4162 - accuracy: 0.8383\n",
      "Epoch 61/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.4158 - accuracy: 0.8223\n",
      "Epoch 62/1500\n",
      "40/40 [==============================] - 0s 713us/step - loss: 0.4311 - accuracy: 0.8099\n",
      "Epoch 63/1500\n",
      "40/40 [==============================] - 0s 707us/step - loss: 0.4228 - accuracy: 0.8195\n",
      "Epoch 64/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.4113 - accuracy: 0.8287\n",
      "Epoch 65/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.4077 - accuracy: 0.8283\n",
      "Epoch 66/1500\n",
      "40/40 [==============================] - 0s 704us/step - loss: 0.4286 - accuracy: 0.8163\n",
      "Epoch 67/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.4175 - accuracy: 0.8351\n",
      "Epoch 68/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.3985 - accuracy: 0.8223\n",
      "Epoch 69/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.4096 - accuracy: 0.8235\n",
      "Epoch 70/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.4057 - accuracy: 0.8319\n",
      "Epoch 71/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.4032 - accuracy: 0.8311\n",
      "Epoch 72/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.4153 - accuracy: 0.8143\n",
      "Epoch 73/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.4087 - accuracy: 0.8291\n",
      "Epoch 74/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.4056 - accuracy: 0.8251\n",
      "Epoch 75/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.3928 - accuracy: 0.8383\n",
      "Epoch 76/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.4214 - accuracy: 0.8275\n",
      "Epoch 77/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.3838 - accuracy: 0.8415\n",
      "Epoch 78/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.4002 - accuracy: 0.8279\n",
      "Epoch 79/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.3856 - accuracy: 0.8395\n",
      "Epoch 80/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.3959 - accuracy: 0.8331\n",
      "Epoch 81/1500\n",
      "40/40 [==============================] - 0s 708us/step - loss: 0.3940 - accuracy: 0.8367\n",
      "Epoch 82/1500\n",
      "40/40 [==============================] - 0s 722us/step - loss: 0.3910 - accuracy: 0.8347\n",
      "Epoch 83/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.3741 - accuracy: 0.8391\n",
      "Epoch 84/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.3941 - accuracy: 0.8339\n",
      "Epoch 85/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.3581 - accuracy: 0.8482\n",
      "Epoch 86/1500\n",
      "40/40 [==============================] - 0s 677us/step - loss: 0.3804 - accuracy: 0.8395\n",
      "Epoch 87/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.3714 - accuracy: 0.8399\n",
      "Epoch 88/1500\n",
      "40/40 [==============================] - 0s 694us/step - loss: 0.3634 - accuracy: 0.8498\n",
      "Epoch 89/1500\n",
      "40/40 [==============================] - 0s 712us/step - loss: 0.3753 - accuracy: 0.8486\n",
      "Epoch 90/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.3696 - accuracy: 0.8470\n",
      "Epoch 91/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.3792 - accuracy: 0.8450\n",
      "Epoch 92/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.3666 - accuracy: 0.8442\n",
      "Epoch 93/1500\n",
      "40/40 [==============================] - 0s 720us/step - loss: 0.3651 - accuracy: 0.8435\n",
      "Epoch 94/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.3643 - accuracy: 0.8466\n",
      "Epoch 95/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.3696 - accuracy: 0.8462\n",
      "Epoch 96/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.3785 - accuracy: 0.8367\n",
      "Epoch 97/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.3718 - accuracy: 0.8399\n",
      "Epoch 98/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.3635 - accuracy: 0.8522\n",
      "Epoch 99/1500\n",
      "40/40 [==============================] - 0s 726us/step - loss: 0.3563 - accuracy: 0.8502\n",
      "Epoch 100/1500\n",
      "40/40 [==============================] - 0s 703us/step - loss: 0.3616 - accuracy: 0.8446\n",
      "Epoch 101/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.3565 - accuracy: 0.8538\n",
      "Epoch 102/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.3417 - accuracy: 0.8610\n",
      "Epoch 103/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.3623 - accuracy: 0.8490\n",
      "Epoch 104/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.3423 - accuracy: 0.8566\n",
      "Epoch 105/1500\n",
      "40/40 [==============================] - 0s 729us/step - loss: 0.3494 - accuracy: 0.8558\n",
      "Epoch 106/1500\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.3554 - accuracy: 0.8538\n",
      "Epoch 107/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.3455 - accuracy: 0.8538\n",
      "Epoch 108/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3467 - accuracy: 0.8554\n",
      "Epoch 109/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.3396 - accuracy: 0.8630\n",
      "Epoch 110/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.3428 - accuracy: 0.8578\n",
      "Epoch 111/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.3445 - accuracy: 0.8522\n",
      "Epoch 112/1500\n",
      "40/40 [==============================] - 0s 731us/step - loss: 0.3353 - accuracy: 0.8614\n",
      "Epoch 113/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.3451 - accuracy: 0.8582\n",
      "Epoch 114/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.3447 - accuracy: 0.8550\n",
      "Epoch 115/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.3476 - accuracy: 0.8582\n",
      "Epoch 116/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.3456 - accuracy: 0.8554\n",
      "Epoch 117/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.3330 - accuracy: 0.8594\n",
      "Epoch 118/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.3562 - accuracy: 0.8498\n",
      "Epoch 119/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.3340 - accuracy: 0.8562\n",
      "Epoch 120/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.3264 - accuracy: 0.8694\n",
      "Epoch 121/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.3214 - accuracy: 0.8686\n",
      "Epoch 122/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.3349 - accuracy: 0.8658\n",
      "Epoch 123/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.3269 - accuracy: 0.8626\n",
      "Epoch 124/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.3476 - accuracy: 0.8498\n",
      "Epoch 125/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.3371 - accuracy: 0.8594\n",
      "Epoch 126/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.3167 - accuracy: 0.8702\n",
      "Epoch 127/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3238 - accuracy: 0.8678\n",
      "Epoch 128/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.3307 - accuracy: 0.8590\n",
      "Epoch 129/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3345 - accuracy: 0.8586\n",
      "Epoch 130/1500\n",
      "40/40 [==============================] - 0s 721us/step - loss: 0.3327 - accuracy: 0.8658\n",
      "Epoch 131/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.3248 - accuracy: 0.8594\n",
      "Epoch 132/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3134 - accuracy: 0.8798\n",
      "Epoch 133/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.3286 - accuracy: 0.8630\n",
      "Epoch 134/1500\n",
      "40/40 [==============================] - 0s 726us/step - loss: 0.3192 - accuracy: 0.8714\n",
      "Epoch 135/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3010 - accuracy: 0.8750\n",
      "Epoch 136/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.3181 - accuracy: 0.8710\n",
      "Epoch 137/1500\n",
      "40/40 [==============================] - 0s 712us/step - loss: 0.3153 - accuracy: 0.8734\n",
      "Epoch 138/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.3007 - accuracy: 0.8754\n",
      "Epoch 139/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3205 - accuracy: 0.8670\n",
      "Epoch 140/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.3082 - accuracy: 0.8734\n",
      "Epoch 141/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.2935 - accuracy: 0.8834\n",
      "Epoch 142/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3106 - accuracy: 0.8730\n",
      "Epoch 143/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.2978 - accuracy: 0.8774\n",
      "Epoch 144/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3316 - accuracy: 0.8622\n",
      "Epoch 145/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.3095 - accuracy: 0.8706\n",
      "Epoch 146/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.2944 - accuracy: 0.8826\n",
      "Epoch 147/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.3110 - accuracy: 0.8774\n",
      "Epoch 148/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3110 - accuracy: 0.8778\n",
      "Epoch 149/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.3192 - accuracy: 0.8646\n",
      "Epoch 150/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.3120 - accuracy: 0.8746\n",
      "Epoch 151/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.3117 - accuracy: 0.8690\n",
      "Epoch 152/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2973 - accuracy: 0.8710\n",
      "Epoch 153/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3115 - accuracy: 0.8698\n",
      "Epoch 154/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.3107 - accuracy: 0.8734\n",
      "Epoch 155/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2979 - accuracy: 0.8762\n",
      "Epoch 156/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2978 - accuracy: 0.8834\n",
      "Epoch 157/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.3246 - accuracy: 0.8698\n",
      "Epoch 158/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2919 - accuracy: 0.8814\n",
      "Epoch 159/1500\n",
      "40/40 [==============================] - 0s 729us/step - loss: 0.3085 - accuracy: 0.8762\n",
      "Epoch 160/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.3093 - accuracy: 0.8710\n",
      "Epoch 161/1500\n",
      "40/40 [==============================] - 0s 707us/step - loss: 0.2973 - accuracy: 0.8770\n",
      "Epoch 162/1500\n",
      "40/40 [==============================] - 0s 705us/step - loss: 0.2999 - accuracy: 0.8750\n",
      "Epoch 163/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.2922 - accuracy: 0.8890\n",
      "Epoch 164/1500\n",
      "40/40 [==============================] - 0s 721us/step - loss: 0.2981 - accuracy: 0.8714\n",
      "Epoch 165/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.2950 - accuracy: 0.8854\n",
      "Epoch 166/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2869 - accuracy: 0.8810\n",
      "Epoch 167/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.3024 - accuracy: 0.8698\n",
      "Epoch 168/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.3012 - accuracy: 0.8778\n",
      "Epoch 169/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.2903 - accuracy: 0.8870\n",
      "Epoch 170/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.2870 - accuracy: 0.8838\n",
      "Epoch 171/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3128 - accuracy: 0.8730\n",
      "Epoch 172/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.2862 - accuracy: 0.8850\n",
      "Epoch 173/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.2921 - accuracy: 0.8798\n",
      "Epoch 174/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2771 - accuracy: 0.8922\n",
      "Epoch 175/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.2873 - accuracy: 0.8854\n",
      "Epoch 176/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2755 - accuracy: 0.8898\n",
      "Epoch 177/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2851 - accuracy: 0.8782\n",
      "Epoch 178/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.2754 - accuracy: 0.8894\n",
      "Epoch 179/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2724 - accuracy: 0.8886\n",
      "Epoch 180/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.2771 - accuracy: 0.8862\n",
      "Epoch 181/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.2781 - accuracy: 0.8910\n",
      "Epoch 182/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.2876 - accuracy: 0.8870\n",
      "Epoch 183/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2803 - accuracy: 0.8882\n",
      "Epoch 184/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2798 - accuracy: 0.8906\n",
      "Epoch 185/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.2943 - accuracy: 0.8830\n",
      "Epoch 186/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2714 - accuracy: 0.8946\n",
      "Epoch 187/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2799 - accuracy: 0.8838\n",
      "Epoch 188/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.2928 - accuracy: 0.8790\n",
      "Epoch 189/1500\n",
      "40/40 [==============================] - 0s 721us/step - loss: 0.2868 - accuracy: 0.8762\n",
      "Epoch 190/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.2684 - accuracy: 0.8866\n",
      "Epoch 191/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2769 - accuracy: 0.8910\n",
      "Epoch 192/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.2808 - accuracy: 0.8890\n",
      "Epoch 193/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.2823 - accuracy: 0.8822\n",
      "Epoch 194/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2653 - accuracy: 0.8946\n",
      "Epoch 195/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.2853 - accuracy: 0.8862\n",
      "Epoch 196/1500\n",
      "40/40 [==============================] - 0s 701us/step - loss: 0.2781 - accuracy: 0.8838\n",
      "Epoch 197/1500\n",
      "40/40 [==============================] - 0s 729us/step - loss: 0.2660 - accuracy: 0.8950\n",
      "Epoch 198/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2725 - accuracy: 0.8850\n",
      "Epoch 199/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.2716 - accuracy: 0.8874\n",
      "Epoch 200/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.2765 - accuracy: 0.8898\n",
      "Epoch 201/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.2675 - accuracy: 0.8974\n",
      "Epoch 202/1500\n",
      "40/40 [==============================] - 0s 728us/step - loss: 0.2686 - accuracy: 0.8906\n",
      "Epoch 203/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.2579 - accuracy: 0.9054\n",
      "Epoch 204/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.2706 - accuracy: 0.8874\n",
      "Epoch 205/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2709 - accuracy: 0.8914\n",
      "Epoch 206/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2474 - accuracy: 0.8950\n",
      "Epoch 207/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2651 - accuracy: 0.8978\n",
      "Epoch 208/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.2755 - accuracy: 0.8910\n",
      "Epoch 209/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.2544 - accuracy: 0.8950\n",
      "Epoch 210/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.2628 - accuracy: 0.8966\n",
      "Epoch 211/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.2587 - accuracy: 0.9006\n",
      "Epoch 212/1500\n",
      "40/40 [==============================] - 0s 711us/step - loss: 0.2650 - accuracy: 0.8942\n",
      "Epoch 213/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2561 - accuracy: 0.8982\n",
      "Epoch 214/1500\n",
      "40/40 [==============================] - 0s 721us/step - loss: 0.2574 - accuracy: 0.8998\n",
      "Epoch 215/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.2630 - accuracy: 0.8930\n",
      "Epoch 216/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2791 - accuracy: 0.8866\n",
      "Epoch 217/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2532 - accuracy: 0.8966\n",
      "Epoch 218/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.2641 - accuracy: 0.8934\n",
      "Epoch 219/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.2680 - accuracy: 0.8910\n",
      "Epoch 220/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2693 - accuracy: 0.8910\n",
      "Epoch 221/1500\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.2789 - accuracy: 0.8938\n",
      "Epoch 222/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.2697 - accuracy: 0.8902\n",
      "Epoch 223/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.2481 - accuracy: 0.9058\n",
      "Epoch 224/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.2490 - accuracy: 0.8978\n",
      "Epoch 225/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.2403 - accuracy: 0.9038\n",
      "Epoch 226/1500\n",
      "40/40 [==============================] - 0s 868us/step - loss: 0.2541 - accuracy: 0.9002\n",
      "Epoch 227/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.2535 - accuracy: 0.8990\n",
      "Epoch 228/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.2700 - accuracy: 0.8902\n",
      "Epoch 229/1500\n",
      "40/40 [==============================] - 0s 870us/step - loss: 0.2451 - accuracy: 0.9109\n",
      "Epoch 230/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2614 - accuracy: 0.9006\n",
      "Epoch 231/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.2518 - accuracy: 0.8990\n",
      "Epoch 232/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2475 - accuracy: 0.9018\n",
      "Epoch 233/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.2513 - accuracy: 0.9038\n",
      "Epoch 234/1500\n",
      "40/40 [==============================] - 0s 726us/step - loss: 0.2507 - accuracy: 0.9034\n",
      "Epoch 235/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.2416 - accuracy: 0.9069\n",
      "Epoch 236/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.2527 - accuracy: 0.9026\n",
      "Epoch 237/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.2492 - accuracy: 0.8938\n",
      "Epoch 238/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.8910\n",
      "Epoch 239/1500\n",
      "40/40 [==============================] - 0s 990us/step - loss: 0.2524 - accuracy: 0.8990\n",
      "Epoch 240/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.2410 - accuracy: 0.9065\n",
      "Epoch 241/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.8934\n",
      "Epoch 242/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2513 - accuracy: 0.8946\n",
      "Epoch 243/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.8958\n",
      "Epoch 244/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2458 - accuracy: 0.8998\n",
      "Epoch 245/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2459 - accuracy: 0.8986\n",
      "Epoch 246/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9137\n",
      "Epoch 247/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.9010\n",
      "Epoch 248/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9038\n",
      "Epoch 249/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.9034\n",
      "Epoch 250/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9077\n",
      "Epoch 251/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.8974\n",
      "Epoch 252/1500\n",
      "40/40 [==============================] - 0s 935us/step - loss: 0.2399 - accuracy: 0.9054\n",
      "Epoch 253/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.2432 - accuracy: 0.9050\n",
      "Epoch 254/1500\n",
      "40/40 [==============================] - 0s 939us/step - loss: 0.2626 - accuracy: 0.8878\n",
      "Epoch 255/1500\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.2501 - accuracy: 0.8958\n",
      "Epoch 256/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.2458 - accuracy: 0.9014\n",
      "Epoch 257/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.2430 - accuracy: 0.9089\n",
      "Epoch 258/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.2404 - accuracy: 0.9097\n",
      "Epoch 259/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.2489 - accuracy: 0.9030\n",
      "Epoch 260/1500\n",
      "40/40 [==============================] - 0s 717us/step - loss: 0.2429 - accuracy: 0.8998\n",
      "Epoch 261/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2341 - accuracy: 0.9042\n",
      "Epoch 262/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.8970\n",
      "Epoch 263/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.8974\n",
      "Epoch 264/1500\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2429 - accuracy: 0.9034\n",
      "Epoch 265/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.8998\n",
      "Epoch 266/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9073\n",
      "Epoch 267/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9050\n",
      "Epoch 268/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.9058\n",
      "Epoch 269/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2360 - accuracy: 0.9077\n",
      "Epoch 270/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.9069\n",
      "Epoch 271/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.9054\n",
      "Epoch 272/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.9137\n",
      "Epoch 273/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2342 - accuracy: 0.9058\n",
      "Epoch 274/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2335 - accuracy: 0.9046\n",
      "Epoch 275/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9050\n",
      "Epoch 276/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.2202 - accuracy: 0.9129\n",
      "Epoch 277/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.2354 - accuracy: 0.9050\n",
      "Epoch 278/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.2370 - accuracy: 0.8986\n",
      "Epoch 279/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.2216 - accuracy: 0.9141\n",
      "Epoch 280/1500\n",
      "40/40 [==============================] - 0s 910us/step - loss: 0.2411 - accuracy: 0.9085\n",
      "Epoch 281/1500\n",
      "40/40 [==============================] - 0s 927us/step - loss: 0.2248 - accuracy: 0.9054\n",
      "Epoch 282/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9097\n",
      "Epoch 283/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9014\n",
      "Epoch 284/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2300 - accuracy: 0.9085\n",
      "Epoch 285/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.9062\n",
      "Epoch 286/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9042\n",
      "Epoch 287/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2246 - accuracy: 0.9173\n",
      "Epoch 288/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9077\n",
      "Epoch 289/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9161\n",
      "Epoch 290/1500\n",
      "40/40 [==============================] - 0s 988us/step - loss: 0.2354 - accuracy: 0.9069\n",
      "Epoch 291/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2159 - accuracy: 0.9141\n",
      "Epoch 292/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9181\n",
      "Epoch 293/1500\n",
      "40/40 [==============================] - 0s 998us/step - loss: 0.2131 - accuracy: 0.9173\n",
      "Epoch 294/1500\n",
      "40/40 [==============================] - 0s 923us/step - loss: 0.2201 - accuracy: 0.9093\n",
      "Epoch 295/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9018\n",
      "Epoch 296/1500\n",
      "40/40 [==============================] - 0s 944us/step - loss: 0.2305 - accuracy: 0.9069\n",
      "Epoch 297/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.2155 - accuracy: 0.9161\n",
      "Epoch 298/1500\n",
      "40/40 [==============================] - 0s 870us/step - loss: 0.2227 - accuracy: 0.9050\n",
      "Epoch 299/1500\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.2219 - accuracy: 0.9149\n",
      "Epoch 300/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.2072 - accuracy: 0.9225\n",
      "Epoch 301/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.2150 - accuracy: 0.9165\n",
      "Epoch 302/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.2206 - accuracy: 0.9113\n",
      "Epoch 303/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.2114 - accuracy: 0.9165\n",
      "Epoch 304/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2063 - accuracy: 0.9121\n",
      "Epoch 305/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.2155 - accuracy: 0.9137\n",
      "Epoch 306/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.2210 - accuracy: 0.9058\n",
      "Epoch 307/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.2104 - accuracy: 0.9157\n",
      "Epoch 308/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9081\n",
      "Epoch 309/1500\n",
      "40/40 [==============================] - 0s 971us/step - loss: 0.2130 - accuracy: 0.9117\n",
      "Epoch 310/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9081\n",
      "Epoch 311/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9129\n",
      "Epoch 312/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9169\n",
      "Epoch 313/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9137\n",
      "Epoch 314/1500\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2020 - accuracy: 0.9213\n",
      "Epoch 315/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2219 - accuracy: 0.9065\n",
      "Epoch 316/1500\n",
      "40/40 [==============================] - 0s 829us/step - loss: 0.2110 - accuracy: 0.9165\n",
      "Epoch 317/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.2242 - accuracy: 0.9145\n",
      "Epoch 318/1500\n",
      "40/40 [==============================] - 0s 983us/step - loss: 0.2269 - accuracy: 0.9093\n",
      "Epoch 319/1500\n",
      "40/40 [==============================] - 0s 977us/step - loss: 0.2093 - accuracy: 0.9205\n",
      "Epoch 320/1500\n",
      "40/40 [==============================] - 0s 996us/step - loss: 0.2114 - accuracy: 0.9129\n",
      "Epoch 321/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9121\n",
      "Epoch 322/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9121\n",
      "Epoch 323/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.2212 - accuracy: 0.9137\n",
      "Epoch 324/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9189\n",
      "Epoch 325/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9165\n",
      "Epoch 326/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9117\n",
      "Epoch 327/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9193\n",
      "Epoch 328/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2244 - accuracy: 0.9085\n",
      "Epoch 329/1500\n",
      "40/40 [==============================] - 0s 998us/step - loss: 0.2291 - accuracy: 0.9105\n",
      "Epoch 330/1500\n",
      "40/40 [==============================] - 0s 884us/step - loss: 0.2076 - accuracy: 0.9141\n",
      "Epoch 331/1500\n",
      "40/40 [==============================] - 0s 953us/step - loss: 0.2157 - accuracy: 0.9133\n",
      "Epoch 332/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9209\n",
      "Epoch 333/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.2091 - accuracy: 0.9241\n",
      "Epoch 334/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.2164 - accuracy: 0.9165\n",
      "Epoch 335/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.2076 - accuracy: 0.9185\n",
      "Epoch 336/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.2163 - accuracy: 0.9153\n",
      "Epoch 337/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.2103 - accuracy: 0.9169\n",
      "Epoch 338/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.1909 - accuracy: 0.9257\n",
      "Epoch 339/1500\n",
      "40/40 [==============================] - 0s 708us/step - loss: 0.2120 - accuracy: 0.9101\n",
      "Epoch 340/1500\n",
      "40/40 [==============================] - 0s 722us/step - loss: 0.2084 - accuracy: 0.9217\n",
      "Epoch 341/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2112 - accuracy: 0.9093\n",
      "Epoch 342/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.2261 - accuracy: 0.9117\n",
      "Epoch 343/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.2092 - accuracy: 0.9145\n",
      "Epoch 344/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.2012 - accuracy: 0.9245\n",
      "Epoch 345/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.2063 - accuracy: 0.9165\n",
      "Epoch 346/1500\n",
      "40/40 [==============================] - 0s 714us/step - loss: 0.2020 - accuracy: 0.9217\n",
      "Epoch 347/1500\n",
      "40/40 [==============================] - 0s 699us/step - loss: 0.2016 - accuracy: 0.9225\n",
      "Epoch 348/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2079 - accuracy: 0.9149\n",
      "Epoch 349/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2163 - accuracy: 0.9165\n",
      "Epoch 350/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.1998 - accuracy: 0.9185\n",
      "Epoch 351/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.2179 - accuracy: 0.9101\n",
      "Epoch 352/1500\n",
      "40/40 [==============================] - 0s 698us/step - loss: 0.2079 - accuracy: 0.9177\n",
      "Epoch 353/1500\n",
      "40/40 [==============================] - 0s 717us/step - loss: 0.2002 - accuracy: 0.9225\n",
      "Epoch 354/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2066 - accuracy: 0.9157\n",
      "Epoch 355/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.2026 - accuracy: 0.9205\n",
      "Epoch 356/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.1874 - accuracy: 0.9237\n",
      "Epoch 357/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.2027 - accuracy: 0.9149\n",
      "Epoch 358/1500\n",
      "40/40 [==============================] - 0s 714us/step - loss: 0.2127 - accuracy: 0.9161\n",
      "Epoch 359/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.1869 - accuracy: 0.9277\n",
      "Epoch 360/1500\n",
      "40/40 [==============================] - 0s 710us/step - loss: 0.2004 - accuracy: 0.9237\n",
      "Epoch 361/1500\n",
      "40/40 [==============================] - 0s 722us/step - loss: 0.2179 - accuracy: 0.9133\n",
      "Epoch 362/1500\n",
      "40/40 [==============================] - 0s 710us/step - loss: 0.1959 - accuracy: 0.9253\n",
      "Epoch 363/1500\n",
      "40/40 [==============================] - 0s 710us/step - loss: 0.1999 - accuracy: 0.9161\n",
      "Epoch 364/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.1867 - accuracy: 0.9345\n",
      "Epoch 365/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2009 - accuracy: 0.9221\n",
      "Epoch 366/1500\n",
      "40/40 [==============================] - 0s 713us/step - loss: 0.2009 - accuracy: 0.9225\n",
      "Epoch 367/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.1996 - accuracy: 0.9213\n",
      "Epoch 368/1500\n",
      "40/40 [==============================] - 0s 699us/step - loss: 0.2003 - accuracy: 0.9189\n",
      "Epoch 369/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.1905 - accuracy: 0.9297\n",
      "Epoch 370/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.1994 - accuracy: 0.9257\n",
      "Epoch 371/1500\n",
      "40/40 [==============================] - 0s 727us/step - loss: 0.1913 - accuracy: 0.9225\n",
      "Epoch 372/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.1983 - accuracy: 0.9233\n",
      "Epoch 373/1500\n",
      "40/40 [==============================] - 0s 700us/step - loss: 0.2022 - accuracy: 0.9225\n",
      "Epoch 374/1500\n",
      "40/40 [==============================] - 0s 721us/step - loss: 0.2040 - accuracy: 0.9197\n",
      "Epoch 375/1500\n",
      "40/40 [==============================] - 0s 726us/step - loss: 0.1985 - accuracy: 0.9185\n",
      "Epoch 376/1500\n",
      "40/40 [==============================] - 0s 695us/step - loss: 0.1789 - accuracy: 0.9265\n",
      "Epoch 377/1500\n",
      "40/40 [==============================] - 0s 688us/step - loss: 0.2119 - accuracy: 0.9221\n",
      "Epoch 378/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.1960 - accuracy: 0.9285\n",
      "Epoch 379/1500\n",
      "40/40 [==============================] - 0s 685us/step - loss: 0.1949 - accuracy: 0.9213\n",
      "Epoch 380/1500\n",
      "40/40 [==============================] - 0s 699us/step - loss: 0.2046 - accuracy: 0.9197\n",
      "Epoch 381/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.1930 - accuracy: 0.9277\n",
      "Epoch 382/1500\n",
      "40/40 [==============================] - 0s 710us/step - loss: 0.1955 - accuracy: 0.9253\n",
      "Epoch 383/1500\n",
      "40/40 [==============================] - 0s 696us/step - loss: 0.1856 - accuracy: 0.9285\n",
      "Epoch 384/1500\n",
      "40/40 [==============================] - 0s 708us/step - loss: 0.1956 - accuracy: 0.9201\n",
      "Epoch 385/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.1823 - accuracy: 0.9309\n",
      "Epoch 386/1500\n",
      "40/40 [==============================] - 0s 715us/step - loss: 0.2047 - accuracy: 0.9193\n",
      "Epoch 387/1500\n",
      "40/40 [==============================] - 0s 710us/step - loss: 0.1861 - accuracy: 0.9277\n",
      "Epoch 388/1500\n",
      "40/40 [==============================] - 0s 711us/step - loss: 0.1996 - accuracy: 0.9257\n",
      "Epoch 389/1500\n",
      "40/40 [==============================] - 0s 720us/step - loss: 0.1829 - accuracy: 0.9301\n",
      "Epoch 390/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.1878 - accuracy: 0.9261\n",
      "Epoch 391/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.1995 - accuracy: 0.9197\n",
      "Epoch 392/1500\n",
      "40/40 [==============================] - 0s 729us/step - loss: 0.2105 - accuracy: 0.9193\n",
      "Epoch 393/1500\n",
      "40/40 [==============================] - 0s 695us/step - loss: 0.1951 - accuracy: 0.9249\n",
      "Epoch 394/1500\n",
      "40/40 [==============================] - 0s 697us/step - loss: 0.1853 - accuracy: 0.9305\n",
      "Epoch 395/1500\n",
      "40/40 [==============================] - 0s 701us/step - loss: 0.1848 - accuracy: 0.9281\n",
      "Epoch 396/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.1774 - accuracy: 0.9289\n",
      "Epoch 397/1500\n",
      "40/40 [==============================] - 0s 699us/step - loss: 0.2008 - accuracy: 0.9197\n",
      "Epoch 398/1500\n",
      "40/40 [==============================] - 0s 715us/step - loss: 0.2044 - accuracy: 0.9161\n",
      "Epoch 399/1500\n",
      "40/40 [==============================] - 0s 714us/step - loss: 0.1932 - accuracy: 0.9209\n",
      "Epoch 400/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.1826 - accuracy: 0.9285\n",
      "Epoch 401/1500\n",
      "40/40 [==============================] - 0s 704us/step - loss: 0.1783 - accuracy: 0.9337\n",
      "Epoch 402/1500\n",
      "40/40 [==============================] - 0s 694us/step - loss: 0.1905 - accuracy: 0.9177\n",
      "Epoch 403/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.1902 - accuracy: 0.9225\n",
      "Epoch 404/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.1883 - accuracy: 0.9209\n",
      "Epoch 405/1500\n",
      "40/40 [==============================] - 0s 715us/step - loss: 0.2002 - accuracy: 0.9181\n",
      "Epoch 406/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.1864 - accuracy: 0.9273\n",
      "Epoch 407/1500\n",
      "40/40 [==============================] - 0s 886us/step - loss: 0.2092 - accuracy: 0.9213\n",
      "Epoch 408/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.1962 - accuracy: 0.9257\n",
      "Epoch 409/1500\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.1880 - accuracy: 0.9253\n",
      "Epoch 410/1500\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.1799 - accuracy: 0.9313\n",
      "Epoch 411/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.1929 - accuracy: 0.9257\n",
      "Epoch 412/1500\n",
      "40/40 [==============================] - 0s 711us/step - loss: 0.1969 - accuracy: 0.9233\n",
      "Epoch 413/1500\n",
      "40/40 [==============================] - 0s 686us/step - loss: 0.1871 - accuracy: 0.9277\n",
      "Epoch 414/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.1890 - accuracy: 0.9317\n",
      "Epoch 415/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.1917 - accuracy: 0.9313\n",
      "Epoch 416/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.1822 - accuracy: 0.9313\n",
      "Epoch 417/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.1844 - accuracy: 0.9261\n",
      "Epoch 418/1500\n",
      "40/40 [==============================] - 0s 899us/step - loss: 0.1902 - accuracy: 0.9241\n",
      "Epoch 419/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.1955 - accuracy: 0.9229\n",
      "Epoch 420/1500\n",
      "40/40 [==============================] - 0s 859us/step - loss: 0.1897 - accuracy: 0.9229\n",
      "Epoch 421/1500\n",
      "40/40 [==============================] - 0s 856us/step - loss: 0.1852 - accuracy: 0.9353\n",
      "Epoch 422/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.2054 - accuracy: 0.9177\n",
      "Epoch 423/1500\n",
      "40/40 [==============================] - 0s 852us/step - loss: 0.1948 - accuracy: 0.9229\n",
      "Epoch 424/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.1894 - accuracy: 0.9273\n",
      "Epoch 425/1500\n",
      "40/40 [==============================] - 0s 720us/step - loss: 0.1861 - accuracy: 0.9325\n",
      "Epoch 426/1500\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.1420 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 396.\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.1952 - accuracy: 0.9257\n",
      "Epoch 426: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.7962 - accuracy: 0.7059\n",
      "5/5 [==============================] - 0s 792us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.68 (17/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.7961812019348145, Accuracy: 0.7058823704719543, Precision: 0.6240503507056987, Recall: 0.7142646625405246, F1 Score: 0.6241660106244841\n",
      "Confusion Matrix:\n",
      " [[82  6 28]\n",
      " [ 8 16  0]\n",
      " [ 3  0 10]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'070A'}\n",
      "Moved to Test Set:\n",
      "{'070A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A'\n",
      " '011A' '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '031A' '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A'\n",
      " '045A' '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '059A' '060A' '062A' '064A' '065A' '069A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A' '039A'\n",
      " '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A' '067A'\n",
      " '068A' '070A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "700\n",
      "Length of y_train_val:\n",
      "700\n",
      "Length of groups_train_val:\n",
      "700\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     443\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     145\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 886, 2: 685, 1: 600})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.0338 - accuracy: 0.5154\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 999us/step - loss: 0.8713 - accuracy: 0.6076\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8005 - accuracy: 0.6490\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.7824 - accuracy: 0.6610\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7764 - accuracy: 0.6578\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.7486 - accuracy: 0.6730\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7314 - accuracy: 0.6684\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6956 - accuracy: 0.6905\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.7070\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6814 - accuracy: 0.7094\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.6832 - accuracy: 0.7047\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.6658 - accuracy: 0.7080\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6423 - accuracy: 0.7292\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6495 - accuracy: 0.7057\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.6515 - accuracy: 0.7135\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.6495 - accuracy: 0.7135\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.6113 - accuracy: 0.7471\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.6120 - accuracy: 0.7324\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.5923 - accuracy: 0.7388\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.5858 - accuracy: 0.7421\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.5901 - accuracy: 0.7513\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.5965 - accuracy: 0.7407\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.5720 - accuracy: 0.7550\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.5714 - accuracy: 0.7411\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.5728 - accuracy: 0.7434\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.5678 - accuracy: 0.7476\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.5581 - accuracy: 0.7462\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.5458 - accuracy: 0.7614\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.5376 - accuracy: 0.7637\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.5322 - accuracy: 0.7711\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.5358 - accuracy: 0.7669\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.5282 - accuracy: 0.7761\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.5546 - accuracy: 0.7545\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.5131 - accuracy: 0.7844\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.5199 - accuracy: 0.7775\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.5226 - accuracy: 0.7711\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.5155 - accuracy: 0.7817\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.5153 - accuracy: 0.7688\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.5036 - accuracy: 0.7803\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 713us/step - loss: 0.5137 - accuracy: 0.7683\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 723us/step - loss: 0.5136 - accuracy: 0.7729\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.5065 - accuracy: 0.7830\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.5104 - accuracy: 0.7748\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4944 - accuracy: 0.7798\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.4868 - accuracy: 0.7872\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4959 - accuracy: 0.7900\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.4843 - accuracy: 0.7904\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.4961 - accuracy: 0.7854\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.4780 - accuracy: 0.7881\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.4703 - accuracy: 0.8019\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.4697 - accuracy: 0.7987\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.4757 - accuracy: 0.7941\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.4617 - accuracy: 0.7973\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.4629 - accuracy: 0.7946\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.4604 - accuracy: 0.8033\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.4544 - accuracy: 0.8038\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.4485 - accuracy: 0.8148\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.4430 - accuracy: 0.8052\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.4580 - accuracy: 0.7982\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.8052\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.4472 - accuracy: 0.7996\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8116\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.4419 - accuracy: 0.8010\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.4363 - accuracy: 0.8070\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.4384 - accuracy: 0.8181\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.4567 - accuracy: 0.8029\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.4521 - accuracy: 0.8015\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.4339 - accuracy: 0.8194\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.4420 - accuracy: 0.8171\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.4161 - accuracy: 0.8250\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.4234 - accuracy: 0.8158\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.4222 - accuracy: 0.8144\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.4270 - accuracy: 0.8158\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.4055 - accuracy: 0.8319\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.4212 - accuracy: 0.8217\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 943us/step - loss: 0.4316 - accuracy: 0.8213\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.4129 - accuracy: 0.8259\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.4055 - accuracy: 0.8144\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.4046 - accuracy: 0.8323\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.4062 - accuracy: 0.8213\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.4089 - accuracy: 0.8245\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.3887 - accuracy: 0.8259\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.4013 - accuracy: 0.8314\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.4029 - accuracy: 0.8194\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.4036 - accuracy: 0.8213\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.4178 - accuracy: 0.8287\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3948 - accuracy: 0.8337\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8333\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.3897 - accuracy: 0.8383\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.3982 - accuracy: 0.8333\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.4017 - accuracy: 0.8356\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 993us/step - loss: 0.3881 - accuracy: 0.8300\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.8356\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.3911 - accuracy: 0.8392\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 959us/step - loss: 0.3919 - accuracy: 0.8245\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.3782 - accuracy: 0.8411\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.3726 - accuracy: 0.8443\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.3695 - accuracy: 0.8521\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.3795 - accuracy: 0.8443\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.3588 - accuracy: 0.8512\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.3744 - accuracy: 0.8443\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.3912 - accuracy: 0.8291\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8480\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 984us/step - loss: 0.3676 - accuracy: 0.8480\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.3698 - accuracy: 0.8457\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.3655 - accuracy: 0.8494\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.3652 - accuracy: 0.8388\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.3501 - accuracy: 0.8475\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.3653 - accuracy: 0.8498\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.3573 - accuracy: 0.8563\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3566 - accuracy: 0.8452\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3553 - accuracy: 0.8600\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.3612 - accuracy: 0.8485\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.3672 - accuracy: 0.8448\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.3554 - accuracy: 0.8475\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.3499 - accuracy: 0.8544\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 965us/step - loss: 0.3400 - accuracy: 0.8673\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.3605 - accuracy: 0.8466\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3631 - accuracy: 0.8526\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.3459 - accuracy: 0.8591\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.3401 - accuracy: 0.8623\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3504 - accuracy: 0.8558\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3496 - accuracy: 0.8485\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.3549 - accuracy: 0.8512\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.3317 - accuracy: 0.8591\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.3426 - accuracy: 0.8544\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3354 - accuracy: 0.8646\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.3256 - accuracy: 0.8655\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8609\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8567\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.3214 - accuracy: 0.8678\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 912us/step - loss: 0.3346 - accuracy: 0.8600\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.3237 - accuracy: 0.8687\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.3239 - accuracy: 0.8738\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.3256 - accuracy: 0.8627\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.3279 - accuracy: 0.8655\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.3299 - accuracy: 0.8591\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.3301 - accuracy: 0.8646\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3196 - accuracy: 0.8664\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.3167 - accuracy: 0.8692\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 999us/step - loss: 0.3247 - accuracy: 0.8706\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8719\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 999us/step - loss: 0.3220 - accuracy: 0.8660\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.3309 - accuracy: 0.8660\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.3138 - accuracy: 0.8660\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.3262 - accuracy: 0.8706\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.3119 - accuracy: 0.8752\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.3178 - accuracy: 0.8729\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.3065 - accuracy: 0.8793\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.3010 - accuracy: 0.8729\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3268 - accuracy: 0.8637\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3138 - accuracy: 0.8683\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.3124 - accuracy: 0.8678\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.3106 - accuracy: 0.8669\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.3143 - accuracy: 0.8701\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.3172 - accuracy: 0.8701\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2974 - accuracy: 0.8802\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8770\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.3016 - accuracy: 0.8816\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2923 - accuracy: 0.8729\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.2963 - accuracy: 0.8770\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8775\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2970 - accuracy: 0.8821\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.2944 - accuracy: 0.8862\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 943us/step - loss: 0.2893 - accuracy: 0.8789\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.2949 - accuracy: 0.8724\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.2934 - accuracy: 0.8775\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2921 - accuracy: 0.8821\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3080 - accuracy: 0.8683\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.2913 - accuracy: 0.8881\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.2849 - accuracy: 0.8871\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2787 - accuracy: 0.8858\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2952 - accuracy: 0.8807\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2741 - accuracy: 0.8913\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.2788 - accuracy: 0.8899\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.2899 - accuracy: 0.8775\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2908 - accuracy: 0.8895\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2892 - accuracy: 0.8825\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.2755 - accuracy: 0.8871\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2740 - accuracy: 0.8876\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 981us/step - loss: 0.2815 - accuracy: 0.8881\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 0.2683 - accuracy: 0.8968\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.2691 - accuracy: 0.8904\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.2783 - accuracy: 0.8871\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2838 - accuracy: 0.8904\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 997us/step - loss: 0.2710 - accuracy: 0.8936\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.2821 - accuracy: 0.8835\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2952 - accuracy: 0.8724\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2735 - accuracy: 0.8858\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2663 - accuracy: 0.8904\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 700us/step - loss: 0.2824 - accuracy: 0.8871\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2679 - accuracy: 0.8987\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2628 - accuracy: 0.8931\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2635 - accuracy: 0.8964\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2630 - accuracy: 0.8927\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.2720 - accuracy: 0.8931\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2630 - accuracy: 0.8973\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8968\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.2657 - accuracy: 0.8945\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.2552 - accuracy: 0.8968\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.2727 - accuracy: 0.8835\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2529 - accuracy: 0.9023\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2717 - accuracy: 0.8899\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2649 - accuracy: 0.8922\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.2574 - accuracy: 0.8964\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2582 - accuracy: 0.8996\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2720 - accuracy: 0.8913\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2551 - accuracy: 0.8977\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2585 - accuracy: 0.8954\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2550 - accuracy: 0.9000\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2613 - accuracy: 0.8973\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2613 - accuracy: 0.8913\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 710us/step - loss: 0.2552 - accuracy: 0.8950\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 705us/step - loss: 0.2627 - accuracy: 0.8950\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2476 - accuracy: 0.8996\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2540 - accuracy: 0.8913\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.2487 - accuracy: 0.9051\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2442 - accuracy: 0.9000\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2462 - accuracy: 0.9065\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2452 - accuracy: 0.9093\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2344 - accuracy: 0.9093\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2704 - accuracy: 0.8858\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2427 - accuracy: 0.8991\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.2466 - accuracy: 0.9047\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2500 - accuracy: 0.8968\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2487 - accuracy: 0.9065\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2407 - accuracy: 0.9028\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2456 - accuracy: 0.8977\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.2475 - accuracy: 0.9028\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 736us/step - loss: 0.2436 - accuracy: 0.9033\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 714us/step - loss: 0.2438 - accuracy: 0.9056\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 712us/step - loss: 0.2419 - accuracy: 0.9037\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 704us/step - loss: 0.2515 - accuracy: 0.9014\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2468 - accuracy: 0.9000\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2375 - accuracy: 0.9088\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2248 - accuracy: 0.9088\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.2247 - accuracy: 0.9083\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 694us/step - loss: 0.2408 - accuracy: 0.9037\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2557 - accuracy: 0.8968\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2408 - accuracy: 0.9083\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.2183 - accuracy: 0.9139\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2348 - accuracy: 0.9120\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2305 - accuracy: 0.9171\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2150 - accuracy: 0.9162\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 717us/step - loss: 0.2365 - accuracy: 0.9051\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.2367 - accuracy: 0.9083\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2254 - accuracy: 0.9093\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2270 - accuracy: 0.9152\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2263 - accuracy: 0.9120\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2320 - accuracy: 0.9033\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.2377 - accuracy: 0.9079\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.2375 - accuracy: 0.9051\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.2273 - accuracy: 0.9083\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2284 - accuracy: 0.9111\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2393 - accuracy: 0.9070\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.2215 - accuracy: 0.9106\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2347 - accuracy: 0.9005\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2217 - accuracy: 0.9097\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2369 - accuracy: 0.9065\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.2238 - accuracy: 0.9116\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2273 - accuracy: 0.9116\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2282 - accuracy: 0.9033\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2115 - accuracy: 0.9231\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2280 - accuracy: 0.9143\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 736us/step - loss: 0.2139 - accuracy: 0.9162\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 708us/step - loss: 0.2303 - accuracy: 0.9079\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 717us/step - loss: 0.2232 - accuracy: 0.9208\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 703us/step - loss: 0.2169 - accuracy: 0.9152\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.2246 - accuracy: 0.9203\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.2199 - accuracy: 0.9102\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 717us/step - loss: 0.2204 - accuracy: 0.9143\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 712us/step - loss: 0.2200 - accuracy: 0.9189\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 700us/step - loss: 0.2216 - accuracy: 0.9180\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 723us/step - loss: 0.2123 - accuracy: 0.9194\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 709us/step - loss: 0.2148 - accuracy: 0.9116\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.2113 - accuracy: 0.9194\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.2163 - accuracy: 0.9148\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.2081 - accuracy: 0.9152\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.2032 - accuracy: 0.9208\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2143 - accuracy: 0.9212\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.2196 - accuracy: 0.9097\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 708us/step - loss: 0.2187 - accuracy: 0.9125\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2070 - accuracy: 0.9152\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2205 - accuracy: 0.9157\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 700us/step - loss: 0.2005 - accuracy: 0.9212\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.2095 - accuracy: 0.9212\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.2216 - accuracy: 0.9102\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.2009 - accuracy: 0.9212\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.2218 - accuracy: 0.9060\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2238 - accuracy: 0.9143\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2212 - accuracy: 0.9116\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2030 - accuracy: 0.9189\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.1895 - accuracy: 0.9171\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.2227 - accuracy: 0.9060\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.2084 - accuracy: 0.9139\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2176 - accuracy: 0.9097\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1935 - accuracy: 0.9235\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.1879 - accuracy: 0.9291\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2034 - accuracy: 0.9199\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2017 - accuracy: 0.9217\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.2083 - accuracy: 0.9162\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.2049 - accuracy: 0.9258\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 714us/step - loss: 0.2120 - accuracy: 0.9222\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.1899 - accuracy: 0.9199\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2214 - accuracy: 0.9120\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 721us/step - loss: 0.1888 - accuracy: 0.9254\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1931 - accuracy: 0.9286\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.1985 - accuracy: 0.9254\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 722us/step - loss: 0.2096 - accuracy: 0.9162\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.2121 - accuracy: 0.9106\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2080 - accuracy: 0.9189\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.1951 - accuracy: 0.9194\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.1919 - accuracy: 0.9226\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 708us/step - loss: 0.1944 - accuracy: 0.9189\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.1964 - accuracy: 0.9240\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1897 - accuracy: 0.9208\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1945 - accuracy: 0.9217\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2107 - accuracy: 0.9203\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 715us/step - loss: 0.1978 - accuracy: 0.9226\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 709us/step - loss: 0.1887 - accuracy: 0.9226\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2008 - accuracy: 0.9222\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2037 - accuracy: 0.9235\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.1891 - accuracy: 0.9300\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 698us/step - loss: 0.1937 - accuracy: 0.9309\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 712us/step - loss: 0.1852 - accuracy: 0.9337\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.1966 - accuracy: 0.9226\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 917us/step - loss: 0.1936 - accuracy: 0.9235\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2044 - accuracy: 0.9226\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1961 - accuracy: 0.9175\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.1931 - accuracy: 0.9231\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.2030 - accuracy: 0.9222\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.1890 - accuracy: 0.9240\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.1795 - accuracy: 0.9268\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.1948 - accuracy: 0.9199\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.1909 - accuracy: 0.9258\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1877 - accuracy: 0.9277\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2109 - accuracy: 0.9189\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.1937 - accuracy: 0.9249\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 705us/step - loss: 0.1888 - accuracy: 0.9231\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2019 - accuracy: 0.9226\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1817 - accuracy: 0.9309\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1861 - accuracy: 0.9327\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.1960 - accuracy: 0.9254\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1996 - accuracy: 0.9268\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 714us/step - loss: 0.1920 - accuracy: 0.9272\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 710us/step - loss: 0.2008 - accuracy: 0.9222\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.1772 - accuracy: 0.9300\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 726us/step - loss: 0.2025 - accuracy: 0.9226\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 703us/step - loss: 0.2012 - accuracy: 0.9217\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1848 - accuracy: 0.9281\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.1844 - accuracy: 0.9249\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 682us/step - loss: 0.1782 - accuracy: 0.9300\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.1698 - accuracy: 0.9346\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 701us/step - loss: 0.2132 - accuracy: 0.9152\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1900 - accuracy: 0.9171\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1783 - accuracy: 0.9304\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2105 - accuracy: 0.9111\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 722us/step - loss: 0.1855 - accuracy: 0.9240\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.1720 - accuracy: 0.9346\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 732us/step - loss: 0.1844 - accuracy: 0.9304\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1829 - accuracy: 0.9263\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.1878 - accuracy: 0.9240\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 716us/step - loss: 0.1808 - accuracy: 0.9309\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1905 - accuracy: 0.9268\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.1662 - accuracy: 0.9341\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.1792 - accuracy: 0.9291\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.1740 - accuracy: 0.9341\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.1878 - accuracy: 0.9235\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1798 - accuracy: 0.9291\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.1585 - accuracy: 0.9378\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1853 - accuracy: 0.9300\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 717us/step - loss: 0.1626 - accuracy: 0.9415\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1889 - accuracy: 0.9208\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1665 - accuracy: 0.9374\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.1895 - accuracy: 0.9226\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.1745 - accuracy: 0.9364\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1664 - accuracy: 0.9351\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 714us/step - loss: 0.1794 - accuracy: 0.9286\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 715us/step - loss: 0.1671 - accuracy: 0.9392\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.1817 - accuracy: 0.9314\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 718us/step - loss: 0.1580 - accuracy: 0.9406\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1781 - accuracy: 0.9281\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.1792 - accuracy: 0.9277\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1826 - accuracy: 0.9304\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.1822 - accuracy: 0.9309\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.1731 - accuracy: 0.9304\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.1727 - accuracy: 0.9327\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1773 - accuracy: 0.9291\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.1786 - accuracy: 0.9332\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1800 - accuracy: 0.9277\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.1734 - accuracy: 0.9378\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.1607 - accuracy: 0.9300\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.1807 - accuracy: 0.9300\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1869 - accuracy: 0.9277\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.1660 - accuracy: 0.9323\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.1716 - accuracy: 0.9424\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.1620 - accuracy: 0.9374\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 719us/step - loss: 0.1843 - accuracy: 0.9268\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1632 - accuracy: 0.9461\n",
      "Epoch 400/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1308 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 370.\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1638 - accuracy: 0.9383\n",
      "Epoch 400: early stopping\n",
      "8/8 [==============================] - 0s 778us/step - loss: 0.6988 - accuracy: 0.7342\n",
      "8/8 [==============================] - 0s 601us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "Final Test Results - Loss: 0.69877028465271, Accuracy: 0.7341772317886353, Precision: 0.7359755537174891, Recall: 0.6061071807472639, F1 Score: 0.6456139564839966\n",
      "Confusion Matrix:\n",
      " [[131   3  11]\n",
      " [ 23  28   0]\n",
      " [ 26   0  15]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 902, 1: 810, 2: 505})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "35/35 [==============================] - 0s 947us/step - loss: 1.0802 - accuracy: 0.5133\n",
      "Epoch 2/1500\n",
      "35/35 [==============================] - 0s 992us/step - loss: 0.8891 - accuracy: 0.6189\n",
      "Epoch 3/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.8117 - accuracy: 0.6504\n",
      "Epoch 4/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.7356 - accuracy: 0.6960\n",
      "Epoch 5/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.6830 - accuracy: 0.7217\n",
      "Epoch 6/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.6697 - accuracy: 0.7212\n",
      "Epoch 7/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.6650 - accuracy: 0.7226\n",
      "Epoch 8/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.6045 - accuracy: 0.7397\n",
      "Epoch 9/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.6195 - accuracy: 0.7334\n",
      "Epoch 10/1500\n",
      "35/35 [==============================] - 0s 793us/step - loss: 0.5980 - accuracy: 0.7510\n",
      "Epoch 11/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.5731 - accuracy: 0.7623\n",
      "Epoch 12/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.5603 - accuracy: 0.7686\n",
      "Epoch 13/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.5514 - accuracy: 0.7632\n",
      "Epoch 14/1500\n",
      "35/35 [==============================] - 0s 799us/step - loss: 0.5384 - accuracy: 0.7767\n",
      "Epoch 15/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.5518 - accuracy: 0.7713\n",
      "Epoch 16/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.5347 - accuracy: 0.7731\n",
      "Epoch 17/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.5302 - accuracy: 0.7830\n",
      "Epoch 18/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.5150 - accuracy: 0.7907\n",
      "Epoch 19/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.4996 - accuracy: 0.7970\n",
      "Epoch 20/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.4884 - accuracy: 0.7898\n",
      "Epoch 21/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.5065 - accuracy: 0.7880\n",
      "Epoch 22/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.4840 - accuracy: 0.7952\n",
      "Epoch 23/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.4971 - accuracy: 0.7876\n",
      "Epoch 24/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.4778 - accuracy: 0.8038\n",
      "Epoch 25/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.4600 - accuracy: 0.8115\n",
      "Epoch 26/1500\n",
      "35/35 [==============================] - 0s 785us/step - loss: 0.4543 - accuracy: 0.8088\n",
      "Epoch 27/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.4542 - accuracy: 0.8110\n",
      "Epoch 28/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.4538 - accuracy: 0.8128\n",
      "Epoch 29/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.4308 - accuracy: 0.8295\n",
      "Epoch 30/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.4306 - accuracy: 0.8209\n",
      "Epoch 31/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.4328 - accuracy: 0.8218\n",
      "Epoch 32/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.4323 - accuracy: 0.8214\n",
      "Epoch 33/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.4369 - accuracy: 0.8205\n",
      "Epoch 34/1500\n",
      "35/35 [==============================] - 0s 954us/step - loss: 0.4214 - accuracy: 0.8250\n",
      "Epoch 35/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.4160 - accuracy: 0.8218\n",
      "Epoch 36/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.4180 - accuracy: 0.8254\n",
      "Epoch 37/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.4069 - accuracy: 0.8340\n",
      "Epoch 38/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.4120 - accuracy: 0.8281\n",
      "Epoch 39/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.4034 - accuracy: 0.8259\n",
      "Epoch 40/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.4044 - accuracy: 0.8254\n",
      "Epoch 41/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.4028 - accuracy: 0.8286\n",
      "Epoch 42/1500\n",
      "35/35 [==============================] - 0s 984us/step - loss: 0.4093 - accuracy: 0.8223\n",
      "Epoch 43/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.3927 - accuracy: 0.8403\n",
      "Epoch 44/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.3858 - accuracy: 0.8421\n",
      "Epoch 45/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.3981 - accuracy: 0.8385\n",
      "Epoch 46/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.3874 - accuracy: 0.8358\n",
      "Epoch 47/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.3693 - accuracy: 0.8593\n",
      "Epoch 48/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.3879 - accuracy: 0.8412\n",
      "Epoch 49/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.3791 - accuracy: 0.8475\n",
      "Epoch 50/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.3707 - accuracy: 0.8457\n",
      "Epoch 51/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.3784 - accuracy: 0.8430\n",
      "Epoch 52/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.3797 - accuracy: 0.8439\n",
      "Epoch 53/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.3712 - accuracy: 0.8435\n",
      "Epoch 54/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.3683 - accuracy: 0.8471\n",
      "Epoch 55/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.3795 - accuracy: 0.8457\n",
      "Epoch 56/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.3501 - accuracy: 0.8602\n",
      "Epoch 57/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.3545 - accuracy: 0.8507\n",
      "Epoch 58/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.3550 - accuracy: 0.8602\n",
      "Epoch 59/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.3545 - accuracy: 0.8480\n",
      "Epoch 60/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.3513 - accuracy: 0.8696\n",
      "Epoch 61/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.3596 - accuracy: 0.8457\n",
      "Epoch 62/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.3508 - accuracy: 0.8534\n",
      "Epoch 63/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.3535 - accuracy: 0.8521\n",
      "Epoch 64/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.3441 - accuracy: 0.8660\n",
      "Epoch 65/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.3284 - accuracy: 0.8606\n",
      "Epoch 66/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.3382 - accuracy: 0.8615\n",
      "Epoch 67/1500\n",
      "35/35 [==============================] - 0s 921us/step - loss: 0.3339 - accuracy: 0.8656\n",
      "Epoch 68/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.3313 - accuracy: 0.8683\n",
      "Epoch 69/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.3302 - accuracy: 0.8638\n",
      "Epoch 70/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.3308 - accuracy: 0.8629\n",
      "Epoch 71/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.3317 - accuracy: 0.8620\n",
      "Epoch 72/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.3536 - accuracy: 0.8534\n",
      "Epoch 73/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.3361 - accuracy: 0.8629\n",
      "Epoch 74/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.3440 - accuracy: 0.8548\n",
      "Epoch 75/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.3297 - accuracy: 0.8642\n",
      "Epoch 76/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.3203 - accuracy: 0.8683\n",
      "Epoch 77/1500\n",
      "35/35 [==============================] - 0s 964us/step - loss: 0.3112 - accuracy: 0.8714\n",
      "Epoch 78/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.3049 - accuracy: 0.8814\n",
      "Epoch 79/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.3217 - accuracy: 0.8647\n",
      "Epoch 80/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.3234 - accuracy: 0.8656\n",
      "Epoch 81/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.3170 - accuracy: 0.8791\n",
      "Epoch 82/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.3202 - accuracy: 0.8642\n",
      "Epoch 83/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.3177 - accuracy: 0.8611\n",
      "Epoch 84/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.3126 - accuracy: 0.8737\n",
      "Epoch 85/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.2967 - accuracy: 0.8823\n",
      "Epoch 86/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.2969 - accuracy: 0.8787\n",
      "Epoch 87/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.3217 - accuracy: 0.8665\n",
      "Epoch 88/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.3096 - accuracy: 0.8737\n",
      "Epoch 89/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.3022 - accuracy: 0.8778\n",
      "Epoch 90/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.2851 - accuracy: 0.8827\n",
      "Epoch 91/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.3081 - accuracy: 0.8760\n",
      "Epoch 92/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.3005 - accuracy: 0.8742\n",
      "Epoch 93/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.3075 - accuracy: 0.8705\n",
      "Epoch 94/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.3025 - accuracy: 0.8755\n",
      "Epoch 95/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.3183 - accuracy: 0.8660\n",
      "Epoch 96/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.2949 - accuracy: 0.8809\n",
      "Epoch 97/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.2985 - accuracy: 0.8796\n",
      "Epoch 98/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.2992 - accuracy: 0.8764\n",
      "Epoch 99/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.2921 - accuracy: 0.8904\n",
      "Epoch 100/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.2987 - accuracy: 0.8755\n",
      "Epoch 101/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.2931 - accuracy: 0.8773\n",
      "Epoch 102/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.2896 - accuracy: 0.8886\n",
      "Epoch 103/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.2850 - accuracy: 0.8778\n",
      "Epoch 104/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.2889 - accuracy: 0.8791\n",
      "Epoch 105/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.2716 - accuracy: 0.8881\n",
      "Epoch 106/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.2861 - accuracy: 0.8827\n",
      "Epoch 107/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2817 - accuracy: 0.8787\n",
      "Epoch 108/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.2824 - accuracy: 0.8787\n",
      "Epoch 109/1500\n",
      "35/35 [==============================] - 0s 760us/step - loss: 0.2884 - accuracy: 0.8814\n",
      "Epoch 110/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.2720 - accuracy: 0.8841\n",
      "Epoch 111/1500\n",
      "35/35 [==============================] - 0s 728us/step - loss: 0.2972 - accuracy: 0.8800\n",
      "Epoch 112/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.2803 - accuracy: 0.8854\n",
      "Epoch 113/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.2758 - accuracy: 0.8841\n",
      "Epoch 114/1500\n",
      "35/35 [==============================] - 0s 717us/step - loss: 0.2679 - accuracy: 0.8972\n",
      "Epoch 115/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.2931 - accuracy: 0.8764\n",
      "Epoch 116/1500\n",
      "35/35 [==============================] - 0s 716us/step - loss: 0.2786 - accuracy: 0.8931\n",
      "Epoch 117/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.2693 - accuracy: 0.8913\n",
      "Epoch 118/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.2592 - accuracy: 0.8890\n",
      "Epoch 119/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.2777 - accuracy: 0.8850\n",
      "Epoch 120/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.2718 - accuracy: 0.8954\n",
      "Epoch 121/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2831 - accuracy: 0.8854\n",
      "Epoch 122/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.2588 - accuracy: 0.8899\n",
      "Epoch 123/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.2796 - accuracy: 0.8782\n",
      "Epoch 124/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.2658 - accuracy: 0.8872\n",
      "Epoch 125/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.2573 - accuracy: 0.8926\n",
      "Epoch 126/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.2579 - accuracy: 0.8954\n",
      "Epoch 127/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.2520 - accuracy: 0.9008\n",
      "Epoch 128/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.2617 - accuracy: 0.8972\n",
      "Epoch 129/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.2478 - accuracy: 0.8985\n",
      "Epoch 130/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.2642 - accuracy: 0.8954\n",
      "Epoch 131/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.2535 - accuracy: 0.8940\n",
      "Epoch 132/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.2716 - accuracy: 0.8895\n",
      "Epoch 133/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.2544 - accuracy: 0.8958\n",
      "Epoch 134/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.2570 - accuracy: 0.9030\n",
      "Epoch 135/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.2696 - accuracy: 0.8945\n",
      "Epoch 136/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.2550 - accuracy: 0.8908\n",
      "Epoch 137/1500\n",
      "35/35 [==============================] - 0s 752us/step - loss: 0.2538 - accuracy: 0.9021\n",
      "Epoch 138/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.2542 - accuracy: 0.8967\n",
      "Epoch 139/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.2815 - accuracy: 0.8868\n",
      "Epoch 140/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2422 - accuracy: 0.9057\n",
      "Epoch 141/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.2454 - accuracy: 0.9003\n",
      "Epoch 142/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.2592 - accuracy: 0.8877\n",
      "Epoch 143/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.2405 - accuracy: 0.9062\n",
      "Epoch 144/1500\n",
      "35/35 [==============================] - 0s 726us/step - loss: 0.2428 - accuracy: 0.9012\n",
      "Epoch 145/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.2481 - accuracy: 0.9071\n",
      "Epoch 146/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.2581 - accuracy: 0.8940\n",
      "Epoch 147/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.2553 - accuracy: 0.8999\n",
      "Epoch 148/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.2422 - accuracy: 0.8976\n",
      "Epoch 149/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.2477 - accuracy: 0.9008\n",
      "Epoch 150/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.2508 - accuracy: 0.8994\n",
      "Epoch 151/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2550 - accuracy: 0.8954\n",
      "Epoch 152/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.2139 - accuracy: 0.9211\n",
      "Epoch 153/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.2232 - accuracy: 0.9134\n",
      "Epoch 154/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.2265 - accuracy: 0.9107\n",
      "Epoch 155/1500\n",
      "35/35 [==============================] - 0s 728us/step - loss: 0.2472 - accuracy: 0.8990\n",
      "Epoch 156/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.2219 - accuracy: 0.9039\n",
      "Epoch 157/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.2430 - accuracy: 0.9066\n",
      "Epoch 158/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.2311 - accuracy: 0.9057\n",
      "Epoch 159/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.2425 - accuracy: 0.8994\n",
      "Epoch 160/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.2267 - accuracy: 0.9075\n",
      "Epoch 161/1500\n",
      "35/35 [==============================] - 0s 718us/step - loss: 0.2285 - accuracy: 0.9075\n",
      "Epoch 162/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2304 - accuracy: 0.9062\n",
      "Epoch 163/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.2439 - accuracy: 0.9053\n",
      "Epoch 164/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.2380 - accuracy: 0.9044\n",
      "Epoch 165/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2191 - accuracy: 0.9188\n",
      "Epoch 166/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.2323 - accuracy: 0.9089\n",
      "Epoch 167/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.2431 - accuracy: 0.9035\n",
      "Epoch 168/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.2349 - accuracy: 0.9066\n",
      "Epoch 169/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.2164 - accuracy: 0.9157\n",
      "Epoch 170/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.2252 - accuracy: 0.9098\n",
      "Epoch 171/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.2327 - accuracy: 0.9125\n",
      "Epoch 172/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.2235 - accuracy: 0.9143\n",
      "Epoch 173/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.2449 - accuracy: 0.8976\n",
      "Epoch 174/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.2304 - accuracy: 0.9098\n",
      "Epoch 175/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.9039\n",
      "Epoch 176/1500\n",
      "35/35 [==============================] - 0s 969us/step - loss: 0.2204 - accuracy: 0.9071\n",
      "Epoch 177/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2222 - accuracy: 0.9134\n",
      "Epoch 178/1500\n",
      "35/35 [==============================] - 0s 997us/step - loss: 0.2179 - accuracy: 0.9161\n",
      "Epoch 179/1500\n",
      "35/35 [==============================] - 0s 995us/step - loss: 0.2165 - accuracy: 0.9120\n",
      "Epoch 180/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.2238 - accuracy: 0.9093\n",
      "Epoch 181/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.2210 - accuracy: 0.9053\n",
      "Epoch 182/1500\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.0877 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 152.\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.2342 - accuracy: 0.9035\n",
      "Epoch 182: early stopping\n",
      "7/7 [==============================] - 0s 719us/step - loss: 0.9147 - accuracy: 0.6413\n",
      "7/7 [==============================] - 0s 561us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 0.9147074818611145, Accuracy: 0.6412556171417236, Precision: 0.5971032628846866, Recall: 0.721932568647897, F1 Score: 0.6370440668741603\n",
      "Confusion Matrix:\n",
      " [[101   7  29]\n",
      " [  0   9   0]\n",
      " [ 44   0  33]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 864, 1: 730, 2: 655})\n",
      "Epoch 1/1500\n",
      "36/36 [==============================] - 0s 975us/step - loss: 1.4851 - accuracy: 0.3708\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 923us/step - loss: 1.0483 - accuracy: 0.5385\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.9413 - accuracy: 0.5967\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.8557 - accuracy: 0.6327\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.8249 - accuracy: 0.6541\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.7853 - accuracy: 0.6687\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.7574 - accuracy: 0.6799\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.7173 - accuracy: 0.6896\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.7130 - accuracy: 0.6954\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 0.6906 - accuracy: 0.6976\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 758us/step - loss: 0.6894 - accuracy: 0.7159\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 0.6770 - accuracy: 0.7065\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.6558 - accuracy: 0.7230\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 0.6393 - accuracy: 0.7177\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.6382 - accuracy: 0.7163\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 806us/step - loss: 0.6142 - accuracy: 0.7403\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.5973 - accuracy: 0.7390\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.6182 - accuracy: 0.7354\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.6009 - accuracy: 0.7359\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.5924 - accuracy: 0.7399\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 0.5934 - accuracy: 0.7377\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.5865 - accuracy: 0.7448\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 711us/step - loss: 0.5604 - accuracy: 0.7568\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.5511 - accuracy: 0.7594\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.5618 - accuracy: 0.7666\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.5422 - accuracy: 0.7666\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.5512 - accuracy: 0.7768\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.5475 - accuracy: 0.7670\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.5337 - accuracy: 0.7715\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.5368 - accuracy: 0.7723\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 758us/step - loss: 0.5284 - accuracy: 0.7639\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 0.5120 - accuracy: 0.7777\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.5183 - accuracy: 0.7812\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.5093 - accuracy: 0.7635\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 707us/step - loss: 0.5146 - accuracy: 0.7830\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.4989 - accuracy: 0.7959\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - 0s 723us/step - loss: 0.5112 - accuracy: 0.7803\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.5074 - accuracy: 0.7892\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.5054 - accuracy: 0.7852\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.4777 - accuracy: 0.7937\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.4944 - accuracy: 0.7852\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 707us/step - loss: 0.4886 - accuracy: 0.7955\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 747us/step - loss: 0.4699 - accuracy: 0.7959\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.4947 - accuracy: 0.7799\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.4757 - accuracy: 0.8048\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.4918 - accuracy: 0.7924\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.4642 - accuracy: 0.8052\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.4631 - accuracy: 0.8061\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.4643 - accuracy: 0.8039\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 965us/step - loss: 0.4740 - accuracy: 0.7995\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.4526 - accuracy: 0.8115\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 936us/step - loss: 0.4599 - accuracy: 0.8084\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 911us/step - loss: 0.4603 - accuracy: 0.8097\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 911us/step - loss: 0.4503 - accuracy: 0.8088\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.4475 - accuracy: 0.8181\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.4359 - accuracy: 0.8186\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.4422 - accuracy: 0.8141\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 818us/step - loss: 0.4395 - accuracy: 0.8164\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.4254 - accuracy: 0.8199\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.4108 - accuracy: 0.8208\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.4259 - accuracy: 0.8181\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.4393 - accuracy: 0.8128\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.4192 - accuracy: 0.8141\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 889us/step - loss: 0.4340 - accuracy: 0.8084\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.4186 - accuracy: 0.8204\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.4297 - accuracy: 0.8173\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.4280 - accuracy: 0.8230\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 0.4108 - accuracy: 0.8319\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.4255 - accuracy: 0.8230\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.4221 - accuracy: 0.8239\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.4066 - accuracy: 0.8373\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 990us/step - loss: 0.3924 - accuracy: 0.8341\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.4126 - accuracy: 0.8248\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.4146 - accuracy: 0.8266\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 936us/step - loss: 0.4046 - accuracy: 0.8444\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3989 - accuracy: 0.8399\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.3972 - accuracy: 0.8355\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 887us/step - loss: 0.3876 - accuracy: 0.8382\n",
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 995us/step - loss: 0.3935 - accuracy: 0.8324\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 981us/step - loss: 0.4002 - accuracy: 0.8390\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8364\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.3975 - accuracy: 0.8368\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.3932 - accuracy: 0.8404\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.3812 - accuracy: 0.8502\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 713us/step - loss: 0.4003 - accuracy: 0.8284\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 758us/step - loss: 0.3769 - accuracy: 0.8417\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.4000 - accuracy: 0.8413\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.3683 - accuracy: 0.8524\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.3887 - accuracy: 0.8417\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.3906 - accuracy: 0.8346\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.3695 - accuracy: 0.8462\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 975us/step - loss: 0.3641 - accuracy: 0.8502\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 925us/step - loss: 0.3787 - accuracy: 0.8462\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.3693 - accuracy: 0.8493\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.3696 - accuracy: 0.8466\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.3688 - accuracy: 0.8453\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.3630 - accuracy: 0.8502\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.3634 - accuracy: 0.8470\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 710us/step - loss: 0.3738 - accuracy: 0.8435\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.3694 - accuracy: 0.8479\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.3598 - accuracy: 0.8542\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.3796 - accuracy: 0.8448\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 882us/step - loss: 0.3510 - accuracy: 0.8622\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.3622 - accuracy: 0.8524\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.3569 - accuracy: 0.8524\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 938us/step - loss: 0.3531 - accuracy: 0.8564\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 958us/step - loss: 0.3529 - accuracy: 0.8577\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 994us/step - loss: 0.3644 - accuracy: 0.8475\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 957us/step - loss: 0.3574 - accuracy: 0.8537\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.3563 - accuracy: 0.8533\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.3500 - accuracy: 0.8510\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.3620 - accuracy: 0.8497\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.3476 - accuracy: 0.8564\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 806us/step - loss: 0.3559 - accuracy: 0.8559\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.3576 - accuracy: 0.8510\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.3339 - accuracy: 0.8604\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.3526 - accuracy: 0.8586\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.3455 - accuracy: 0.8546\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.3486 - accuracy: 0.8542\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.3542 - accuracy: 0.8648\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 719us/step - loss: 0.3296 - accuracy: 0.8662\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.3362 - accuracy: 0.8595\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.3367 - accuracy: 0.8648\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 730us/step - loss: 0.3289 - accuracy: 0.8657\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.3327 - accuracy: 0.8622\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.3414 - accuracy: 0.8604\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 706us/step - loss: 0.3538 - accuracy: 0.8479\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 717us/step - loss: 0.3485 - accuracy: 0.8564\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 0.3332 - accuracy: 0.8626\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.3266 - accuracy: 0.8724\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.3201 - accuracy: 0.8737\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.3345 - accuracy: 0.8648\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 732us/step - loss: 0.3291 - accuracy: 0.8657\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.3254 - accuracy: 0.8639\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.3281 - accuracy: 0.8706\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.3229 - accuracy: 0.8733\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 723us/step - loss: 0.3224 - accuracy: 0.8671\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.3446 - accuracy: 0.8582\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 766us/step - loss: 0.3270 - accuracy: 0.8648\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.3317 - accuracy: 0.8675\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 723us/step - loss: 0.3293 - accuracy: 0.8631\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 716us/step - loss: 0.3186 - accuracy: 0.8724\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 0.3161 - accuracy: 0.8706\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 725us/step - loss: 0.3077 - accuracy: 0.8711\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.3256 - accuracy: 0.8719\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.3158 - accuracy: 0.8724\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 731us/step - loss: 0.2944 - accuracy: 0.8804\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 737us/step - loss: 0.3343 - accuracy: 0.8693\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.2994 - accuracy: 0.8759\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.3130 - accuracy: 0.8662\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.3123 - accuracy: 0.8777\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.3008 - accuracy: 0.8826\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 710us/step - loss: 0.3013 - accuracy: 0.8724\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 0.3132 - accuracy: 0.8751\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 0.3060 - accuracy: 0.8742\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 980us/step - loss: 0.3033 - accuracy: 0.8728\n",
      "Epoch 157/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.8804\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 991us/step - loss: 0.3209 - accuracy: 0.8715\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3008 - accuracy: 0.8773\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8715\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 957us/step - loss: 0.2976 - accuracy: 0.8786\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3107 - accuracy: 0.8759\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 827us/step - loss: 0.3045 - accuracy: 0.8839\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.3127 - accuracy: 0.8724\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.3061 - accuracy: 0.8742\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.2980 - accuracy: 0.8746\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.3039 - accuracy: 0.8826\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.3019 - accuracy: 0.8724\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.2888 - accuracy: 0.8822\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2897 - accuracy: 0.8808\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.2924 - accuracy: 0.8808\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.2925 - accuracy: 0.8839\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 997us/step - loss: 0.2979 - accuracy: 0.8817\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8862\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 947us/step - loss: 0.2883 - accuracy: 0.8813\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 905us/step - loss: 0.2832 - accuracy: 0.8902\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.2996 - accuracy: 0.8826\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - 0s 984us/step - loss: 0.2707 - accuracy: 0.8942\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.3082 - accuracy: 0.8826\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 0.2843 - accuracy: 0.8808\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 742us/step - loss: 0.2806 - accuracy: 0.8928\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.2783 - accuracy: 0.8897\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.2990 - accuracy: 0.8777\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.2936 - accuracy: 0.8817\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.2874 - accuracy: 0.8853\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.2854 - accuracy: 0.8866\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2762 - accuracy: 0.8928\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8822\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2872 - accuracy: 0.8791\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8928\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.8933\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.2797 - accuracy: 0.8946\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 984us/step - loss: 0.2773 - accuracy: 0.8875\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 947us/step - loss: 0.2768 - accuracy: 0.8884\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 982us/step - loss: 0.2714 - accuracy: 0.8857\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2798 - accuracy: 0.8884\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.2734 - accuracy: 0.8871\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.2764 - accuracy: 0.8880\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.2768 - accuracy: 0.8862\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.2728 - accuracy: 0.8915\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 949us/step - loss: 0.2801 - accuracy: 0.8866\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.2685 - accuracy: 0.8933\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.2867 - accuracy: 0.8822\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 944us/step - loss: 0.2690 - accuracy: 0.8960\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.9071\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8893\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.2790 - accuracy: 0.8875\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 766us/step - loss: 0.2763 - accuracy: 0.8915\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2674 - accuracy: 0.8888\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.2695 - accuracy: 0.8920\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 911us/step - loss: 0.2771 - accuracy: 0.8915\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.2713 - accuracy: 0.8906\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.2566 - accuracy: 0.8960\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.2483 - accuracy: 0.9022\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.2705 - accuracy: 0.8924\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 720us/step - loss: 0.2618 - accuracy: 0.8937\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 721us/step - loss: 0.2689 - accuracy: 0.8942\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 742us/step - loss: 0.2552 - accuracy: 0.8995\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 717us/step - loss: 0.2541 - accuracy: 0.9022\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.2573 - accuracy: 0.8995\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.2646 - accuracy: 0.8906\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.2474 - accuracy: 0.8960\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2729 - accuracy: 0.8920\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.2414 - accuracy: 0.9008\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.2675 - accuracy: 0.8937\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.2613 - accuracy: 0.9000\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.2600 - accuracy: 0.8995\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 714us/step - loss: 0.2563 - accuracy: 0.8964\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 711us/step - loss: 0.2600 - accuracy: 0.8946\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 722us/step - loss: 0.2484 - accuracy: 0.8968\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 736us/step - loss: 0.2412 - accuracy: 0.9040\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2629 - accuracy: 0.8982\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.2483 - accuracy: 0.9071\n",
      "Epoch 234/1500\n",
      "36/36 [==============================] - 0s 738us/step - loss: 0.2504 - accuracy: 0.9004\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.2692 - accuracy: 0.8968\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 707us/step - loss: 0.2567 - accuracy: 0.9013\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 702us/step - loss: 0.2554 - accuracy: 0.8982\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 709us/step - loss: 0.2409 - accuracy: 0.9040\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.2563 - accuracy: 0.8960\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.2412 - accuracy: 0.9040\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 724us/step - loss: 0.2534 - accuracy: 0.8995\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.2503 - accuracy: 0.9062\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.2500 - accuracy: 0.9040\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 0.2494 - accuracy: 0.9013\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 719us/step - loss: 0.2405 - accuracy: 0.9022\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2527 - accuracy: 0.8937\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.2526 - accuracy: 0.9035\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.2506 - accuracy: 0.9066\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 716us/step - loss: 0.2475 - accuracy: 0.8924\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.2420 - accuracy: 0.9066\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.2488 - accuracy: 0.8973\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 729us/step - loss: 0.2480 - accuracy: 0.9040\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 747us/step - loss: 0.2683 - accuracy: 0.8964\n",
      "Epoch 254/1500\n",
      " 1/36 [..............................] - ETA: 0s - loss: 0.2972 - accuracy: 0.8438Restoring model weights from the end of the best epoch: 224.\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2411 - accuracy: 0.9048\n",
      "Epoch 254: early stopping\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.5165 - accuracy: 0.7807\n",
      "8/8 [==============================] - 0s 616us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n",
      "Final Test Results - Loss: 0.5165460109710693, Accuracy: 0.780701756477356, Precision: 0.7544229497354498, Recall: 0.8261702127659575, F1 Score: 0.7768412174861972\n",
      "Confusion Matrix:\n",
      " [[117   5  34]\n",
      " [  2  23   0]\n",
      " [  9   0  38]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6709163128672095\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7315512448549271\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7155042439699173\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.677888029260831\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7171186561754107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, kitten, kitten, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, adult, kitten, adult, adult, a...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[kitten, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[senior, senior, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, kitten, kitten, senior, adult, adult, ...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "76    070A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, kitten, adult, adult, adult, ad...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, kitten, adult, adult, ad...         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A     [adult, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [senior, adult, kitten, adult, adult, adult, a...         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, adult, kitten, adult, adult, a...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "25    023A       [kitten, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, senior, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, senior, senior, senior, adult, ...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "12    011A                                   [senior, senior]        senior           senior                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C              [adult, adult, adult, senior, senior]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, kitten, adult, adult, ki...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, senior, adult, se...         adult            adult                   True\n",
       "41    035A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "52    047A  [kitten, adult, adult, kitten, adult, adult, k...         adult           kitten                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, senior, senior, senior, adult,...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "42    036A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "40    034A            [senior, senior, senior, senior, adult]        senior            adult                  False\n",
       "92    097A  [senior, senior, senior, adult, senior, senior...         adult           senior                  False\n",
       "57    051B  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "71    065A  [senior, adult, kitten, adult, adult, senior, ...        senior            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "47    041A                                    [adult, kitten]         adult           kitten                  False\n",
       "69    063A  [senior, adult, senior, senior, adult, senior,...        senior            adult                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, s...         adult           senior                  False"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     61\n",
      "kitten    10\n",
      "senior    12\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             61  83.561644\n",
      "1           kitten           15             10  66.666667\n",
      "2           senior           22             12  54.545455\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          554            431  77.797834\n",
      "1           kitten          109             76  69.724771\n",
      "2           senior          178             96  53.932584\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    218      171     78.44\n",
      "1          M    337      245     72.70\n",
      "2          X    286      187     65.38\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
