{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 87, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1204, 1: 1093, 2: 796})\n",
      "Epoch 1/1500\n",
      "49/49 [==============================] - 0s 943us/step - loss: 1.1425 - accuracy: 0.5105\n",
      "Epoch 2/1500\n",
      "49/49 [==============================] - 0s 928us/step - loss: 0.8981 - accuracy: 0.6120\n",
      "Epoch 3/1500\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.8203 - accuracy: 0.6427\n",
      "Epoch 4/1500\n",
      "49/49 [==============================] - 0s 795us/step - loss: 0.7777 - accuracy: 0.6654\n",
      "Epoch 5/1500\n",
      "49/49 [==============================] - 0s 792us/step - loss: 0.7374 - accuracy: 0.6964\n",
      "Epoch 6/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.7095 - accuracy: 0.7022\n",
      "Epoch 7/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.7005 - accuracy: 0.7074\n",
      "Epoch 8/1500\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.6687 - accuracy: 0.7106\n",
      "Epoch 9/1500\n",
      "49/49 [==============================] - 0s 740us/step - loss: 0.6448 - accuracy: 0.7226\n",
      "Epoch 10/1500\n",
      "49/49 [==============================] - 0s 747us/step - loss: 0.6352 - accuracy: 0.7252\n",
      "Epoch 11/1500\n",
      "49/49 [==============================] - 0s 759us/step - loss: 0.6189 - accuracy: 0.7365\n",
      "Epoch 12/1500\n",
      "49/49 [==============================] - 0s 741us/step - loss: 0.6244 - accuracy: 0.7404\n",
      "Epoch 13/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.6092 - accuracy: 0.7384\n",
      "Epoch 14/1500\n",
      "49/49 [==============================] - 0s 789us/step - loss: 0.5891 - accuracy: 0.7562\n",
      "Epoch 15/1500\n",
      "49/49 [==============================] - 0s 758us/step - loss: 0.6032 - accuracy: 0.7501\n",
      "Epoch 16/1500\n",
      "49/49 [==============================] - 0s 754us/step - loss: 0.5909 - accuracy: 0.7491\n",
      "Epoch 17/1500\n",
      "49/49 [==============================] - 0s 843us/step - loss: 0.5983 - accuracy: 0.7397\n",
      "Epoch 18/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.5565 - accuracy: 0.7692\n",
      "Epoch 19/1500\n",
      "49/49 [==============================] - 0s 755us/step - loss: 0.5592 - accuracy: 0.7666\n",
      "Epoch 20/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7701\n",
      "Epoch 21/1500\n",
      "49/49 [==============================] - 0s 792us/step - loss: 0.5307 - accuracy: 0.7756\n",
      "Epoch 22/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.5479 - accuracy: 0.7614\n",
      "Epoch 23/1500\n",
      "49/49 [==============================] - 0s 749us/step - loss: 0.5437 - accuracy: 0.7704\n",
      "Epoch 24/1500\n",
      "49/49 [==============================] - 0s 793us/step - loss: 0.5113 - accuracy: 0.7837\n",
      "Epoch 25/1500\n",
      "49/49 [==============================] - 0s 764us/step - loss: 0.5227 - accuracy: 0.7814\n",
      "Epoch 26/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.5147 - accuracy: 0.7847\n",
      "Epoch 27/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.5002 - accuracy: 0.7915\n",
      "Epoch 28/1500\n",
      "49/49 [==============================] - 0s 776us/step - loss: 0.4975 - accuracy: 0.7928\n",
      "Epoch 29/1500\n",
      "49/49 [==============================] - 0s 751us/step - loss: 0.4847 - accuracy: 0.7983\n",
      "Epoch 30/1500\n",
      "49/49 [==============================] - 0s 763us/step - loss: 0.4841 - accuracy: 0.8060\n",
      "Epoch 31/1500\n",
      "49/49 [==============================] - 0s 722us/step - loss: 0.4848 - accuracy: 0.7953\n",
      "Epoch 32/1500\n",
      "49/49 [==============================] - 0s 725us/step - loss: 0.4877 - accuracy: 0.7963\n",
      "Epoch 33/1500\n",
      "49/49 [==============================] - 0s 770us/step - loss: 0.4783 - accuracy: 0.7911\n",
      "Epoch 34/1500\n",
      "49/49 [==============================] - 0s 792us/step - loss: 0.4769 - accuracy: 0.8028\n",
      "Epoch 35/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.4731 - accuracy: 0.7995\n",
      "Epoch 36/1500\n",
      "49/49 [==============================] - 0s 754us/step - loss: 0.4723 - accuracy: 0.8034\n",
      "Epoch 37/1500\n",
      "49/49 [==============================] - 0s 779us/step - loss: 0.4536 - accuracy: 0.8160\n",
      "Epoch 38/1500\n",
      "49/49 [==============================] - 0s 775us/step - loss: 0.4619 - accuracy: 0.8060\n",
      "Epoch 39/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.8025\n",
      "Epoch 40/1500\n",
      "49/49 [==============================] - 0s 731us/step - loss: 0.4629 - accuracy: 0.8044\n",
      "Epoch 41/1500\n",
      "49/49 [==============================] - 0s 768us/step - loss: 0.4478 - accuracy: 0.8086\n",
      "Epoch 42/1500\n",
      "49/49 [==============================] - 0s 746us/step - loss: 0.4550 - accuracy: 0.8076\n",
      "Epoch 43/1500\n",
      "49/49 [==============================] - 0s 753us/step - loss: 0.4598 - accuracy: 0.8115\n",
      "Epoch 44/1500\n",
      "49/49 [==============================] - 0s 744us/step - loss: 0.4485 - accuracy: 0.8060\n",
      "Epoch 45/1500\n",
      "49/49 [==============================] - 0s 755us/step - loss: 0.4448 - accuracy: 0.8092\n",
      "Epoch 46/1500\n",
      "49/49 [==============================] - 0s 768us/step - loss: 0.4339 - accuracy: 0.8228\n",
      "Epoch 47/1500\n",
      "49/49 [==============================] - 0s 751us/step - loss: 0.4389 - accuracy: 0.8170\n",
      "Epoch 48/1500\n",
      "49/49 [==============================] - 0s 752us/step - loss: 0.4433 - accuracy: 0.8157\n",
      "Epoch 49/1500\n",
      "49/49 [==============================] - 0s 753us/step - loss: 0.4354 - accuracy: 0.8160\n",
      "Epoch 50/1500\n",
      "49/49 [==============================] - 0s 763us/step - loss: 0.4182 - accuracy: 0.8244\n",
      "Epoch 51/1500\n",
      "49/49 [==============================] - 0s 750us/step - loss: 0.4221 - accuracy: 0.8248\n",
      "Epoch 52/1500\n",
      "49/49 [==============================] - 0s 757us/step - loss: 0.4206 - accuracy: 0.8199\n",
      "Epoch 53/1500\n",
      "49/49 [==============================] - 0s 753us/step - loss: 0.4230 - accuracy: 0.8196\n",
      "Epoch 54/1500\n",
      "49/49 [==============================] - 0s 754us/step - loss: 0.4163 - accuracy: 0.8215\n",
      "Epoch 55/1500\n",
      "49/49 [==============================] - 0s 761us/step - loss: 0.4187 - accuracy: 0.8222\n",
      "Epoch 56/1500\n",
      "49/49 [==============================] - 0s 810us/step - loss: 0.3982 - accuracy: 0.8325\n",
      "Epoch 57/1500\n",
      "49/49 [==============================] - 0s 810us/step - loss: 0.4089 - accuracy: 0.8270\n",
      "Epoch 58/1500\n",
      "49/49 [==============================] - 0s 795us/step - loss: 0.4135 - accuracy: 0.8215\n",
      "Epoch 59/1500\n",
      "49/49 [==============================] - 0s 796us/step - loss: 0.4222 - accuracy: 0.8183\n",
      "Epoch 60/1500\n",
      "49/49 [==============================] - 0s 785us/step - loss: 0.3958 - accuracy: 0.8351\n",
      "Epoch 61/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.4166 - accuracy: 0.8164\n",
      "Epoch 62/1500\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.4100 - accuracy: 0.8238\n",
      "Epoch 63/1500\n",
      "49/49 [==============================] - 0s 749us/step - loss: 0.3998 - accuracy: 0.8299\n",
      "Epoch 64/1500\n",
      "49/49 [==============================] - 0s 775us/step - loss: 0.3998 - accuracy: 0.8377\n",
      "Epoch 65/1500\n",
      "49/49 [==============================] - 0s 779us/step - loss: 0.3929 - accuracy: 0.8374\n",
      "Epoch 66/1500\n",
      "49/49 [==============================] - 0s 756us/step - loss: 0.3927 - accuracy: 0.8400\n",
      "Epoch 67/1500\n",
      "49/49 [==============================] - 0s 786us/step - loss: 0.3708 - accuracy: 0.8442\n",
      "Epoch 68/1500\n",
      "49/49 [==============================] - 0s 806us/step - loss: 0.3890 - accuracy: 0.8345\n",
      "Epoch 69/1500\n",
      "49/49 [==============================] - 0s 820us/step - loss: 0.3958 - accuracy: 0.8319\n",
      "Epoch 70/1500\n",
      "49/49 [==============================] - 0s 810us/step - loss: 0.3800 - accuracy: 0.8374\n",
      "Epoch 71/1500\n",
      "49/49 [==============================] - 0s 803us/step - loss: 0.3606 - accuracy: 0.8468\n",
      "Epoch 72/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.3740 - accuracy: 0.8400\n",
      "Epoch 73/1500\n",
      "49/49 [==============================] - 0s 746us/step - loss: 0.3882 - accuracy: 0.8432\n",
      "Epoch 74/1500\n",
      "49/49 [==============================] - 0s 756us/step - loss: 0.3786 - accuracy: 0.8438\n",
      "Epoch 75/1500\n",
      "49/49 [==============================] - 0s 757us/step - loss: 0.3635 - accuracy: 0.8510\n",
      "Epoch 76/1500\n",
      "49/49 [==============================] - 0s 760us/step - loss: 0.3855 - accuracy: 0.8351\n",
      "Epoch 77/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.3723 - accuracy: 0.8438\n",
      "Epoch 78/1500\n",
      "49/49 [==============================] - 0s 766us/step - loss: 0.3647 - accuracy: 0.8510\n",
      "Epoch 79/1500\n",
      "49/49 [==============================] - 0s 775us/step - loss: 0.3745 - accuracy: 0.8429\n",
      "Epoch 80/1500\n",
      "49/49 [==============================] - 0s 793us/step - loss: 0.3558 - accuracy: 0.8458\n",
      "Epoch 81/1500\n",
      "49/49 [==============================] - 0s 762us/step - loss: 0.3600 - accuracy: 0.8477\n",
      "Epoch 82/1500\n",
      "49/49 [==============================] - 0s 757us/step - loss: 0.3782 - accuracy: 0.8461\n",
      "Epoch 83/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8565\n",
      "Epoch 84/1500\n",
      "49/49 [==============================] - 0s 893us/step - loss: 0.3558 - accuracy: 0.8539\n",
      "Epoch 85/1500\n",
      "49/49 [==============================] - 0s 846us/step - loss: 0.3660 - accuracy: 0.8500\n",
      "Epoch 86/1500\n",
      "49/49 [==============================] - 0s 832us/step - loss: 0.3612 - accuracy: 0.8487\n",
      "Epoch 87/1500\n",
      "49/49 [==============================] - 0s 839us/step - loss: 0.3548 - accuracy: 0.8451\n",
      "Epoch 88/1500\n",
      "49/49 [==============================] - 0s 806us/step - loss: 0.3371 - accuracy: 0.8616\n",
      "Epoch 89/1500\n",
      "49/49 [==============================] - 0s 811us/step - loss: 0.3537 - accuracy: 0.8464\n",
      "Epoch 90/1500\n",
      "49/49 [==============================] - 0s 744us/step - loss: 0.3448 - accuracy: 0.8522\n",
      "Epoch 91/1500\n",
      "49/49 [==============================] - 0s 814us/step - loss: 0.3499 - accuracy: 0.8561\n",
      "Epoch 92/1500\n",
      "49/49 [==============================] - 0s 776us/step - loss: 0.3487 - accuracy: 0.8506\n",
      "Epoch 93/1500\n",
      "49/49 [==============================] - 0s 819us/step - loss: 0.3392 - accuracy: 0.8642\n",
      "Epoch 94/1500\n",
      "49/49 [==============================] - 0s 803us/step - loss: 0.3422 - accuracy: 0.8529\n",
      "Epoch 95/1500\n",
      "49/49 [==============================] - 0s 825us/step - loss: 0.3384 - accuracy: 0.8594\n",
      "Epoch 96/1500\n",
      "49/49 [==============================] - 0s 856us/step - loss: 0.3406 - accuracy: 0.8600\n",
      "Epoch 97/1500\n",
      "49/49 [==============================] - 0s 813us/step - loss: 0.3468 - accuracy: 0.8587\n",
      "Epoch 98/1500\n",
      "49/49 [==============================] - 0s 808us/step - loss: 0.3306 - accuracy: 0.8678\n",
      "Epoch 99/1500\n",
      "49/49 [==============================] - 0s 810us/step - loss: 0.3251 - accuracy: 0.8691\n",
      "Epoch 100/1500\n",
      "49/49 [==============================] - 0s 797us/step - loss: 0.3395 - accuracy: 0.8597\n",
      "Epoch 101/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.3319 - accuracy: 0.8691\n",
      "Epoch 102/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.3392 - accuracy: 0.8574\n",
      "Epoch 103/1500\n",
      "49/49 [==============================] - 0s 785us/step - loss: 0.3421 - accuracy: 0.8636\n",
      "Epoch 104/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.3280 - accuracy: 0.8668\n",
      "Epoch 105/1500\n",
      "49/49 [==============================] - 0s 858us/step - loss: 0.3413 - accuracy: 0.8652\n",
      "Epoch 106/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.3201 - accuracy: 0.8671\n",
      "Epoch 107/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.3275 - accuracy: 0.8704\n",
      "Epoch 108/1500\n",
      "49/49 [==============================] - 0s 785us/step - loss: 0.3325 - accuracy: 0.8584\n",
      "Epoch 109/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.3266 - accuracy: 0.8655\n",
      "Epoch 110/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.3189 - accuracy: 0.8678\n",
      "Epoch 111/1500\n",
      "49/49 [==============================] - 0s 746us/step - loss: 0.3260 - accuracy: 0.8704\n",
      "Epoch 112/1500\n",
      "49/49 [==============================] - 0s 763us/step - loss: 0.3153 - accuracy: 0.8697\n",
      "Epoch 113/1500\n",
      "49/49 [==============================] - 0s 789us/step - loss: 0.3214 - accuracy: 0.8642\n",
      "Epoch 114/1500\n",
      "49/49 [==============================] - 0s 848us/step - loss: 0.3107 - accuracy: 0.8736\n",
      "Epoch 115/1500\n",
      "49/49 [==============================] - 0s 958us/step - loss: 0.3187 - accuracy: 0.8687\n",
      "Epoch 116/1500\n",
      "49/49 [==============================] - 0s 938us/step - loss: 0.3258 - accuracy: 0.8700\n",
      "Epoch 117/1500\n",
      "49/49 [==============================] - 0s 890us/step - loss: 0.3127 - accuracy: 0.8762\n",
      "Epoch 118/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8716\n",
      "Epoch 119/1500\n",
      "49/49 [==============================] - 0s 942us/step - loss: 0.3257 - accuracy: 0.8707\n",
      "Epoch 120/1500\n",
      "49/49 [==============================] - 0s 912us/step - loss: 0.3157 - accuracy: 0.8768\n",
      "Epoch 121/1500\n",
      "49/49 [==============================] - 0s 936us/step - loss: 0.2997 - accuracy: 0.8749\n",
      "Epoch 122/1500\n",
      "49/49 [==============================] - 0s 908us/step - loss: 0.3155 - accuracy: 0.8726\n",
      "Epoch 123/1500\n",
      "49/49 [==============================] - 0s 829us/step - loss: 0.3221 - accuracy: 0.8652\n",
      "Epoch 124/1500\n",
      "49/49 [==============================] - 0s 770us/step - loss: 0.3039 - accuracy: 0.8716\n",
      "Epoch 125/1500\n",
      "49/49 [==============================] - 0s 810us/step - loss: 0.2994 - accuracy: 0.8775\n",
      "Epoch 126/1500\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.3212 - accuracy: 0.8700\n",
      "Epoch 127/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.2941 - accuracy: 0.8843\n",
      "Epoch 128/1500\n",
      "49/49 [==============================] - 0s 754us/step - loss: 0.3058 - accuracy: 0.8765\n",
      "Epoch 129/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.3024 - accuracy: 0.8762\n",
      "Epoch 130/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.3009 - accuracy: 0.8746\n",
      "Epoch 131/1500\n",
      "49/49 [==============================] - 0s 822us/step - loss: 0.3136 - accuracy: 0.8736\n",
      "Epoch 132/1500\n",
      "49/49 [==============================] - 0s 819us/step - loss: 0.3039 - accuracy: 0.8700\n",
      "Epoch 133/1500\n",
      "49/49 [==============================] - 0s 886us/step - loss: 0.2974 - accuracy: 0.8807\n",
      "Epoch 134/1500\n",
      "49/49 [==============================] - 0s 855us/step - loss: 0.3058 - accuracy: 0.8755\n",
      "Epoch 135/1500\n",
      "49/49 [==============================] - 0s 864us/step - loss: 0.2921 - accuracy: 0.8849\n",
      "Epoch 136/1500\n",
      "49/49 [==============================] - 0s 843us/step - loss: 0.2983 - accuracy: 0.8775\n",
      "Epoch 137/1500\n",
      "49/49 [==============================] - 0s 797us/step - loss: 0.2928 - accuracy: 0.8833\n",
      "Epoch 138/1500\n",
      "49/49 [==============================] - 0s 780us/step - loss: 0.2936 - accuracy: 0.8849\n",
      "Epoch 139/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.2973 - accuracy: 0.8810\n",
      "Epoch 140/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.3021 - accuracy: 0.8781\n",
      "Epoch 141/1500\n",
      "49/49 [==============================] - 0s 766us/step - loss: 0.2880 - accuracy: 0.8794\n",
      "Epoch 142/1500\n",
      "49/49 [==============================] - 0s 824us/step - loss: 0.2897 - accuracy: 0.8794\n",
      "Epoch 143/1500\n",
      "49/49 [==============================] - 0s 829us/step - loss: 0.2934 - accuracy: 0.8839\n",
      "Epoch 144/1500\n",
      "49/49 [==============================] - 0s 834us/step - loss: 0.2944 - accuracy: 0.8804\n",
      "Epoch 145/1500\n",
      "49/49 [==============================] - 0s 878us/step - loss: 0.2931 - accuracy: 0.8813\n",
      "Epoch 146/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.2874 - accuracy: 0.8810\n",
      "Epoch 147/1500\n",
      "49/49 [==============================] - 0s 853us/step - loss: 0.2978 - accuracy: 0.8833\n",
      "Epoch 148/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8801\n",
      "Epoch 149/1500\n",
      "49/49 [==============================] - 0s 826us/step - loss: 0.2988 - accuracy: 0.8836\n",
      "Epoch 150/1500\n",
      "49/49 [==============================] - 0s 823us/step - loss: 0.2918 - accuracy: 0.8781\n",
      "Epoch 151/1500\n",
      "49/49 [==============================] - 0s 840us/step - loss: 0.2910 - accuracy: 0.8852\n",
      "Epoch 152/1500\n",
      "49/49 [==============================] - 0s 804us/step - loss: 0.2974 - accuracy: 0.8804\n",
      "Epoch 153/1500\n",
      "49/49 [==============================] - 0s 814us/step - loss: 0.2933 - accuracy: 0.8894\n",
      "Epoch 154/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.2764 - accuracy: 0.8878\n",
      "Epoch 155/1500\n",
      "49/49 [==============================] - 0s 883us/step - loss: 0.2827 - accuracy: 0.8826\n",
      "Epoch 156/1500\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.2813 - accuracy: 0.8898\n",
      "Epoch 157/1500\n",
      "49/49 [==============================] - 0s 800us/step - loss: 0.2804 - accuracy: 0.8910\n",
      "Epoch 158/1500\n",
      "49/49 [==============================] - 0s 839us/step - loss: 0.2897 - accuracy: 0.8826\n",
      "Epoch 159/1500\n",
      "49/49 [==============================] - 0s 841us/step - loss: 0.2858 - accuracy: 0.8855\n",
      "Epoch 160/1500\n",
      "49/49 [==============================] - 0s 843us/step - loss: 0.2684 - accuracy: 0.8943\n",
      "Epoch 161/1500\n",
      "49/49 [==============================] - 0s 816us/step - loss: 0.2739 - accuracy: 0.8904\n",
      "Epoch 162/1500\n",
      "49/49 [==============================] - 0s 791us/step - loss: 0.2947 - accuracy: 0.8817\n",
      "Epoch 163/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.2776 - accuracy: 0.8875\n",
      "Epoch 164/1500\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.2745 - accuracy: 0.8901\n",
      "Epoch 165/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2730 - accuracy: 0.8927\n",
      "Epoch 166/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2829 - accuracy: 0.8826\n",
      "Epoch 167/1500\n",
      "49/49 [==============================] - 0s 809us/step - loss: 0.2759 - accuracy: 0.8898\n",
      "Epoch 168/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.2733 - accuracy: 0.8907\n",
      "Epoch 169/1500\n",
      "49/49 [==============================] - 0s 788us/step - loss: 0.2729 - accuracy: 0.8927\n",
      "Epoch 170/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.2626 - accuracy: 0.8965\n",
      "Epoch 171/1500\n",
      "49/49 [==============================] - 0s 804us/step - loss: 0.2685 - accuracy: 0.8920\n",
      "Epoch 172/1500\n",
      "49/49 [==============================] - 0s 772us/step - loss: 0.2767 - accuracy: 0.8875\n",
      "Epoch 173/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.2656 - accuracy: 0.8946\n",
      "Epoch 174/1500\n",
      "49/49 [==============================] - 0s 823us/step - loss: 0.2553 - accuracy: 0.8933\n",
      "Epoch 175/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2596 - accuracy: 0.8965\n",
      "Epoch 176/1500\n",
      "49/49 [==============================] - 0s 799us/step - loss: 0.2654 - accuracy: 0.8894\n",
      "Epoch 177/1500\n",
      "49/49 [==============================] - 0s 761us/step - loss: 0.2739 - accuracy: 0.8923\n",
      "Epoch 178/1500\n",
      "49/49 [==============================] - 0s 780us/step - loss: 0.2697 - accuracy: 0.8988\n",
      "Epoch 179/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.2597 - accuracy: 0.8940\n",
      "Epoch 180/1500\n",
      "49/49 [==============================] - 0s 738us/step - loss: 0.2691 - accuracy: 0.8910\n",
      "Epoch 181/1500\n",
      "49/49 [==============================] - 0s 789us/step - loss: 0.2485 - accuracy: 0.9027\n",
      "Epoch 182/1500\n",
      "49/49 [==============================] - 0s 812us/step - loss: 0.2723 - accuracy: 0.8936\n",
      "Epoch 183/1500\n",
      "49/49 [==============================] - 0s 900us/step - loss: 0.2747 - accuracy: 0.8904\n",
      "Epoch 184/1500\n",
      "49/49 [==============================] - 0s 914us/step - loss: 0.2695 - accuracy: 0.8920\n",
      "Epoch 185/1500\n",
      "49/49 [==============================] - 0s 908us/step - loss: 0.2524 - accuracy: 0.8985\n",
      "Epoch 186/1500\n",
      "49/49 [==============================] - 0s 891us/step - loss: 0.2552 - accuracy: 0.8985\n",
      "Epoch 187/1500\n",
      "49/49 [==============================] - 0s 933us/step - loss: 0.2622 - accuracy: 0.8962\n",
      "Epoch 188/1500\n",
      "49/49 [==============================] - 0s 924us/step - loss: 0.2614 - accuracy: 0.8952\n",
      "Epoch 189/1500\n",
      "49/49 [==============================] - 0s 823us/step - loss: 0.2601 - accuracy: 0.8959\n",
      "Epoch 190/1500\n",
      "49/49 [==============================] - 0s 810us/step - loss: 0.2744 - accuracy: 0.8959\n",
      "Epoch 191/1500\n",
      "49/49 [==============================] - 0s 871us/step - loss: 0.2582 - accuracy: 0.8933\n",
      "Epoch 192/1500\n",
      "49/49 [==============================] - 0s 814us/step - loss: 0.2659 - accuracy: 0.8975\n",
      "Epoch 193/1500\n",
      "49/49 [==============================] - 0s 768us/step - loss: 0.2560 - accuracy: 0.8943\n",
      "Epoch 194/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.2611 - accuracy: 0.8978\n",
      "Epoch 195/1500\n",
      "49/49 [==============================] - 0s 786us/step - loss: 0.2543 - accuracy: 0.9011\n",
      "Epoch 196/1500\n",
      "49/49 [==============================] - 0s 851us/step - loss: 0.2417 - accuracy: 0.8982\n",
      "Epoch 197/1500\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.2483 - accuracy: 0.9040\n",
      "Epoch 198/1500\n",
      "49/49 [==============================] - 0s 781us/step - loss: 0.2560 - accuracy: 0.8978\n",
      "Epoch 199/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.2595 - accuracy: 0.9056\n",
      "Epoch 200/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.9114\n",
      "Epoch 201/1500\n",
      "49/49 [==============================] - 0s 815us/step - loss: 0.2376 - accuracy: 0.9033\n",
      "Epoch 202/1500\n",
      "49/49 [==============================] - 0s 870us/step - loss: 0.2501 - accuracy: 0.8943\n",
      "Epoch 203/1500\n",
      "49/49 [==============================] - 0s 883us/step - loss: 0.2579 - accuracy: 0.9046\n",
      "Epoch 204/1500\n",
      "49/49 [==============================] - 0s 860us/step - loss: 0.2535 - accuracy: 0.9001\n",
      "Epoch 205/1500\n",
      "49/49 [==============================] - 0s 780us/step - loss: 0.2611 - accuracy: 0.8946\n",
      "Epoch 206/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.2408 - accuracy: 0.9027\n",
      "Epoch 207/1500\n",
      "49/49 [==============================] - 0s 796us/step - loss: 0.2340 - accuracy: 0.9075\n",
      "Epoch 208/1500\n",
      "49/49 [==============================] - 0s 801us/step - loss: 0.2621 - accuracy: 0.8940\n",
      "Epoch 209/1500\n",
      "49/49 [==============================] - 0s 819us/step - loss: 0.2498 - accuracy: 0.9007\n",
      "Epoch 210/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.2508 - accuracy: 0.8985\n",
      "Epoch 211/1500\n",
      "49/49 [==============================] - 0s 788us/step - loss: 0.2327 - accuracy: 0.9075\n",
      "Epoch 212/1500\n",
      "49/49 [==============================] - 0s 759us/step - loss: 0.2532 - accuracy: 0.8998\n",
      "Epoch 213/1500\n",
      "49/49 [==============================] - 0s 766us/step - loss: 0.2463 - accuracy: 0.8988\n",
      "Epoch 214/1500\n",
      "49/49 [==============================] - 0s 800us/step - loss: 0.2425 - accuracy: 0.9020\n",
      "Epoch 215/1500\n",
      "49/49 [==============================] - 0s 813us/step - loss: 0.2487 - accuracy: 0.9020\n",
      "Epoch 216/1500\n",
      "49/49 [==============================] - 0s 846us/step - loss: 0.2395 - accuracy: 0.9104\n",
      "Epoch 217/1500\n",
      "49/49 [==============================] - 0s 841us/step - loss: 0.2396 - accuracy: 0.8972\n",
      "Epoch 218/1500\n",
      "49/49 [==============================] - 0s 823us/step - loss: 0.2442 - accuracy: 0.9082\n",
      "Epoch 219/1500\n",
      "49/49 [==============================] - 0s 819us/step - loss: 0.2326 - accuracy: 0.9130\n",
      "Epoch 220/1500\n",
      "49/49 [==============================] - 0s 825us/step - loss: 0.2551 - accuracy: 0.8965\n",
      "Epoch 221/1500\n",
      "49/49 [==============================] - 0s 795us/step - loss: 0.2332 - accuracy: 0.9049\n",
      "Epoch 222/1500\n",
      "49/49 [==============================] - 0s 741us/step - loss: 0.2449 - accuracy: 0.9069\n",
      "Epoch 223/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.9098\n",
      "Epoch 224/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2474 - accuracy: 0.9007\n",
      "Epoch 225/1500\n",
      "49/49 [==============================] - 0s 822us/step - loss: 0.2527 - accuracy: 0.9037\n",
      "Epoch 226/1500\n",
      "49/49 [==============================] - 0s 826us/step - loss: 0.2334 - accuracy: 0.9114\n",
      "Epoch 227/1500\n",
      "49/49 [==============================] - 0s 835us/step - loss: 0.2339 - accuracy: 0.9079\n",
      "Epoch 228/1500\n",
      "49/49 [==============================] - 0s 854us/step - loss: 0.2273 - accuracy: 0.9124\n",
      "Epoch 229/1500\n",
      "49/49 [==============================] - 0s 874us/step - loss: 0.2447 - accuracy: 0.9043\n",
      "Epoch 230/1500\n",
      "49/49 [==============================] - 0s 889us/step - loss: 0.2385 - accuracy: 0.9072\n",
      "Epoch 231/1500\n",
      "49/49 [==============================] - 0s 899us/step - loss: 0.2358 - accuracy: 0.9059\n",
      "Epoch 232/1500\n",
      "49/49 [==============================] - 0s 865us/step - loss: 0.2384 - accuracy: 0.9130\n",
      "Epoch 233/1500\n",
      "49/49 [==============================] - 0s 878us/step - loss: 0.2263 - accuracy: 0.9121\n",
      "Epoch 234/1500\n",
      "49/49 [==============================] - 0s 911us/step - loss: 0.2317 - accuracy: 0.9088\n",
      "Epoch 235/1500\n",
      "49/49 [==============================] - 0s 894us/step - loss: 0.2313 - accuracy: 0.9053\n",
      "Epoch 236/1500\n",
      "49/49 [==============================] - 0s 801us/step - loss: 0.2340 - accuracy: 0.9137\n",
      "Epoch 237/1500\n",
      "49/49 [==============================] - 0s 795us/step - loss: 0.2471 - accuracy: 0.8998\n",
      "Epoch 238/1500\n",
      "49/49 [==============================] - 0s 759us/step - loss: 0.2367 - accuracy: 0.9091\n",
      "Epoch 239/1500\n",
      "49/49 [==============================] - 0s 768us/step - loss: 0.2398 - accuracy: 0.9062\n",
      "Epoch 240/1500\n",
      "49/49 [==============================] - 0s 760us/step - loss: 0.2334 - accuracy: 0.9062\n",
      "Epoch 241/1500\n",
      "49/49 [==============================] - 0s 756us/step - loss: 0.2309 - accuracy: 0.9075\n",
      "Epoch 242/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.2397 - accuracy: 0.9033\n",
      "Epoch 243/1500\n",
      "49/49 [==============================] - 0s 763us/step - loss: 0.2219 - accuracy: 0.9179\n",
      "Epoch 244/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.9098\n",
      "Epoch 245/1500\n",
      "49/49 [==============================] - 0s 806us/step - loss: 0.2437 - accuracy: 0.9037\n",
      "Epoch 246/1500\n",
      "49/49 [==============================] - 0s 876us/step - loss: 0.2268 - accuracy: 0.9143\n",
      "Epoch 247/1500\n",
      "49/49 [==============================] - 0s 957us/step - loss: 0.2225 - accuracy: 0.9124\n",
      "Epoch 248/1500\n",
      "49/49 [==============================] - 0s 951us/step - loss: 0.2173 - accuracy: 0.9169\n",
      "Epoch 249/1500\n",
      "49/49 [==============================] - 0s 945us/step - loss: 0.2167 - accuracy: 0.9143\n",
      "Epoch 250/1500\n",
      "49/49 [==============================] - 0s 940us/step - loss: 0.2211 - accuracy: 0.9111\n",
      "Epoch 251/1500\n",
      "49/49 [==============================] - 0s 934us/step - loss: 0.2224 - accuracy: 0.9104\n",
      "Epoch 252/1500\n",
      "49/49 [==============================] - 0s 910us/step - loss: 0.2266 - accuracy: 0.9124\n",
      "Epoch 253/1500\n",
      "49/49 [==============================] - 0s 876us/step - loss: 0.2296 - accuracy: 0.9066\n",
      "Epoch 254/1500\n",
      "49/49 [==============================] - 0s 890us/step - loss: 0.2244 - accuracy: 0.9095\n",
      "Epoch 255/1500\n",
      "49/49 [==============================] - 0s 911us/step - loss: 0.2191 - accuracy: 0.9156\n",
      "Epoch 256/1500\n",
      "49/49 [==============================] - 0s 884us/step - loss: 0.2268 - accuracy: 0.9085\n",
      "Epoch 257/1500\n",
      "49/49 [==============================] - 0s 835us/step - loss: 0.2201 - accuracy: 0.9117\n",
      "Epoch 258/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2172 - accuracy: 0.9172\n",
      "Epoch 259/1500\n",
      "49/49 [==============================] - 0s 775us/step - loss: 0.2259 - accuracy: 0.9085\n",
      "Epoch 260/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.2312 - accuracy: 0.9098\n",
      "Epoch 261/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.2300 - accuracy: 0.9121\n",
      "Epoch 262/1500\n",
      "49/49 [==============================] - 0s 766us/step - loss: 0.2367 - accuracy: 0.9011\n",
      "Epoch 263/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2209 - accuracy: 0.9091\n",
      "Epoch 264/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2180 - accuracy: 0.9130\n",
      "Epoch 265/1500\n",
      "49/49 [==============================] - 0s 761us/step - loss: 0.2291 - accuracy: 0.9079\n",
      "Epoch 266/1500\n",
      "49/49 [==============================] - 0s 799us/step - loss: 0.2224 - accuracy: 0.9114\n",
      "Epoch 267/1500\n",
      "49/49 [==============================] - 0s 844us/step - loss: 0.2169 - accuracy: 0.9159\n",
      "Epoch 268/1500\n",
      "49/49 [==============================] - 0s 893us/step - loss: 0.2302 - accuracy: 0.9066\n",
      "Epoch 269/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2241 - accuracy: 0.9114\n",
      "Epoch 270/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.2260 - accuracy: 0.9121\n",
      "Epoch 271/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.2298 - accuracy: 0.9072\n",
      "Epoch 272/1500\n",
      "49/49 [==============================] - 0s 772us/step - loss: 0.2156 - accuracy: 0.9150\n",
      "Epoch 273/1500\n",
      "49/49 [==============================] - 0s 776us/step - loss: 0.2279 - accuracy: 0.9056\n",
      "Epoch 274/1500\n",
      "49/49 [==============================] - 0s 841us/step - loss: 0.2214 - accuracy: 0.9134\n",
      "Epoch 275/1500\n",
      "49/49 [==============================] - 0s 845us/step - loss: 0.2222 - accuracy: 0.9172\n",
      "Epoch 276/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.2194 - accuracy: 0.9130\n",
      "Epoch 277/1500\n",
      "49/49 [==============================] - 0s 770us/step - loss: 0.2249 - accuracy: 0.9143\n",
      "Epoch 278/1500\n",
      "49/49 [==============================] - 0s 767us/step - loss: 0.2193 - accuracy: 0.9146\n",
      "Epoch 279/1500\n",
      "49/49 [==============================] - 0s 796us/step - loss: 0.2133 - accuracy: 0.9179\n",
      "Epoch 280/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.2241 - accuracy: 0.9072\n",
      "Epoch 281/1500\n",
      "49/49 [==============================] - 0s 779us/step - loss: 0.2295 - accuracy: 0.9072\n",
      "Epoch 282/1500\n",
      "49/49 [==============================] - 0s 781us/step - loss: 0.2086 - accuracy: 0.9159\n",
      "Epoch 283/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9143\n",
      "Epoch 284/1500\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.2155 - accuracy: 0.9185\n",
      "Epoch 285/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.2103 - accuracy: 0.9179\n",
      "Epoch 286/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.2160 - accuracy: 0.9163\n",
      "Epoch 287/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2037 - accuracy: 0.9195\n",
      "Epoch 288/1500\n",
      "49/49 [==============================] - 0s 752us/step - loss: 0.2190 - accuracy: 0.9146\n",
      "Epoch 289/1500\n",
      "49/49 [==============================] - 0s 740us/step - loss: 0.2053 - accuracy: 0.9208\n",
      "Epoch 290/1500\n",
      "49/49 [==============================] - 0s 776us/step - loss: 0.2112 - accuracy: 0.9172\n",
      "Epoch 291/1500\n",
      "49/49 [==============================] - 0s 751us/step - loss: 0.2136 - accuracy: 0.9143\n",
      "Epoch 292/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.2214 - accuracy: 0.9140\n",
      "Epoch 293/1500\n",
      "49/49 [==============================] - 0s 753us/step - loss: 0.2067 - accuracy: 0.9169\n",
      "Epoch 294/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.2017 - accuracy: 0.9192\n",
      "Epoch 295/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2039 - accuracy: 0.9179\n",
      "Epoch 296/1500\n",
      "49/49 [==============================] - 0s 767us/step - loss: 0.2173 - accuracy: 0.9143\n",
      "Epoch 297/1500\n",
      "49/49 [==============================] - 0s 767us/step - loss: 0.2068 - accuracy: 0.9185\n",
      "Epoch 298/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2154 - accuracy: 0.9134\n",
      "Epoch 299/1500\n",
      "49/49 [==============================] - 0s 762us/step - loss: 0.2018 - accuracy: 0.9188\n",
      "Epoch 300/1500\n",
      "49/49 [==============================] - 0s 764us/step - loss: 0.2055 - accuracy: 0.9208\n",
      "Epoch 301/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9153\n",
      "Epoch 302/1500\n",
      "49/49 [==============================] - 0s 810us/step - loss: 0.2133 - accuracy: 0.9143\n",
      "Epoch 303/1500\n",
      "49/49 [==============================] - 0s 803us/step - loss: 0.1998 - accuracy: 0.9231\n",
      "Epoch 304/1500\n",
      "49/49 [==============================] - 0s 763us/step - loss: 0.2043 - accuracy: 0.9192\n",
      "Epoch 305/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2180 - accuracy: 0.9182\n",
      "Epoch 306/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.2102 - accuracy: 0.9201\n",
      "Epoch 307/1500\n",
      "49/49 [==============================] - 0s 770us/step - loss: 0.2061 - accuracy: 0.9185\n",
      "Epoch 308/1500\n",
      "49/49 [==============================] - 0s 773us/step - loss: 0.2181 - accuracy: 0.9153\n",
      "Epoch 309/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.2084 - accuracy: 0.9195\n",
      "Epoch 310/1500\n",
      "49/49 [==============================] - 0s 773us/step - loss: 0.2011 - accuracy: 0.9159\n",
      "Epoch 311/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.2045 - accuracy: 0.9182\n",
      "Epoch 312/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.2095 - accuracy: 0.9166\n",
      "Epoch 313/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2053 - accuracy: 0.9192\n",
      "Epoch 314/1500\n",
      "49/49 [==============================] - 0s 793us/step - loss: 0.1976 - accuracy: 0.9231\n",
      "Epoch 315/1500\n",
      "49/49 [==============================] - 0s 765us/step - loss: 0.2036 - accuracy: 0.9247\n",
      "Epoch 316/1500\n",
      "49/49 [==============================] - 0s 756us/step - loss: 0.1993 - accuracy: 0.9231\n",
      "Epoch 317/1500\n",
      "49/49 [==============================] - 0s 770us/step - loss: 0.2131 - accuracy: 0.9205\n",
      "Epoch 318/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2085 - accuracy: 0.9172\n",
      "Epoch 319/1500\n",
      "49/49 [==============================] - 0s 818us/step - loss: 0.1988 - accuracy: 0.9247\n",
      "Epoch 320/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.2058 - accuracy: 0.9195\n",
      "Epoch 321/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.2005 - accuracy: 0.9198\n",
      "Epoch 322/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.2135 - accuracy: 0.9182\n",
      "Epoch 323/1500\n",
      "49/49 [==============================] - 0s 760us/step - loss: 0.2129 - accuracy: 0.9176\n",
      "Epoch 324/1500\n",
      "49/49 [==============================] - 0s 793us/step - loss: 0.1926 - accuracy: 0.9240\n",
      "Epoch 325/1500\n",
      "49/49 [==============================] - 0s 773us/step - loss: 0.1943 - accuracy: 0.9224\n",
      "Epoch 326/1500\n",
      "49/49 [==============================] - 0s 758us/step - loss: 0.2026 - accuracy: 0.9211\n",
      "Epoch 327/1500\n",
      "49/49 [==============================] - 0s 768us/step - loss: 0.1946 - accuracy: 0.9211\n",
      "Epoch 328/1500\n",
      "49/49 [==============================] - 0s 786us/step - loss: 0.1975 - accuracy: 0.9266\n",
      "Epoch 329/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.2199 - accuracy: 0.9169\n",
      "Epoch 330/1500\n",
      "49/49 [==============================] - 0s 773us/step - loss: 0.2033 - accuracy: 0.9153\n",
      "Epoch 331/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.1938 - accuracy: 0.9247\n",
      "Epoch 332/1500\n",
      "49/49 [==============================] - 0s 772us/step - loss: 0.2033 - accuracy: 0.9201\n",
      "Epoch 333/1500\n",
      "49/49 [==============================] - 0s 773us/step - loss: 0.2037 - accuracy: 0.9195\n",
      "Epoch 334/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9273\n",
      "Epoch 335/1500\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.1907 - accuracy: 0.9279\n",
      "Epoch 336/1500\n",
      "49/49 [==============================] - 0s 776us/step - loss: 0.1921 - accuracy: 0.9211\n",
      "Epoch 337/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.2030 - accuracy: 0.9218\n",
      "Epoch 338/1500\n",
      "49/49 [==============================] - 0s 779us/step - loss: 0.1920 - accuracy: 0.9266\n",
      "Epoch 339/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.2013 - accuracy: 0.9231\n",
      "Epoch 340/1500\n",
      "49/49 [==============================] - 0s 775us/step - loss: 0.1785 - accuracy: 0.9298\n",
      "Epoch 341/1500\n",
      "49/49 [==============================] - 0s 758us/step - loss: 0.1984 - accuracy: 0.9218\n",
      "Epoch 342/1500\n",
      "49/49 [==============================] - 0s 786us/step - loss: 0.1895 - accuracy: 0.9250\n",
      "Epoch 343/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.1965 - accuracy: 0.9208\n",
      "Epoch 344/1500\n",
      "49/49 [==============================] - 0s 770us/step - loss: 0.1804 - accuracy: 0.9324\n",
      "Epoch 345/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.2008 - accuracy: 0.9227\n",
      "Epoch 346/1500\n",
      "49/49 [==============================] - 0s 788us/step - loss: 0.1981 - accuracy: 0.9269\n",
      "Epoch 347/1500\n",
      "49/49 [==============================] - 0s 769us/step - loss: 0.2028 - accuracy: 0.9198\n",
      "Epoch 348/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.1856 - accuracy: 0.9295\n",
      "Epoch 349/1500\n",
      "49/49 [==============================] - 0s 771us/step - loss: 0.1939 - accuracy: 0.9205\n",
      "Epoch 350/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.9295\n",
      "Epoch 351/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.1822 - accuracy: 0.9234\n",
      "Epoch 352/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.1887 - accuracy: 0.9211\n",
      "Epoch 353/1500\n",
      "49/49 [==============================] - 0s 762us/step - loss: 0.1842 - accuracy: 0.9305\n",
      "Epoch 354/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.2060 - accuracy: 0.9240\n",
      "Epoch 355/1500\n",
      "49/49 [==============================] - 0s 760us/step - loss: 0.1833 - accuracy: 0.9256\n",
      "Epoch 356/1500\n",
      "49/49 [==============================] - 0s 753us/step - loss: 0.1856 - accuracy: 0.9273\n",
      "Epoch 357/1500\n",
      "49/49 [==============================] - 0s 780us/step - loss: 0.1866 - accuracy: 0.9266\n",
      "Epoch 358/1500\n",
      "49/49 [==============================] - 0s 787us/step - loss: 0.1757 - accuracy: 0.9292\n",
      "Epoch 359/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.1877 - accuracy: 0.9260\n",
      "Epoch 360/1500\n",
      "49/49 [==============================] - 0s 780us/step - loss: 0.1922 - accuracy: 0.9253\n",
      "Epoch 361/1500\n",
      "49/49 [==============================] - 0s 792us/step - loss: 0.1931 - accuracy: 0.9211\n",
      "Epoch 362/1500\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.1884 - accuracy: 0.9231\n",
      "Epoch 363/1500\n",
      "49/49 [==============================] - 0s 751us/step - loss: 0.1776 - accuracy: 0.9331\n",
      "Epoch 364/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.2068 - accuracy: 0.9185\n",
      "Epoch 365/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9289\n",
      "Epoch 366/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.2021 - accuracy: 0.9179\n",
      "Epoch 367/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.1877 - accuracy: 0.9353\n",
      "Epoch 368/1500\n",
      "49/49 [==============================] - 0s 775us/step - loss: 0.1823 - accuracy: 0.9311\n",
      "Epoch 369/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.1974 - accuracy: 0.9237\n",
      "Epoch 370/1500\n",
      "49/49 [==============================] - 0s 785us/step - loss: 0.1759 - accuracy: 0.9334\n",
      "Epoch 371/1500\n",
      "49/49 [==============================] - 0s 779us/step - loss: 0.1940 - accuracy: 0.9211\n",
      "Epoch 372/1500\n",
      "49/49 [==============================] - 0s 761us/step - loss: 0.1765 - accuracy: 0.9350\n",
      "Epoch 373/1500\n",
      "49/49 [==============================] - 0s 767us/step - loss: 0.1867 - accuracy: 0.9282\n",
      "Epoch 374/1500\n",
      "49/49 [==============================] - 0s 781us/step - loss: 0.2013 - accuracy: 0.9231\n",
      "Epoch 375/1500\n",
      "49/49 [==============================] - 0s 760us/step - loss: 0.1644 - accuracy: 0.9328\n",
      "Epoch 376/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.1875 - accuracy: 0.9305\n",
      "Epoch 377/1500\n",
      "49/49 [==============================] - 0s 793us/step - loss: 0.1928 - accuracy: 0.9253\n",
      "Epoch 378/1500\n",
      "49/49 [==============================] - 0s 773us/step - loss: 0.1862 - accuracy: 0.9266\n",
      "Epoch 379/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2013 - accuracy: 0.9218\n",
      "Epoch 380/1500\n",
      "49/49 [==============================] - 0s 812us/step - loss: 0.1859 - accuracy: 0.9266\n",
      "Epoch 381/1500\n",
      "49/49 [==============================] - 0s 803us/step - loss: 0.1918 - accuracy: 0.9276\n",
      "Epoch 382/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.1913 - accuracy: 0.9211\n",
      "Epoch 383/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.1824 - accuracy: 0.9266\n",
      "Epoch 384/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.1786 - accuracy: 0.9331\n",
      "Epoch 385/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.1898 - accuracy: 0.9289\n",
      "Epoch 386/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.1735 - accuracy: 0.9295\n",
      "Epoch 387/1500\n",
      "49/49 [==============================] - 0s 745us/step - loss: 0.1786 - accuracy: 0.9285\n",
      "Epoch 388/1500\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.1817 - accuracy: 0.9289\n",
      "Epoch 389/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.1873 - accuracy: 0.9260\n",
      "Epoch 390/1500\n",
      "49/49 [==============================] - 0s 766us/step - loss: 0.1734 - accuracy: 0.9289\n",
      "Epoch 391/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.1831 - accuracy: 0.9260\n",
      "Epoch 392/1500\n",
      "49/49 [==============================] - 0s 785us/step - loss: 0.1992 - accuracy: 0.9198\n",
      "Epoch 393/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9331\n",
      "Epoch 394/1500\n",
      "49/49 [==============================] - 0s 875us/step - loss: 0.1834 - accuracy: 0.9276\n",
      "Epoch 395/1500\n",
      "49/49 [==============================] - 0s 801us/step - loss: 0.1853 - accuracy: 0.9311\n",
      "Epoch 396/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.1839 - accuracy: 0.9295\n",
      "Epoch 397/1500\n",
      "49/49 [==============================] - 0s 779us/step - loss: 0.1764 - accuracy: 0.9279\n",
      "Epoch 398/1500\n",
      "49/49 [==============================] - 0s 792us/step - loss: 0.1705 - accuracy: 0.9357\n",
      "Epoch 399/1500\n",
      "49/49 [==============================] - 0s 768us/step - loss: 0.1755 - accuracy: 0.9353\n",
      "Epoch 400/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.1706 - accuracy: 0.9340\n",
      "Epoch 401/1500\n",
      "49/49 [==============================] - 0s 788us/step - loss: 0.1774 - accuracy: 0.9315\n",
      "Epoch 402/1500\n",
      "49/49 [==============================] - 0s 779us/step - loss: 0.1822 - accuracy: 0.9298\n",
      "Epoch 403/1500\n",
      "49/49 [==============================] - 0s 777us/step - loss: 0.1742 - accuracy: 0.9334\n",
      "Epoch 404/1500\n",
      "49/49 [==============================] - 0s 783us/step - loss: 0.1748 - accuracy: 0.9308\n",
      "Epoch 405/1500\n",
      " 1/49 [..............................] - ETA: 0s - loss: 0.2700 - accuracy: 0.8594Restoring model weights from the end of the best epoch: 375.\n",
      "49/49 [==============================] - 0s 866us/step - loss: 0.1807 - accuracy: 0.9282\n",
      "Epoch 405: early stopping\n",
      "6/6 [==============================] - 0s 832us/step - loss: 0.9292 - accuracy: 0.6667\n",
      "6/6 [==============================] - 0s 749us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Results - Loss: 0.9291595816612244, Accuracy: 0.6666666865348816, Precision: 0.6618488953394613, Recall: 0.7192956713132795, F1 Score: 0.6856522426316477\n",
      "Confusion Matrix:\n",
      " [[68  4 22]\n",
      " [ 1  9  0]\n",
      " [27  0 31]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "        ..\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, Length: 83, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'110A'}\n",
      "Moved to Test Set:\n",
      "{'110A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '047A' '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A'\n",
      " '097B' '099A' '101A' '102A' '103A' '104A' '108A' '109A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '048A' '049A' '051B' '054A' '057A' '068A'\n",
      " '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A' '110A']\n",
      "Length of X_train_val:\n",
      "717\n",
      "Length of y_train_val:\n",
      "717\n",
      "Length of groups_train_val:\n",
      "717\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    163\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({1: 1107, 0: 1023, 2: 870})\n",
      "Epoch 1/1500\n",
      "47/47 [==============================] - 0s 910us/step - loss: 1.0766 - accuracy: 0.4977\n",
      "Epoch 2/1500\n",
      "47/47 [==============================] - 0s 865us/step - loss: 0.8691 - accuracy: 0.6113\n",
      "Epoch 3/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.7803 - accuracy: 0.6687\n",
      "Epoch 4/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.7164 - accuracy: 0.6983\n",
      "Epoch 5/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.6678 - accuracy: 0.7167\n",
      "Epoch 6/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.6361 - accuracy: 0.7287\n",
      "Epoch 7/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.6240 - accuracy: 0.7357\n",
      "Epoch 8/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.5798 - accuracy: 0.7673\n",
      "Epoch 9/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.5889 - accuracy: 0.7507\n",
      "Epoch 10/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.5813 - accuracy: 0.7637\n",
      "Epoch 11/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.5503 - accuracy: 0.7800\n",
      "Epoch 12/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.5495 - accuracy: 0.7670\n",
      "Epoch 13/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.5223 - accuracy: 0.7870\n",
      "Epoch 14/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.5113 - accuracy: 0.7877\n",
      "Epoch 15/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.5046 - accuracy: 0.7967\n",
      "Epoch 16/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.4964 - accuracy: 0.8017\n",
      "Epoch 17/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.4950 - accuracy: 0.7963\n",
      "Epoch 18/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.5051 - accuracy: 0.7973\n",
      "Epoch 19/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.4855 - accuracy: 0.8037\n",
      "Epoch 20/1500\n",
      "47/47 [==============================] - 0s 812us/step - loss: 0.4778 - accuracy: 0.8003\n",
      "Epoch 21/1500\n",
      "47/47 [==============================] - 0s 829us/step - loss: 0.4667 - accuracy: 0.8113\n",
      "Epoch 22/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.4720 - accuracy: 0.8007\n",
      "Epoch 23/1500\n",
      "47/47 [==============================] - 0s 805us/step - loss: 0.4595 - accuracy: 0.8107\n",
      "Epoch 24/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.4529 - accuracy: 0.8083\n",
      "Epoch 25/1500\n",
      "47/47 [==============================] - 0s 814us/step - loss: 0.4424 - accuracy: 0.8193\n",
      "Epoch 26/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.4458 - accuracy: 0.8140\n",
      "Epoch 27/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.4346 - accuracy: 0.8237\n",
      "Epoch 28/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.4358 - accuracy: 0.8173\n",
      "Epoch 29/1500\n",
      "47/47 [==============================] - 0s 823us/step - loss: 0.4227 - accuracy: 0.8237\n",
      "Epoch 30/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.4333 - accuracy: 0.8173\n",
      "Epoch 31/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.4086 - accuracy: 0.8320\n",
      "Epoch 32/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.4215 - accuracy: 0.8297\n",
      "Epoch 33/1500\n",
      "47/47 [==============================] - 0s 795us/step - loss: 0.4142 - accuracy: 0.8357\n",
      "Epoch 34/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.4201 - accuracy: 0.8260\n",
      "Epoch 35/1500\n",
      "47/47 [==============================] - 0s 862us/step - loss: 0.4122 - accuracy: 0.8343\n",
      "Epoch 36/1500\n",
      "47/47 [==============================] - 0s 945us/step - loss: 0.4075 - accuracy: 0.8373\n",
      "Epoch 37/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8283\n",
      "Epoch 38/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8343\n",
      "Epoch 39/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.4117 - accuracy: 0.8330\n",
      "Epoch 40/1500\n",
      "47/47 [==============================] - 0s 848us/step - loss: 0.3940 - accuracy: 0.8413\n",
      "Epoch 41/1500\n",
      "47/47 [==============================] - 0s 836us/step - loss: 0.4012 - accuracy: 0.8327\n",
      "Epoch 42/1500\n",
      "47/47 [==============================] - 0s 946us/step - loss: 0.3818 - accuracy: 0.8410\n",
      "Epoch 43/1500\n",
      "47/47 [==============================] - 0s 972us/step - loss: 0.3942 - accuracy: 0.8330\n",
      "Epoch 44/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.3885 - accuracy: 0.8430\n",
      "Epoch 45/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8533\n",
      "Epoch 46/1500\n",
      "47/47 [==============================] - 0s 984us/step - loss: 0.3838 - accuracy: 0.8417\n",
      "Epoch 47/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8487\n",
      "Epoch 48/1500\n",
      "47/47 [==============================] - 0s 843us/step - loss: 0.3784 - accuracy: 0.8460\n",
      "Epoch 49/1500\n",
      "47/47 [==============================] - 0s 936us/step - loss: 0.3657 - accuracy: 0.8553\n",
      "Epoch 50/1500\n",
      "47/47 [==============================] - 0s 902us/step - loss: 0.3712 - accuracy: 0.8480\n",
      "Epoch 51/1500\n",
      "47/47 [==============================] - 0s 897us/step - loss: 0.3677 - accuracy: 0.8480\n",
      "Epoch 52/1500\n",
      "47/47 [==============================] - 0s 920us/step - loss: 0.3587 - accuracy: 0.8533\n",
      "Epoch 53/1500\n",
      "47/47 [==============================] - 0s 870us/step - loss: 0.3548 - accuracy: 0.8543\n",
      "Epoch 54/1500\n",
      "47/47 [==============================] - 0s 880us/step - loss: 0.3605 - accuracy: 0.8540\n",
      "Epoch 55/1500\n",
      "47/47 [==============================] - 0s 859us/step - loss: 0.3567 - accuracy: 0.8467\n",
      "Epoch 56/1500\n",
      "47/47 [==============================] - 0s 846us/step - loss: 0.3519 - accuracy: 0.8550\n",
      "Epoch 57/1500\n",
      "47/47 [==============================] - 0s 841us/step - loss: 0.3632 - accuracy: 0.8547\n",
      "Epoch 58/1500\n",
      "47/47 [==============================] - 0s 847us/step - loss: 0.3587 - accuracy: 0.8623\n",
      "Epoch 59/1500\n",
      "47/47 [==============================] - 0s 892us/step - loss: 0.3358 - accuracy: 0.8633\n",
      "Epoch 60/1500\n",
      "47/47 [==============================] - 0s 861us/step - loss: 0.3491 - accuracy: 0.8580\n",
      "Epoch 61/1500\n",
      "47/47 [==============================] - 0s 857us/step - loss: 0.3334 - accuracy: 0.8673\n",
      "Epoch 62/1500\n",
      "47/47 [==============================] - 0s 887us/step - loss: 0.3507 - accuracy: 0.8663\n",
      "Epoch 63/1500\n",
      "47/47 [==============================] - 0s 879us/step - loss: 0.3322 - accuracy: 0.8707\n",
      "Epoch 64/1500\n",
      "47/47 [==============================] - 0s 903us/step - loss: 0.3374 - accuracy: 0.8650\n",
      "Epoch 65/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.3316 - accuracy: 0.8667\n",
      "Epoch 66/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.3362 - accuracy: 0.8647\n",
      "Epoch 67/1500\n",
      "47/47 [==============================] - 0s 832us/step - loss: 0.3529 - accuracy: 0.8553\n",
      "Epoch 68/1500\n",
      "47/47 [==============================] - 0s 839us/step - loss: 0.3352 - accuracy: 0.8617\n",
      "Epoch 69/1500\n",
      "47/47 [==============================] - 0s 831us/step - loss: 0.3295 - accuracy: 0.8713\n",
      "Epoch 70/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.3234 - accuracy: 0.8750\n",
      "Epoch 71/1500\n",
      "47/47 [==============================] - 0s 793us/step - loss: 0.3268 - accuracy: 0.8660\n",
      "Epoch 72/1500\n",
      "47/47 [==============================] - 0s 844us/step - loss: 0.3094 - accuracy: 0.8693\n",
      "Epoch 73/1500\n",
      "47/47 [==============================] - 0s 883us/step - loss: 0.3253 - accuracy: 0.8730\n",
      "Epoch 74/1500\n",
      "47/47 [==============================] - 0s 867us/step - loss: 0.3171 - accuracy: 0.8710\n",
      "Epoch 75/1500\n",
      "47/47 [==============================] - 0s 873us/step - loss: 0.3241 - accuracy: 0.8697\n",
      "Epoch 76/1500\n",
      "47/47 [==============================] - 0s 873us/step - loss: 0.3068 - accuracy: 0.8753\n",
      "Epoch 77/1500\n",
      "47/47 [==============================] - 0s 872us/step - loss: 0.3219 - accuracy: 0.8697\n",
      "Epoch 78/1500\n",
      "47/47 [==============================] - 0s 864us/step - loss: 0.3222 - accuracy: 0.8713\n",
      "Epoch 79/1500\n",
      "47/47 [==============================] - 0s 848us/step - loss: 0.3135 - accuracy: 0.8787\n",
      "Epoch 80/1500\n",
      "47/47 [==============================] - 0s 863us/step - loss: 0.3178 - accuracy: 0.8800\n",
      "Epoch 81/1500\n",
      "47/47 [==============================] - 0s 855us/step - loss: 0.3242 - accuracy: 0.8677\n",
      "Epoch 82/1500\n",
      "47/47 [==============================] - 0s 804us/step - loss: 0.3099 - accuracy: 0.8827\n",
      "Epoch 83/1500\n",
      "47/47 [==============================] - 0s 805us/step - loss: 0.3038 - accuracy: 0.8843\n",
      "Epoch 84/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.3073 - accuracy: 0.8820\n",
      "Epoch 85/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.3142 - accuracy: 0.8787\n",
      "Epoch 86/1500\n",
      "47/47 [==============================] - 0s 857us/step - loss: 0.3064 - accuracy: 0.8823\n",
      "Epoch 87/1500\n",
      "47/47 [==============================] - 0s 887us/step - loss: 0.3032 - accuracy: 0.8807\n",
      "Epoch 88/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8823\n",
      "Epoch 89/1500\n",
      "47/47 [==============================] - 0s 909us/step - loss: 0.3083 - accuracy: 0.8770\n",
      "Epoch 90/1500\n",
      "47/47 [==============================] - 0s 892us/step - loss: 0.3046 - accuracy: 0.8820\n",
      "Epoch 91/1500\n",
      "47/47 [==============================] - 0s 886us/step - loss: 0.3007 - accuracy: 0.8810\n",
      "Epoch 92/1500\n",
      "47/47 [==============================] - 0s 895us/step - loss: 0.2976 - accuracy: 0.8820\n",
      "Epoch 93/1500\n",
      "47/47 [==============================] - 0s 820us/step - loss: 0.2976 - accuracy: 0.8810\n",
      "Epoch 94/1500\n",
      "47/47 [==============================] - 0s 836us/step - loss: 0.2955 - accuracy: 0.8840\n",
      "Epoch 95/1500\n",
      "47/47 [==============================] - 0s 865us/step - loss: 0.2894 - accuracy: 0.8870\n",
      "Epoch 96/1500\n",
      "47/47 [==============================] - 0s 884us/step - loss: 0.2843 - accuracy: 0.8920\n",
      "Epoch 97/1500\n",
      "47/47 [==============================] - 0s 873us/step - loss: 0.3054 - accuracy: 0.8757\n",
      "Epoch 98/1500\n",
      "47/47 [==============================] - 0s 827us/step - loss: 0.2820 - accuracy: 0.8927\n",
      "Epoch 99/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.2940 - accuracy: 0.8810\n",
      "Epoch 100/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.2834 - accuracy: 0.8883\n",
      "Epoch 101/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2928 - accuracy: 0.8830\n",
      "Epoch 102/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.2885 - accuracy: 0.8890\n",
      "Epoch 103/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.2844 - accuracy: 0.8847\n",
      "Epoch 104/1500\n",
      "47/47 [==============================] - 0s 783us/step - loss: 0.2827 - accuracy: 0.8930\n",
      "Epoch 105/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.2816 - accuracy: 0.8900\n",
      "Epoch 106/1500\n",
      "47/47 [==============================] - 0s 869us/step - loss: 0.2884 - accuracy: 0.8897\n",
      "Epoch 107/1500\n",
      "47/47 [==============================] - 0s 853us/step - loss: 0.2803 - accuracy: 0.8870\n",
      "Epoch 108/1500\n",
      "47/47 [==============================] - 0s 823us/step - loss: 0.2689 - accuracy: 0.8900\n",
      "Epoch 109/1500\n",
      "47/47 [==============================] - 0s 827us/step - loss: 0.2752 - accuracy: 0.8877\n",
      "Epoch 110/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.2803 - accuracy: 0.8890\n",
      "Epoch 111/1500\n",
      "47/47 [==============================] - 0s 857us/step - loss: 0.2706 - accuracy: 0.8933\n",
      "Epoch 112/1500\n",
      "47/47 [==============================] - 0s 932us/step - loss: 0.2816 - accuracy: 0.8833\n",
      "Epoch 113/1500\n",
      "47/47 [==============================] - 0s 858us/step - loss: 0.2618 - accuracy: 0.8980\n",
      "Epoch 114/1500\n",
      "47/47 [==============================] - 0s 858us/step - loss: 0.2676 - accuracy: 0.8993\n",
      "Epoch 115/1500\n",
      "47/47 [==============================] - 0s 887us/step - loss: 0.2781 - accuracy: 0.8847\n",
      "Epoch 116/1500\n",
      "47/47 [==============================] - 0s 886us/step - loss: 0.2728 - accuracy: 0.8910\n",
      "Epoch 117/1500\n",
      "47/47 [==============================] - 0s 894us/step - loss: 0.2618 - accuracy: 0.8963\n",
      "Epoch 118/1500\n",
      "47/47 [==============================] - 0s 858us/step - loss: 0.2767 - accuracy: 0.8917\n",
      "Epoch 119/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.2772 - accuracy: 0.8907\n",
      "Epoch 120/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2713 - accuracy: 0.8980\n",
      "Epoch 121/1500\n",
      "47/47 [==============================] - 0s 813us/step - loss: 0.2762 - accuracy: 0.8877\n",
      "Epoch 122/1500\n",
      "47/47 [==============================] - 0s 831us/step - loss: 0.2706 - accuracy: 0.8937\n",
      "Epoch 123/1500\n",
      "47/47 [==============================] - 0s 872us/step - loss: 0.2597 - accuracy: 0.8907\n",
      "Epoch 124/1500\n",
      "47/47 [==============================] - 0s 823us/step - loss: 0.2627 - accuracy: 0.8950\n",
      "Epoch 125/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.2637 - accuracy: 0.8927\n",
      "Epoch 126/1500\n",
      "47/47 [==============================] - 0s 832us/step - loss: 0.2655 - accuracy: 0.8983\n",
      "Epoch 127/1500\n",
      "47/47 [==============================] - 0s 844us/step - loss: 0.2637 - accuracy: 0.8983\n",
      "Epoch 128/1500\n",
      "47/47 [==============================] - 0s 926us/step - loss: 0.2678 - accuracy: 0.8940\n",
      "Epoch 129/1500\n",
      "47/47 [==============================] - 0s 962us/step - loss: 0.2549 - accuracy: 0.9023\n",
      "Epoch 130/1500\n",
      "47/47 [==============================] - 0s 916us/step - loss: 0.2570 - accuracy: 0.8987\n",
      "Epoch 131/1500\n",
      "47/47 [==============================] - 0s 932us/step - loss: 0.2573 - accuracy: 0.8977\n",
      "Epoch 132/1500\n",
      "47/47 [==============================] - 0s 820us/step - loss: 0.2607 - accuracy: 0.8980\n",
      "Epoch 133/1500\n",
      "47/47 [==============================] - 0s 840us/step - loss: 0.2656 - accuracy: 0.9003\n",
      "Epoch 134/1500\n",
      "47/47 [==============================] - 0s 834us/step - loss: 0.2552 - accuracy: 0.9063\n",
      "Epoch 135/1500\n",
      "47/47 [==============================] - 0s 842us/step - loss: 0.2540 - accuracy: 0.9037\n",
      "Epoch 136/1500\n",
      "47/47 [==============================] - 0s 871us/step - loss: 0.2443 - accuracy: 0.9047\n",
      "Epoch 137/1500\n",
      "47/47 [==============================] - 0s 852us/step - loss: 0.2634 - accuracy: 0.8937\n",
      "Epoch 138/1500\n",
      "47/47 [==============================] - 0s 887us/step - loss: 0.2518 - accuracy: 0.8953\n",
      "Epoch 139/1500\n",
      "47/47 [==============================] - 0s 892us/step - loss: 0.2507 - accuracy: 0.9010\n",
      "Epoch 140/1500\n",
      "47/47 [==============================] - 0s 865us/step - loss: 0.2491 - accuracy: 0.9053\n",
      "Epoch 141/1500\n",
      "47/47 [==============================] - 0s 876us/step - loss: 0.2709 - accuracy: 0.8927\n",
      "Epoch 142/1500\n",
      "47/47 [==============================] - 0s 859us/step - loss: 0.2440 - accuracy: 0.9037\n",
      "Epoch 143/1500\n",
      "47/47 [==============================] - 0s 825us/step - loss: 0.2440 - accuracy: 0.9077\n",
      "Epoch 144/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.2496 - accuracy: 0.8990\n",
      "Epoch 145/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.2457 - accuracy: 0.9023\n",
      "Epoch 146/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.2639 - accuracy: 0.8903\n",
      "Epoch 147/1500\n",
      "47/47 [==============================] - 0s 842us/step - loss: 0.2494 - accuracy: 0.9023\n",
      "Epoch 148/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.2450 - accuracy: 0.9070\n",
      "Epoch 149/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.9040\n",
      "Epoch 150/1500\n",
      "47/47 [==============================] - 0s 847us/step - loss: 0.2552 - accuracy: 0.9013\n",
      "Epoch 151/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2555 - accuracy: 0.9010\n",
      "Epoch 152/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.2467 - accuracy: 0.9013\n",
      "Epoch 153/1500\n",
      "47/47 [==============================] - 0s 802us/step - loss: 0.2376 - accuracy: 0.9057\n",
      "Epoch 154/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.2533 - accuracy: 0.9013\n",
      "Epoch 155/1500\n",
      "47/47 [==============================] - 0s 783us/step - loss: 0.2404 - accuracy: 0.9060\n",
      "Epoch 156/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.2358 - accuracy: 0.9020\n",
      "Epoch 157/1500\n",
      "47/47 [==============================] - 0s 846us/step - loss: 0.2296 - accuracy: 0.9053\n",
      "Epoch 158/1500\n",
      "47/47 [==============================] - 0s 817us/step - loss: 0.2491 - accuracy: 0.9037\n",
      "Epoch 159/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.2383 - accuracy: 0.9057\n",
      "Epoch 160/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.2463 - accuracy: 0.9000\n",
      "Epoch 161/1500\n",
      "47/47 [==============================] - 0s 832us/step - loss: 0.2321 - accuracy: 0.9137\n",
      "Epoch 162/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.2427 - accuracy: 0.9063\n",
      "Epoch 163/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.2543 - accuracy: 0.9010\n",
      "Epoch 164/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.2299 - accuracy: 0.9127\n",
      "Epoch 165/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.2326 - accuracy: 0.9123\n",
      "Epoch 166/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.2413 - accuracy: 0.9033\n",
      "Epoch 167/1500\n",
      "47/47 [==============================] - 0s 795us/step - loss: 0.2332 - accuracy: 0.9073\n",
      "Epoch 168/1500\n",
      "47/47 [==============================] - 0s 805us/step - loss: 0.2287 - accuracy: 0.9150\n",
      "Epoch 169/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.2322 - accuracy: 0.9140\n",
      "Epoch 170/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.2354 - accuracy: 0.9110\n",
      "Epoch 171/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.2327 - accuracy: 0.9137\n",
      "Epoch 172/1500\n",
      "47/47 [==============================] - 0s 910us/step - loss: 0.2379 - accuracy: 0.9090\n",
      "Epoch 173/1500\n",
      "47/47 [==============================] - 0s 925us/step - loss: 0.2232 - accuracy: 0.9157\n",
      "Epoch 174/1500\n",
      "47/47 [==============================] - 0s 888us/step - loss: 0.2328 - accuracy: 0.9077\n",
      "Epoch 175/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9107\n",
      "Epoch 176/1500\n",
      "47/47 [==============================] - 0s 831us/step - loss: 0.2429 - accuracy: 0.9020\n",
      "Epoch 177/1500\n",
      "47/47 [==============================] - 0s 798us/step - loss: 0.2337 - accuracy: 0.9067\n",
      "Epoch 178/1500\n",
      "47/47 [==============================] - 0s 820us/step - loss: 0.2393 - accuracy: 0.9070\n",
      "Epoch 179/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.2146 - accuracy: 0.9227\n",
      "Epoch 180/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.2264 - accuracy: 0.9107\n",
      "Epoch 181/1500\n",
      "47/47 [==============================] - 0s 846us/step - loss: 0.2208 - accuracy: 0.9117\n",
      "Epoch 182/1500\n",
      "47/47 [==============================] - 0s 839us/step - loss: 0.2195 - accuracy: 0.9167\n",
      "Epoch 183/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.2322 - accuracy: 0.9143\n",
      "Epoch 184/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.2214 - accuracy: 0.9150\n",
      "Epoch 185/1500\n",
      "47/47 [==============================] - 0s 841us/step - loss: 0.2234 - accuracy: 0.9160\n",
      "Epoch 186/1500\n",
      "47/47 [==============================] - 0s 819us/step - loss: 0.2275 - accuracy: 0.9090\n",
      "Epoch 187/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2210 - accuracy: 0.9133\n",
      "Epoch 188/1500\n",
      "47/47 [==============================] - 0s 853us/step - loss: 0.2187 - accuracy: 0.9133\n",
      "Epoch 189/1500\n",
      "47/47 [==============================] - 0s 845us/step - loss: 0.2220 - accuracy: 0.9167\n",
      "Epoch 190/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.2274 - accuracy: 0.9103\n",
      "Epoch 191/1500\n",
      "47/47 [==============================] - 0s 821us/step - loss: 0.2269 - accuracy: 0.9090\n",
      "Epoch 192/1500\n",
      "47/47 [==============================] - 0s 825us/step - loss: 0.2088 - accuracy: 0.9097\n",
      "Epoch 193/1500\n",
      "47/47 [==============================] - 0s 851us/step - loss: 0.2069 - accuracy: 0.9173\n",
      "Epoch 194/1500\n",
      "47/47 [==============================] - 0s 816us/step - loss: 0.2145 - accuracy: 0.9190\n",
      "Epoch 195/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.2201 - accuracy: 0.9117\n",
      "Epoch 196/1500\n",
      "47/47 [==============================] - 0s 796us/step - loss: 0.2278 - accuracy: 0.9113\n",
      "Epoch 197/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.2131 - accuracy: 0.9193\n",
      "Epoch 198/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.2221 - accuracy: 0.9150\n",
      "Epoch 199/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2277 - accuracy: 0.9060\n",
      "Epoch 200/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.2141 - accuracy: 0.9180\n",
      "Epoch 201/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.2205 - accuracy: 0.9187\n",
      "Epoch 202/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.2236 - accuracy: 0.9110\n",
      "Epoch 203/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.2133 - accuracy: 0.9243\n",
      "Epoch 204/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.2266 - accuracy: 0.9053\n",
      "Epoch 205/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.2112 - accuracy: 0.9207\n",
      "Epoch 206/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9223\n",
      "Epoch 207/1500\n",
      "47/47 [==============================] - 0s 961us/step - loss: 0.2153 - accuracy: 0.9083\n",
      "Epoch 208/1500\n",
      "47/47 [==============================] - 0s 876us/step - loss: 0.2236 - accuracy: 0.9163\n",
      "Epoch 209/1500\n",
      "47/47 [==============================] - 0s 917us/step - loss: 0.2099 - accuracy: 0.9223\n",
      "Epoch 210/1500\n",
      "47/47 [==============================] - 0s 859us/step - loss: 0.2138 - accuracy: 0.9157\n",
      "Epoch 211/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.2049 - accuracy: 0.9257\n",
      "Epoch 212/1500\n",
      "47/47 [==============================] - 0s 845us/step - loss: 0.1983 - accuracy: 0.9200\n",
      "Epoch 213/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.2133 - accuracy: 0.9207\n",
      "Epoch 214/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.2067 - accuracy: 0.9207\n",
      "Epoch 215/1500\n",
      "47/47 [==============================] - 0s 879us/step - loss: 0.2119 - accuracy: 0.9197\n",
      "Epoch 216/1500\n",
      "47/47 [==============================] - 0s 862us/step - loss: 0.2143 - accuracy: 0.9177\n",
      "Epoch 217/1500\n",
      "47/47 [==============================] - 0s 907us/step - loss: 0.1900 - accuracy: 0.9273\n",
      "Epoch 218/1500\n",
      "47/47 [==============================] - 0s 881us/step - loss: 0.2139 - accuracy: 0.9170\n",
      "Epoch 219/1500\n",
      "47/47 [==============================] - 0s 844us/step - loss: 0.2119 - accuracy: 0.9150\n",
      "Epoch 220/1500\n",
      "47/47 [==============================] - 0s 855us/step - loss: 0.2174 - accuracy: 0.9177\n",
      "Epoch 221/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9220\n",
      "Epoch 222/1500\n",
      "47/47 [==============================] - 0s 879us/step - loss: 0.2097 - accuracy: 0.9217\n",
      "Epoch 223/1500\n",
      "47/47 [==============================] - 0s 914us/step - loss: 0.2156 - accuracy: 0.9167\n",
      "Epoch 224/1500\n",
      "47/47 [==============================] - 0s 907us/step - loss: 0.2066 - accuracy: 0.9200\n",
      "Epoch 225/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.2077 - accuracy: 0.9167\n",
      "Epoch 226/1500\n",
      "47/47 [==============================] - 0s 849us/step - loss: 0.2139 - accuracy: 0.9183\n",
      "Epoch 227/1500\n",
      "47/47 [==============================] - 0s 860us/step - loss: 0.2027 - accuracy: 0.9200\n",
      "Epoch 228/1500\n",
      "47/47 [==============================] - 0s 849us/step - loss: 0.2156 - accuracy: 0.9150\n",
      "Epoch 229/1500\n",
      "47/47 [==============================] - 0s 867us/step - loss: 0.2073 - accuracy: 0.9213\n",
      "Epoch 230/1500\n",
      "47/47 [==============================] - 0s 874us/step - loss: 0.1966 - accuracy: 0.9280\n",
      "Epoch 231/1500\n",
      "47/47 [==============================] - 0s 891us/step - loss: 0.2051 - accuracy: 0.9243\n",
      "Epoch 232/1500\n",
      "47/47 [==============================] - 0s 818us/step - loss: 0.2097 - accuracy: 0.9177\n",
      "Epoch 233/1500\n",
      "47/47 [==============================] - 0s 841us/step - loss: 0.2041 - accuracy: 0.9217\n",
      "Epoch 234/1500\n",
      "47/47 [==============================] - 0s 842us/step - loss: 0.1954 - accuracy: 0.9217\n",
      "Epoch 235/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.2053 - accuracy: 0.9177\n",
      "Epoch 236/1500\n",
      "47/47 [==============================] - 0s 823us/step - loss: 0.1965 - accuracy: 0.9270\n",
      "Epoch 237/1500\n",
      "47/47 [==============================] - 0s 839us/step - loss: 0.2091 - accuracy: 0.9233\n",
      "Epoch 238/1500\n",
      "47/47 [==============================] - 0s 817us/step - loss: 0.2017 - accuracy: 0.9267\n",
      "Epoch 239/1500\n",
      "47/47 [==============================] - 0s 856us/step - loss: 0.1930 - accuracy: 0.9290\n",
      "Epoch 240/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.2125 - accuracy: 0.9147\n",
      "Epoch 241/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.1941 - accuracy: 0.9227\n",
      "Epoch 242/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9180\n",
      "Epoch 243/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2048 - accuracy: 0.9200\n",
      "Epoch 244/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.1998 - accuracy: 0.9253\n",
      "Epoch 245/1500\n",
      "47/47 [==============================] - 0s 786us/step - loss: 0.1959 - accuracy: 0.9257\n",
      "Epoch 246/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.2027 - accuracy: 0.9233\n",
      "Epoch 247/1500\n",
      " 1/47 [..............................] - ETA: 0s - loss: 0.1192 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 217.\n",
      "47/47 [==============================] - 0s 866us/step - loss: 0.2068 - accuracy: 0.9230\n",
      "Epoch 247: early stopping\n",
      "7/7 [==============================] - 0s 752us/step - loss: 0.9142 - accuracy: 0.6636\n",
      "7/7 [==============================] - 0s 643us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 382, Predictions: 382, Actuals: 382, Gender: 382\n",
      "Final Test Results - Loss: 0.9142130613327026, Accuracy: 0.6636363863945007, Precision: 0.5905773420479302, Recall: 0.6318910256410256, F1 Score: 0.603558746333179\n",
      "Confusion Matrix:\n",
      " [[111   4  45]\n",
      " [  3   5   0]\n",
      " [ 22   0  30]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "        ..\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 82, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 909, 1: 891, 2: 819})\n",
      "Epoch 1/1500\n",
      "41/41 [==============================] - 0s 941us/step - loss: 1.1541 - accuracy: 0.4670\n",
      "Epoch 2/1500\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.8794 - accuracy: 0.6056\n",
      "Epoch 3/1500\n",
      "41/41 [==============================] - 0s 921us/step - loss: 0.8090 - accuracy: 0.6399\n",
      "Epoch 4/1500\n",
      "41/41 [==============================] - 0s 876us/step - loss: 0.8086 - accuracy: 0.6350\n",
      "Epoch 5/1500\n",
      "41/41 [==============================] - 0s 932us/step - loss: 0.7434 - accuracy: 0.6686\n",
      "Epoch 6/1500\n",
      "41/41 [==============================] - 0s 937us/step - loss: 0.7447 - accuracy: 0.6732\n",
      "Epoch 7/1500\n",
      "41/41 [==============================] - 0s 890us/step - loss: 0.7113 - accuracy: 0.6800\n",
      "Epoch 8/1500\n",
      "41/41 [==============================] - 0s 832us/step - loss: 0.7080 - accuracy: 0.6861\n",
      "Epoch 9/1500\n",
      "41/41 [==============================] - 0s 883us/step - loss: 0.6838 - accuracy: 0.7060\n",
      "Epoch 10/1500\n",
      "41/41 [==============================] - 0s 804us/step - loss: 0.6770 - accuracy: 0.6884\n",
      "Epoch 11/1500\n",
      "41/41 [==============================] - 0s 817us/step - loss: 0.6673 - accuracy: 0.6945\n",
      "Epoch 12/1500\n",
      "41/41 [==============================] - 0s 820us/step - loss: 0.6591 - accuracy: 0.6934\n",
      "Epoch 13/1500\n",
      "41/41 [==============================] - 0s 828us/step - loss: 0.6478 - accuracy: 0.7090\n",
      "Epoch 14/1500\n",
      "41/41 [==============================] - 0s 836us/step - loss: 0.6382 - accuracy: 0.7186\n",
      "Epoch 15/1500\n",
      "41/41 [==============================] - 0s 781us/step - loss: 0.6406 - accuracy: 0.7129\n",
      "Epoch 16/1500\n",
      "41/41 [==============================] - 0s 827us/step - loss: 0.6117 - accuracy: 0.7308\n",
      "Epoch 17/1500\n",
      "41/41 [==============================] - 0s 836us/step - loss: 0.6227 - accuracy: 0.7274\n",
      "Epoch 18/1500\n",
      "41/41 [==============================] - 0s 796us/step - loss: 0.5956 - accuracy: 0.7384\n",
      "Epoch 19/1500\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.5892 - accuracy: 0.7510\n",
      "Epoch 20/1500\n",
      "41/41 [==============================] - 0s 854us/step - loss: 0.5868 - accuracy: 0.7438\n",
      "Epoch 21/1500\n",
      "41/41 [==============================] - 0s 837us/step - loss: 0.5681 - accuracy: 0.7541\n",
      "Epoch 22/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.5757 - accuracy: 0.7491\n",
      "Epoch 23/1500\n",
      "41/41 [==============================] - 0s 831us/step - loss: 0.5782 - accuracy: 0.7530\n",
      "Epoch 24/1500\n",
      "41/41 [==============================] - 0s 825us/step - loss: 0.5480 - accuracy: 0.7629\n",
      "Epoch 25/1500\n",
      "41/41 [==============================] - 0s 782us/step - loss: 0.5518 - accuracy: 0.7514\n",
      "Epoch 26/1500\n",
      "41/41 [==============================] - 0s 790us/step - loss: 0.5463 - accuracy: 0.7602\n",
      "Epoch 27/1500\n",
      "41/41 [==============================] - 0s 857us/step - loss: 0.5537 - accuracy: 0.7595\n",
      "Epoch 28/1500\n",
      "41/41 [==============================] - 0s 782us/step - loss: 0.5406 - accuracy: 0.7617\n",
      "Epoch 29/1500\n",
      "41/41 [==============================] - 0s 784us/step - loss: 0.5535 - accuracy: 0.7633\n",
      "Epoch 30/1500\n",
      "41/41 [==============================] - 0s 801us/step - loss: 0.5198 - accuracy: 0.7732\n",
      "Epoch 31/1500\n",
      "41/41 [==============================] - 0s 782us/step - loss: 0.5265 - accuracy: 0.7690\n",
      "Epoch 32/1500\n",
      "41/41 [==============================] - 0s 779us/step - loss: 0.5215 - accuracy: 0.7755\n",
      "Epoch 33/1500\n",
      "41/41 [==============================] - 0s 782us/step - loss: 0.5136 - accuracy: 0.7805\n",
      "Epoch 34/1500\n",
      "41/41 [==============================] - 0s 797us/step - loss: 0.5091 - accuracy: 0.7812\n",
      "Epoch 35/1500\n",
      "41/41 [==============================] - 0s 761us/step - loss: 0.5116 - accuracy: 0.7785\n",
      "Epoch 36/1500\n",
      "41/41 [==============================] - 0s 842us/step - loss: 0.5078 - accuracy: 0.7728\n",
      "Epoch 37/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7808\n",
      "Epoch 38/1500\n",
      "41/41 [==============================] - 0s 807us/step - loss: 0.5034 - accuracy: 0.7820\n",
      "Epoch 39/1500\n",
      "41/41 [==============================] - 0s 784us/step - loss: 0.4957 - accuracy: 0.7881\n",
      "Epoch 40/1500\n",
      "41/41 [==============================] - 0s 819us/step - loss: 0.4813 - accuracy: 0.7885\n",
      "Epoch 41/1500\n",
      "41/41 [==============================] - 0s 787us/step - loss: 0.4822 - accuracy: 0.7942\n",
      "Epoch 42/1500\n",
      "41/41 [==============================] - 0s 804us/step - loss: 0.4904 - accuracy: 0.7847\n",
      "Epoch 43/1500\n",
      "41/41 [==============================] - 0s 780us/step - loss: 0.4910 - accuracy: 0.7847\n",
      "Epoch 44/1500\n",
      "41/41 [==============================] - 0s 780us/step - loss: 0.4948 - accuracy: 0.7911\n",
      "Epoch 45/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.4664 - accuracy: 0.7938\n",
      "Epoch 46/1500\n",
      "41/41 [==============================] - 0s 801us/step - loss: 0.4572 - accuracy: 0.7984\n",
      "Epoch 47/1500\n",
      "41/41 [==============================] - 0s 783us/step - loss: 0.4749 - accuracy: 0.8011\n",
      "Epoch 48/1500\n",
      "41/41 [==============================] - 0s 824us/step - loss: 0.4818 - accuracy: 0.7866\n",
      "Epoch 49/1500\n",
      "41/41 [==============================] - 0s 864us/step - loss: 0.4691 - accuracy: 0.7976\n",
      "Epoch 50/1500\n",
      "41/41 [==============================] - 0s 850us/step - loss: 0.4710 - accuracy: 0.8007\n",
      "Epoch 51/1500\n",
      "41/41 [==============================] - 0s 805us/step - loss: 0.4640 - accuracy: 0.7980\n",
      "Epoch 52/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.4665 - accuracy: 0.8076\n",
      "Epoch 53/1500\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.4570 - accuracy: 0.8015\n",
      "Epoch 54/1500\n",
      "41/41 [==============================] - 0s 829us/step - loss: 0.4561 - accuracy: 0.7992\n",
      "Epoch 55/1500\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.4484 - accuracy: 0.8083\n",
      "Epoch 56/1500\n",
      "41/41 [==============================] - 0s 808us/step - loss: 0.4512 - accuracy: 0.7992\n",
      "Epoch 57/1500\n",
      "41/41 [==============================] - 0s 845us/step - loss: 0.4522 - accuracy: 0.8049\n",
      "Epoch 58/1500\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.4582 - accuracy: 0.8037\n",
      "Epoch 59/1500\n",
      "41/41 [==============================] - 0s 834us/step - loss: 0.4403 - accuracy: 0.8183\n",
      "Epoch 60/1500\n",
      "41/41 [==============================] - 0s 832us/step - loss: 0.4277 - accuracy: 0.8247\n",
      "Epoch 61/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.4521 - accuracy: 0.8068\n",
      "Epoch 62/1500\n",
      "41/41 [==============================] - 0s 754us/step - loss: 0.4289 - accuracy: 0.8267\n",
      "Epoch 63/1500\n",
      "41/41 [==============================] - 0s 782us/step - loss: 0.4367 - accuracy: 0.8137\n",
      "Epoch 64/1500\n",
      "41/41 [==============================] - 0s 810us/step - loss: 0.4429 - accuracy: 0.8072\n",
      "Epoch 65/1500\n",
      "41/41 [==============================] - 0s 797us/step - loss: 0.4389 - accuracy: 0.8087\n",
      "Epoch 66/1500\n",
      "41/41 [==============================] - 0s 741us/step - loss: 0.4425 - accuracy: 0.8114\n",
      "Epoch 67/1500\n",
      "41/41 [==============================] - 0s 779us/step - loss: 0.4331 - accuracy: 0.8232\n",
      "Epoch 68/1500\n",
      "41/41 [==============================] - 0s 790us/step - loss: 0.4352 - accuracy: 0.8121\n",
      "Epoch 69/1500\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.4286 - accuracy: 0.8175\n",
      "Epoch 70/1500\n",
      "41/41 [==============================] - 0s 784us/step - loss: 0.4318 - accuracy: 0.8179\n",
      "Epoch 71/1500\n",
      "41/41 [==============================] - 0s 786us/step - loss: 0.4290 - accuracy: 0.8121\n",
      "Epoch 72/1500\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.4250 - accuracy: 0.8129\n",
      "Epoch 73/1500\n",
      "41/41 [==============================] - 0s 788us/step - loss: 0.4394 - accuracy: 0.8072\n",
      "Epoch 74/1500\n",
      "41/41 [==============================] - 0s 771us/step - loss: 0.4269 - accuracy: 0.8167\n",
      "Epoch 75/1500\n",
      "41/41 [==============================] - 0s 782us/step - loss: 0.4228 - accuracy: 0.8221\n",
      "Epoch 76/1500\n",
      "41/41 [==============================] - 0s 794us/step - loss: 0.4210 - accuracy: 0.8225\n",
      "Epoch 77/1500\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.4136 - accuracy: 0.8274\n",
      "Epoch 78/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.4109 - accuracy: 0.8309\n",
      "Epoch 79/1500\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.4077 - accuracy: 0.8240\n",
      "Epoch 80/1500\n",
      "41/41 [==============================] - 0s 797us/step - loss: 0.4082 - accuracy: 0.8331\n",
      "Epoch 81/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.4108 - accuracy: 0.8347\n",
      "Epoch 82/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8217\n",
      "Epoch 83/1500\n",
      "41/41 [==============================] - 0s 815us/step - loss: 0.4166 - accuracy: 0.8163\n",
      "Epoch 84/1500\n",
      "41/41 [==============================] - 0s 794us/step - loss: 0.4133 - accuracy: 0.8228\n",
      "Epoch 85/1500\n",
      "41/41 [==============================] - 0s 773us/step - loss: 0.4202 - accuracy: 0.8263\n",
      "Epoch 86/1500\n",
      "41/41 [==============================] - 0s 778us/step - loss: 0.4096 - accuracy: 0.8286\n",
      "Epoch 87/1500\n",
      "41/41 [==============================] - 0s 769us/step - loss: 0.4118 - accuracy: 0.8209\n",
      "Epoch 88/1500\n",
      "41/41 [==============================] - 0s 813us/step - loss: 0.3947 - accuracy: 0.8339\n",
      "Epoch 89/1500\n",
      "41/41 [==============================] - 0s 768us/step - loss: 0.4089 - accuracy: 0.8289\n",
      "Epoch 90/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.4019 - accuracy: 0.8377\n",
      "Epoch 91/1500\n",
      "41/41 [==============================] - 0s 781us/step - loss: 0.3967 - accuracy: 0.8297\n",
      "Epoch 92/1500\n",
      "41/41 [==============================] - 0s 786us/step - loss: 0.4027 - accuracy: 0.8236\n",
      "Epoch 93/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.4048 - accuracy: 0.8286\n",
      "Epoch 94/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.3905 - accuracy: 0.8377\n",
      "Epoch 95/1500\n",
      "41/41 [==============================] - 0s 790us/step - loss: 0.4101 - accuracy: 0.8240\n",
      "Epoch 96/1500\n",
      "41/41 [==============================] - 0s 803us/step - loss: 0.3892 - accuracy: 0.8362\n",
      "Epoch 97/1500\n",
      "41/41 [==============================] - 0s 765us/step - loss: 0.3974 - accuracy: 0.8347\n",
      "Epoch 98/1500\n",
      "41/41 [==============================] - 0s 775us/step - loss: 0.3837 - accuracy: 0.8373\n",
      "Epoch 99/1500\n",
      "41/41 [==============================] - 0s 808us/step - loss: 0.3759 - accuracy: 0.8419\n",
      "Epoch 100/1500\n",
      "41/41 [==============================] - 0s 788us/step - loss: 0.3879 - accuracy: 0.8347\n",
      "Epoch 101/1500\n",
      "41/41 [==============================] - 0s 796us/step - loss: 0.3886 - accuracy: 0.8393\n",
      "Epoch 102/1500\n",
      "41/41 [==============================] - 0s 784us/step - loss: 0.3896 - accuracy: 0.8400\n",
      "Epoch 103/1500\n",
      "41/41 [==============================] - 0s 791us/step - loss: 0.3778 - accuracy: 0.8442\n",
      "Epoch 104/1500\n",
      "41/41 [==============================] - 0s 771us/step - loss: 0.3878 - accuracy: 0.8373\n",
      "Epoch 105/1500\n",
      "41/41 [==============================] - 0s 785us/step - loss: 0.3784 - accuracy: 0.8480\n",
      "Epoch 106/1500\n",
      "41/41 [==============================] - 0s 785us/step - loss: 0.3890 - accuracy: 0.8408\n",
      "Epoch 107/1500\n",
      "41/41 [==============================] - 0s 778us/step - loss: 0.3749 - accuracy: 0.8393\n",
      "Epoch 108/1500\n",
      "41/41 [==============================] - 0s 785us/step - loss: 0.3838 - accuracy: 0.8331\n",
      "Epoch 109/1500\n",
      "41/41 [==============================] - 0s 786us/step - loss: 0.3810 - accuracy: 0.8324\n",
      "Epoch 110/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.3688 - accuracy: 0.8503\n",
      "Epoch 111/1500\n",
      "41/41 [==============================] - 0s 808us/step - loss: 0.3673 - accuracy: 0.8461\n",
      "Epoch 112/1500\n",
      "41/41 [==============================] - 0s 787us/step - loss: 0.3778 - accuracy: 0.8435\n",
      "Epoch 113/1500\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.3630 - accuracy: 0.8561\n",
      "Epoch 114/1500\n",
      "41/41 [==============================] - 0s 773us/step - loss: 0.3811 - accuracy: 0.8419\n",
      "Epoch 115/1500\n",
      "41/41 [==============================] - 0s 763us/step - loss: 0.3698 - accuracy: 0.8465\n",
      "Epoch 116/1500\n",
      "41/41 [==============================] - 0s 804us/step - loss: 0.3627 - accuracy: 0.8519\n",
      "Epoch 117/1500\n",
      "41/41 [==============================] - 0s 791us/step - loss: 0.3676 - accuracy: 0.8484\n",
      "Epoch 118/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8469\n",
      "Epoch 119/1500\n",
      "41/41 [==============================] - 0s 800us/step - loss: 0.3682 - accuracy: 0.8519\n",
      "Epoch 120/1500\n",
      "41/41 [==============================] - 0s 808us/step - loss: 0.3601 - accuracy: 0.8499\n",
      "Epoch 121/1500\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.3573 - accuracy: 0.8480\n",
      "Epoch 122/1500\n",
      "41/41 [==============================] - 0s 804us/step - loss: 0.3583 - accuracy: 0.8492\n",
      "Epoch 123/1500\n",
      "41/41 [==============================] - 0s 791us/step - loss: 0.3620 - accuracy: 0.8465\n",
      "Epoch 124/1500\n",
      "41/41 [==============================] - 0s 828us/step - loss: 0.3464 - accuracy: 0.8614\n",
      "Epoch 125/1500\n",
      "41/41 [==============================] - 0s 794us/step - loss: 0.3574 - accuracy: 0.8496\n",
      "Epoch 126/1500\n",
      "41/41 [==============================] - 0s 786us/step - loss: 0.3652 - accuracy: 0.8557\n",
      "Epoch 127/1500\n",
      "41/41 [==============================] - 0s 795us/step - loss: 0.3464 - accuracy: 0.8580\n",
      "Epoch 128/1500\n",
      "41/41 [==============================] - 0s 754us/step - loss: 0.3639 - accuracy: 0.8526\n",
      "Epoch 129/1500\n",
      "41/41 [==============================] - 0s 808us/step - loss: 0.3468 - accuracy: 0.8545\n",
      "Epoch 130/1500\n",
      "41/41 [==============================] - 0s 794us/step - loss: 0.3583 - accuracy: 0.8553\n",
      "Epoch 131/1500\n",
      "41/41 [==============================] - 0s 789us/step - loss: 0.3585 - accuracy: 0.8484\n",
      "Epoch 132/1500\n",
      "41/41 [==============================] - 0s 769us/step - loss: 0.3547 - accuracy: 0.8435\n",
      "Epoch 133/1500\n",
      "41/41 [==============================] - 0s 791us/step - loss: 0.3601 - accuracy: 0.8526\n",
      "Epoch 134/1500\n",
      "41/41 [==============================] - 0s 817us/step - loss: 0.3578 - accuracy: 0.8450\n",
      "Epoch 135/1500\n",
      "41/41 [==============================] - 0s 797us/step - loss: 0.3469 - accuracy: 0.8541\n",
      "Epoch 136/1500\n",
      "41/41 [==============================] - 0s 789us/step - loss: 0.3484 - accuracy: 0.8545\n",
      "Epoch 137/1500\n",
      "41/41 [==============================] - 0s 803us/step - loss: 0.3408 - accuracy: 0.8530\n",
      "Epoch 138/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.3299 - accuracy: 0.8667\n",
      "Epoch 139/1500\n",
      "41/41 [==============================] - 0s 779us/step - loss: 0.3482 - accuracy: 0.8583\n",
      "Epoch 140/1500\n",
      "41/41 [==============================] - 0s 785us/step - loss: 0.3534 - accuracy: 0.8545\n",
      "Epoch 141/1500\n",
      "41/41 [==============================] - 0s 778us/step - loss: 0.3405 - accuracy: 0.8610\n",
      "Epoch 142/1500\n",
      "41/41 [==============================] - 0s 761us/step - loss: 0.3425 - accuracy: 0.8580\n",
      "Epoch 143/1500\n",
      "41/41 [==============================] - 0s 763us/step - loss: 0.3524 - accuracy: 0.8557\n",
      "Epoch 144/1500\n",
      "41/41 [==============================] - 0s 783us/step - loss: 0.3426 - accuracy: 0.8522\n",
      "Epoch 145/1500\n",
      "41/41 [==============================] - 0s 773us/step - loss: 0.3473 - accuracy: 0.8606\n",
      "Epoch 146/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.3233 - accuracy: 0.8690\n",
      "Epoch 147/1500\n",
      "41/41 [==============================] - 0s 791us/step - loss: 0.3487 - accuracy: 0.8492\n",
      "Epoch 148/1500\n",
      "41/41 [==============================] - 0s 806us/step - loss: 0.3287 - accuracy: 0.8694\n",
      "Epoch 149/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8652\n",
      "Epoch 150/1500\n",
      "41/41 [==============================] - 0s 814us/step - loss: 0.3300 - accuracy: 0.8648\n",
      "Epoch 151/1500\n",
      "41/41 [==============================] - 0s 785us/step - loss: 0.3232 - accuracy: 0.8618\n",
      "Epoch 152/1500\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.3490 - accuracy: 0.8576\n",
      "Epoch 153/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.3390 - accuracy: 0.8618\n",
      "Epoch 154/1500\n",
      "41/41 [==============================] - 0s 781us/step - loss: 0.3264 - accuracy: 0.8683\n",
      "Epoch 155/1500\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.3374 - accuracy: 0.8564\n",
      "Epoch 156/1500\n",
      "41/41 [==============================] - 0s 796us/step - loss: 0.3249 - accuracy: 0.8599\n",
      "Epoch 157/1500\n",
      "41/41 [==============================] - 0s 796us/step - loss: 0.3262 - accuracy: 0.8583\n",
      "Epoch 158/1500\n",
      "41/41 [==============================] - 0s 900us/step - loss: 0.3397 - accuracy: 0.8591\n",
      "Epoch 159/1500\n",
      "41/41 [==============================] - 0s 857us/step - loss: 0.3191 - accuracy: 0.8694\n",
      "Epoch 160/1500\n",
      "41/41 [==============================] - 0s 825us/step - loss: 0.3215 - accuracy: 0.8671\n",
      "Epoch 161/1500\n",
      "41/41 [==============================] - 0s 849us/step - loss: 0.3133 - accuracy: 0.8759\n",
      "Epoch 162/1500\n",
      "41/41 [==============================] - 0s 821us/step - loss: 0.3264 - accuracy: 0.8618\n",
      "Epoch 163/1500\n",
      "41/41 [==============================] - 0s 846us/step - loss: 0.3299 - accuracy: 0.8679\n",
      "Epoch 164/1500\n",
      "41/41 [==============================] - 0s 858us/step - loss: 0.3267 - accuracy: 0.8637\n",
      "Epoch 165/1500\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.3307 - accuracy: 0.8652\n",
      "Epoch 166/1500\n",
      "41/41 [==============================] - 0s 802us/step - loss: 0.3204 - accuracy: 0.8763\n",
      "Epoch 167/1500\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.3239 - accuracy: 0.8633\n",
      "Epoch 168/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.3262 - accuracy: 0.8641\n",
      "Epoch 169/1500\n",
      "41/41 [==============================] - 0s 839us/step - loss: 0.3262 - accuracy: 0.8664\n",
      "Epoch 170/1500\n",
      "41/41 [==============================] - 0s 821us/step - loss: 0.3221 - accuracy: 0.8667\n",
      "Epoch 171/1500\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.3210 - accuracy: 0.8679\n",
      "Epoch 172/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.3031 - accuracy: 0.8706\n",
      "Epoch 173/1500\n",
      "41/41 [==============================] - 0s 788us/step - loss: 0.3057 - accuracy: 0.8793\n",
      "Epoch 174/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.3170 - accuracy: 0.8729\n",
      "Epoch 175/1500\n",
      "41/41 [==============================] - 0s 872us/step - loss: 0.3092 - accuracy: 0.8736\n",
      "Epoch 176/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.8675\n",
      "Epoch 177/1500\n",
      "41/41 [==============================] - 0s 876us/step - loss: 0.3069 - accuracy: 0.8740\n",
      "Epoch 178/1500\n",
      "41/41 [==============================] - 0s 860us/step - loss: 0.3279 - accuracy: 0.8622\n",
      "Epoch 179/1500\n",
      "41/41 [==============================] - 0s 807us/step - loss: 0.3122 - accuracy: 0.8767\n",
      "Epoch 180/1500\n",
      "41/41 [==============================] - 0s 852us/step - loss: 0.3117 - accuracy: 0.8748\n",
      "Epoch 181/1500\n",
      "41/41 [==============================] - 0s 822us/step - loss: 0.3054 - accuracy: 0.8751\n",
      "Epoch 182/1500\n",
      "41/41 [==============================] - 0s 801us/step - loss: 0.3105 - accuracy: 0.8717\n",
      "Epoch 183/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.3017 - accuracy: 0.8767\n",
      "Epoch 184/1500\n",
      "41/41 [==============================] - 0s 800us/step - loss: 0.3046 - accuracy: 0.8767\n",
      "Epoch 185/1500\n",
      "41/41 [==============================] - 0s 831us/step - loss: 0.3069 - accuracy: 0.8805\n",
      "Epoch 186/1500\n",
      "41/41 [==============================] - 0s 815us/step - loss: 0.3025 - accuracy: 0.8709\n",
      "Epoch 187/1500\n",
      "41/41 [==============================] - 0s 866us/step - loss: 0.3090 - accuracy: 0.8702\n",
      "Epoch 188/1500\n",
      "41/41 [==============================] - 0s 831us/step - loss: 0.3223 - accuracy: 0.8667\n",
      "Epoch 189/1500\n",
      "41/41 [==============================] - 0s 853us/step - loss: 0.2939 - accuracy: 0.8771\n",
      "Epoch 190/1500\n",
      "41/41 [==============================] - 0s 842us/step - loss: 0.3056 - accuracy: 0.8809\n",
      "Epoch 191/1500\n",
      "41/41 [==============================] - 0s 774us/step - loss: 0.2987 - accuracy: 0.8767\n",
      "Epoch 192/1500\n",
      "41/41 [==============================] - 0s 806us/step - loss: 0.3010 - accuracy: 0.8797\n",
      "Epoch 193/1500\n",
      "41/41 [==============================] - 0s 864us/step - loss: 0.3058 - accuracy: 0.8713\n",
      "Epoch 194/1500\n",
      "41/41 [==============================] - 0s 824us/step - loss: 0.3041 - accuracy: 0.8797\n",
      "Epoch 195/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.2966 - accuracy: 0.8782\n",
      "Epoch 196/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2951 - accuracy: 0.8744\n",
      "Epoch 197/1500\n",
      "41/41 [==============================] - 0s 813us/step - loss: 0.2890 - accuracy: 0.8786\n",
      "Epoch 198/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2913 - accuracy: 0.8874\n",
      "Epoch 199/1500\n",
      "41/41 [==============================] - 0s 805us/step - loss: 0.2982 - accuracy: 0.8755\n",
      "Epoch 200/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2979 - accuracy: 0.8767\n",
      "Epoch 201/1500\n",
      "41/41 [==============================] - 0s 818us/step - loss: 0.2898 - accuracy: 0.8832\n",
      "Epoch 202/1500\n",
      "41/41 [==============================] - 0s 809us/step - loss: 0.2950 - accuracy: 0.8797\n",
      "Epoch 203/1500\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.2947 - accuracy: 0.8847\n",
      "Epoch 204/1500\n",
      "41/41 [==============================] - 0s 788us/step - loss: 0.2981 - accuracy: 0.8782\n",
      "Epoch 205/1500\n",
      "41/41 [==============================] - 0s 801us/step - loss: 0.2766 - accuracy: 0.8912\n",
      "Epoch 206/1500\n",
      "41/41 [==============================] - 0s 784us/step - loss: 0.2920 - accuracy: 0.8790\n",
      "Epoch 207/1500\n",
      "41/41 [==============================] - 0s 795us/step - loss: 0.2795 - accuracy: 0.8828\n",
      "Epoch 208/1500\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.3070 - accuracy: 0.8782\n",
      "Epoch 209/1500\n",
      "41/41 [==============================] - 0s 819us/step - loss: 0.3110 - accuracy: 0.8698\n",
      "Epoch 210/1500\n",
      "41/41 [==============================] - 0s 857us/step - loss: 0.2970 - accuracy: 0.8698\n",
      "Epoch 211/1500\n",
      "41/41 [==============================] - 0s 866us/step - loss: 0.2880 - accuracy: 0.8774\n",
      "Epoch 212/1500\n",
      "41/41 [==============================] - 0s 845us/step - loss: 0.2939 - accuracy: 0.8786\n",
      "Epoch 213/1500\n",
      "41/41 [==============================] - 0s 867us/step - loss: 0.2948 - accuracy: 0.8793\n",
      "Epoch 214/1500\n",
      "41/41 [==============================] - 0s 814us/step - loss: 0.2899 - accuracy: 0.8828\n",
      "Epoch 215/1500\n",
      "41/41 [==============================] - 0s 834us/step - loss: 0.2826 - accuracy: 0.8866\n",
      "Epoch 216/1500\n",
      "41/41 [==============================] - 0s 814us/step - loss: 0.2968 - accuracy: 0.8790\n",
      "Epoch 217/1500\n",
      "41/41 [==============================] - 0s 844us/step - loss: 0.2858 - accuracy: 0.8824\n",
      "Epoch 218/1500\n",
      "41/41 [==============================] - 0s 834us/step - loss: 0.2979 - accuracy: 0.8820\n",
      "Epoch 219/1500\n",
      "41/41 [==============================] - 0s 827us/step - loss: 0.2745 - accuracy: 0.8839\n",
      "Epoch 220/1500\n",
      "41/41 [==============================] - 0s 870us/step - loss: 0.2944 - accuracy: 0.8805\n",
      "Epoch 221/1500\n",
      "41/41 [==============================] - 0s 827us/step - loss: 0.2870 - accuracy: 0.8866\n",
      "Epoch 222/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8877\n",
      "Epoch 223/1500\n",
      "41/41 [==============================] - 0s 904us/step - loss: 0.2737 - accuracy: 0.8862\n",
      "Epoch 224/1500\n",
      "41/41 [==============================] - 0s 834us/step - loss: 0.2778 - accuracy: 0.8881\n",
      "Epoch 225/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.2728 - accuracy: 0.8897\n",
      "Epoch 226/1500\n",
      "41/41 [==============================] - 0s 795us/step - loss: 0.2887 - accuracy: 0.8839\n",
      "Epoch 227/1500\n",
      "41/41 [==============================] - 0s 819us/step - loss: 0.2679 - accuracy: 0.8931\n",
      "Epoch 228/1500\n",
      "41/41 [==============================] - 0s 757us/step - loss: 0.2726 - accuracy: 0.8981\n",
      "Epoch 229/1500\n",
      "41/41 [==============================] - 0s 786us/step - loss: 0.2757 - accuracy: 0.8862\n",
      "Epoch 230/1500\n",
      "41/41 [==============================] - 0s 822us/step - loss: 0.2774 - accuracy: 0.8793\n",
      "Epoch 231/1500\n",
      "41/41 [==============================] - 0s 804us/step - loss: 0.2729 - accuracy: 0.8877\n",
      "Epoch 232/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2917 - accuracy: 0.8767\n",
      "Epoch 233/1500\n",
      "41/41 [==============================] - 0s 829us/step - loss: 0.2693 - accuracy: 0.8950\n",
      "Epoch 234/1500\n",
      "41/41 [==============================] - 0s 847us/step - loss: 0.2649 - accuracy: 0.8897\n",
      "Epoch 235/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2876 - accuracy: 0.8820\n",
      "Epoch 236/1500\n",
      "41/41 [==============================] - 0s 843us/step - loss: 0.2668 - accuracy: 0.8904\n",
      "Epoch 237/1500\n",
      "41/41 [==============================] - 0s 835us/step - loss: 0.2748 - accuracy: 0.8939\n",
      "Epoch 238/1500\n",
      "41/41 [==============================] - 0s 932us/step - loss: 0.2557 - accuracy: 0.8984\n",
      "Epoch 239/1500\n",
      "41/41 [==============================] - 0s 867us/step - loss: 0.2701 - accuracy: 0.8908\n",
      "Epoch 240/1500\n",
      "41/41 [==============================] - 0s 824us/step - loss: 0.2655 - accuracy: 0.8908\n",
      "Epoch 241/1500\n",
      "41/41 [==============================] - 0s 830us/step - loss: 0.2640 - accuracy: 0.8961\n",
      "Epoch 242/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.8942\n",
      "Epoch 243/1500\n",
      "41/41 [==============================] - 0s 901us/step - loss: 0.2723 - accuracy: 0.8881\n",
      "Epoch 244/1500\n",
      "41/41 [==============================] - 0s 919us/step - loss: 0.2658 - accuracy: 0.8874\n",
      "Epoch 245/1500\n",
      "41/41 [==============================] - 0s 850us/step - loss: 0.2593 - accuracy: 0.8992\n",
      "Epoch 246/1500\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.2661 - accuracy: 0.8939\n",
      "Epoch 247/1500\n",
      "41/41 [==============================] - 0s 819us/step - loss: 0.2565 - accuracy: 0.8988\n",
      "Epoch 248/1500\n",
      "41/41 [==============================] - 0s 852us/step - loss: 0.2666 - accuracy: 0.8931\n",
      "Epoch 249/1500\n",
      "41/41 [==============================] - 0s 863us/step - loss: 0.2518 - accuracy: 0.9015\n",
      "Epoch 250/1500\n",
      "41/41 [==============================] - 0s 818us/step - loss: 0.2643 - accuracy: 0.8977\n",
      "Epoch 251/1500\n",
      "41/41 [==============================] - 0s 808us/step - loss: 0.2822 - accuracy: 0.8897\n",
      "Epoch 252/1500\n",
      "41/41 [==============================] - 0s 826us/step - loss: 0.2699 - accuracy: 0.8900\n",
      "Epoch 253/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2566 - accuracy: 0.8935\n",
      "Epoch 254/1500\n",
      "41/41 [==============================] - 0s 806us/step - loss: 0.2550 - accuracy: 0.8969\n",
      "Epoch 255/1500\n",
      "41/41 [==============================] - 0s 855us/step - loss: 0.2513 - accuracy: 0.9034\n",
      "Epoch 256/1500\n",
      "41/41 [==============================] - 0s 901us/step - loss: 0.2710 - accuracy: 0.8942\n",
      "Epoch 257/1500\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.2779 - accuracy: 0.8889\n",
      "Epoch 258/1500\n",
      "41/41 [==============================] - 0s 867us/step - loss: 0.2715 - accuracy: 0.8908\n",
      "Epoch 259/1500\n",
      "41/41 [==============================] - 0s 839us/step - loss: 0.2730 - accuracy: 0.8897\n",
      "Epoch 260/1500\n",
      "41/41 [==============================] - 0s 801us/step - loss: 0.2554 - accuracy: 0.8992\n",
      "Epoch 261/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8927\n",
      "Epoch 262/1500\n",
      "41/41 [==============================] - 0s 884us/step - loss: 0.2484 - accuracy: 0.8958\n",
      "Epoch 263/1500\n",
      "41/41 [==============================] - 0s 821us/step - loss: 0.2689 - accuracy: 0.8912\n",
      "Epoch 264/1500\n",
      "41/41 [==============================] - 0s 793us/step - loss: 0.2535 - accuracy: 0.8984\n",
      "Epoch 265/1500\n",
      "41/41 [==============================] - 0s 818us/step - loss: 0.2621 - accuracy: 0.8939\n",
      "Epoch 266/1500\n",
      "41/41 [==============================] - 0s 824us/step - loss: 0.2559 - accuracy: 0.8996\n",
      "Epoch 267/1500\n",
      "41/41 [==============================] - 0s 802us/step - loss: 0.2692 - accuracy: 0.8889\n",
      "Epoch 268/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2592 - accuracy: 0.8942\n",
      "Epoch 269/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2592 - accuracy: 0.8984\n",
      "Epoch 270/1500\n",
      "41/41 [==============================] - 0s 822us/step - loss: 0.2521 - accuracy: 0.9011\n",
      "Epoch 271/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.2433 - accuracy: 0.9049\n",
      "Epoch 272/1500\n",
      "41/41 [==============================] - 0s 797us/step - loss: 0.2479 - accuracy: 0.9030\n",
      "Epoch 273/1500\n",
      "41/41 [==============================] - 0s 827us/step - loss: 0.2596 - accuracy: 0.8942\n",
      "Epoch 274/1500\n",
      "41/41 [==============================] - 0s 835us/step - loss: 0.2535 - accuracy: 0.9038\n",
      "Epoch 275/1500\n",
      "41/41 [==============================] - 0s 821us/step - loss: 0.2566 - accuracy: 0.8946\n",
      "Epoch 276/1500\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.2616 - accuracy: 0.8958\n",
      "Epoch 277/1500\n",
      "41/41 [==============================] - 0s 861us/step - loss: 0.2308 - accuracy: 0.9042\n",
      "Epoch 278/1500\n",
      "41/41 [==============================] - 0s 837us/step - loss: 0.2479 - accuracy: 0.8969\n",
      "Epoch 279/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.9015\n",
      "Epoch 280/1500\n",
      "41/41 [==============================] - 0s 974us/step - loss: 0.2472 - accuracy: 0.8996\n",
      "Epoch 281/1500\n",
      "41/41 [==============================] - 0s 839us/step - loss: 0.2486 - accuracy: 0.9015\n",
      "Epoch 282/1500\n",
      "41/41 [==============================] - 0s 833us/step - loss: 0.2516 - accuracy: 0.8977\n",
      "Epoch 283/1500\n",
      "41/41 [==============================] - 0s 826us/step - loss: 0.2568 - accuracy: 0.8931\n",
      "Epoch 284/1500\n",
      "41/41 [==============================] - 0s 808us/step - loss: 0.2563 - accuracy: 0.8981\n",
      "Epoch 285/1500\n",
      "41/41 [==============================] - 0s 837us/step - loss: 0.2628 - accuracy: 0.8927\n",
      "Epoch 286/1500\n",
      "41/41 [==============================] - 0s 810us/step - loss: 0.2473 - accuracy: 0.9015\n",
      "Epoch 287/1500\n",
      "41/41 [==============================] - 0s 802us/step - loss: 0.2462 - accuracy: 0.8992\n",
      "Epoch 288/1500\n",
      "41/41 [==============================] - 0s 817us/step - loss: 0.2344 - accuracy: 0.9133\n",
      "Epoch 289/1500\n",
      "41/41 [==============================] - 0s 771us/step - loss: 0.2490 - accuracy: 0.9003\n",
      "Epoch 290/1500\n",
      "41/41 [==============================] - 0s 772us/step - loss: 0.2467 - accuracy: 0.9007\n",
      "Epoch 291/1500\n",
      "41/41 [==============================] - 0s 807us/step - loss: 0.2383 - accuracy: 0.9091\n",
      "Epoch 292/1500\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.2401 - accuracy: 0.9076\n",
      "Epoch 293/1500\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.2568 - accuracy: 0.8954\n",
      "Epoch 294/1500\n",
      "41/41 [==============================] - 0s 822us/step - loss: 0.2481 - accuracy: 0.9053\n",
      "Epoch 295/1500\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.2442 - accuracy: 0.9000\n",
      "Epoch 296/1500\n",
      "41/41 [==============================] - 0s 827us/step - loss: 0.2466 - accuracy: 0.8958\n",
      "Epoch 297/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2458 - accuracy: 0.9045\n",
      "Epoch 298/1500\n",
      "41/41 [==============================] - 0s 944us/step - loss: 0.2260 - accuracy: 0.9118\n",
      "Epoch 299/1500\n",
      "41/41 [==============================] - 0s 827us/step - loss: 0.2443 - accuracy: 0.9003\n",
      "Epoch 300/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.2393 - accuracy: 0.9011\n",
      "Epoch 301/1500\n",
      "41/41 [==============================] - 0s 815us/step - loss: 0.2470 - accuracy: 0.9030\n",
      "Epoch 302/1500\n",
      "41/41 [==============================] - 0s 813us/step - loss: 0.2338 - accuracy: 0.9030\n",
      "Epoch 303/1500\n",
      "41/41 [==============================] - 0s 820us/step - loss: 0.2535 - accuracy: 0.8942\n",
      "Epoch 304/1500\n",
      "41/41 [==============================] - 0s 820us/step - loss: 0.2405 - accuracy: 0.9011\n",
      "Epoch 305/1500\n",
      "41/41 [==============================] - 0s 851us/step - loss: 0.2406 - accuracy: 0.9007\n",
      "Epoch 306/1500\n",
      "41/41 [==============================] - 0s 809us/step - loss: 0.2343 - accuracy: 0.9026\n",
      "Epoch 307/1500\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.2389 - accuracy: 0.9045\n",
      "Epoch 308/1500\n",
      "41/41 [==============================] - 0s 826us/step - loss: 0.2363 - accuracy: 0.9095\n",
      "Epoch 309/1500\n",
      "41/41 [==============================] - 0s 846us/step - loss: 0.2232 - accuracy: 0.9087\n",
      "Epoch 310/1500\n",
      "41/41 [==============================] - 0s 826us/step - loss: 0.2314 - accuracy: 0.9030\n",
      "Epoch 311/1500\n",
      "41/41 [==============================] - 0s 842us/step - loss: 0.2463 - accuracy: 0.9007\n",
      "Epoch 312/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.2388 - accuracy: 0.9091\n",
      "Epoch 313/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2412 - accuracy: 0.9034\n",
      "Epoch 314/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.9068\n",
      "Epoch 315/1500\n",
      "41/41 [==============================] - 0s 806us/step - loss: 0.2265 - accuracy: 0.9065\n",
      "Epoch 316/1500\n",
      "41/41 [==============================] - 0s 809us/step - loss: 0.2349 - accuracy: 0.9076\n",
      "Epoch 317/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.2475 - accuracy: 0.8973\n",
      "Epoch 318/1500\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.2435 - accuracy: 0.9026\n",
      "Epoch 319/1500\n",
      "41/41 [==============================] - 0s 818us/step - loss: 0.2437 - accuracy: 0.9023\n",
      "Epoch 320/1500\n",
      "41/41 [==============================] - 0s 809us/step - loss: 0.2265 - accuracy: 0.9126\n",
      "Epoch 321/1500\n",
      "41/41 [==============================] - 0s 776us/step - loss: 0.2364 - accuracy: 0.9080\n",
      "Epoch 322/1500\n",
      "41/41 [==============================] - 0s 818us/step - loss: 0.2303 - accuracy: 0.9042\n",
      "Epoch 323/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.2452 - accuracy: 0.9038\n",
      "Epoch 324/1500\n",
      "41/41 [==============================] - 0s 779us/step - loss: 0.2234 - accuracy: 0.9168\n",
      "Epoch 325/1500\n",
      "41/41 [==============================] - 0s 806us/step - loss: 0.2261 - accuracy: 0.9061\n",
      "Epoch 326/1500\n",
      "41/41 [==============================] - 0s 810us/step - loss: 0.2354 - accuracy: 0.9042\n",
      "Epoch 327/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2230 - accuracy: 0.9095\n",
      "Epoch 328/1500\n",
      "41/41 [==============================] - 0s 851us/step - loss: 0.2268 - accuracy: 0.9114\n",
      "Epoch 329/1500\n",
      "41/41 [==============================] - 0s 830us/step - loss: 0.2293 - accuracy: 0.9084\n",
      "Epoch 330/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9126\n",
      "Epoch 331/1500\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.2326 - accuracy: 0.9057\n",
      "Epoch 332/1500\n",
      "41/41 [==============================] - 0s 871us/step - loss: 0.2210 - accuracy: 0.9126\n",
      "Epoch 333/1500\n",
      "41/41 [==============================] - 0s 889us/step - loss: 0.2366 - accuracy: 0.9080\n",
      "Epoch 334/1500\n",
      "41/41 [==============================] - 0s 900us/step - loss: 0.2339 - accuracy: 0.9080\n",
      "Epoch 335/1500\n",
      "41/41 [==============================] - 0s 907us/step - loss: 0.2168 - accuracy: 0.9122\n",
      "Epoch 336/1500\n",
      "41/41 [==============================] - 0s 838us/step - loss: 0.2220 - accuracy: 0.9145\n",
      "Epoch 337/1500\n",
      "41/41 [==============================] - 0s 884us/step - loss: 0.2301 - accuracy: 0.9141\n",
      "Epoch 338/1500\n",
      "41/41 [==============================] - 0s 906us/step - loss: 0.2145 - accuracy: 0.9122\n",
      "Epoch 339/1500\n",
      "41/41 [==============================] - 0s 824us/step - loss: 0.2214 - accuracy: 0.9152\n",
      "Epoch 340/1500\n",
      "41/41 [==============================] - 0s 842us/step - loss: 0.2254 - accuracy: 0.9110\n",
      "Epoch 341/1500\n",
      "41/41 [==============================] - 0s 815us/step - loss: 0.2149 - accuracy: 0.9160\n",
      "Epoch 342/1500\n",
      "41/41 [==============================] - 0s 803us/step - loss: 0.2248 - accuracy: 0.9103\n",
      "Epoch 343/1500\n",
      "41/41 [==============================] - 0s 821us/step - loss: 0.2196 - accuracy: 0.9129\n",
      "Epoch 344/1500\n",
      "41/41 [==============================] - 0s 830us/step - loss: 0.2273 - accuracy: 0.9118\n",
      "Epoch 345/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9149\n",
      "Epoch 346/1500\n",
      "41/41 [==============================] - 0s 936us/step - loss: 0.2152 - accuracy: 0.9160\n",
      "Epoch 347/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.2315 - accuracy: 0.9076\n",
      "Epoch 348/1500\n",
      "41/41 [==============================] - 0s 818us/step - loss: 0.2208 - accuracy: 0.9072\n",
      "Epoch 349/1500\n",
      "41/41 [==============================] - 0s 860us/step - loss: 0.2291 - accuracy: 0.9087\n",
      "Epoch 350/1500\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9164\n",
      "Epoch 351/1500\n",
      "41/41 [==============================] - 0s 915us/step - loss: 0.2154 - accuracy: 0.9168\n",
      "Epoch 352/1500\n",
      "41/41 [==============================] - 0s 937us/step - loss: 0.2189 - accuracy: 0.9168\n",
      "Epoch 353/1500\n",
      "41/41 [==============================] - 0s 926us/step - loss: 0.2377 - accuracy: 0.9087\n",
      "Epoch 354/1500\n",
      "41/41 [==============================] - 0s 942us/step - loss: 0.2161 - accuracy: 0.9152\n",
      "Epoch 355/1500\n",
      "41/41 [==============================] - 0s 937us/step - loss: 0.2151 - accuracy: 0.9175\n",
      "Epoch 356/1500\n",
      "41/41 [==============================] - 0s 907us/step - loss: 0.2313 - accuracy: 0.9065\n",
      "Epoch 357/1500\n",
      "41/41 [==============================] - 0s 919us/step - loss: 0.2203 - accuracy: 0.9133\n",
      "Epoch 358/1500\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2122 - accuracy: 0.9171\n",
      "Epoch 359/1500\n",
      "41/41 [==============================] - 0s 964us/step - loss: 0.2163 - accuracy: 0.9183\n",
      "Epoch 360/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9076\n",
      "Epoch 361/1500\n",
      "41/41 [==============================] - 0s 875us/step - loss: 0.2170 - accuracy: 0.9114\n",
      "Epoch 362/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.2174 - accuracy: 0.9149\n",
      "Epoch 363/1500\n",
      "41/41 [==============================] - 0s 822us/step - loss: 0.2168 - accuracy: 0.9099\n",
      "Epoch 364/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2289 - accuracy: 0.9114\n",
      "Epoch 365/1500\n",
      "41/41 [==============================] - 0s 838us/step - loss: 0.2171 - accuracy: 0.9141\n",
      "Epoch 366/1500\n",
      "41/41 [==============================] - 0s 859us/step - loss: 0.2153 - accuracy: 0.9118\n",
      "Epoch 367/1500\n",
      "41/41 [==============================] - 0s 847us/step - loss: 0.2095 - accuracy: 0.9149\n",
      "Epoch 368/1500\n",
      "41/41 [==============================] - 0s 862us/step - loss: 0.2208 - accuracy: 0.9118\n",
      "Epoch 369/1500\n",
      "41/41 [==============================] - 0s 819us/step - loss: 0.2230 - accuracy: 0.9118\n",
      "Epoch 370/1500\n",
      "41/41 [==============================] - 0s 887us/step - loss: 0.2109 - accuracy: 0.9141\n",
      "Epoch 371/1500\n",
      "41/41 [==============================] - 0s 849us/step - loss: 0.2166 - accuracy: 0.9171\n",
      "Epoch 372/1500\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.2196 - accuracy: 0.9080\n",
      "Epoch 373/1500\n",
      "41/41 [==============================] - 0s 820us/step - loss: 0.2149 - accuracy: 0.9141\n",
      "Epoch 374/1500\n",
      "41/41 [==============================] - 0s 796us/step - loss: 0.2032 - accuracy: 0.9191\n",
      "Epoch 375/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9229\n",
      "Epoch 376/1500\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9179\n",
      "Epoch 377/1500\n",
      "41/41 [==============================] - 0s 858us/step - loss: 0.1965 - accuracy: 0.9202\n",
      "Epoch 378/1500\n",
      "41/41 [==============================] - 0s 813us/step - loss: 0.2187 - accuracy: 0.9225\n",
      "Epoch 379/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.2078 - accuracy: 0.9110\n",
      "Epoch 380/1500\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.2122 - accuracy: 0.9187\n",
      "Epoch 381/1500\n",
      "41/41 [==============================] - 0s 931us/step - loss: 0.2112 - accuracy: 0.9156\n",
      "Epoch 382/1500\n",
      "41/41 [==============================] - 0s 975us/step - loss: 0.2167 - accuracy: 0.9141\n",
      "Epoch 383/1500\n",
      "41/41 [==============================] - 0s 897us/step - loss: 0.2232 - accuracy: 0.9164\n",
      "Epoch 384/1500\n",
      "41/41 [==============================] - 0s 828us/step - loss: 0.2139 - accuracy: 0.9156\n",
      "Epoch 385/1500\n",
      "41/41 [==============================] - 0s 846us/step - loss: 0.2187 - accuracy: 0.9152\n",
      "Epoch 386/1500\n",
      "41/41 [==============================] - 0s 924us/step - loss: 0.2164 - accuracy: 0.9152\n",
      "Epoch 387/1500\n",
      "41/41 [==============================] - 0s 985us/step - loss: 0.1948 - accuracy: 0.9255\n",
      "Epoch 388/1500\n",
      "41/41 [==============================] - 0s 982us/step - loss: 0.2041 - accuracy: 0.9175\n",
      "Epoch 389/1500\n",
      "41/41 [==============================] - 0s 951us/step - loss: 0.2157 - accuracy: 0.9107\n",
      "Epoch 390/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9171\n",
      "Epoch 391/1500\n",
      "41/41 [==============================] - 0s 940us/step - loss: 0.2141 - accuracy: 0.9091\n",
      "Epoch 392/1500\n",
      "41/41 [==============================] - 0s 961us/step - loss: 0.2181 - accuracy: 0.9126\n",
      "Epoch 393/1500\n",
      "41/41 [==============================] - 0s 941us/step - loss: 0.2093 - accuracy: 0.9183\n",
      "Epoch 394/1500\n",
      "41/41 [==============================] - 0s 953us/step - loss: 0.1980 - accuracy: 0.9248\n",
      "Epoch 395/1500\n",
      "41/41 [==============================] - 0s 943us/step - loss: 0.2015 - accuracy: 0.9240\n",
      "Epoch 396/1500\n",
      "41/41 [==============================] - 0s 931us/step - loss: 0.2242 - accuracy: 0.9191\n",
      "Epoch 397/1500\n",
      "41/41 [==============================] - 0s 951us/step - loss: 0.2001 - accuracy: 0.9175\n",
      "Epoch 398/1500\n",
      "41/41 [==============================] - 0s 949us/step - loss: 0.2044 - accuracy: 0.9183\n",
      "Epoch 399/1500\n",
      "41/41 [==============================] - 0s 892us/step - loss: 0.2012 - accuracy: 0.9183\n",
      "Epoch 400/1500\n",
      "41/41 [==============================] - 0s 907us/step - loss: 0.2039 - accuracy: 0.9156\n",
      "Epoch 401/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.2129 - accuracy: 0.9099\n",
      "Epoch 402/1500\n",
      "41/41 [==============================] - 0s 850us/step - loss: 0.2120 - accuracy: 0.9183\n",
      "Epoch 403/1500\n",
      "41/41 [==============================] - 0s 826us/step - loss: 0.1975 - accuracy: 0.9210\n",
      "Epoch 404/1500\n",
      "41/41 [==============================] - 0s 939us/step - loss: 0.1997 - accuracy: 0.9187\n",
      "Epoch 405/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9175\n",
      "Epoch 406/1500\n",
      "41/41 [==============================] - 0s 981us/step - loss: 0.2011 - accuracy: 0.9236\n",
      "Epoch 407/1500\n",
      "41/41 [==============================] - 0s 872us/step - loss: 0.2184 - accuracy: 0.9164\n",
      "Epoch 408/1500\n",
      "41/41 [==============================] - 0s 940us/step - loss: 0.1885 - accuracy: 0.9255\n",
      "Epoch 409/1500\n",
      "41/41 [==============================] - 0s 834us/step - loss: 0.2034 - accuracy: 0.9233\n",
      "Epoch 410/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.1922 - accuracy: 0.9267\n",
      "Epoch 411/1500\n",
      "41/41 [==============================] - 0s 825us/step - loss: 0.1986 - accuracy: 0.9229\n",
      "Epoch 412/1500\n",
      "41/41 [==============================] - 0s 825us/step - loss: 0.2163 - accuracy: 0.9114\n",
      "Epoch 413/1500\n",
      "41/41 [==============================] - 0s 842us/step - loss: 0.1962 - accuracy: 0.9179\n",
      "Epoch 414/1500\n",
      "41/41 [==============================] - 0s 850us/step - loss: 0.2064 - accuracy: 0.9171\n",
      "Epoch 415/1500\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9221\n",
      "Epoch 416/1500\n",
      "41/41 [==============================] - 0s 839us/step - loss: 0.1974 - accuracy: 0.9255\n",
      "Epoch 417/1500\n",
      "41/41 [==============================] - 0s 802us/step - loss: 0.2066 - accuracy: 0.9175\n",
      "Epoch 418/1500\n",
      "41/41 [==============================] - 0s 803us/step - loss: 0.2131 - accuracy: 0.9194\n",
      "Epoch 419/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9175\n",
      "Epoch 420/1500\n",
      "41/41 [==============================] - 0s 825us/step - loss: 0.2036 - accuracy: 0.9187\n",
      "Epoch 421/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.1906 - accuracy: 0.9259\n",
      "Epoch 422/1500\n",
      "41/41 [==============================] - 0s 832us/step - loss: 0.1829 - accuracy: 0.9198\n",
      "Epoch 423/1500\n",
      "41/41 [==============================] - 0s 813us/step - loss: 0.2234 - accuracy: 0.9118\n",
      "Epoch 424/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.2067 - accuracy: 0.9171\n",
      "Epoch 425/1500\n",
      "41/41 [==============================] - 0s 790us/step - loss: 0.1914 - accuracy: 0.9248\n",
      "Epoch 426/1500\n",
      "41/41 [==============================] - 0s 800us/step - loss: 0.1799 - accuracy: 0.9271\n",
      "Epoch 427/1500\n",
      "41/41 [==============================] - 0s 807us/step - loss: 0.2090 - accuracy: 0.9171\n",
      "Epoch 428/1500\n",
      "41/41 [==============================] - 0s 830us/step - loss: 0.1834 - accuracy: 0.9317\n",
      "Epoch 429/1500\n",
      "41/41 [==============================] - 0s 829us/step - loss: 0.1996 - accuracy: 0.9210\n",
      "Epoch 430/1500\n",
      "41/41 [==============================] - 0s 818us/step - loss: 0.1996 - accuracy: 0.9236\n",
      "Epoch 431/1500\n",
      "41/41 [==============================] - 0s 816us/step - loss: 0.1844 - accuracy: 0.9271\n",
      "Epoch 432/1500\n",
      "41/41 [==============================] - 0s 784us/step - loss: 0.1977 - accuracy: 0.9202\n",
      "Epoch 433/1500\n",
      "41/41 [==============================] - 0s 809us/step - loss: 0.2085 - accuracy: 0.9164\n",
      "Epoch 434/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.9252\n",
      "Epoch 435/1500\n",
      "41/41 [==============================] - 0s 940us/step - loss: 0.2011 - accuracy: 0.9210\n",
      "Epoch 436/1500\n",
      "41/41 [==============================] - 0s 825us/step - loss: 0.1969 - accuracy: 0.9233\n",
      "Epoch 437/1500\n",
      "41/41 [==============================] - 0s 812us/step - loss: 0.2014 - accuracy: 0.9198\n",
      "Epoch 438/1500\n",
      "41/41 [==============================] - 0s 856us/step - loss: 0.2078 - accuracy: 0.9183\n",
      "Epoch 439/1500\n",
      "41/41 [==============================] - 0s 775us/step - loss: 0.2088 - accuracy: 0.9175\n",
      "Epoch 440/1500\n",
      "41/41 [==============================] - 0s 803us/step - loss: 0.1974 - accuracy: 0.9206\n",
      "Epoch 441/1500\n",
      "41/41 [==============================] - 0s 804us/step - loss: 0.2001 - accuracy: 0.9233\n",
      "Epoch 442/1500\n",
      "41/41 [==============================] - 0s 778us/step - loss: 0.1886 - accuracy: 0.9286\n",
      "Epoch 443/1500\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.1878 - accuracy: 0.9271\n",
      "Epoch 444/1500\n",
      "41/41 [==============================] - 0s 838us/step - loss: 0.2007 - accuracy: 0.9171\n",
      "Epoch 445/1500\n",
      "41/41 [==============================] - 0s 815us/step - loss: 0.1865 - accuracy: 0.9233\n",
      "Epoch 446/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.2050 - accuracy: 0.9149\n",
      "Epoch 447/1500\n",
      "41/41 [==============================] - 0s 822us/step - loss: 0.1815 - accuracy: 0.9309\n",
      "Epoch 448/1500\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1844 - accuracy: 0.9252\n",
      "Epoch 449/1500\n",
      "41/41 [==============================] - 0s 825us/step - loss: 0.1938 - accuracy: 0.9252\n",
      "Epoch 450/1500\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.1905 - accuracy: 0.9252\n",
      "Epoch 451/1500\n",
      "41/41 [==============================] - 0s 822us/step - loss: 0.1875 - accuracy: 0.9259\n",
      "Epoch 452/1500\n",
      "41/41 [==============================] - 0s 846us/step - loss: 0.1997 - accuracy: 0.9267\n",
      "Epoch 453/1500\n",
      "41/41 [==============================] - 0s 840us/step - loss: 0.1947 - accuracy: 0.9252\n",
      "Epoch 454/1500\n",
      "41/41 [==============================] - 0s 799us/step - loss: 0.1985 - accuracy: 0.9236\n",
      "Epoch 455/1500\n",
      "41/41 [==============================] - 0s 835us/step - loss: 0.2085 - accuracy: 0.9171\n",
      "Epoch 456/1500\n",
      " 1/41 [..............................] - ETA: 0s - loss: 0.1857 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 426.\n",
      "41/41 [==============================] - 0s 929us/step - loss: 0.1860 - accuracy: 0.9286\n",
      "Epoch 456: early stopping\n",
      "10/10 [==============================] - 0s 750us/step - loss: 0.6327 - accuracy: 0.7508\n",
      "10/10 [==============================] - 0s 587us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 382, Predictions: 382, Actuals: 382, Gender: 382\n",
      "After appending - Cat IDs: 687, Predictions: 687, Actuals: 687, Gender: 687\n",
      "Final Test Results - Loss: 0.6326549053192139, Accuracy: 0.7508196830749512, Precision: 0.7489583333333334, Recall: 0.7504436892287359, F1 Score: 0.7428629600543643\n",
      "Confusion Matrix:\n",
      " [[165   0  49]\n",
      " [  3  33   0]\n",
      " [ 24   0  31]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 84, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'092A'}\n",
      "Moved to Test Set:\n",
      "{'092A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '073A' '074A' '075A' '076A' '088A' '090A' '091A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '038A'\n",
      " '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '087A' '092A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "787\n",
      "Length of y_train_val:\n",
      "787\n",
      "Length of groups_train_val:\n",
      "787\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     506\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     82\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1236, 2: 1109, 1: 812})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "50/50 [==============================] - 0s 902us/step - loss: 1.2599 - accuracy: 0.4599\n",
      "Epoch 2/1500\n",
      "50/50 [==============================] - 0s 860us/step - loss: 0.9918 - accuracy: 0.5727\n",
      "Epoch 3/1500\n",
      "50/50 [==============================] - 0s 777us/step - loss: 0.8884 - accuracy: 0.6161\n",
      "Epoch 4/1500\n",
      "50/50 [==============================] - 0s 835us/step - loss: 0.8527 - accuracy: 0.6417\n",
      "Epoch 5/1500\n",
      "50/50 [==============================] - 0s 764us/step - loss: 0.7990 - accuracy: 0.6646\n",
      "Epoch 6/1500\n",
      "50/50 [==============================] - 0s 785us/step - loss: 0.7975 - accuracy: 0.6658\n",
      "Epoch 7/1500\n",
      "50/50 [==============================] - 0s 802us/step - loss: 0.7589 - accuracy: 0.6810\n",
      "Epoch 8/1500\n",
      "50/50 [==============================] - 0s 824us/step - loss: 0.7212 - accuracy: 0.6896\n",
      "Epoch 9/1500\n",
      "50/50 [==============================] - 0s 806us/step - loss: 0.7244 - accuracy: 0.6908\n",
      "Epoch 10/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.7107 - accuracy: 0.6988\n",
      "Epoch 11/1500\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.7019\n",
      "Epoch 12/1500\n",
      "50/50 [==============================] - 0s 825us/step - loss: 0.6785 - accuracy: 0.7076\n",
      "Epoch 13/1500\n",
      "50/50 [==============================] - 0s 768us/step - loss: 0.6459 - accuracy: 0.7241\n",
      "Epoch 14/1500\n",
      "50/50 [==============================] - 0s 898us/step - loss: 0.6240 - accuracy: 0.7418\n",
      "Epoch 15/1500\n",
      "50/50 [==============================] - 0s 868us/step - loss: 0.6261 - accuracy: 0.7298\n",
      "Epoch 16/1500\n",
      "50/50 [==============================] - 0s 912us/step - loss: 0.6063 - accuracy: 0.7409\n",
      "Epoch 17/1500\n",
      "50/50 [==============================] - 0s 881us/step - loss: 0.6265 - accuracy: 0.7241\n",
      "Epoch 18/1500\n",
      "50/50 [==============================] - 0s 869us/step - loss: 0.5854 - accuracy: 0.7501\n",
      "Epoch 19/1500\n",
      "50/50 [==============================] - 0s 796us/step - loss: 0.5890 - accuracy: 0.7460\n",
      "Epoch 20/1500\n",
      "50/50 [==============================] - 0s 822us/step - loss: 0.5771 - accuracy: 0.7555\n",
      "Epoch 21/1500\n",
      "50/50 [==============================] - 0s 714us/step - loss: 0.5826 - accuracy: 0.7456\n",
      "Epoch 22/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.5651 - accuracy: 0.7536\n",
      "Epoch 23/1500\n",
      "50/50 [==============================] - 0s 759us/step - loss: 0.5427 - accuracy: 0.7688\n",
      "Epoch 24/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7605\n",
      "Epoch 25/1500\n",
      "50/50 [==============================] - 0s 730us/step - loss: 0.5421 - accuracy: 0.7694\n",
      "Epoch 26/1500\n",
      "50/50 [==============================] - 0s 871us/step - loss: 0.5452 - accuracy: 0.7643\n",
      "Epoch 27/1500\n",
      "50/50 [==============================] - 0s 837us/step - loss: 0.5313 - accuracy: 0.7675\n",
      "Epoch 28/1500\n",
      "50/50 [==============================] - 0s 801us/step - loss: 0.5340 - accuracy: 0.7681\n",
      "Epoch 29/1500\n",
      "50/50 [==============================] - 0s 771us/step - loss: 0.5218 - accuracy: 0.7776\n",
      "Epoch 30/1500\n",
      "50/50 [==============================] - 0s 792us/step - loss: 0.5236 - accuracy: 0.7780\n",
      "Epoch 31/1500\n",
      "50/50 [==============================] - 0s 765us/step - loss: 0.5343 - accuracy: 0.7738\n",
      "Epoch 32/1500\n",
      "50/50 [==============================] - 0s 813us/step - loss: 0.5154 - accuracy: 0.7723\n",
      "Epoch 33/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.5144 - accuracy: 0.7865\n",
      "Epoch 34/1500\n",
      "50/50 [==============================] - 0s 789us/step - loss: 0.5137 - accuracy: 0.7745\n",
      "Epoch 35/1500\n",
      "50/50 [==============================] - 0s 881us/step - loss: 0.4873 - accuracy: 0.7897\n",
      "Epoch 36/1500\n",
      "50/50 [==============================] - 0s 838us/step - loss: 0.5120 - accuracy: 0.7795\n",
      "Epoch 37/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.4957 - accuracy: 0.7767\n",
      "Epoch 38/1500\n",
      "50/50 [==============================] - 0s 785us/step - loss: 0.5022 - accuracy: 0.7871\n",
      "Epoch 39/1500\n",
      "50/50 [==============================] - 0s 801us/step - loss: 0.4818 - accuracy: 0.7985\n",
      "Epoch 40/1500\n",
      "50/50 [==============================] - 0s 793us/step - loss: 0.4885 - accuracy: 0.7881\n",
      "Epoch 41/1500\n",
      "50/50 [==============================] - 0s 762us/step - loss: 0.4708 - accuracy: 0.7906\n",
      "Epoch 42/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.4795 - accuracy: 0.7976\n",
      "Epoch 43/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.4985 - accuracy: 0.7840\n",
      "Epoch 44/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.4857 - accuracy: 0.7916\n",
      "Epoch 45/1500\n",
      "50/50 [==============================] - 0s 730us/step - loss: 0.4617 - accuracy: 0.8030\n",
      "Epoch 46/1500\n",
      "50/50 [==============================] - 0s 728us/step - loss: 0.4627 - accuracy: 0.7979\n",
      "Epoch 47/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.4656 - accuracy: 0.7992\n",
      "Epoch 48/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.4485 - accuracy: 0.8090\n",
      "Epoch 49/1500\n",
      "50/50 [==============================] - 0s 717us/step - loss: 0.4501 - accuracy: 0.8080\n",
      "Epoch 50/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.4482 - accuracy: 0.8065\n",
      "Epoch 51/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.4479 - accuracy: 0.8017\n",
      "Epoch 52/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.4443 - accuracy: 0.8090\n",
      "Epoch 53/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.4435 - accuracy: 0.8071\n",
      "Epoch 54/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.4445 - accuracy: 0.8179\n",
      "Epoch 55/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.4408 - accuracy: 0.8128\n",
      "Epoch 56/1500\n",
      "50/50 [==============================] - 0s 725us/step - loss: 0.4418 - accuracy: 0.8042\n",
      "Epoch 57/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.4216 - accuracy: 0.8194\n",
      "Epoch 58/1500\n",
      "50/50 [==============================] - 0s 731us/step - loss: 0.4340 - accuracy: 0.8118\n",
      "Epoch 59/1500\n",
      "50/50 [==============================] - 0s 775us/step - loss: 0.4342 - accuracy: 0.8226\n",
      "Epoch 60/1500\n",
      "50/50 [==============================] - 0s 761us/step - loss: 0.4299 - accuracy: 0.8169\n",
      "Epoch 61/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.4322 - accuracy: 0.8210\n",
      "Epoch 62/1500\n",
      "50/50 [==============================] - 0s 727us/step - loss: 0.4210 - accuracy: 0.8194\n",
      "Epoch 63/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.4183 - accuracy: 0.8245\n",
      "Epoch 64/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.4172 - accuracy: 0.8198\n",
      "Epoch 65/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.4246 - accuracy: 0.8255\n",
      "Epoch 66/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.4084 - accuracy: 0.8328\n",
      "Epoch 67/1500\n",
      "50/50 [==============================] - 0s 738us/step - loss: 0.4232 - accuracy: 0.8144\n",
      "Epoch 68/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.4115 - accuracy: 0.8217\n",
      "Epoch 69/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.4003 - accuracy: 0.8375\n",
      "Epoch 70/1500\n",
      "50/50 [==============================] - 0s 760us/step - loss: 0.3940 - accuracy: 0.8350\n",
      "Epoch 71/1500\n",
      "50/50 [==============================] - 0s 770us/step - loss: 0.3988 - accuracy: 0.8369\n",
      "Epoch 72/1500\n",
      "50/50 [==============================] - 0s 756us/step - loss: 0.4190 - accuracy: 0.8188\n",
      "Epoch 73/1500\n",
      "50/50 [==============================] - 0s 786us/step - loss: 0.4046 - accuracy: 0.8267\n",
      "Epoch 74/1500\n",
      "50/50 [==============================] - 0s 815us/step - loss: 0.4065 - accuracy: 0.8296\n",
      "Epoch 75/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3984 - accuracy: 0.8347\n",
      "Epoch 76/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.4055 - accuracy: 0.8264\n",
      "Epoch 77/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3962 - accuracy: 0.8296\n",
      "Epoch 78/1500\n",
      "50/50 [==============================] - 0s 818us/step - loss: 0.4030 - accuracy: 0.8394\n",
      "Epoch 79/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3954 - accuracy: 0.8343\n",
      "Epoch 80/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.3715 - accuracy: 0.8451\n",
      "Epoch 81/1500\n",
      "50/50 [==============================] - 0s 734us/step - loss: 0.3991 - accuracy: 0.8312\n",
      "Epoch 82/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.4039 - accuracy: 0.8302\n",
      "Epoch 83/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.3809 - accuracy: 0.8438\n",
      "Epoch 84/1500\n",
      "50/50 [==============================] - 0s 716us/step - loss: 0.3855 - accuracy: 0.8343\n",
      "Epoch 85/1500\n",
      "50/50 [==============================] - 0s 759us/step - loss: 0.3789 - accuracy: 0.8359\n",
      "Epoch 86/1500\n",
      "50/50 [==============================] - 0s 735us/step - loss: 0.3881 - accuracy: 0.8350\n",
      "Epoch 87/1500\n",
      "50/50 [==============================] - 0s 862us/step - loss: 0.3695 - accuracy: 0.8483\n",
      "Epoch 88/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3706 - accuracy: 0.8435\n",
      "Epoch 89/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3719 - accuracy: 0.8362\n",
      "Epoch 90/1500\n",
      "50/50 [==============================] - 0s 759us/step - loss: 0.3680 - accuracy: 0.8492\n",
      "Epoch 91/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.3729 - accuracy: 0.8442\n",
      "Epoch 92/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.3772 - accuracy: 0.8426\n",
      "Epoch 93/1500\n",
      "50/50 [==============================] - 0s 759us/step - loss: 0.3665 - accuracy: 0.8416\n",
      "Epoch 94/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.3760 - accuracy: 0.8419\n",
      "Epoch 95/1500\n",
      "50/50 [==============================] - 0s 731us/step - loss: 0.3669 - accuracy: 0.8426\n",
      "Epoch 96/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.3630 - accuracy: 0.8480\n",
      "Epoch 97/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.3717 - accuracy: 0.8470\n",
      "Epoch 98/1500\n",
      "50/50 [==============================] - 0s 772us/step - loss: 0.3482 - accuracy: 0.8603\n",
      "Epoch 99/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.3470 - accuracy: 0.8575\n",
      "Epoch 100/1500\n",
      "50/50 [==============================] - 0s 724us/step - loss: 0.3641 - accuracy: 0.8438\n",
      "Epoch 101/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3626 - accuracy: 0.8457\n",
      "Epoch 102/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.3610 - accuracy: 0.8486\n",
      "Epoch 103/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.3691 - accuracy: 0.8476\n",
      "Epoch 104/1500\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3597 - accuracy: 0.8438\n",
      "Epoch 105/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3535 - accuracy: 0.8502\n",
      "Epoch 106/1500\n",
      "50/50 [==============================] - 0s 723us/step - loss: 0.3514 - accuracy: 0.8597\n",
      "Epoch 107/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3612 - accuracy: 0.8461\n",
      "Epoch 108/1500\n",
      "50/50 [==============================] - 0s 722us/step - loss: 0.3593 - accuracy: 0.8473\n",
      "Epoch 109/1500\n",
      "50/50 [==============================] - 0s 735us/step - loss: 0.3494 - accuracy: 0.8587\n",
      "Epoch 110/1500\n",
      "50/50 [==============================] - 0s 760us/step - loss: 0.3366 - accuracy: 0.8571\n",
      "Epoch 111/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.3463 - accuracy: 0.8568\n",
      "Epoch 112/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3482 - accuracy: 0.8590\n",
      "Epoch 113/1500\n",
      "50/50 [==============================] - 0s 733us/step - loss: 0.3471 - accuracy: 0.8565\n",
      "Epoch 114/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3389 - accuracy: 0.8676\n",
      "Epoch 115/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.3302 - accuracy: 0.8625\n",
      "Epoch 116/1500\n",
      "50/50 [==============================] - 0s 779us/step - loss: 0.3396 - accuracy: 0.8530\n",
      "Epoch 117/1500\n",
      "50/50 [==============================] - 0s 726us/step - loss: 0.3389 - accuracy: 0.8644\n",
      "Epoch 118/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.3343 - accuracy: 0.8632\n",
      "Epoch 119/1500\n",
      "50/50 [==============================] - 0s 733us/step - loss: 0.3382 - accuracy: 0.8635\n",
      "Epoch 120/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.3341 - accuracy: 0.8644\n",
      "Epoch 121/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.3269 - accuracy: 0.8742\n",
      "Epoch 122/1500\n",
      "50/50 [==============================] - 0s 734us/step - loss: 0.3347 - accuracy: 0.8685\n",
      "Epoch 123/1500\n",
      "50/50 [==============================] - 0s 728us/step - loss: 0.3220 - accuracy: 0.8651\n",
      "Epoch 124/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3266 - accuracy: 0.8625\n",
      "Epoch 125/1500\n",
      "50/50 [==============================] - 0s 712us/step - loss: 0.3353 - accuracy: 0.8628\n",
      "Epoch 126/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.3271 - accuracy: 0.8685\n",
      "Epoch 127/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3456 - accuracy: 0.8584\n",
      "Epoch 128/1500\n",
      "50/50 [==============================] - 0s 756us/step - loss: 0.3383 - accuracy: 0.8590\n",
      "Epoch 129/1500\n",
      "50/50 [==============================] - 0s 719us/step - loss: 0.3216 - accuracy: 0.8692\n",
      "Epoch 130/1500\n",
      "50/50 [==============================] - 0s 726us/step - loss: 0.3259 - accuracy: 0.8657\n",
      "Epoch 131/1500\n",
      "50/50 [==============================] - 0s 727us/step - loss: 0.3301 - accuracy: 0.8616\n",
      "Epoch 132/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.3216 - accuracy: 0.8692\n",
      "Epoch 133/1500\n",
      "50/50 [==============================] - 0s 719us/step - loss: 0.3193 - accuracy: 0.8714\n",
      "Epoch 134/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3382 - accuracy: 0.8600\n",
      "Epoch 135/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.3318 - accuracy: 0.8632\n",
      "Epoch 136/1500\n",
      "50/50 [==============================] - 0s 732us/step - loss: 0.3218 - accuracy: 0.8657\n",
      "Epoch 137/1500\n",
      "50/50 [==============================] - 0s 747us/step - loss: 0.3200 - accuracy: 0.8676\n",
      "Epoch 138/1500\n",
      "50/50 [==============================] - 0s 731us/step - loss: 0.3257 - accuracy: 0.8641\n",
      "Epoch 139/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.3136 - accuracy: 0.8717\n",
      "Epoch 140/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.3065 - accuracy: 0.8730\n",
      "Epoch 141/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.3208 - accuracy: 0.8717\n",
      "Epoch 142/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.3132 - accuracy: 0.8736\n",
      "Epoch 143/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3027 - accuracy: 0.8771\n",
      "Epoch 144/1500\n",
      "50/50 [==============================] - 0s 730us/step - loss: 0.3287 - accuracy: 0.8673\n",
      "Epoch 145/1500\n",
      "50/50 [==============================] - 0s 738us/step - loss: 0.3021 - accuracy: 0.8796\n",
      "Epoch 146/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.3143 - accuracy: 0.8752\n",
      "Epoch 147/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3062 - accuracy: 0.8758\n",
      "Epoch 148/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.3065 - accuracy: 0.8774\n",
      "Epoch 149/1500\n",
      "50/50 [==============================] - 0s 725us/step - loss: 0.3270 - accuracy: 0.8635\n",
      "Epoch 150/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.3002 - accuracy: 0.8853\n",
      "Epoch 151/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8809\n",
      "Epoch 152/1500\n",
      "50/50 [==============================] - 0s 730us/step - loss: 0.2955 - accuracy: 0.8752\n",
      "Epoch 153/1500\n",
      "50/50 [==============================] - 0s 727us/step - loss: 0.3168 - accuracy: 0.8730\n",
      "Epoch 154/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.3080 - accuracy: 0.8733\n",
      "Epoch 155/1500\n",
      "50/50 [==============================] - 0s 738us/step - loss: 0.2926 - accuracy: 0.8838\n",
      "Epoch 156/1500\n",
      "50/50 [==============================] - 0s 733us/step - loss: 0.3018 - accuracy: 0.8682\n",
      "Epoch 157/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.3004 - accuracy: 0.8739\n",
      "Epoch 158/1500\n",
      "50/50 [==============================] - 0s 727us/step - loss: 0.3040 - accuracy: 0.8777\n",
      "Epoch 159/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.2982 - accuracy: 0.8780\n",
      "Epoch 160/1500\n",
      "50/50 [==============================] - 0s 765us/step - loss: 0.2888 - accuracy: 0.8857\n",
      "Epoch 161/1500\n",
      "50/50 [==============================] - 0s 734us/step - loss: 0.2982 - accuracy: 0.8777\n",
      "Epoch 162/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.3089 - accuracy: 0.8771\n",
      "Epoch 163/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.2947 - accuracy: 0.8825\n",
      "Epoch 164/1500\n",
      "50/50 [==============================] - 0s 718us/step - loss: 0.2928 - accuracy: 0.8784\n",
      "Epoch 165/1500\n",
      "50/50 [==============================] - 0s 724us/step - loss: 0.3030 - accuracy: 0.8717\n",
      "Epoch 166/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.3025 - accuracy: 0.8752\n",
      "Epoch 167/1500\n",
      "50/50 [==============================] - 0s 776us/step - loss: 0.2959 - accuracy: 0.8796\n",
      "Epoch 168/1500\n",
      "50/50 [==============================] - 0s 734us/step - loss: 0.2967 - accuracy: 0.8803\n",
      "Epoch 169/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2925 - accuracy: 0.8831\n",
      "Epoch 170/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2864 - accuracy: 0.8844\n",
      "Epoch 171/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.2869 - accuracy: 0.8847\n",
      "Epoch 172/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.2949 - accuracy: 0.8796\n",
      "Epoch 173/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.2983 - accuracy: 0.8825\n",
      "Epoch 174/1500\n",
      "50/50 [==============================] - 0s 732us/step - loss: 0.2873 - accuracy: 0.8860\n",
      "Epoch 175/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2858 - accuracy: 0.8784\n",
      "Epoch 176/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2873 - accuracy: 0.8825\n",
      "Epoch 177/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2962 - accuracy: 0.8809\n",
      "Epoch 178/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2838 - accuracy: 0.8822\n",
      "Epoch 179/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.2840 - accuracy: 0.8847\n",
      "Epoch 180/1500\n",
      "50/50 [==============================] - 0s 731us/step - loss: 0.2877 - accuracy: 0.8818\n",
      "Epoch 181/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2845 - accuracy: 0.8860\n",
      "Epoch 182/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2812 - accuracy: 0.8885\n",
      "Epoch 183/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2809 - accuracy: 0.8901\n",
      "Epoch 184/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.2704 - accuracy: 0.8850\n",
      "Epoch 185/1500\n",
      "50/50 [==============================] - 0s 720us/step - loss: 0.2785 - accuracy: 0.8866\n",
      "Epoch 186/1500\n",
      "50/50 [==============================] - 0s 787us/step - loss: 0.2659 - accuracy: 0.8936\n",
      "Epoch 187/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2875 - accuracy: 0.8822\n",
      "Epoch 188/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2828 - accuracy: 0.8847\n",
      "Epoch 189/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.2723 - accuracy: 0.8964\n",
      "Epoch 190/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.2853 - accuracy: 0.8850\n",
      "Epoch 191/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.2772 - accuracy: 0.8891\n",
      "Epoch 192/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2767 - accuracy: 0.8872\n",
      "Epoch 193/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2650 - accuracy: 0.8907\n",
      "Epoch 194/1500\n",
      "50/50 [==============================] - 0s 735us/step - loss: 0.2706 - accuracy: 0.8939\n",
      "Epoch 195/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2817 - accuracy: 0.8850\n",
      "Epoch 196/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.2948 - accuracy: 0.8793\n",
      "Epoch 197/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2780 - accuracy: 0.8850\n",
      "Epoch 198/1500\n",
      "50/50 [==============================] - 0s 762us/step - loss: 0.2604 - accuracy: 0.8939\n",
      "Epoch 199/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2777 - accuracy: 0.8888\n",
      "Epoch 200/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.2883 - accuracy: 0.8822\n",
      "Epoch 201/1500\n",
      "50/50 [==============================] - 0s 721us/step - loss: 0.2674 - accuracy: 0.8882\n",
      "Epoch 202/1500\n",
      "50/50 [==============================] - 0s 747us/step - loss: 0.2636 - accuracy: 0.8926\n",
      "Epoch 203/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.2646 - accuracy: 0.8866\n",
      "Epoch 204/1500\n",
      "50/50 [==============================] - 0s 831us/step - loss: 0.2698 - accuracy: 0.8945\n",
      "Epoch 205/1500\n",
      "50/50 [==============================] - 0s 779us/step - loss: 0.2734 - accuracy: 0.8863\n",
      "Epoch 206/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2724 - accuracy: 0.8891\n",
      "Epoch 207/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.2617 - accuracy: 0.8914\n",
      "Epoch 208/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.2699 - accuracy: 0.8945\n",
      "Epoch 209/1500\n",
      "50/50 [==============================] - 0s 791us/step - loss: 0.2554 - accuracy: 0.8974\n",
      "Epoch 210/1500\n",
      "50/50 [==============================] - 0s 817us/step - loss: 0.2602 - accuracy: 0.8901\n",
      "Epoch 211/1500\n",
      "50/50 [==============================] - 0s 790us/step - loss: 0.2715 - accuracy: 0.8879\n",
      "Epoch 212/1500\n",
      "50/50 [==============================] - 0s 807us/step - loss: 0.2674 - accuracy: 0.8882\n",
      "Epoch 213/1500\n",
      "50/50 [==============================] - 0s 820us/step - loss: 0.2568 - accuracy: 0.8952\n",
      "Epoch 214/1500\n",
      "50/50 [==============================] - 0s 830us/step - loss: 0.2511 - accuracy: 0.9015\n",
      "Epoch 215/1500\n",
      "50/50 [==============================] - 0s 890us/step - loss: 0.2560 - accuracy: 0.8958\n",
      "Epoch 216/1500\n",
      "50/50 [==============================] - 0s 904us/step - loss: 0.2658 - accuracy: 0.8936\n",
      "Epoch 217/1500\n",
      "50/50 [==============================] - 0s 902us/step - loss: 0.2589 - accuracy: 0.8939\n",
      "Epoch 218/1500\n",
      "50/50 [==============================] - 0s 890us/step - loss: 0.2572 - accuracy: 0.8955\n",
      "Epoch 219/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2633 - accuracy: 0.8967\n",
      "Epoch 220/1500\n",
      "50/50 [==============================] - 0s 948us/step - loss: 0.2701 - accuracy: 0.8939\n",
      "Epoch 221/1500\n",
      "50/50 [==============================] - 0s 907us/step - loss: 0.2625 - accuracy: 0.8948\n",
      "Epoch 222/1500\n",
      "50/50 [==============================] - 0s 783us/step - loss: 0.2587 - accuracy: 0.8926\n",
      "Epoch 223/1500\n",
      "50/50 [==============================] - 0s 807us/step - loss: 0.2596 - accuracy: 0.8974\n",
      "Epoch 224/1500\n",
      "50/50 [==============================] - 0s 784us/step - loss: 0.2594 - accuracy: 0.8974\n",
      "Epoch 225/1500\n",
      "50/50 [==============================] - 0s 747us/step - loss: 0.2626 - accuracy: 0.8980\n",
      "Epoch 226/1500\n",
      "50/50 [==============================] - 0s 803us/step - loss: 0.2533 - accuracy: 0.8996\n",
      "Epoch 227/1500\n",
      "50/50 [==============================] - 0s 815us/step - loss: 0.2506 - accuracy: 0.8983\n",
      "Epoch 228/1500\n",
      "50/50 [==============================] - 0s 768us/step - loss: 0.2633 - accuracy: 0.8929\n",
      "Epoch 229/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.2598 - accuracy: 0.8904\n",
      "Epoch 230/1500\n",
      "50/50 [==============================] - 0s 783us/step - loss: 0.2363 - accuracy: 0.9034\n",
      "Epoch 231/1500\n",
      "50/50 [==============================] - 0s 817us/step - loss: 0.2493 - accuracy: 0.8967\n",
      "Epoch 232/1500\n",
      "50/50 [==============================] - 0s 808us/step - loss: 0.2586 - accuracy: 0.8910\n",
      "Epoch 233/1500\n",
      "50/50 [==============================] - 0s 886us/step - loss: 0.2499 - accuracy: 0.9015\n",
      "Epoch 234/1500\n",
      "50/50 [==============================] - 0s 826us/step - loss: 0.2535 - accuracy: 0.9012\n",
      "Epoch 235/1500\n",
      "50/50 [==============================] - 0s 869us/step - loss: 0.2480 - accuracy: 0.9002\n",
      "Epoch 236/1500\n",
      "50/50 [==============================] - 0s 833us/step - loss: 0.2597 - accuracy: 0.8942\n",
      "Epoch 237/1500\n",
      "50/50 [==============================] - 0s 799us/step - loss: 0.2590 - accuracy: 0.8971\n",
      "Epoch 238/1500\n",
      "50/50 [==============================] - 0s 814us/step - loss: 0.2581 - accuracy: 0.8999\n",
      "Epoch 239/1500\n",
      "50/50 [==============================] - 0s 862us/step - loss: 0.2548 - accuracy: 0.8977\n",
      "Epoch 240/1500\n",
      "50/50 [==============================] - 0s 963us/step - loss: 0.2302 - accuracy: 0.9062\n",
      "Epoch 241/1500\n",
      "50/50 [==============================] - 0s 966us/step - loss: 0.2556 - accuracy: 0.8933\n",
      "Epoch 242/1500\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8980\n",
      "Epoch 243/1500\n",
      "50/50 [==============================] - 0s 862us/step - loss: 0.2381 - accuracy: 0.9005\n",
      "Epoch 244/1500\n",
      "50/50 [==============================] - 0s 796us/step - loss: 0.2445 - accuracy: 0.9021\n",
      "Epoch 245/1500\n",
      "50/50 [==============================] - 0s 767us/step - loss: 0.2397 - accuracy: 0.9050\n",
      "Epoch 246/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.8990\n",
      "Epoch 247/1500\n",
      "50/50 [==============================] - 0s 786us/step - loss: 0.2490 - accuracy: 0.8990\n",
      "Epoch 248/1500\n",
      "50/50 [==============================] - 0s 788us/step - loss: 0.2380 - accuracy: 0.8964\n",
      "Epoch 249/1500\n",
      "50/50 [==============================] - 0s 841us/step - loss: 0.2449 - accuracy: 0.8996\n",
      "Epoch 250/1500\n",
      "50/50 [==============================] - 0s 812us/step - loss: 0.2434 - accuracy: 0.9066\n",
      "Epoch 251/1500\n",
      "50/50 [==============================] - 0s 799us/step - loss: 0.2505 - accuracy: 0.8996\n",
      "Epoch 252/1500\n",
      "50/50 [==============================] - 0s 791us/step - loss: 0.2467 - accuracy: 0.8958\n",
      "Epoch 253/1500\n",
      "50/50 [==============================] - 0s 787us/step - loss: 0.2506 - accuracy: 0.8996\n",
      "Epoch 254/1500\n",
      "50/50 [==============================] - 0s 817us/step - loss: 0.2433 - accuracy: 0.8990\n",
      "Epoch 255/1500\n",
      "50/50 [==============================] - 0s 761us/step - loss: 0.2332 - accuracy: 0.9129\n",
      "Epoch 256/1500\n",
      "50/50 [==============================] - 0s 829us/step - loss: 0.2413 - accuracy: 0.9012\n",
      "Epoch 257/1500\n",
      "50/50 [==============================] - 0s 798us/step - loss: 0.2310 - accuracy: 0.9050\n",
      "Epoch 258/1500\n",
      "50/50 [==============================] - 0s 756us/step - loss: 0.2466 - accuracy: 0.8955\n",
      "Epoch 259/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2450 - accuracy: 0.8986\n",
      "Epoch 260/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.2373 - accuracy: 0.9028\n",
      "Epoch 261/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2347 - accuracy: 0.9034\n",
      "Epoch 262/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.2402 - accuracy: 0.9075\n",
      "Epoch 263/1500\n",
      "50/50 [==============================] - 0s 768us/step - loss: 0.2373 - accuracy: 0.9009\n",
      "Epoch 264/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.2376 - accuracy: 0.9012\n",
      "Epoch 265/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.2371 - accuracy: 0.9040\n",
      "Epoch 266/1500\n",
      "50/50 [==============================] - 0s 763us/step - loss: 0.2312 - accuracy: 0.9078\n",
      "Epoch 267/1500\n",
      "50/50 [==============================] - 0s 786us/step - loss: 0.2203 - accuracy: 0.9151\n",
      "Epoch 268/1500\n",
      "50/50 [==============================] - 0s 763us/step - loss: 0.2364 - accuracy: 0.9129\n",
      "Epoch 269/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.2425 - accuracy: 0.9015\n",
      "Epoch 270/1500\n",
      "50/50 [==============================] - 0s 781us/step - loss: 0.2418 - accuracy: 0.9015\n",
      "Epoch 271/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.9091\n",
      "Epoch 272/1500\n",
      "50/50 [==============================] - 0s 850us/step - loss: 0.2332 - accuracy: 0.9059\n",
      "Epoch 273/1500\n",
      "50/50 [==============================] - 0s 872us/step - loss: 0.2396 - accuracy: 0.9056\n",
      "Epoch 274/1500\n",
      "50/50 [==============================] - 0s 815us/step - loss: 0.2331 - accuracy: 0.9012\n",
      "Epoch 275/1500\n",
      "50/50 [==============================] - 0s 875us/step - loss: 0.2363 - accuracy: 0.9040\n",
      "Epoch 276/1500\n",
      "50/50 [==============================] - 0s 952us/step - loss: 0.2434 - accuracy: 0.9066\n",
      "Epoch 277/1500\n",
      "50/50 [==============================] - 0s 891us/step - loss: 0.2138 - accuracy: 0.9189\n",
      "Epoch 278/1500\n",
      "50/50 [==============================] - 0s 919us/step - loss: 0.2250 - accuracy: 0.9107\n",
      "Epoch 279/1500\n",
      "50/50 [==============================] - 0s 924us/step - loss: 0.2294 - accuracy: 0.9072\n",
      "Epoch 280/1500\n",
      "50/50 [==============================] - 0s 916us/step - loss: 0.2407 - accuracy: 0.9072\n",
      "Epoch 281/1500\n",
      "50/50 [==============================] - 0s 903us/step - loss: 0.2393 - accuracy: 0.9053\n",
      "Epoch 282/1500\n",
      "50/50 [==============================] - 0s 889us/step - loss: 0.2264 - accuracy: 0.9104\n",
      "Epoch 283/1500\n",
      "50/50 [==============================] - 0s 785us/step - loss: 0.2227 - accuracy: 0.9126\n",
      "Epoch 284/1500\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9123\n",
      "Epoch 285/1500\n",
      "50/50 [==============================] - 0s 999us/step - loss: 0.2404 - accuracy: 0.9031\n",
      "Epoch 286/1500\n",
      "50/50 [==============================] - 0s 788us/step - loss: 0.2169 - accuracy: 0.9091\n",
      "Epoch 287/1500\n",
      "50/50 [==============================] - 0s 788us/step - loss: 0.2395 - accuracy: 0.9078\n",
      "Epoch 288/1500\n",
      "50/50 [==============================] - 0s 875us/step - loss: 0.2229 - accuracy: 0.9116\n",
      "Epoch 289/1500\n",
      "50/50 [==============================] - 0s 956us/step - loss: 0.2470 - accuracy: 0.8996\n",
      "Epoch 290/1500\n",
      "50/50 [==============================] - 0s 921us/step - loss: 0.2197 - accuracy: 0.9132\n",
      "Epoch 291/1500\n",
      "50/50 [==============================] - 0s 904us/step - loss: 0.2236 - accuracy: 0.9094\n",
      "Epoch 292/1500\n",
      "50/50 [==============================] - 0s 870us/step - loss: 0.2330 - accuracy: 0.9040\n",
      "Epoch 293/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.9113\n",
      "Epoch 294/1500\n",
      "50/50 [==============================] - 0s 824us/step - loss: 0.2231 - accuracy: 0.9031\n",
      "Epoch 295/1500\n",
      "50/50 [==============================] - 0s 826us/step - loss: 0.2298 - accuracy: 0.9031\n",
      "Epoch 296/1500\n",
      "50/50 [==============================] - 0s 849us/step - loss: 0.2203 - accuracy: 0.9078\n",
      "Epoch 297/1500\n",
      "50/50 [==============================] - 0s 790us/step - loss: 0.2244 - accuracy: 0.9126\n",
      "Epoch 298/1500\n",
      "50/50 [==============================] - 0s 848us/step - loss: 0.2342 - accuracy: 0.9037\n",
      "Epoch 299/1500\n",
      "50/50 [==============================] - 0s 817us/step - loss: 0.2167 - accuracy: 0.9085\n",
      "Epoch 300/1500\n",
      "50/50 [==============================] - 0s 815us/step - loss: 0.2202 - accuracy: 0.9157\n",
      "Epoch 301/1500\n",
      "50/50 [==============================] - 0s 793us/step - loss: 0.2131 - accuracy: 0.9148\n",
      "Epoch 302/1500\n",
      "50/50 [==============================] - 0s 813us/step - loss: 0.2179 - accuracy: 0.9132\n",
      "Epoch 303/1500\n",
      "50/50 [==============================] - 0s 865us/step - loss: 0.2123 - accuracy: 0.9145\n",
      "Epoch 304/1500\n",
      "50/50 [==============================] - 0s 826us/step - loss: 0.2291 - accuracy: 0.9062\n",
      "Epoch 305/1500\n",
      "50/50 [==============================] - 0s 795us/step - loss: 0.2217 - accuracy: 0.9104\n",
      "Epoch 306/1500\n",
      "50/50 [==============================] - 0s 838us/step - loss: 0.2130 - accuracy: 0.9142\n",
      "Epoch 307/1500\n",
      "50/50 [==============================] - 0s 854us/step - loss: 0.2283 - accuracy: 0.9066\n",
      "Epoch 308/1500\n",
      "50/50 [==============================] - 0s 854us/step - loss: 0.2192 - accuracy: 0.9161\n",
      "Epoch 309/1500\n",
      "50/50 [==============================] - 0s 859us/step - loss: 0.2298 - accuracy: 0.9072\n",
      "Epoch 310/1500\n",
      "50/50 [==============================] - 0s 877us/step - loss: 0.2304 - accuracy: 0.9062\n",
      "Epoch 311/1500\n",
      "50/50 [==============================] - 0s 896us/step - loss: 0.2288 - accuracy: 0.9085\n",
      "Epoch 312/1500\n",
      "50/50 [==============================] - 0s 853us/step - loss: 0.2141 - accuracy: 0.9183\n",
      "Epoch 313/1500\n",
      "50/50 [==============================] - 0s 873us/step - loss: 0.2144 - accuracy: 0.9170\n",
      "Epoch 314/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9107\n",
      "Epoch 315/1500\n",
      "50/50 [==============================] - 0s 853us/step - loss: 0.2168 - accuracy: 0.9161\n",
      "Epoch 316/1500\n",
      "50/50 [==============================] - 0s 811us/step - loss: 0.2283 - accuracy: 0.9100\n",
      "Epoch 317/1500\n",
      "50/50 [==============================] - 0s 863us/step - loss: 0.2219 - accuracy: 0.9100\n",
      "Epoch 318/1500\n",
      "50/50 [==============================] - 0s 874us/step - loss: 0.2228 - accuracy: 0.9104\n",
      "Epoch 319/1500\n",
      "50/50 [==============================] - 0s 858us/step - loss: 0.2267 - accuracy: 0.9043\n",
      "Epoch 320/1500\n",
      "50/50 [==============================] - 0s 805us/step - loss: 0.2141 - accuracy: 0.9138\n",
      "Epoch 321/1500\n",
      "50/50 [==============================] - 0s 813us/step - loss: 0.2200 - accuracy: 0.9107\n",
      "Epoch 322/1500\n",
      "50/50 [==============================] - 0s 807us/step - loss: 0.2216 - accuracy: 0.9157\n",
      "Epoch 323/1500\n",
      "50/50 [==============================] - 0s 816us/step - loss: 0.2168 - accuracy: 0.9094\n",
      "Epoch 324/1500\n",
      "50/50 [==============================] - 0s 949us/step - loss: 0.1956 - accuracy: 0.9233\n",
      "Epoch 325/1500\n",
      "50/50 [==============================] - 0s 851us/step - loss: 0.2272 - accuracy: 0.9047\n",
      "Epoch 326/1500\n",
      "50/50 [==============================] - 0s 803us/step - loss: 0.2180 - accuracy: 0.9059\n",
      "Epoch 327/1500\n",
      "50/50 [==============================] - 0s 733us/step - loss: 0.2290 - accuracy: 0.9116\n",
      "Epoch 328/1500\n",
      "50/50 [==============================] - 0s 767us/step - loss: 0.2132 - accuracy: 0.9119\n",
      "Epoch 329/1500\n",
      "50/50 [==============================] - 0s 789us/step - loss: 0.2111 - accuracy: 0.9157\n",
      "Epoch 330/1500\n",
      "50/50 [==============================] - 0s 807us/step - loss: 0.2206 - accuracy: 0.9085\n",
      "Epoch 331/1500\n",
      "50/50 [==============================] - 0s 847us/step - loss: 0.2201 - accuracy: 0.9138\n",
      "Epoch 332/1500\n",
      "50/50 [==============================] - 0s 841us/step - loss: 0.2144 - accuracy: 0.9123\n",
      "Epoch 333/1500\n",
      "50/50 [==============================] - 0s 812us/step - loss: 0.2204 - accuracy: 0.9113\n",
      "Epoch 334/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9078\n",
      "Epoch 335/1500\n",
      "50/50 [==============================] - 0s 824us/step - loss: 0.2093 - accuracy: 0.9167\n",
      "Epoch 336/1500\n",
      "50/50 [==============================] - 0s 784us/step - loss: 0.2228 - accuracy: 0.9126\n",
      "Epoch 337/1500\n",
      "50/50 [==============================] - 0s 840us/step - loss: 0.2198 - accuracy: 0.9100\n",
      "Epoch 338/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2181 - accuracy: 0.9173\n",
      "Epoch 339/1500\n",
      "50/50 [==============================] - 0s 780us/step - loss: 0.2044 - accuracy: 0.9205\n",
      "Epoch 340/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2134 - accuracy: 0.9145\n",
      "Epoch 341/1500\n",
      "50/50 [==============================] - 0s 802us/step - loss: 0.2177 - accuracy: 0.9129\n",
      "Epoch 342/1500\n",
      "50/50 [==============================] - 0s 818us/step - loss: 0.2004 - accuracy: 0.9214\n",
      "Epoch 343/1500\n",
      "50/50 [==============================] - 0s 780us/step - loss: 0.2020 - accuracy: 0.9195\n",
      "Epoch 344/1500\n",
      "50/50 [==============================] - 0s 800us/step - loss: 0.2105 - accuracy: 0.9161\n",
      "Epoch 345/1500\n",
      "50/50 [==============================] - 0s 788us/step - loss: 0.2059 - accuracy: 0.9148\n",
      "Epoch 346/1500\n",
      "50/50 [==============================] - 0s 811us/step - loss: 0.2186 - accuracy: 0.9110\n",
      "Epoch 347/1500\n",
      "50/50 [==============================] - 0s 818us/step - loss: 0.2127 - accuracy: 0.9205\n",
      "Epoch 348/1500\n",
      "50/50 [==============================] - 0s 835us/step - loss: 0.2173 - accuracy: 0.9135\n",
      "Epoch 349/1500\n",
      "50/50 [==============================] - 0s 797us/step - loss: 0.2015 - accuracy: 0.9259\n",
      "Epoch 350/1500\n",
      "50/50 [==============================] - 0s 792us/step - loss: 0.2167 - accuracy: 0.9110\n",
      "Epoch 351/1500\n",
      "50/50 [==============================] - 0s 800us/step - loss: 0.2233 - accuracy: 0.9078\n",
      "Epoch 352/1500\n",
      "50/50 [==============================] - 0s 791us/step - loss: 0.2061 - accuracy: 0.9180\n",
      "Epoch 353/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9176\n",
      "Epoch 354/1500\n",
      " 1/50 [..............................] - ETA: 0s - loss: 0.1868 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 324.\n",
      "50/50 [==============================] - 0s 780us/step - loss: 0.2067 - accuracy: 0.9148\n",
      "Epoch 354: early stopping\n",
      "5/5 [==============================] - 0s 813us/step - loss: 0.5133 - accuracy: 0.8067\n",
      "5/5 [==============================] - 0s 702us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.86 (24/28)\n",
      "Before appending - Cat IDs: 687, Predictions: 687, Actuals: 687, Gender: 687\n",
      "After appending - Cat IDs: 837, Predictions: 837, Actuals: 837, Gender: 837\n",
      "Final Test Results - Loss: 0.5132957100868225, Accuracy: 0.8066666722297668, Precision: 0.7163512155917219, Recall: 0.7562738074933196, F1 Score: 0.7263589493768522\n",
      "Confusion Matrix:\n",
      " [[67  4 11]\n",
      " [ 7 46  2]\n",
      " [ 5  0  8]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6896082245990107\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7473308145999908\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7219473570585251\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6794339465781116\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7144760484190902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 837, Predictions: 837, Actuals: 837, Gender: 837\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.78 (86/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, kitten, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, senior, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, kitten...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, kitten, adult, senior, adult, a...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, senior, adult, senior, adult, ...         adult            adult                   True\n",
       "68    062A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "63    057A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A       [adult, adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A              [adult, senior, adult, senior, adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [adult, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A   [senior, senior, senior, senior, senior, senior]        senior           senior                   True\n",
       "101   106A  [adult, senior, senior, senior, senior, adult,...        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, senior, senior, adult, senior, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "87    092A                                     [adult, adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "20    019A  [senior, adult, adult, adult, senior, adult, a...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, senior, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A      [adult, senior, adult, adult, senior, senior]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, senior, adult, senior, adult, a...         adult            adult                   True\n",
       "30    025C              [senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "42    036A  [adult, senior, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "104   110A                                     [adult, adult]         adult           kitten                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "56    051A  [adult, senior, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, adult, s...         adult           senior                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "90    095A  [senior, adult, senior, senior, senior, kitten...        senior            adult                  False\n",
       "89    094A  [senior, adult, adult, adult, adult, senior, s...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, senior, s...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "36    029A  [senior, adult, adult, adult, senior, senior, ...        senior            adult                  False\n",
       "34    027A  [adult, senior, adult, senior, senior, adult, ...        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     63\n",
      "kitten    12\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count  accuracy\n",
      "0            adult           73             63  86.30137\n",
      "1           kitten           15             12  80.00000\n",
      "2           senior           22             11  50.00000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABn60lEQVR4nO3deVgVdf//8ecBQQQUEUXEfVcy9wWXcl9zTTMru01zuzW3zNsyU0tts8wt07TMzNzKfUtLcwXNBZdE3AJR3EOURdbz+4Mf8+UIKAIKeF6P6/K6PDNzZt5zOHPO63zmM58xmc1mMyIiIiIiVsImuwsQEREREXmSFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIrlYXFxcdpeQ5Z7GfRKRnCVPdhcgkl5RUVG0a9eOiIgIACpXrszSpUuzuSrJjPPnz/P1119z7NgxIiIiKFSoEE2bNmXs2LFpPqdu3boWjwsUKMDvv/+OjY3l7/nPPvuMVatWWUybOHEinTp1ylCthw4dYvDgwQAUK1aMDRs2ZGg9j2LSpEls3LgRgAEDBjBo0CCL+du2bWPVqlUsWLAgS7cbExND27ZtuXv3LgBvvPEGb731VprLd+zYkatXrwLQv39/43V6VHfv3uXbb7+lYMGCvPnmmxlaR1bbsGEDH374IQC1a9fm22+/zdZ6PvzwQ4v33rJly6hYsWI2VpR+YWFhbNq0iZ07d3L58mVCQ0PJkycPRYoUoVq1anTs2JH69etnd5liJdQCLLnG9u3bjfALEBAQwN9//52NFUlmxMbGMmTIEHbv3k1YWBhxcXFcv36da9euPdJ67ty5g7+/f4rpBw8ezKpSc5ybN28yYMAAxo0bZwTPrGRvb0/Lli2Nx9u3b09z2ZMnT1rU0L59+wxtc+fOnbz44ossW7ZMLcBpiIiI4Pfff7eYtnr16myq5tHs3buXnj17Mn36dI4ePcr169eJjY0lKiqKixcvsnnzZoYMGcK4ceOIiYnJ7nLFCqgFWHKNdevWpZi2Zs0annnmmWyoRjLr/Pnz3Lp1y3jcvn17ChYsSPXq1R95XQcPHrR4H1y/fp2goKAsqTOJh4cHffr0ASB//vxZuu60NGnSBDc3NwBq1qxpTA8MDOTo0aOPddvt2rVj7dq1AFy+fJm///471WPtjz/+MP7v5eVF6dKlM7S9Xbt2ERoamqHnWovt27cTFRVlMW3Lli2MGDECBweHbKrq4Xbs2MH//vc/47GjoyMNGjSgWLFi3L59mwMHDhifBdu2bcPJyYn3338/u8oVK6EALLlCYGAgx44dAxJPed+5cwdI/LAcNWoUTk5O2VmeZEDy1nx3d3cmT578yOtwcHDg3r17HDx4kL59+xrTk7f+5suXL0VoyIgSJUowbNiwTK/nUbRq1YpWrVo90W0mqVOnDkWLFjVa5Ldv355qAN6xY4fx/3bt2j2x+qxR8kaApM/B8PBwtm3bRufOnbOxsrRdunTJ6EICUL9+faZOnYqrq6sxLSYmhsmTJ7NlyxYA1q5dS+/evTP8Y0okPRSAJVdI/sH/0ksv4evry99//01kZCRbt26le/fuaT739OnTLFmyhCNHjnD79m0KFSpE+fLl6dWrF40aNUqxfHh4OEuXLmXnzp1cunQJOzs7PD09adOmDS+99BKOjo7Gsg/qo/mgPqNJ/Vjd3NxYsGABkyZNwt/fnwIFCvC///2Pli1bEhMTw9KlS9m+fTvBwcFER0fj5ORE2bJl6d69Oy+88EKGa+/Xrx/Hjx8HYOTIkfTu3dtiPcuWLePLL78EElshZ8yYkebrmyQuLo4NGzawefNm/vnnH6KioihatCiNGzfm9ddfx93d3Vi2U6dOXLlyxXh8/fp14zVZv349np6eD90eQPXq1Tl48CDHjx8nOjqavHnzAvDXX38Zy9SoUQNfX99Un3/z5k2+++47fHx8uH79OvHx8RQsWBAvLy/69u1r0Rqdnj7A27ZtY/369Zw9e5a7d+/i5uZG/fr1ef311ylTpozFsvPnzzf67r777rvcuXOHn3/+maioKLy8vIz3xf3vr+TTAK5cuULdunUpVqwY77//vtFX18XFhd9++408ef7vYz4uLo527dpx+/ZtAH788Ue8vLxSfW1MJhNt27blxx9/BBID8IgRIzCZTMYy/v7+XL58GQBbW1vatGljzLt9+zarVq1ix44dhISEYDabKV26NK1bt6Znz54WLZb39+tesGABCxYsSHFM/f7776xcuZKAgADi4+MpWbIkrVu35tVXX03RAhoZGcmSJUvYtWsXwcHBxMTE4OzsTMWKFenSpUuGu2rcvHmTWbNmsXfvXmJjY6lcuTJ9+vThueeeAyAhIYFOnToZPxw+++wzi+4kAF9++SXLli0DEj/PHtTnPcn58+c5ceIE8H9nIz777DMg8UzYgwLwpUuXmDdvHr6+vkRFRVGlShUGDBiAg4MD/fv3BxL7cU+aNMnieY/yeqdl8eLFxo/dYsWK8cUXX1h8hkJil5v333+ff//9F3d3d8qXL4+dnZ0xPz3HSpITJ06wcuVK/Pz8uHnzJvnz56datWr07NkTb29vi+0+7JhO/jk1b948432a/Bj86quvyJ8/P99++y0nT57Ezs6O+vXrM3ToUEqUKJGu10iyhwKw5HhxcXFs2rTJeNypUyc8PDyM/r9r1qxJMwBv3LiRyZMnEx8fb0y7du0a165dY//+/bz11lu88cYbxryrV6/y3//+l+DgYGPavXv3CAgIICAggD/++IN58+al+ADPqHv37vHWW28REhICwK1bt6hUqRIJCQm8//777Ny502L5u3fvcvz4cY4fP86lS5cswsGj1N65c2cjAG/bti1FAE7e57Njx44P3Y/bt28zevRoo5U+ycWLF7l48SIbN25k2rRpKYJOZtWpU4eDBw8SHR3N0aNHjS+4Q4cOAVCqVCkKFy6c6nNDQ0MZOHAgFy9etJh+69Yt9uzZw/79+5k1axYNGjR4aB3R0dGMGzeOXbt2WUy/cuUK69atY8uWLUycOJG2bdum+vzVq1dz5swZ47GHh8dDt5ma+vXr4+HhwdWrVwkLC8PX15cmTZoY8w8dOmSE33LlyqUZfpO0b9/eCMDXrl3j+PHj1KhRw5ifvPtDvXr1jNfa39+f0aNHc/36dYv1+fv74+/vz8aNG5k9ezZFixZN976ldlHj2bNnOXv2LL///jvffPMNLi4uQOL7vn///havKSRehHXo0CEOHTrEpUuXGDBgQLq3D4nvjT59+lj0U/fz88PPz4+3336bV199FRsbGzp27Mh3330HJB5fyQOw2Wy2eN3Se1Fm8kaAjh070r59e2bMmEF0dDQnTpzg3LlzVKhQIcXzTp8+zX//+1/jgkaAY8eOMWzYMLp165bm9h7l9U5LQkKCxRmC7t27p/nZ6eDgwNdff/3A9cGDj5Xvv/+eefPmkZCQYEz7999/2b17N7t37+aVV15h9OjRD93Go9i9ezfr16+3+I7Zvn07Bw4cYN68eVSqVClLtydZRxfBSY63Z88e/v33XwBq1apFiRIlaNOmDfny5QMSP+BTuwjqwoULTJ061fhgqlixIi+99JJFK8CcOXMICAgwHr///vtGgHR2dqZjx4506dLF6GJx6tQpvvnmmyzbt4iICEJCQnjuuefo1q0bDRo0oGTJkuzdu9cIv05OTnTp0oVevXpZfJj+/PPPmM3mDNXepk0b44vo1KlTXLp0yVjP1atXjZamAgUK8Pzzzz90Pz788EMj/ObJk4fmzZvTrVs3I+DcvXuXd955x9hO9+7dLcKgk5MTffr0oU+fPjg7O6f79atTp47x/6RW36CgICOgJJ9/vx9++MEIv8WLF6dXr168+OKLRoiLj49n+fLl6apj1qxZRvg1mUw0atSI7t27G6dwY2JimDhxovG63u/MmTMULlyYnj17Urt27TSDMiS2yKf22nXv3h0bGxuLQLVt2zaL5z7qD5uKFStSvnz5VJ8PqXd/uHv3LmPGjDHCb8GCBenUqRNt27Y13nMXLlzg7bffNi5269Onj8V2atSoQZ8+fYx+z5s2bTLCmMlk4vnnn6d79+7GWYUzZ87w+eefG8/fvHmzEZJcXV3p3Lkzr776qsUIAwsWLLB436dH0nurSZMmvPjiixYBfubMmQQGBgKJoTappXzv3r1ERkYayx07dsx4bdLzIwQSLxjdvHmzsf8dO3bE2dnZIlindjFcQkICH3zwgRF+8+bNS/v27enQoQOOjo5pXkD3qK93WkJCQggLCzMeJ+/HnlFpHSs7duxg7ty5RvitUqUKL730ErVr1zaeu2zZMn766adM15DcmjVrsLOzo3379rRv3944C3Xnzh3Gjx9v8RktOYtagCXHS97ykfTl7uTkRKtWrYxTVqtXr05x0cSyZcuIjY0FoFmzZnz66afG6eApU6awdu1anJycOHjwIJUrV+bYsWNGiHNycuKnn34yTmF16tSJ/v37Y2try99//01CQkKKYbcyqnnz5kybNs1imr29PV27duXs2bMMHjyYhg0bAoktW61btyYqKoqIiAhu376Nq6vrI9fu6OhIq1atWL9+PZAYlPr16wcknvZM+tBu06YN9vb2D6z/2LFj7NmzB0g8Df7NN99Qq1YtILFLxpAhQzh16hTh4eEsXLiQSZMm8cYbb3Do0CF+++03IDFoZ6R/bbVq1Sz6AYNl94c6deqk2f2hZMmStG3blosXLzJz5kwKFSoEJLZ6JrUMJp3ef5CrV69atJRNnjzZCIMxMTGMHTuWPXv2EBcXx+zZs9McRmv27NnpGs6qVatWFCxYMM3XrnPnzixcuBCz2cyuXbuMriFxcXH8+eefQOLfqUOHDg/dFiS+HnPmzAES3xtvv/02NjY2nDlzxvgBkTdvXpo3bw7AqlWrjFEhPD09+f77740fFYGBgfTp04eIiAgCAgLYsmULnTp1YtiwYdy6dYvz588DiS3Zyc9uLF682Pj/u+++a5zxGTp0KL169eL69ets376dYcOG4eHhYfF3Gzp0KF27djUef/3111y9epWyZctatNql1//+9z969uwJJIacfv36ERgYSHx8POvWrWPEiBGUKFGCunXr8tdffxEdHc3u3buN90TyHxGpdWNKza5du4yW+6RGAIAuXboYwXjLli0MHz7comvCoUOH+Oeff4DEv/m3335r9OMODAzktddeIzo6OsX2HvX1Tkvyi1wB4xhLcuDAAYYOHZrqc1PrkpEktWMl6T0KiT+wx44da3xGL1q0yGhdXrBgAV27dn2kH9oPYmtry8KFC6lSpQoAPXr0oH///pjNZi5cuMDBgwfTdRZJnjy1AEuOdv36dXx8fIDEi5mSXxDUpUsX4//btm2zaGWB/zsNDtCzZ0+LvpBDhw5l7dq1/Pnnn7z++uspln/++ect+m/VrFmTn376id27d/P9999nWfgFUm3t8/b2Zvz48SxevJiGDRsSHR2Nn58fS5YssWhRSPryykjt979+SZIPs5SeVsLky7dp08YIv5DYEp18/Nhdu3ZZnJ7MrDx58hj9dAMCAggLC7O4AO5BXS569OjB1KlTWbJkCYUKFSIsLIy9e/dadLdJLRzcb8eOHcY+1axZ0+JCMHt7e4tTrkePHjWCTHLlypXLsrFcixUrZrR0RkREsG/fPiDxwsCk1rgGDRqk2TXkfu3atTNaM2/evMmRI0cAy+4Pzz//vHGmIfn7oV+/fhbbKVOmDL169TIe39/FJzU3b97kwoULANjZ2VmE2QIFCtC0aVMgsbUz6cdPUhgBmDZtGu+88w4rVqwwugNMnjyZfv36PfJFVi4uLhbdrQoUKMCLL75oPD558qTx/+THV9KPleRdAmxtbdMdgO/v/pCkdu3alCxZEkhseb9/iLTkXZIaNmxocRFjmTJlUv0RlJHXOy1JraFJMvKD436pHSsBAQHGjzEHBweGDx9u8Rn9n//8h2LFigGJx8TD6n4UzZs3t3i/1ahRw2iwAFJ0C5OcQy3AkqNt2LDB+NC0tbXlnXfesZhvMpkwm81ERETw22+/WfRpS97/MOnDL4mrq6vFVcgPWx4sv1TTI72nvlLbFiS2LK5evRpfX1/jIpT7JQWvjNReo0YNypQpQ2BgIOfOneOff/4hX758xpd4mTJlqFat2kPrT97nOLXtJJ929+5dwsLCUrz2mZHUDzjpC/nw4cMAlC5d+qEh7+TJk6xbt47Dhw+n6AsMpCusP2z/S5QogZOTExEREZjNZi5fvkzBggUtlknrPZBRXbp04cCBA0Bii2OLFi0euftDEg8PD2rVqmUE3+3bt1O3bl2L7g/Jg9SjvB/S0wUh+RjDsbGxD2xNS2rtbNWqlfFjJjo6mj///NNo/S5QoADNmjXj9ddfp2zZsg/dfnLFixfH1tbWYlryixuTt3g2b96c/Pnzc/fuXXx9fbl79y5nz57lxo0bQPp/hFy9etX4W0LiCAlbt241Ht+7d8/4/+rVqy3+tknbAlIN+6ntf0Ze77Tc38f72rVrFtv09PQ0hhaExO4iSWcB0pLasZL8PVeyZMkUowLZ2tpSsWJF44K25Ms/SHqO/9Re1zJlyrB//34gZSu45BwKwJJjmc1m4xQ9JJ5Of9DNDdasWZPmRR2P2vKQkZaK+wNvUveLh0ltCLeki1QiIyMxmUzUrFmT2rVrU716daZMmWLxxXa/R6m9S5cuzJw5E0hsBU5+gUp6Q1LylvXU3P+6JB9FICsk7+f7008/Ga2cD+r/C4ldZKZPn47ZbMbBwYGmTZtSs2ZNPDw8eO+999K9/Yft//1S2/+sHsavWbNmuLi4EBYWxp49e7hz547RRzl//vxGK156tWvXzgjAO3bsoHv37kb4cXFxsWjxetT3w8MkDyE2NjYP/PGUtG6TycSHH35It27d2LJlCz4+PsaFpnfu3GH9+vVs2bKFefPmWVzU9zCp3aAj+fGWfN/z5s1Lu3btWLVqFbGxsezcudPiWoX0tv5u2LDB4jVIung1NcePH+f8+fNGf+rkr3V6z7xk5PVOi6urK8WLFze6pBw6dMjiGoySJUtadN9J3g0mLakdK+k5BpPXmtoxmNrrk54bsqR2047kI1hk9eedZB0FYMmxDh8+nK4+mElOnTpFQEAAlStXBhLHlk36pR8YGGjRUnPx4kV+/fVXypUrR+XKlalSpYrFMF2p3UThm2++IX/+/JQvX55atWrh4OBgcZoteUsMkOqp7tQk/7BMMn36dKNLR/I+pZD6h3JGaofEL+Gvv/6auLg4YwB6SPziS28f0eQtMskvKExtWoECBR565fijeuaZZ4x+wMlPQT8oAN+5c4fZs2djNpuxs7Nj5cqVxtBrSad/0+th+3/p0iVjGCgbGxuKFy+eYpnU3gOZYW9vT/v27Vm+fDn37t1j2rRpxtjZrVu3TnFq+mFatWrFtGnTiI2NJTQ01OICqNatW1sEkGLFihkXXQUEBKRoBU7+GpUqVeqh207+3razs2PLli0Wx118fHyKVtkkZcqUYcyYMeTJk4erV6/i5+fHL7/8gp+fH7GxsSxcuJDZs2c/tIYkly5d4t69exb9bJOfObi/RbdLly5G//CtW7ca4c7Z2ZlmzZo9dHtms/mRb7m9Zs0a40xZkSJFUq0zyblz51JMy8zrnZp27doZI2Ikje97/xmQJOkJ6akdK8mPweDgYCIiIiyCcnx8vMW+JnUbSb4f939+JyQkGMfMg6T2GiZ/rZP/DSRnUR9gybGS7kIF0KtXL2P4ovv/Jb+yO/lVzckD0MqVKy1aZFeuXMnSpUuZPHmy8eGcfHkfHx+LlojTp0/z3XffMWPGDEaOHGn86i9QoICxzP3BKXkfyQdJrYXg7Nmzxv+Tf1n4+PhY3C0r6QsjI7VD4kUpSeOXBgUFcerUKSDxIqTkX4QPknyUiN9++w0/Pz/jcUREhMXQRs2aNcvyFhE7O7tU7x73oAAcFBRkvA62trYWd3ZLuqgI0veFnHz/jx49atHVIDY2lq+++sqiptR+ADzqa5L8izutVqrkfVCTbjAAj9b9IUmBAgVo3Lix8Tj53/j+m18kfz2+//57bt68aTwOCgpixYoVxuOkC+cAi5CVfJ88PDyMHw3R0dH8+uuvxryoqCi6du1Kly5dGDVqlBFGPvjgA9q0aUOrVq2MzwQPDw/atWtHjx49jOc/6m23k8YWThIeHm5xAeT9oxxUqVLF+EF+8OBB43R4en+EHDhwwGi5dnFxwdfXN9XPwOQ3kdm8ebPRdz15f3wfHx/j+IbE0RSSd6VIkpHX+0F69uxpfIbdvn2bUaNGpRgeLyYmhkWLFqUYtSQ1qR0rlSpVMkLwvXv3mDNnjkWL75IlS4zuD87OztSrVw+wvKPjnTt3LN6ru3btStdZvKS/SZJz584Z3R/A8m8gOYtagCVHunv3rsUFMg+6G1bbtm2NrhFbt25l5MiR5MuXj169erFx40bi4uI4ePAgr7zyCvXq1ePy5csWH1Avv/wykPjlVb16deOmCn379qVp06Y4ODhYhJoOHToYwTf5xRj79+/nk08+oXLlyuzatcu4+CgjChcubHzxjRs3jjZt2nDr1i12795tsVzSF11Gak/SpUuXFBcjPUpIqlOnDrVq1eLo0aPEx8czePBgnn/+eVxcXPDx8TH6FObPn/+Rx11Nr9q1a1t0j3lY/9/k8+7du0ffvn1p0KAB/v7+FqeY03MRXIkSJWjfvr0RMseNG8fGjRspVqwYhw4dMobGsrOzs7ggMDOSt27duHGDiRMnAljccatixYp4eXlZhJ5SpUpl6FbTkBh0k/rRJilevHiK0NejRw9+/fVXQkNDuXz5Mq+88gpNmjQhLi6OXbt2GWc2vLy8LMJz8n1av3494eHhVKxYkRdffJFXX33VGCnls88+Y8+ePZQqVYoDBw4YwSYuLs7oj1mhQgXj7/Hll1/i4+NDyZIljTFhkzxK94ck8+fP5/jx45QoUYL9+/cbZ6ny5s2b6s0ounTpkmLIsPQeX8kvfmvWrFmap/qbNm1K3rx5iY6O5s6dO/z++++88MIL1KlTh3LlynHhwgUSEhIYOHAgLVq0wGw2s3PnzlRP3wOP/Ho/iJubG+PHj2fs2LHEx8dz4sQJunXrRqNGjShWrBihoaH4+PikOGP2KN2CTCYTb775JlOmTAESRyI5efIk1apV4/z580b3HYBBgwYZ6y5VqpTxupnNZkaOHEm3bt0ICQlJ9xCIZrOZYcOG0axZMxwcHNixY4fxuVGpUiWLYdgkZ1ELsORIW7ZsMT5EihQp8sAvqhYtWhinxZIuhoPEL8H33nvPaC0LDAxk1apVFuG3b9++FiMFTJkyxWj9iIyMZMuWLaxZs4bw8HAg8QrkkSNHWmw7+SntX3/9lY8//ph9+/bx0ksvZXj/k0amgMSWiV9++YWdO3cSHx9vMXxP8os5HrX2JA0bNrQ4Tefk5JSu07NJbGxs+OSTT6hatSqQ+MW4Y8cO1qxZY4TfAgUK8OWXX2b5xV5J7h/t4WH9f4sVK2bxoyowMJAVK1Zw/Phx8uTJY5ziDgsLS9dp0Pfee8/o22g2m9m3bx+//PKLEX7z5s3L5MmTU72VcEaULVvWoiV506ZNbNmyJUVr8P2BLCOtv0mee+65FKEktRFMChcuzOeff46bmxuQeMORDRs2sGXLFiP8VqhQgS+++MKiJTt5kL516xarVq0yrqB/6aWXLLa1f/9+li9fbvRDdnZ25rPPPjM+B3r37k3r1q2BxNPfe/bs4eeff2br1q1GDWXKlGHIkCGP9Bq0bt0aNzc3fHx8WLVqlRF+bWxsePfdd1MdEiz52LCQGLrSE7zDwsIsbqzyoEYAR0dHi5b3NWvWGHVNnjzZ+Lvdu3ePzZs3s2XLFhISEozXCCxbVh/19X6YZs2a8fXXXxvviejoaHbu3MnPP//Mli1bLMJv/vz5GTRoEKNGjUrXupN07dqVN954w9gPf39/Vq1aZRF+X3vtNV555RXjsb29vdEAAolnyz755BMWL15M0aJFLc4upqVu3brY2Niwfft2NmzYYHR3cnFxydDt3eXJUQCWHCl5y0eLFi0eeIo4f/78Frc0Tvrwh8TWl0WLFhlfXLa2thQoUIAGDRrwxRdfpBiD0tPTkyVLltCvXz/Kli1L3rx5yZs3L+XLl2fgwIEsXrzYInjky5ePhQsX0r59ewoWLIiDgwPVqlVjypQpqYbN9HrppZf49NNP8fLywtHRkXz58lGtWjUmT55ssd7k3SwetfYktra2FsGsVatW6b7NaZLChQuzaNEi3nvvPWrXro2Liwv29vaULFmSV155hRUrVjzWlpCkfsBJHhaAAT766COGDBlCmTJlsLe3x8XFhSZNmrBw4ULj1LzZbDZGO7j/4qDkHB0dmT17NlOmTKFRo0a4ublhZ2eHh4cHXbp04eeff35ggHlUdnZ2TJs2DS8vL+zs7ChQoAB169ZN0WKdvLXXZDKlu193avLmzUuLFi0spqV1O+FatWqxfPlyBgwYQKVKlYz3cNWqVRkxYgQ//PBDii42LVq0YNCgQbi7u5MnTx6KFi1qtDDa2NgwZcoUJk+eTL169SzeXy+++CJLly61GLHE1taWqVOn8vnnn+Pt7U2xYsXIkycPTk5OVK1alcGDB/Pjjz8+8mgknp6eLF26lE6dOhnHe+3atZkzZ06ad3TLnz+/RUtpev8GW7ZsMVpoXVxcjNP2aUkeWP38/IywWrlyZRYvXkzz5s0pUKAA+fLlo0GDBnz//fcWQTzpxkLw6K93etStW5dff/2V0aNHU79+fQoVKoStrS1OTk6UKlWKdu3aMWnSJDZv3syAAQMe+eJSgLfeeouFCxfSoUMHihUrhp2dHa6urjz//PPMnTs31VA9bNgwRo4cSenSpbG3t6dYsWK8/vrr/Pjjj+m6XqFWrVp899131KtXDwcHB1xcXIxbiCe/uYvkPCazblMiYtUuXrxIr169jC/b+fPnpytAWpsffvjBGGy/fPnyFn1Zc6qPPvrIGEmlTp06zJ8/P5srsj5Hjhxh4MCBQOKPkHXr1hkXXD5uV69eZcuWLRQsWBAXFxdq1aplEfo//PBD4yK7kSNHprgluqRu0qRJbNy4EYABAwZY3LRFcg/1ARaxQleuXGHlypXEx8ezdetWI/yWL19e4fc+W7duZdq0aRa3dH1cXTmywi+//ML169c5ffq0RXefzHTJkUdz+vRptm/fTmRkpMWNVRo3bvzEwi8knsFIfhFqyZIladSoETY2Npw7d864IYTJZKJJkyZPrC6RnCDHBuBr167x8ssv88UXX1j07wsODmb69OkcPXoUW1tbWrVqxbBhwyz6RUZGRjJ79mx27NhBZGQktWrV4u2337YYBkvEmplMJour2SHxtPqYMWOyqaKc6++//7YIv5B4x7uc6tSpUxbjZ0PinQVbtmyZTRVZn6ioKIvbCUNiv9kRI0Y80TqKFStGt27djG5hwcHBqZ65ePXVV/X9KFYnRwbgq1evMmzYMOPinSR3795l8ODBuLm5MWnSJEJDQ5k1axYhISEWYzm+//77nDx5kuHDh+Pk5MSCBQsYPHgwK1euTHEFvIg1KlKkCCVLluT69es4ODhQuXJl+vXr98BbB1szFxcXIiMj8fT05OWXX85UX9rHrVKlShQsWJCoqCiKFClCq1at6N+/vwbkf4I8PT3x8PDg33//JX/+/FSrVo2BAwc+8p3nssK4ceOoUaMGv/32G2fPnjUuOHNxcaFy5cp07do1Rd9uEWuQo/oAJyQksGnTJmbMmAEkXgU7b94840t50aJFfPfdd2zcuNEYV3Dfvn2MGDGChQsXUrNmTY4fP06/fv2YOXOmMW5laGgonTt35o033uDNN9/Mjl0TERERkRwiR40CcfbsWT755BNeeOEFi/Esk/j4+FCrVi2LGwN4e3vj5ORkjLnq4+NDvnz5LG636OrqSu3atTM1LquIiIiIPB1yVAD28PBgzZo1vP3226kOwxQYGJji1pm2trZ4enoat38NDAykePHiKW7VWLJkyVRvESsiIiIi1iVH9QF2cXF54Lh74eHhqd4dxtHR0Rh8Oj3LPKqAgADjuekd+FtEREREnqzY2FhMJtNDb0OdowLwwyQfiP5+SQPTp2eZjEjqKp3WrSNFREREJHfIVQHY2dnZuI1lchEREcZdhZydnfn3339TXSb5UGmPonLlypw4cQKz2UyFChUytA4RERERebzOnTuXrlFvclUALl26NMHBwRbT4uPjCQkJMW5dWrp0aXx9fUlISLBo8Q0ODs70OIcmkwlHR8dMrUNEREREHo/0DvmYoy6Cexhvb2+OHDlCaGioMc3X15fIyEhj1Advb28iIiLw8fExlgkNDeXo0aMWI0OIiIiIiHXKVQG4R48e5M2bl6FDh7Jz507Wrl3LBx98QKNGjahRowYAtWvXpk6dOnzwwQesXbuWnTt3MmTIEPLnz0+PHj2yeQ9EREREJLvlqi4Qrq6uzJs3j+nTpzN+/HicnJxo2bIlI0eOtFhu2rRpfPXVV8ycOZOEhARq1KjBJ598orvAiYiIiEjOuhNcTnbixAkAnn322WyuRERERERSk968lqu6QIiIiIiIZJYCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWJU92FyCS3Jo1a1i2bBkhISF4eHjQs2dPXnrpJUwmEwDXr19n1qxZ+Pj4EBcXxzPPPMPw4cOpUqXKA9e7YcMGlixZwuXLlylatCg9e/bk5ZdfNtYrIiIi1kMBWHKMtWvXMnXqVF5++WWaNm3K0aNHmTZtGjExMfTu3ZuIiAgGDBiAvb097733Hnnz5mXhwoUMHTqUFStWULhw4TTXO2XKFP7zn//g7e3NyZMn+eqrr4iMjKRfv35PeC9FREQkuykAS46xfv16atasyZgxYwCoX78+QUFBrFy5kt69e7Ns2TLCwsL45ZdfjLBbtWpVXn/9dQ4dOkS7du1SXe+iRYto2bIlw4cPN9Z78eJFVqxYoQAsIiJihRSAJceIjo5O0Yrr4uJCWFgYAH/88QctW7a0WKZw4cJs2bLlgeudMWMGefPmtZhmZ2dHTExMFlUuIiIiuYkugpMc45VXXsHX15fNmzcTHh6Oj48PmzZtokOHDsTFxXHhwgVKly7NN998Q9u2bWnQoAGDBg3i/PnzD1xv2bJl8fT0xGw2ExYWxtq1a9m0aRM9evR4QnsmIiIiOYlagCXHaNu2LYcPH2bChAnGtIYNGzJ69Gju3LlDfHw8P//8M8WLF+eDDz4gJiaGefPmMXDgQJYvX06RIkUeuP4TJ04YXR68vLzo3bv3Y90fERERyZnUAiw5xujRo/njjz8YPnw48+fPZ8yYMZw6dYqxY8dadFeYPXs2TZo0oUWLFsyaNYvIyEhWrlz50PUXK1aM+fPnM3HiRG7evEm/fv24d+/e49wlERERyYHUAiw5wrFjx9i/fz/jx4+na9euANSpU4fixYszcuRIOnXqZExzdHQ0nufh4UHZsmUJCAh46DaKFClCkSJFjPUOHDiQ33//nY4dOz6WfRIREZGcSS3AkiNcuXIFgBo1alhMr127NgCBgYG4urqmeuFaXFxciovckkRGRrJ161aCg4MtpieNG3zz5s1M1y4iIiK5iwKw5AhlypQB4OjRoxbTjx07BkCJEiVo3LgxBw8e5Pbt28b8wMBAgoKCqFmzZqrrtbW1ZfLkyfz4448W0319fQGoUKFC1uyAiIiI5BrqAiE5QpUqVWjRogVfffUVd+7coVq1aly4cIFvv/2WqlWr0qxZM6pUqcKff/7J0KFDGTBgALGxscydO5eiRYsa3SYg8WI3V1dXSpQoQd68eenbty/z58+nUKFC1K1blzNnzrBgwQLq169P48aNs2+nRUREJFuYzGazObuLyA1OnDgBwLPPPpvNlTy9YmNj+e6779i8eTM3btzAw8ODZs2aMWDAAKPf74ULF5g9ezaHDx/GxsaGBg0a8Pbbb1O0aFFjPXXr1qVjx45MmjQJALPZzK+//srKlSu5fPkyBQsWpF27dgwcODDNrhMiIiKS+6Q3rykAp5MCsIiIiEjOlt68pj7AIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIpuhGGlEsxmbEym7C5D0qC/jzyqNWvWsGzZMkJCQvDw8KBnz5689NJLmP7/+yg4OJjp06dz9OhRbG1tadWqFcOGDcPZ2fmB6z116hQzZszA398fJycnOnXqxMCBA7Gzs3sSuyUi8lgoAFspG5OJ5b5nuH4nMrtLkfu4F3Ckl3el7C5DcpG1a9cydepUXn75ZZo2bcrRo0eZNm0aMTEx9O7dm7t37zJ48GDc3NyYNGkSoaGhzJo1i5CQEGbPnp3mei9dusSQIUOoXr06n3zyCYGBgcydO5ewsDDGjRv3BPdQRCRrKQBbset3IgkJjcjuMkQkk9avX0/NmjUZM2YMAPXr1ycoKIiVK1fSu3dvfvnlF8LCwli6dCkFCxYEwN3dnREjRuDn50fNmjVTXe/ixYtxcnLiyy+/xM7OjiZNmuDg4MDnn39Ov3798PDweEJ7KCKStdQHWEQkl4uOjsbJyclimouLC2FhYQD4+PhQq1YtI/wCeHt74+TkxL59+9Jcr6+vL40bN7bo7tCyZUsSEhLw8fHJ2p0QEXmCFIBFRHK5V155BV9fXzZv3kx4eDg+Pj5s2rSJDh06ABAYGEipUqUsnmNra4unpydBQUGprvPevXtcuXIlxfNcXV1xcnJK83kiIrmBukCIiORybdu25fDhw0yYMMGY1rBhQ0aPHg1AeHh4ihZiAEdHRyIiUu8GFR4eDpDqRXJOTk5pPk9EJDdQC7CISC43evRo/vjjD4YPH878+fMZM2YMp06dYuzYsZjNZhISEtJ8ro1N6l8DZrP5gds0aZQSEcnF1AIsIpKLHTt2jP379zN+/Hi6du0KQJ06dShevDgjR45k7969ODs7ExmZcsSXiIgI3N3dU11vUotxai29ERERDx0+TUQkJ1MLsIhILnblyhUAatSoYTG9du3aAJw/f57SpUsTHBxsMT8+Pp6QkBDKlCmT6nodHR1xd3fn0qVLFtP//fdfIiIiKFu2bBbtgYjIk6cALCKSiyUF2KNHj1pMP3bsGAAlSpTA29ubI0eOEBoaasz39fUlMjISb2/vNNfdoEED9uzZQ0xMjDFtx44d2NraUq9evSzcCxGRJ0tdIEREcrEqVarQokULvvrqK+7cuUO1atW4cOEC3377LVWrVqVZs2bUqVOHFStWMHToUAYMGEBYWBizZs2iUaNGFi3HJ06cwNXVlRIlSgDQp08ftm3bxvDhw3nttdcICgpi7ty5dOvWTWMAi0iuZjI/7EoHARK/GACeffbZbK4k68za5qcbYeRAnq5ODG9TM7vLkFwkNjaW7777js2bN3Pjxg08PDxo1qwZAwYMwNHREYBz584xffp0jh07hpOTE02bNmXkyJEWo0PUrVuXjh07MmnSJGPa0aNHmTlzJmfOnKFgwYJ06NCBwYMHkyeP2k9EJOdJb15TAE4nBWB5UhSARUREMia9eU19gEVERETEquTKc1hr1qxh2bJlhISE4OHhQc+ePXnppZeMcSmDg4OZPn06R48exdbWllatWjFs2DAN2yMiIiIiuS8Ar127lqlTp/Lyyy/TtGlTjh49yrRp04iJiaF3797cvXuXwYMH4+bmxqRJkwgNDWXWrFmEhIQwe/bs7C5fRERERLJZrgvA69evp2bNmowZMwaA+vXrExQUxMqVK+nduze//PILYWFhLF26lIIFCwLg7u7OiBEj8PPzo2bNmtlXvIiIiIhku1zXBzg6OjrFPe1dXFwICwsDwMfHh1q1ahnhF8Db2xsnJyf27dv3JEsVERERkRwo1wXgV155BV9fXzZv3kx4eDg+Pj5s2rSJDh06ABAYGEipUqUsnmNra4unpydBQUHZUbKIiIiI5CC5rgtE27ZtOXz4MBMmTDCmNWzYkNGjRwMQHh6eooUYEm/rmdo97R+F2WwmMjIyU+vICUwmE/ny5cvuMuQhoqKi0CiFOU/SxbaSM+mYEbFuZrM5XZ/TuS4Ajx49Gj8/P4YPH84zzzzDuXPn+Pbbbxk7dixffPEFCQkJaT7XxiZzDd6xsbH4+/tnah05Qb58+fDy8sruMuQh/vnnH6KiorK7DEnGzs4Or2eeIY+tbXaXIqmIi4/n1N9/Exsbm92liEg2sre3f+gyuSoAHzt2jP379zN+/Hi6du0KQJ06dShevDgjR45k7969ODs7p9pKGxERgbu7e6a2b2dnR4UKFTK1jpxALVi5Q9myZdWalcOYTCby2Nqy3PcM1+/k/rNBTxP3Ao708q5ExYoVddyIWLFz586la7lcFYCvXLkCYHHveoDatWsDcP78eUqXLk1wcLDF/Pj4eEJCQmjevHmmtm8ymYzbioo8buqmknNdvxOpuyjmUDpuRKxbehv5ctVFcGXKlAES702f3LFjxwAoUaIE3t7eHDlyhNDQUGO+r68vkZGReHt7P7FaRURERCRnylUtwFWqVKFFixZ89dVX3Llzh2rVqnHhwgW+/fZbqlatSrNmzahTpw4rVqxg6NChDBgwgLCwMGbNmkWjRo1StByLiIiIiPXJVQEYYOrUqXz33XesXr2a+fPn4+HhQadOnRgwYAB58uTB1dWVefPmMX36dMaPH4+TkxMtW7Zk5MiR2V26iIiIiOQAuS4A29nZMXjwYAYPHpzmMhUqVGDu3LlPsCoRERERyS1yVR9gEREREZHMUgAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVyZOZJ1+6dIlr164RGhpKnjx5KFiwIOXKlaNAgQJZVZ+IiIiISJZ65AB88uRJ1qxZg6+vLzdu3Eh1mVKlSvHcc8/RqVMnypUrl+kiRURERESySroDsJ+fH7NmzeLkyZMAmM3mNJcNCgri4sWLLF26lJo1azJy5Ei8vLwyX62IiIiISCalKwBPnTqV9evXk5CQAECZMmV49tlnqVixIkWKFMHJyQmAO3fucOPGDc6ePcvp06e5cOECR48epW/fvnTo0IGJEyc+vj0REREREUmHdAXgtWvX4u7uzosvvkirVq0oXbp0ulZ+69Ytfv/9d1avXs2mTZsUgEVEREQk26UrAH/++ec0bdoUG5tHGzTCzc2Nl19+mZdffhlfX98MFSgiIiIikpXSFYCbN2+e6Q15e3tneh0iIiIiIpmVqWHQAMLDw/nmm2/Yu3cvt27dwt3dnXbt2tG3b1/s7OyyokYRERERkSyT6QD80UcfsXPnTuNxcHAwCxcuJCoqihEjRmR29SIiIiIiWSpTATg2NpZdu3bRokULXn/9dQoWLEh4eDjr1q3jt99+UwAWERERkRwnXVe1TZ06lZs3b6aYHh0dTUJCAuXKleOZZ56hRIkSVKlShWeeeYbo6OgsL1ZEREREJLPSPQzali1b6NmzJ2+88YZxq2NnZ2cqVqzId999x9KlS8mfPz+RkZFERETQtGnTx1q4iIiIiEhGpKsF+MMPP8TNzY0lS5bQpUsXFi1axL1794x5ZcqUISoqiuvXrxMeHk716tUZM2bMYy1cRERERCQj0tUC3KFDB9q0acPq1av5/vvvmTt3LitWrKB///5069aNFStWcOXKFf7991/c3d1xd3d/3HWLiIiIiGRIuu9skSdPHnr27MnatWv573//S0xMDJ9//jk9evTgt99+w9PTk2rVqin8ioiIiEiO9mi3dgMcHBzo168f69at4/XXX+fGjRtMmDCBV199lX379j2OGkVEREREsky6A/CtW7fYtGkTS5Ys4bfffsNkMjFs2DDWrl1Lt27d+Oeffxg1ahQDBw7k+PHjj7NmEREREZEMS1cf4EOHDjF69GiioqKMaa6ursyfP58yZcrw3nvv8frrr/PNN9+wfft2+vfvT5MmTZg+ffpjK1xEREREJCPS1QI8a9Ys8uTJQ+PGjWnbti1NmzYlT548zJ0711imRIkSTJ06lZ9++omGDRuyd+/ex1a0iIiIiEhGpasFODAwkFmzZlGzZk1j2t27d+nfv3+KZStVqsTMmTPx8/PLqhpFRERERLJMugKwh4cHkydPplGjRjg7OxMVFYWfnx/FihVL8znJw7KIiIiISE6RrgDcr18/Jk6cyPLlyzGZTJjNZuzs7Cy6QIiIiIiI5AbpCsDt2rWjbNmy7Nq1y7jZRZs2bShRosTjrk9EREREJEulKwADVK5cmcqVKz/OWkREREREHrt0jQIxevRoDh48mOGNnDp1ivHjx2f4+fc7ceIEgwYNokmTJrRp04aJEyfy77//GvODg4MZNWoUzZo1o2XLlnzyySeEh4dn2fZFREREJPdKVwvwnj172LNnDyVKlKBly5Y0a9aMqlWrYmOTen6Oi4vj2LFjHDx4kD179nDu3DkApkyZkumC/f39GTx4MPXr1+eLL77gxo0bzJkzh+DgYL7//nvu3r3L4MGDcXNzY9KkSYSGhjJr1ixCQkKYPXt2prcvIiIiIrlbugLwggUL+Oyzzzh79iyLFy9m8eLF2NnZUbZsWYoUKYKTkxMmk4nIyEiuXr3KxYsXiY6OBsBsNlOlShVGjx6dJQXPmjWLypUr8+WXXxoB3MnJiS+//JLLly+zbds2wsLCWLp0KQULFgTA3d2dESNG4Ofnp9EpRERERKxcugJwjRo1+Omnn/jjjz9YsmQJ/v7+xMTEEBAQwJkzZyyWNZvNAJhMJurXr0/37t1p1qwZJpMp08Xevn2bw4cPM2nSJIvW5xYtWtCiRQsAfHx8qFWrlhF+Aby9vXFycmLfvn0KwCIiIiJWLt0XwdnY2NC6dWtat25NSEgI+/fv59ixY9y4ccPof1uoUCFKlChBzZo1qVevHkWLFs3SYs+dO0dCQgKurq6MHz+e3bt3Yzabad68OWPGjCF//vwEBgbSunVri+fZ2tri6elJUFBQprZvNpuJjIzM1DpyApPJRL58+bK7DHmIqKgo4wel5Aw6dnI+HTci1s1sNqer0TXdATg5T09PevToQY8ePTLy9AwLDQ0F4KOPPqJRo0Z88cUXXLx4ka+//prLly+zcOFCwsPDcXJySvFcR0dHIiIiMrX92NhY/P39M7WOnCBfvnx4eXlldxnyEP/88w9RUVHZXYYko2Mn59NxIyL29vYPXSZDATi7xMbGAlClShU++OADAOrXr0/+/Pl5//33OXDgAAkJCWk+P62L9tLLzs6OChUqZGodOUFWdEeRx69s2bJqycphdOzkfDpuRKxb0sALD5OrArCjoyMAzz33nMX0Ro0aAXD69GmcnZ1T7aYQERGBu7t7prZvMpmMGkQeN51qF3l0Om5ErFt6Gyoy1yT6hJUqVQqAmJgYi+lxcXEAODg4ULp0aYKDgy3mx8fHExISQpkyZZ5InSIiIiKSc+WqAFy2bFk8PT3Ztm2bxSmuXbt2AVCzZk28vb05cuSI0V8YwNfXl8jISLy9vZ94zSIiIiKSs+SqAGwymRg+fDgnTpxg3LhxHDhwgOXLlzN9+nRatGhBlSpV6NGjB3nz5mXo0KHs3LmTtWvX8sEHH9CoUSNq1KiR3bsgIiIiItksQ32AT548SbVq1bK6lnRp1aoVefPmZcGCBYwaNYoCBQrQvXt3/vvf/wLg6urKvHnzmD59OuPHj8fJyYmWLVsycuTIbKlXRERERHKWDAXgvn37UrZsWV544QU6dOhAkSJFsrquB3ruuedSXAiXXIUKFZg7d+4TrEhEREREcosMd4EIDAzk66+/pmPHjrz11lv89ttvxu2PRURERERyqgy1APfp04c//viDS5cuYTabOXjwIAcPHsTR0ZHWrVvzwgsv6JbDIiIiIpIjZSgAv/XWW7z11lsEBATw+++/88cffxAcHExERATr1q1j3bp1eHp60rFjRzp27IiHh0dW1y0iIiIikiGZuhFG5cqVqVy5MkOHDuXMmTOsXLmSdevWARASEsK3337LwoUL6d69O6NHj870ndhEREREskp0dDTPP/888fHxFtPz5cvHnj17ADh16hQzZszA398fJycnOnXqxMCBA7Gzs3vgun19fZk7dy7nz5/Hzc2Nl156id69e+uOkjlEpu8Ed/fuXf744w+2b9/O4cOHMZlMmM1mY5ze+Ph4Vq1aRYECBRg0aFCmCxYRERHJCufPnyc+Pp7JkydTokQJY3pSg92lS5cYMmQI1atX55NPPiEwMJC5c+cSFhbGuHHj0lzviRMnGDlyJK1bt2bw4MH4+fkxa9Ys4uPjeeONNx73bkk6ZCgAR0ZG8ueff7Jt2zYOHjxo3InNbDZjY2NDgwYN6Ny5MyaTidmzZxMSEsLWrVsVgEVERCTHOHPmDLa2trRs2RJ7e/sU8xcvXoyTkxNffvkldnZ2NGnSBAcHBz7//HP69euXZhfP+fPnU7lyZSZPngxAo0aNiIuLY9GiRfTq1QsHB4fHul/ycBkKwK1btyY2NhbAaOn19PSkU6dOKfr8uru78+abb3L9+vUsKFdEREQkawQEBFCmTJlUwy8kdmNo3LixRXeHli1b8umnn+Lj40O3bt1SPCcmJobDhw+naPRr2bIlP/74I35+frozbQ6QoQAcExMDgL29PS1atKBLly7UrVs31WU9PT0ByJ8/fwZLFBEREcl6SS3AQ4cO5dixY9jb2xs3z7K1teXKlSuUKlXK4jmurq44OTkRFBSU6jovX75MbGxsiueVLFkSgKCgIAXgHCBDAbhq1ap07tyZdu3a4ezs/MBl8+XLx9dff03x4sUzVKCIiIhIVjObzZw7dw6z2UzXrl158803OXXqFAsWLOCff/7hk08+AUg15zg5OREREZHqesPDw41lknN0dARI83nyZGUoAP/4449AYl/g2NhY49RAUFAQhQsXtvijOzk5Ub9+/SwoVURERCRrmM1mvvzyS1xdXSlfvjwAtWvXxs3NjQ8++IBDhw498PlpjeaQkJDwwOdpRKycIcN/hXXr1tGxY0dOnDhhTPvpp59o374969evz5LiRERERB4HGxsb6tata4TfJE2aNAESuzJA6i22ERERaZ4BT5oeGRmZ4jnJ50v2ylAA3rdvH1OmTCE8PJxz584Z0wMDA4mKimLKlCkcPHgwy4oUERERyUo3btxgzZo1XL161WJ6dHQ0AIULF8bd3Z1Lly5ZzP/333+JiIigbNmyqa63RIkS2NraEhwcbDE96XGZMmWyaA8kMzIUgJcuXQpAsWLFLH45vfbaa5QsWRKz2cySJUuypkIRERGRLBYfH8/UqVP59ddfLaZv27YNW1tbatWqRYMGDdizZ49x8T/Ajh07sLW1pV69eqmuN2/evNSqVYudO3caI2UlPc/Z2Zlq1ao9nh2SR5KhPsDnz5/HZDIxYcIE6tSpY0xv1qwZLi4uDBw4kLNnz2ZZkSIiIiJZycPDg06dOrFkyRLy5s1L9erV8fPzY9GiRfTs2ZPSpUvTp08ftm3bxvDhw3nttdcICgpi7ty5dOvWzRjyNSYmhoCAANzd3SlatCgAb775JkOGDOHdd9+lc+fOHD9+nCVLlvDWW29pDOAcIkMtwElXOLq6uqaYlzTc2d27dzNRloiIiMjj9d5779G/f382b97MyJEj2bx5M4MGDWLUqFFAYneFOXPmcO/ePcaOHcvPP//Mq6++yjvvvGOs4+bNm/Tt25e1a9ca0+rVq8fnn39OUFAQ77zzDlu3bmXEiBH06dPnSe+ipCFDLcBFixbl0qVLrF692uJNYDabWb58ubGMiIiISE5lb29P//796d+/f5rL1KpVix9++CHN+Z6enqmOGNG8eXOaN2+eFWXKY5ChANysWTOWLFnCypUr8fX1pWLFisTFxXHmzBmuXLmCyWSiadOmWV2riIiIiEimZSgA9+vXjz///JPg4GAuXrzIxYsXjXlms5mSJUvy5ptvZlmRIiIiIiJZJUN9gJ2dnVm0aBFdu3bF2dkZs9mM2WzGycmJrl278v3332ucOxERERHJkTLUAgzg4uLC+++/z7hx47h9+zZmsxlXV9c074wiIiIiIpITZPp+fCaTCVdXVwoVKmSE34SEBPbv35/p4kREREREslqGWoDNZjPff/89u3fv5s6dOxb3vY6Li+P27dvExcVx4MCBLCtURERERCQrZCgAr1ixgnnz5mEymSzucgIY09QVQkRERERyogx1gdi0aRMA+fLlo2TJkphMJp555hnKli1rhN+xY8dmaaEiIiKSeyXc12AmOYc1/m0y1AJ86dIlTCYTn332Ga6urvTu3ZtBgwbRsGFDvvrqK37++WcCAwOzuFQRERHJrWxMJpb7nuH6ncjsLkWScS/gSC/vStldxhOXoQAcHR0NQKlSpShWrBiOjo6cPHmShg0b0q1bN37++Wf27dvH6NGjs7RYERERyb2u34kkJDQiu8sQyVgXiEKFCgEQEBCAyWSiYsWK7Nu3D0hsHQa4fv16FpUoIiIiIpJ1MhSAa9Sogdls5oMPPiA4OJhatWpx6tQpevbsybhx44D/C8kiIiIiIjlJhgJw//79KVCgALGxsRQpUoS2bdtiMpkIDAwkKioKk8lEq1atsrpWEREREZFMy1AALlu2LEuWLGHAgAE4ODhQoUIFJk6cSNGiRSlQoABdunRh0KBBWV2riIiIiEimZegiuH379lG9enX69+9vTOvQoQMdOnTIssJERERERB6HDLUAT5gwgXbt2rF79+6srkdERERE5LHKUAC+d+8esbGxlClTJovLERERERF5vDIUgFu2bAnAzp07s7QYEREREZHHLUN9gCtVqsTevXv5+uuvWb16NeXKlcPZ2Zk8ef5vdSaTiQkTJmRZoSIiIiIiWSFDAXjmzJmYTCYArly5wpUrV1JdTgFYRERERHKaDAVgALPZ/MD5SQFZRERERCQnyVAAXr9+fVbXISIiIiLyRGQoABcrViyr6xAREREReSIyFICPHDmSruVq166dkdWLiIiIiDw2GQrAgwYNemgfX5PJxIEDBzJUlIiIiIjI4/LYLoITEREREcmJMhSABwwYYPHYbDYTExPD1atX2blzJ1WqVKFfv35ZUqCIiIiISFbKUAAeOHBgmvN+//13xo0bx927dzNclIiIiIjI45KhWyE/SIsWLQBYtmxZVq9aRERERCTTsjwA//XXX5jNZs6fP5/VqxYRERERybQMdYEYPHhwimkJCQmEh4dz4cIFAAoVKpS5ykREREREHoMMBeDDhw+nOQxa0ugQHTt2zHhVIiIiIiKPSZYOg2ZnZ0eRIkVo27Yt/fv3z1Rh6TVmzBhOnz7Nhg0bjGnBwcFMnz6do0ePYmtrS6tWrRg2bBjOzs5PpCYRERERybkyFID/+uuvrK4jQzZv3szOnTstbs189+5dBg8ejJubG5MmTSI0NJRZs2YREhLC7Nmzs7FaEREREckJMtwCnJrY2Fjs7OyycpVpunHjBl988QVFixa1mP7LL78QFhbG0qVLKViwIADu7u6MGDECPz8/atas+UTqExEREZGcKcOjQAQEBDBkyBBOnz5tTJs1axb9+/fn7NmzWVLcg0yePJkGDRpQr149i+k+Pj7UqlXLCL8A3t7eODk5sW/fvsdel4iIiIjkbBkKwBcuXGDQoEEcOnTIIuwGBgZy7NgxBg4cSGBgYFbVmMLatWs5ffo0Y8eOTTEvMDCQUqVKWUyztbXF09OToKCgx1aTiIiIiOQOGeoC8f333xMREYG9vb3FaBBVq1blyJEjRERE8MMPPzBp0qSsqtNw5coVvvrqKyZMmGDRypskPDwcJyenFNMdHR2JiIjI1LbNZjORkZGZWkdOYDKZyJcvX3aXIQ8RFRWV6sWmkn107OR8Om5yJh07Od/TcuyYzeY0RypLLkMB2M/PD5PJxPjx42nfvr0xfciQIVSoUIH333+fo0ePZmTVD2Q2m/noo49o1KgRLVu2THWZhISENJ9vY5O5+37Exsbi7++fqXXkBPny5cPLyyu7y5CH+Oeff4iKisruMiQZHTs5n46bnEnHTs73NB079vb2D10mQwH433//BaBatWop5lWuXBmAmzdvZmTVD7Ry5UrOnj3L8uXLiYuLA/5vOLa4uDhsbGxwdnZOtZU2IiICd3f3TG3fzs6OChUqZGodOUF6fhlJ9itbtuxT8Wv8aaJjJ+fTcZMz6djJ+Z6WY+fcuXPpWi5DAdjFxYVbt27x119/UbJkSYt5+/fvByB//vwZWfUD/fHHH9y+fZt27dqlmOft7c2AAQMoXbo0wcHBFvPi4+MJCQmhefPmmdq+yWTC0dExU+sQSS+dLhR5dDpuRDLmaTl20vtjK0MBuG7dumzdupUvv/wSf39/KleuTFxcHKdOnWL79u2YTKYUozNkhXHjxqVo3V2wYAH+/v5Mnz6dIkWKYGNjw48//khoaCiurq4A+Pr6EhkZibe3d5bXJCIiIiK5S4YCcP/+/dm9ezdRUVGsW7fOYp7ZbCZfvny8+eabWVJgcmXKlEkxzcXFBTs7O6NvUY8ePVixYgVDhw5lwIABhIWFMWvWLBo1akSNGjWyvCYRERERyV0ydFVY6dKlmT17NqVKlcJsNlv8K1WqFLNnz041rD4Jrq6uzJs3j4IFCzJ+/Hjmzp1Ly5Yt+eSTT7KlHhERERHJWTJ8J7jq1avzyy+/EBAQQHBwMGazmZIlS1K5cuUn2tk9taHWKlSowNy5c59YDSIiIiKSe2TqVsiRkZGUK1fOGPkhKCiIyMjIVMfhFRERERHJCTI8MO66devo2LEjJ06cMKb99NNPtG/fnvXr12dJcSIiIiIiWS1DAXjfvn1MmTKF8PBwi/HWAgMDiYqKYsqUKRw8eDDLihQRERERySoZCsBLly4FoFixYpQvX96Y/tprr1GyZEnMZjNLlizJmgpFRERERLJQhvoAnz9/HpPJxIQJE6hTp44xvVmzZri4uDBw4EDOnj2bZUWKiIiIiGSVDLUAh4eHAxg3mkgu6Q5wd+/ezURZIiIiIiKPR4YCcNGiRQFYvXq1xXSz2czy5cstlhERERERyUky1AWiWbNmLFmyhJUrV+Lr60vFihWJi4vjzJkzXLlyBZPJRNOmTbO6VhERERGRTMtQAO7Xrx9//vknwcHBXLx4kYsXLxrzkm6I8ThuhSwiIiIiklkZ6gLh7OzMokWL6Nq1K87OzsZtkJ2cnOjatSvff/89zs7OWV2riIiIiEimZfhOcC4uLrz//vuMGzeO27dvYzabcXV1faK3QRYREREReVQZvhNcEpPJhKurK4UKFcJkMhEVFcWaNWv4z3/+kxX1iYiIiIhkqQy3AN/P39+f1atXs23bNqKiorJqtSIiIiIiWSpTATgyMpItW7awdu1aAgICjOlms1ldIUREREQkR8pQAP77779Zs2YN27dvN1p7zWYzALa2tjRt2pTu3btnXZUiIiIiIlkk3QE4IiKCLVu2sGbNGuM2x0mhN4nJZGLjxo0ULlw4a6sUEREREcki6QrAH330Eb///jv37t2zCL2Ojo60aNECDw8PFi5cCKDwKyIiIiI5WroC8IYNGzCZTJjNZvLkyYO3tzft27enadOm5M2bFx8fn8ddp4iIiIhIlnikYdBMJhPu7u5Uq1YNLy8v8ubN+7jqEhERERF5LNLVAlyzZk38/PwAuHLlCvPnz2f+/Pl4eXnRrl073fVNRERERHKNdAXgBQsWcPHiRdauXcvmzZu5desWAKdOneLUqVMWy8bHx2Nra5v1lYqIiIiIZIF0d4EoVaoUw4cPZ9OmTUybNo0mTZoY/YKTj/vbrl07ZsyYwfnz5x9b0SIiIiIiGfXI4wDb2trSrFkzmjVrxs2bN1m/fj0bNmzg0qVLAISFhfHzzz+zbNkyDhw4kOUFi4iIiIhkxiNdBHe/woUL069fP9asWcM333xDu3btsLOzM1qFRURERERymkzdCjm5unXrUrduXcaOHcvmzZtZv359Vq1aRERERCTLZFkATuLs7EzPnj3p2bNnVq9aRERERCTTMtUFQkREREQkt1EAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJV8mR3AY8qISGB1atX88svv3D58mUKFSrE888/z6BBg3B2dgYgODiY6dOnc/ToUWxtbWnVqhXDhg0z5ouIiIiI9cp1AfjHH3/km2++4fXXX6devXpcvHiRefPmcf78eb7++mvCw8MZPHgwbm5uTJo0idDQUGbNmkVISAizZ8/O7vJFREREJJvlqgCckJDA4sWLefHFF3nrrbcAaNCgAS4uLowbNw5/f38OHDhAWFgYS5cupWDBggC4u7szYsQI/Pz8qFmzZvbtgIiIiIhku1zVBzgiIoIOHTrQtm1bi+llypQB4NKlS/j4+FCrVi0j/AJ4e3vj5OTEvn37nmC1IiIiIpIT5aoW4Pz58zNmzJgU0//8808AypUrR2BgIK1bt7aYb2tri6enJ0FBQU+iTBERERHJwXJVAE7NyZMnWbx4Mc899xwVKlQgPDwcJyenFMs5OjoSERGRqW2ZzWYiIyMztY6cwGQykS9fvuwuQx4iKioKs9mc3WVIMjp2cj4dNzmTjp2c72k5dsxmMyaT6aHL5eoA7Ofnx6hRo/D09GTixIlAYj/htNjYZK7HR2xsLP7+/plaR06QL18+vLy8srsMeYh//vmHqKio7C5DktGxk/PpuMmZdOzkfE/TsWNvb//QZXJtAN62bRsffvghpUqVYvbs2UafX2dn51RbaSMiInB3d8/UNu3s7KhQoUKm1pETpOeXkWS/smXLPhW/xp8mOnZyPh03OZOOnZzvaTl2zp07l67lcmUAXrJkCbNmzaJOnTp88cUXFuP7li5dmuDgYIvl4+PjCQkJoXnz5pnarslkwtHRMVPrEEkvnS4UeXQ6bkQy5mk5dtL7YytXjQIB8OuvvzJz5kxatWrF7NmzU9zcwtvbmyNHjhAaGmpM8/X1JTIyEm9v7yddroiIiIjkMLmqBfjmzZtMnz4dT09PXn75ZU6fPm0xv0SJEvTo0YMVK1YwdOhQBgwYQFhYGLNmzaJRo0bUqFEjmyoXERERkZwiVwXgffv2ER0dTUhICP37908xf+LEiXTq1Il58+Yxffp0xo8fj5OTEy1btmTkyJFPvmARERERyXFyVQDu0qULXbp0eehyFSpUYO7cuU+gIhERERHJbXJdH2ARERERkcxQABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqPNUB2NfXl//85z80btyYzp07s2TJEsxmc3aXJSIiIiLZ6KkNwCdOnGDkyJGULl2aadOm0a5dO2bNmsXixYuzuzQRERERyUZ5sruAx2X+/PlUrlyZyZMnA9CoUSPi4uJYtGgRvXr1wsHBIZsrFBEREZHs8FS2AMfExHD48GGaN29uMb1ly5ZERETg5+eXPYWJiIiISLZ7KgPw5cuXiY2NpVSpUhbTS5YsCUBQUFB2lCUiIiIiOcBT2QUiPDwcACcnJ4vpjo6OAERERDzS+gICAoiJiQHg+PHjWVBh9jOZTNQvlEB8QXUFyWlsbRI4ceKELtjMoXTs5Ew6bnI+HTs509N27MTGxmIymR663FMZgBMSEh4438bm0Ru+k17M9LyouYVTXrvsLkEe4Gl6rz1tdOzkXDpucjYdOznX03LsmEwm6w3Azs7OAERGRlpMT2r5TZqfXpUrV86awkREREQk2z2VfYBLlCiBra0twcHBFtOTHpcpUyYbqhIRERGRnOCpDMB58+alVq1a7Ny506JPy44dO3B2dqZatWrZWJ2IiIiIZKenMgADvPnmm5w8eZJ3332Xffv28c0337BkyRL69u2rMYBFRERErJjJ/LRc9peKnTt3Mn/+fIKCgnB3d+ell16id+/e2V2WiIiIiGSjpzoAi4iIiIjc76ntAiEiIiIikhoFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsVk8jAcrTLrX3uN73ImLNFIAlVwoJCaFu3bps2LAhw8+5e/cuEyZM4OjRo4+rTJHHolOnTkyaNCnVefPnz6du3brGYz8/P0aMGGGxzMKFC1myZMnjLFHEqmTkO0mylwKwWK2AgAA2b95MQkJCdpcikmW6du3KokWLjMdr167ln3/+sVhm3rx5REVFPenSRJ5ahQsXZtGiRTRp0iS7S5F0ypPdBYiISNYpWrQoRYsWze4yRKyKvb09zz77bHaXIY9ALcCS7e7du8ecOXPo1q0bDRs2pGnTpgwZMoSAgABjmR07dvDKK6/QuHFjXnvtNc6cOWOxjg0bNlC3bl1CQkIspqd1qvjQoUMMHjwYgMGDBzNw4MCs3zGRJ2TdunXUq1ePhQsXWnSBmDRpEhs3buTKlSvG6dmkeQsWLLDoKnHu3DlGjhxJ06ZNadq0Ke+88w6XLl0y5h86dIi6dety8OBBhg4dSuPGjWnbti2zZs0iPj7+ye6wyCPw9/fnv//9L02bNuX5559nyJAhnDhxwph/9OhRBg4cSOPGjWnRogUTJ04kNDTUmL9hwwYaNGjAyZMn6du3L40aNaJjx44W3YhS6wJx8eJF/ve//9G2bVuaNGnCoEGD8PPzS/Gcn376ie7du9O4cWPWr1//eF8MMSgAS7abOHEi69ev54033mDOnDmMGjWKCxcuMH78eMxmM7t372bs2LFUqFCBL774gtatW/PBBx9kaptVqlRh7NixAIwdO5Z33303K3ZF5Inbtm0bU6dOpX///vTv399iXv/+/WncuDFubm7G6dmk7hFdunQx/h8UFMSbb77Jv//+y6RJk/jggw+4fPmyMS25Dz74gFq1ajFjxgzatm3Ljz/+yNq1a5/Ivoo8qvDwcIYNG0bBggX5/PPP+fjjj4mKiuKtt94iPDycI0eO8N///hcHBwc+/fRT3n77bQ4fPsygQYO4d++esZ6EhATeffdd2rRpw8yZM6lZsyYzZ87Ex8cn1e1euHCB119/nStXrjBmzBimTJmCyWRi8ODBHD582GLZBQsW0KdPHz766CMaNGjwWF8P+T/qAiHZKjY2lsjISMaMGUPr1q0BqFOnDuHh4cyYMYNbt26xcOFCnnnmGSZPngxAw4YNAZgzZ06Gt+vs7EzZsmUBKFu2LOXKlcvknog8eXv27GHChAm88cYbDBo0KMX8EiVK4OrqanF61tXVFQB3d3dj2oIFC3BwcGDu3Lk4OzsDUK9ePbp06cKSJUssLqLr2rWrEbTr1avHrl272Lt3L927d3+s+yqSEf/88w+3b9+mV69e1KhRA4AyZcqwevVqIiIimDNnDqVLl+arr77C1tYWgGeffZaePXuyfv16evbsCSSOmtK/f3+6du0KQI0aNdi5cyd79uwxvpOSW7BgAXZ2dsybNw8nJycAmjRpwssvv8zMmTP58ccfjWVbtWpF586dH+fLIKlQC7BkKzs7O2bPnk3r1q25fv06hw4d4tdff2Xv3r1AYkD29/fnueees3heUlgWsVb+/v68++67uLu7G915Muqvv/6idu3aODg4EBcXR1xcHE5OTtSqVYsDBw5YLHt/P0d3d3ddUCc5Vvny5XF1dWXUqFF8/PHH7Ny5Ezc3N4YPH46LiwsnT56kSZMmmM1m471fvHhxypQpk+K9X716deP/9vb2FCxYMM33/uHDh3nuueeM8AuQJ08e2rRpg7+/P5GRkcb0SpUqZfFeS3qoBViynY+PD19++SWBgYE4OTlRsWJFHB0dAbh+/Tpms5mCBQtaPKdw4cLZUKlIznH+/HmaNGnC3r17WblyJb169crwum7fvs327dvZvn17inlJLcZJHBwcLB6bTCaNpCI5lqOjIwsWLOC7775j+/btrF69mrx58/LCCy/Qt29fEhISWLx4MYsXL07x3Lx581o8vv+9b2Njk+Z42mFhYbi5uaWY7ubmhtlsJiIiwqJGefIUgCVbXbp0iXfeeYemTZsyY8YMihcvjslkYtWqVezfvx8XFxdsbGxS9EMMCwuzeGwymQBSfBEn/5Ut8jRp1KgRM2bM4L333mPu3Lk0a9YMDw+PDK0rf/781K9fn969e6eYl3RaWCS3KlOmDJMnTyY+Pp6///6bzZs388svv+Du7o7JZOLVV1+lbdu2KZ53f+B9FC4uLty6dSvF9KRpLi4u3Lx5M8Prl8xTFwjJVv7+/kRHR/PGG29QokQJI8ju378fSDxlVL16dXbs2GHxS3v37t0W60k6zXTt2jVjWmBgYIqgnJy+2CU3K1SoEACjR4/GxsaGTz/9NNXlbGxSfszfP6127dr8888/VKpUCS8vL7y8vKhatSpLly7lzz//zPLaRZ6U33//nVatWnHz5k1sbW2pXr067777Lvnz5+fWrVtUqVKFwMBA433v5eVFuXLlmD9/foqL1R5F7dq12bNnj0VLb3x8PL/99hteXl7Y29tnxe5JJigAS7aqUqUKtra2zJ49G19fX/bs2cOYMWOMPsD37t1j6NChXLhwgTFjxrB//36WLVvG/PnzLdZTt25d8ubNy4wZM9i3bx/btm1j9OjRuLi4pLnt/PnzA7Bv374Uw6qJ5BaFCxdm6NCh7N27l61bt6aYnz9/fv7991/27dtntDjlz5+fY8eOceTIEcxmMwMGDCA4OJhRo0bx559/4uPjw//+9z+2bdtGxYoVn/QuiWSZmjVrkpCQwDvvvMOff/7JX3/9xdSpUwkPD6dly5YMHToUX19fxo8fz969e9m9ezfDhw/nr7/+okqVKhne7oABA4iOjmbw4MH8/vvv7Nq1i2HDhnH58mWGDh2ahXsoGaUALNmqZMmSTJ06lWvXrjF69Gg+/vhjIPF2riaTiaNHj1KrVi1mzZrF9evXGTNmDKtXr2bChAkW68mfPz/Tpk0jPj6ed955h3nz5jFgwAC8vLzS3Ha5cuVo27YtK1euZPz48Y91P0Uep+7du/PMM8/w5Zdfpjjr0alTJ4oVK8bo0aPZuHEjAH379sXf35/hw4dz7do1KlasyMKFCzGZTEycOJGxY8dy8+ZNvvjiC1q0aJEduySSJQoXLszs2bNxdnZm8uTJjBw5koCAAD7//HPq1q2Lt7c3s2fP5tq1a4wdO5YJEyZga2vL3LlzM3Vji/Lly7Nw4UJcXV356KOPjO+s+fPna6izHMJkTqsHt4iIiIjIU0gtwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWJU82V2AiMjTYMCAARw9ehRIvPnExIkTs7milM6dO8evv/7KwYMHuXnzJjExMbi6ulK1alU6d+5M06ZNs7tEEZEnQjfCEBHJpKCgILp37248dnBwYOvWrTg7O2djVZZ++OEH5s2bR1xcXJrLtG/fng8//BAbG50cFJGnmz7lREQyad26dRaP7927x+bNm7OpmpRWrlzJnDlziIuLo2jRoowbN45Vq1axfPlyRo4ciZOTEwBbtmzh559/zuZqRUQeP7UAi4hkQlxcHC+88AK3bt3C09OTa9euER8fT6VKlXJEmLx58yadOnUiNjaWokWL8uOPP+Lm5maxzL59+xgxYgQARYoUYfPmzZhMpuwoV0TkiVAfYBGRTNi7dy+3bt0CoHPnzpw8eZK9e/dy5swZTp48SbVq1VI8JyQkhDlz5uDr60tsbCy1atXi7bff5uOPP+bIkSPUrl2bb7/91lg+MDCQ+fPn89dffxEZGUmxYsVo3749r7/+Onnz5n1gfRs3biQ2NhaA/v37pwi/AI0bN2bkyJF4enri5eVlhN8NGzbw4YcfAjB9+nQWL17MqVOncHV1ZcmSJbi5uREbG8vy5cvZunUrwcHBAJQvX56uXbvSuXNniyA9cOBAjhw5AsChQ4eM6YcOHWLw4MFAYl/qQYMGWSxfqVIlPvvsM2bOnMlff/2FyWSiYcOGDBs2DE9Pzwfuv4hIahSARUQyIXn3h7Zt21KyZEn27t0LwOrVq1ME4CtXrtCnTx9CQ0ONafv37+fUqVOp9hn++++/GTJkCBEREca0oKAg5s2bx8GDB5k7dy558qT9UZ4UOAG8vb3TXK53794P2EuYOHEid+/eBcDNzQ03NzciIyMZOHAgp0+ftlj2xIkTnDhxgn379vHJJ59ga2v7wHU/TGhoKH379uX27dvGtO3bt3PkyBEWL16Mh4dHptYvItZHfYBFRDLoxo0b7N+/HwAvLy9KlixJ06ZNjT6127dvJzw83OI5c+bMMcJv+/btWbZsGd988w2FChXi0qVLFsuazWY++ugjIiIiKFiwINOmTePXX39lzJgx2NjYcOTIEVasWPHAGq9du2b8v0iRIhbzbt68ybVr11L8i4mJSbGe2NhYpk+fzs8//8zbb78NwIwZM4zw26ZNG3766Se+//57GjRoAMCOHTtYsmTJg1/EdLhx4wYFChRgzpw5LFu2jPbt2wNw69YtZs+enen1i4j1UQAWEcmgDRs2EB8fD0C7du2AxBEgmjdvDkBUVBRbt241lk9ISDBah4sWLcrEiROpWLEi9erVY+rUqSnWf/bsWc6fPw9Ax44d8fLywsHBgWbNmlG7dm0ANm3a9MAak4/ocP8IEP/5z3944YUXUvw7fvx4ivW0atWK559/nkqVKlGrVi0iIiKMbZcvX57JkydTpUoVqlevzhdffGF0tXhYQE+vDz74AG9vbypWrMjEiRMpVqwYAHv27DH+BiIi6aUALCKSAWazmfXr1xuPnZ2d2b9/P/v377c4Jb9mzRrj/6GhoUZXBi8vL4uuCxUrVjRajpNcvHjR+P9PP/1kEVKT+tCeP38+1RbbJEWLFjX+HxIS8qi7aShfvnyK2qKjowGoW7euRTeHfPnyUb16dSCx9TZ514WMMJlMFl1J8uTJg5eXFwCRkZGZXr+IWB/1ARYRyYDDhw9bdFn46KOPUl0uICCAv//+m2eeeQY7OztjenoG4ElP39n4+Hju3LlD4cKFU51fv359o9V57969lCtXzpiXfKi2SZMmsXHjxjS3c3//5IfV9rD9i4+PN9aRFKQftK64uLg0Xz+NWCEij0otwCIiGXD/2L8PktQKXKBAAfLnzw+Av7+/RZeE06dPW1zoBlCyZEnj/0OGDOHQoUPGv59++omtW7dy6NChNMMvJPbNdXBwAGDx4sVptgLfv+373X+hXfHixbG3twcSR3FISEgw5kVFRXHixAkgsQW6YMGCAMby92/v6tWrD9w2JP7gSBIfH09AQACQGMyT1i8ikl4KwCIij+ju3bvs2LEDABcXF3x8fCzC6aFDh9i6davRwrlt2zYj8LVt2xZIvDjtww8/5Ny5c/j6+vL++++n2E758uWpVKkSkNgF4rfffuPSpUts3ryZPn360K5dO8aMGfPAWgsXLsyoUaMACAsLo2/fvqxatYrAwEACAwPZunUrgwYNYufOnY/0Gjg5OdGyZUsgsRvGhAkTOH36NCdOnOB///ufMTRcz549jeckvwhv2bJlJCQkEBAQwOLFix+6vU8//ZQ9e/Zw7tw5Pv30Uy5fvgxAs2bNdOc6EXlk6gIhIvKItmzZYpy279Chg8Wp+SSFCxemadOm7Nixg8jISLZu3Ur37t3p168fO3fu5NatW2zZsoUtW7YA4OHhQb58+YiKijJO6ZtMJkaPHs3w4cO5c+dOipDs4uJijJn7IN27dyc2NpaZM2dy69YtPvvss1SXs7W1pUuXLkb/2ocZM2YMZ86c4fz582zdutXigj+AFi1aWAyv1rZtWzZs2ADAggULWLhwIWazmWefffah/ZPNZrMR5JMUKVKEt956K121iogkp5/NIiKPKHn3hy5duqS5XPfu3Y3/J3WDcHd357vvvqN58+Y4OTnh5OREixYtWLhwodFFIHlXgTp16vDDDz/QunVr3NzcsLOzo2jRonTq1IkffviBChUqpKvmXr16sWrVKvr27UvlypVxcXHBzs6OwoULU79+fd566y02bNjAuHHjcHR0TNc6CxQowJIlSxgxYgRVq1bF0dERBwcHqlWrxvjx4/nss88s+gp7e3szefJkypcvj729PcWKFWPAgAF89dVXD91W0muWL18+nJ2dadOmDYsWLXpg9w8RkbToVsgiIk+Qr68v9vb2uLu74+HhYfStTUhI4LnnniM6Opo2bdrw8ccfZ3Ol2S+tO8eJiGSWukCIiDxBK1asYM+ePQB07dqVPn36EBMTw8aNG41uFentgiAiIhmjACwi8gS9/PLL7Nu3j4SEBNauXcvatWst5hctWpTOnTtnT3EiIlZCfYBFRJ4gb29v5s6dy3PPPYebmxu2trbY29tTokQJunfvzg8//ECBAgWyu0wRkaea+gCLiIiIiFVRC7CIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYlf8HcBQvxbITfpMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          550            411  74.727273\n",
      "1           kitten          109             93  85.321101\n",
      "2           senior          178            100  56.179775\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      168     78.87\n",
      "1          M    338      219     64.79\n",
      "2          X    286      217     75.87\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group distribution: Counter({0: 1030, 2: 975, 1: 658})\n",
      "Epoch 1/1500\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 1.1552 - accuracy: 0.4882\n",
      "Epoch 2/1500\n",
      "42/42 [==============================] - 0s 915us/step - loss: 1.0018 - accuracy: 0.5663\n",
      "Epoch 3/1500\n",
      "42/42 [==============================] - 0s 808us/step - loss: 0.9380 - accuracy: 0.5828\n",
      "Epoch 4/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.8790 - accuracy: 0.6196\n",
      "Epoch 5/1500\n",
      "42/42 [==============================] - 0s 805us/step - loss: 0.8540 - accuracy: 0.6158\n",
      "Epoch 6/1500\n",
      "42/42 [==============================] - 0s 771us/step - loss: 0.8150 - accuracy: 0.6564\n",
      "Epoch 7/1500\n",
      "42/42 [==============================] - 0s 793us/step - loss: 0.7764 - accuracy: 0.6665\n",
      "Epoch 8/1500\n",
      "42/42 [==============================] - 0s 781us/step - loss: 0.7491 - accuracy: 0.6669\n",
      "Epoch 9/1500\n",
      "42/42 [==============================] - 0s 779us/step - loss: 0.7626 - accuracy: 0.6729\n",
      "Epoch 10/1500\n",
      "42/42 [==============================] - 0s 797us/step - loss: 0.7262 - accuracy: 0.6868\n",
      "Epoch 11/1500\n",
      "42/42 [==============================] - 0s 804us/step - loss: 0.6972 - accuracy: 0.7033\n",
      "Epoch 12/1500\n",
      "42/42 [==============================] - 0s 776us/step - loss: 0.6942 - accuracy: 0.6955\n",
      "Epoch 13/1500\n",
      "42/42 [==============================] - 0s 781us/step - loss: 0.6909 - accuracy: 0.7011\n",
      "Epoch 14/1500\n",
      "42/42 [==============================] - 0s 759us/step - loss: 0.6937 - accuracy: 0.7037\n",
      "Epoch 15/1500\n",
      "42/42 [==============================] - 0s 811us/step - loss: 0.6736 - accuracy: 0.7071\n",
      "Epoch 16/1500\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.6579 - accuracy: 0.7217\n",
      "Epoch 17/1500\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.6400 - accuracy: 0.7289\n",
      "Epoch 18/1500\n",
      "42/42 [==============================] - 0s 739us/step - loss: 0.6371 - accuracy: 0.7338\n",
      "Epoch 19/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.6209 - accuracy: 0.7334\n",
      "Epoch 20/1500\n",
      "42/42 [==============================] - 0s 727us/step - loss: 0.6276 - accuracy: 0.7330\n",
      "Epoch 21/1500\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.6142 - accuracy: 0.7315\n",
      "Epoch 22/1500\n",
      "42/42 [==============================] - 0s 734us/step - loss: 0.5972 - accuracy: 0.7518\n",
      "Epoch 23/1500\n",
      "42/42 [==============================] - 0s 696us/step - loss: 0.6009 - accuracy: 0.7323\n",
      "Epoch 24/1500\n",
      "42/42 [==============================] - 0s 707us/step - loss: 0.5922 - accuracy: 0.7492\n",
      "Epoch 25/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.5869 - accuracy: 0.7416\n",
      "Epoch 26/1500\n",
      "42/42 [==============================] - 0s 734us/step - loss: 0.5750 - accuracy: 0.7552\n",
      "Epoch 27/1500\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.5668 - accuracy: 0.7522\n",
      "Epoch 28/1500\n",
      "42/42 [==============================] - 0s 711us/step - loss: 0.5722 - accuracy: 0.7567\n",
      "Epoch 29/1500\n",
      "42/42 [==============================] - 0s 711us/step - loss: 0.5688 - accuracy: 0.7567\n",
      "Epoch 30/1500\n",
      "42/42 [==============================] - 0s 723us/step - loss: 0.5601 - accuracy: 0.7627\n",
      "Epoch 31/1500\n",
      "42/42 [==============================] - 0s 715us/step - loss: 0.5573 - accuracy: 0.7585\n",
      "Epoch 32/1500\n",
      "42/42 [==============================] - 0s 717us/step - loss: 0.5543 - accuracy: 0.7552\n",
      "Epoch 33/1500\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.5604 - accuracy: 0.7623\n",
      "Epoch 34/1500\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.5323 - accuracy: 0.7713\n",
      "Epoch 35/1500\n",
      "42/42 [==============================] - 0s 731us/step - loss: 0.5290 - accuracy: 0.7769\n",
      "Epoch 36/1500\n",
      "42/42 [==============================] - 0s 708us/step - loss: 0.5353 - accuracy: 0.7758\n",
      "Epoch 37/1500\n",
      "42/42 [==============================] - 0s 707us/step - loss: 0.5332 - accuracy: 0.7676\n",
      "Epoch 38/1500\n",
      "42/42 [==============================] - 0s 718us/step - loss: 0.5342 - accuracy: 0.7758\n",
      "Epoch 39/1500\n",
      "42/42 [==============================] - 0s 724us/step - loss: 0.5269 - accuracy: 0.7736\n",
      "Epoch 40/1500\n",
      "42/42 [==============================] - 0s 715us/step - loss: 0.5308 - accuracy: 0.7668\n",
      "Epoch 41/1500\n",
      "42/42 [==============================] - 0s 718us/step - loss: 0.4997 - accuracy: 0.7841\n",
      "Epoch 42/1500\n",
      "42/42 [==============================] - 0s 710us/step - loss: 0.5081 - accuracy: 0.7901\n",
      "Epoch 43/1500\n",
      "42/42 [==============================] - 0s 720us/step - loss: 0.5188 - accuracy: 0.7754\n",
      "Epoch 44/1500\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.5081 - accuracy: 0.7799\n",
      "Epoch 45/1500\n",
      "42/42 [==============================] - 0s 719us/step - loss: 0.4989 - accuracy: 0.7908\n",
      "Epoch 46/1500\n",
      "42/42 [==============================] - 0s 738us/step - loss: 0.4968 - accuracy: 0.7905\n",
      "Epoch 47/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.4913 - accuracy: 0.7938\n",
      "Epoch 48/1500\n",
      "42/42 [==============================] - 0s 802us/step - loss: 0.4824 - accuracy: 0.7897\n",
      "Epoch 49/1500\n",
      "42/42 [==============================] - 0s 832us/step - loss: 0.4930 - accuracy: 0.7863\n",
      "Epoch 50/1500\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.4713 - accuracy: 0.8025\n",
      "Epoch 51/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.4856 - accuracy: 0.7897\n",
      "Epoch 52/1500\n",
      "42/42 [==============================] - 0s 797us/step - loss: 0.4771 - accuracy: 0.8032\n",
      "Epoch 53/1500\n",
      "42/42 [==============================] - 0s 870us/step - loss: 0.4789 - accuracy: 0.7942\n",
      "Epoch 54/1500\n",
      "42/42 [==============================] - 0s 823us/step - loss: 0.4854 - accuracy: 0.7935\n",
      "Epoch 55/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.4781 - accuracy: 0.7897\n",
      "Epoch 56/1500\n",
      "42/42 [==============================] - 0s 801us/step - loss: 0.4696 - accuracy: 0.7983\n",
      "Epoch 57/1500\n",
      "42/42 [==============================] - 0s 800us/step - loss: 0.4746 - accuracy: 0.7968\n",
      "Epoch 58/1500\n",
      "42/42 [==============================] - 0s 776us/step - loss: 0.4545 - accuracy: 0.8029\n",
      "Epoch 59/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.4760 - accuracy: 0.8036\n",
      "Epoch 60/1500\n",
      "42/42 [==============================] - 0s 791us/step - loss: 0.4532 - accuracy: 0.8096\n",
      "Epoch 61/1500\n",
      "42/42 [==============================] - 0s 778us/step - loss: 0.4640 - accuracy: 0.8017\n",
      "Epoch 62/1500\n",
      "42/42 [==============================] - 0s 710us/step - loss: 0.4583 - accuracy: 0.8040\n",
      "Epoch 63/1500\n",
      "42/42 [==============================] - 0s 732us/step - loss: 0.4675 - accuracy: 0.8021\n",
      "Epoch 64/1500\n",
      "42/42 [==============================] - 0s 719us/step - loss: 0.4551 - accuracy: 0.8017\n",
      "Epoch 65/1500\n",
      "42/42 [==============================] - 0s 837us/step - loss: 0.4433 - accuracy: 0.8130\n",
      "Epoch 66/1500\n",
      "42/42 [==============================] - 0s 823us/step - loss: 0.4346 - accuracy: 0.8122\n",
      "Epoch 67/1500\n",
      "42/42 [==============================] - 0s 795us/step - loss: 0.4643 - accuracy: 0.8085\n",
      "Epoch 68/1500\n",
      "42/42 [==============================] - 0s 823us/step - loss: 0.4307 - accuracy: 0.8167\n",
      "Epoch 69/1500\n",
      "42/42 [==============================] - 0s 801us/step - loss: 0.4349 - accuracy: 0.8119\n",
      "Epoch 70/1500\n",
      "42/42 [==============================] - 0s 807us/step - loss: 0.4461 - accuracy: 0.8100\n",
      "Epoch 71/1500\n",
      "42/42 [==============================] - 0s 812us/step - loss: 0.4381 - accuracy: 0.8149\n",
      "Epoch 72/1500\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.4329 - accuracy: 0.8216\n",
      "Epoch 73/1500\n",
      "42/42 [==============================] - 0s 812us/step - loss: 0.4272 - accuracy: 0.8231\n",
      "Epoch 74/1500\n",
      "42/42 [==============================] - 0s 796us/step - loss: 0.4158 - accuracy: 0.8250\n",
      "Epoch 75/1500\n",
      "42/42 [==============================] - 0s 795us/step - loss: 0.4282 - accuracy: 0.8220\n",
      "Epoch 76/1500\n",
      "42/42 [==============================] - 0s 802us/step - loss: 0.4165 - accuracy: 0.8224\n",
      "Epoch 77/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8171\n",
      "Epoch 78/1500\n",
      "42/42 [==============================] - 0s 756us/step - loss: 0.4351 - accuracy: 0.8164\n",
      "Epoch 79/1500\n",
      "42/42 [==============================] - 0s 734us/step - loss: 0.4329 - accuracy: 0.8111\n",
      "Epoch 80/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.4428 - accuracy: 0.8137\n",
      "Epoch 81/1500\n",
      "42/42 [==============================] - 0s 745us/step - loss: 0.4187 - accuracy: 0.8194\n",
      "Epoch 82/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.4058 - accuracy: 0.8321\n",
      "Epoch 83/1500\n",
      "42/42 [==============================] - 0s 694us/step - loss: 0.4185 - accuracy: 0.8235\n",
      "Epoch 84/1500\n",
      "42/42 [==============================] - 0s 737us/step - loss: 0.4170 - accuracy: 0.8228\n",
      "Epoch 85/1500\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.4022 - accuracy: 0.8325\n",
      "Epoch 86/1500\n",
      "42/42 [==============================] - 0s 723us/step - loss: 0.4166 - accuracy: 0.8284\n",
      "Epoch 87/1500\n",
      "42/42 [==============================] - 0s 752us/step - loss: 0.4100 - accuracy: 0.8303\n",
      "Epoch 88/1500\n",
      "42/42 [==============================] - 0s 864us/step - loss: 0.3880 - accuracy: 0.8400\n",
      "Epoch 89/1500\n",
      "42/42 [==============================] - 0s 783us/step - loss: 0.4025 - accuracy: 0.8370\n",
      "Epoch 90/1500\n",
      "42/42 [==============================] - 0s 731us/step - loss: 0.3997 - accuracy: 0.8355\n",
      "Epoch 91/1500\n",
      "42/42 [==============================] - 0s 727us/step - loss: 0.4046 - accuracy: 0.8276\n",
      "Epoch 92/1500\n",
      "42/42 [==============================] - 0s 733us/step - loss: 0.3847 - accuracy: 0.8344\n",
      "Epoch 93/1500\n",
      "42/42 [==============================] - 0s 746us/step - loss: 0.4101 - accuracy: 0.8295\n",
      "Epoch 94/1500\n",
      "42/42 [==============================] - 0s 702us/step - loss: 0.3923 - accuracy: 0.8457\n",
      "Epoch 95/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.3886 - accuracy: 0.8457\n",
      "Epoch 96/1500\n",
      "42/42 [==============================] - 0s 731us/step - loss: 0.4011 - accuracy: 0.8306\n",
      "Epoch 97/1500\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.3957 - accuracy: 0.8385\n",
      "Epoch 98/1500\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.3964 - accuracy: 0.8397\n",
      "Epoch 99/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.3957 - accuracy: 0.8438\n",
      "Epoch 100/1500\n",
      "42/42 [==============================] - 0s 730us/step - loss: 0.4007 - accuracy: 0.8408\n",
      "Epoch 101/1500\n",
      "42/42 [==============================] - 0s 811us/step - loss: 0.3819 - accuracy: 0.8490\n",
      "Epoch 102/1500\n",
      "42/42 [==============================] - 0s 807us/step - loss: 0.4010 - accuracy: 0.8269\n",
      "Epoch 103/1500\n",
      "42/42 [==============================] - 0s 759us/step - loss: 0.3879 - accuracy: 0.8374\n",
      "Epoch 104/1500\n",
      "42/42 [==============================] - 0s 782us/step - loss: 0.3816 - accuracy: 0.8430\n",
      "Epoch 105/1500\n",
      "42/42 [==============================] - 0s 799us/step - loss: 0.3960 - accuracy: 0.8299\n",
      "Epoch 106/1500\n",
      "42/42 [==============================] - 0s 880us/step - loss: 0.3787 - accuracy: 0.8389\n",
      "Epoch 107/1500\n",
      "42/42 [==============================] - 0s 853us/step - loss: 0.3768 - accuracy: 0.8483\n",
      "Epoch 108/1500\n",
      "42/42 [==============================] - 0s 833us/step - loss: 0.3820 - accuracy: 0.8505\n",
      "Epoch 109/1500\n",
      "42/42 [==============================] - 0s 787us/step - loss: 0.3726 - accuracy: 0.8412\n",
      "Epoch 110/1500\n",
      "42/42 [==============================] - 0s 796us/step - loss: 0.3769 - accuracy: 0.8483\n",
      "Epoch 111/1500\n",
      "42/42 [==============================] - 0s 784us/step - loss: 0.3744 - accuracy: 0.8434\n",
      "Epoch 112/1500\n",
      "42/42 [==============================] - 0s 717us/step - loss: 0.3713 - accuracy: 0.8464\n",
      "Epoch 113/1500\n",
      "42/42 [==============================] - 0s 772us/step - loss: 0.3658 - accuracy: 0.8468\n",
      "Epoch 114/1500\n",
      "42/42 [==============================] - 0s 804us/step - loss: 0.3733 - accuracy: 0.8505\n",
      "Epoch 115/1500\n",
      "42/42 [==============================] - 0s 824us/step - loss: 0.3863 - accuracy: 0.8389\n",
      "Epoch 116/1500\n",
      "42/42 [==============================] - 0s 765us/step - loss: 0.3797 - accuracy: 0.8427\n",
      "Epoch 117/1500\n",
      "42/42 [==============================] - 0s 726us/step - loss: 0.3796 - accuracy: 0.8408\n",
      "Epoch 118/1500\n",
      "42/42 [==============================] - 0s 710us/step - loss: 0.3746 - accuracy: 0.8494\n",
      "Epoch 119/1500\n",
      "42/42 [==============================] - 0s 728us/step - loss: 0.3797 - accuracy: 0.8449\n",
      "Epoch 120/1500\n",
      "42/42 [==============================] - 0s 797us/step - loss: 0.3612 - accuracy: 0.8505\n",
      "Epoch 121/1500\n",
      "42/42 [==============================] - 0s 811us/step - loss: 0.3602 - accuracy: 0.8573\n",
      "Epoch 122/1500\n",
      "42/42 [==============================] - 0s 828us/step - loss: 0.3711 - accuracy: 0.8430\n",
      "Epoch 123/1500\n",
      "42/42 [==============================] - 0s 796us/step - loss: 0.3581 - accuracy: 0.8475\n",
      "Epoch 124/1500\n",
      "42/42 [==============================] - 0s 759us/step - loss: 0.3528 - accuracy: 0.8524\n",
      "Epoch 125/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.3693 - accuracy: 0.8419\n",
      "Epoch 126/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.3498 - accuracy: 0.8483\n",
      "Epoch 127/1500\n",
      "42/42 [==============================] - 0s 725us/step - loss: 0.3585 - accuracy: 0.8547\n",
      "Epoch 128/1500\n",
      "42/42 [==============================] - 0s 757us/step - loss: 0.3380 - accuracy: 0.8618\n",
      "Epoch 129/1500\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.3666 - accuracy: 0.8449\n",
      "Epoch 130/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.3391 - accuracy: 0.8592\n",
      "Epoch 131/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.3592 - accuracy: 0.8509\n",
      "Epoch 132/1500\n",
      "42/42 [==============================] - 0s 726us/step - loss: 0.3535 - accuracy: 0.8535\n",
      "Epoch 133/1500\n",
      "42/42 [==============================] - 0s 724us/step - loss: 0.3471 - accuracy: 0.8547\n",
      "Epoch 134/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8532\n",
      "Epoch 135/1500\n",
      "42/42 [==============================] - 0s 739us/step - loss: 0.3518 - accuracy: 0.8415\n",
      "Epoch 136/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.3692 - accuracy: 0.8449\n",
      "Epoch 137/1500\n",
      "42/42 [==============================] - 0s 737us/step - loss: 0.3447 - accuracy: 0.8652\n",
      "Epoch 138/1500\n",
      "42/42 [==============================] - 0s 721us/step - loss: 0.3531 - accuracy: 0.8562\n",
      "Epoch 139/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.3326 - accuracy: 0.8671\n",
      "Epoch 140/1500\n",
      "42/42 [==============================] - 0s 797us/step - loss: 0.3486 - accuracy: 0.8543\n",
      "Epoch 141/1500\n",
      "42/42 [==============================] - 0s 781us/step - loss: 0.3304 - accuracy: 0.8674\n",
      "Epoch 142/1500\n",
      "42/42 [==============================] - 0s 785us/step - loss: 0.3342 - accuracy: 0.8633\n",
      "Epoch 143/1500\n",
      "42/42 [==============================] - 0s 846us/step - loss: 0.3469 - accuracy: 0.8592\n",
      "Epoch 144/1500\n",
      "42/42 [==============================] - 0s 787us/step - loss: 0.3353 - accuracy: 0.8656\n",
      "Epoch 145/1500\n",
      "42/42 [==============================] - 0s 808us/step - loss: 0.3345 - accuracy: 0.8622\n",
      "Epoch 146/1500\n",
      "42/42 [==============================] - 0s 777us/step - loss: 0.3380 - accuracy: 0.8603\n",
      "Epoch 147/1500\n",
      "42/42 [==============================] - 0s 762us/step - loss: 0.3388 - accuracy: 0.8581\n",
      "Epoch 148/1500\n",
      "42/42 [==============================] - 0s 783us/step - loss: 0.3303 - accuracy: 0.8708\n",
      "Epoch 149/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.3387 - accuracy: 0.8562\n",
      "Epoch 150/1500\n",
      "42/42 [==============================] - 0s 727us/step - loss: 0.3362 - accuracy: 0.8667\n",
      "Epoch 151/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.3310 - accuracy: 0.8626\n",
      "Epoch 152/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.3380 - accuracy: 0.8644\n",
      "Epoch 153/1500\n",
      "42/42 [==============================] - 0s 752us/step - loss: 0.3294 - accuracy: 0.8731\n",
      "Epoch 154/1500\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.3313 - accuracy: 0.8618\n",
      "Epoch 155/1500\n",
      "42/42 [==============================] - 0s 726us/step - loss: 0.3269 - accuracy: 0.8659\n",
      "Epoch 156/1500\n",
      "42/42 [==============================] - 0s 769us/step - loss: 0.3288 - accuracy: 0.8742\n",
      "Epoch 157/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.3303 - accuracy: 0.8633\n",
      "Epoch 158/1500\n",
      "42/42 [==============================] - 0s 755us/step - loss: 0.3170 - accuracy: 0.8817\n",
      "Epoch 159/1500\n",
      "42/42 [==============================] - 0s 757us/step - loss: 0.3095 - accuracy: 0.8750\n",
      "Epoch 160/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.3313 - accuracy: 0.8622\n",
      "Epoch 161/1500\n",
      "42/42 [==============================] - 0s 739us/step - loss: 0.3148 - accuracy: 0.8738\n",
      "Epoch 162/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.3320 - accuracy: 0.8614\n",
      "Epoch 163/1500\n",
      "42/42 [==============================] - 0s 734us/step - loss: 0.3290 - accuracy: 0.8659\n",
      "Epoch 164/1500\n",
      "42/42 [==============================] - 0s 718us/step - loss: 0.3204 - accuracy: 0.8708\n",
      "Epoch 165/1500\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.3182 - accuracy: 0.8753\n",
      "Epoch 166/1500\n",
      "42/42 [==============================] - 0s 761us/step - loss: 0.3305 - accuracy: 0.8656\n",
      "Epoch 167/1500\n",
      "42/42 [==============================] - 0s 732us/step - loss: 0.3253 - accuracy: 0.8656\n",
      "Epoch 168/1500\n",
      "42/42 [==============================] - 0s 746us/step - loss: 0.3273 - accuracy: 0.8656\n",
      "Epoch 169/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.3036 - accuracy: 0.8742\n",
      "Epoch 170/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.3193 - accuracy: 0.8701\n",
      "Epoch 171/1500\n",
      "42/42 [==============================] - 0s 792us/step - loss: 0.3094 - accuracy: 0.8761\n",
      "Epoch 172/1500\n",
      "42/42 [==============================] - 0s 774us/step - loss: 0.3172 - accuracy: 0.8727\n",
      "Epoch 173/1500\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.3066 - accuracy: 0.8772\n",
      "Epoch 174/1500\n",
      "42/42 [==============================] - 0s 756us/step - loss: 0.3143 - accuracy: 0.8772\n",
      "Epoch 175/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8828\n",
      "Epoch 176/1500\n",
      "42/42 [==============================] - 0s 755us/step - loss: 0.3093 - accuracy: 0.8787\n",
      "Epoch 177/1500\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.3149 - accuracy: 0.8738\n",
      "Epoch 178/1500\n",
      "42/42 [==============================] - 0s 755us/step - loss: 0.2976 - accuracy: 0.8780\n",
      "Epoch 179/1500\n",
      "42/42 [==============================] - 0s 734us/step - loss: 0.3220 - accuracy: 0.8667\n",
      "Epoch 180/1500\n",
      "42/42 [==============================] - 0s 721us/step - loss: 0.3130 - accuracy: 0.8780\n",
      "Epoch 181/1500\n",
      "42/42 [==============================] - 0s 752us/step - loss: 0.2995 - accuracy: 0.8783\n",
      "Epoch 182/1500\n",
      "42/42 [==============================] - 0s 739us/step - loss: 0.3142 - accuracy: 0.8719\n",
      "Epoch 183/1500\n",
      "42/42 [==============================] - 0s 810us/step - loss: 0.3017 - accuracy: 0.8806\n",
      "Epoch 184/1500\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.3153 - accuracy: 0.8719\n",
      "Epoch 185/1500\n",
      "42/42 [==============================] - 0s 766us/step - loss: 0.3041 - accuracy: 0.8780\n",
      "Epoch 186/1500\n",
      "42/42 [==============================] - 0s 674us/step - loss: 0.3151 - accuracy: 0.8719\n",
      "Epoch 187/1500\n",
      "42/42 [==============================] - 0s 724us/step - loss: 0.3079 - accuracy: 0.8697\n",
      "Epoch 188/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2960 - accuracy: 0.8817\n",
      "Epoch 189/1500\n",
      "42/42 [==============================] - 0s 721us/step - loss: 0.2884 - accuracy: 0.8806\n",
      "Epoch 190/1500\n",
      "42/42 [==============================] - 0s 716us/step - loss: 0.3018 - accuracy: 0.8813\n",
      "Epoch 191/1500\n",
      "42/42 [==============================] - 0s 740us/step - loss: 0.3164 - accuracy: 0.8693\n",
      "Epoch 192/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.2991 - accuracy: 0.8772\n",
      "Epoch 193/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.3046 - accuracy: 0.8761\n",
      "Epoch 194/1500\n",
      "42/42 [==============================] - 0s 733us/step - loss: 0.3191 - accuracy: 0.8693\n",
      "Epoch 195/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.2867 - accuracy: 0.8919\n",
      "Epoch 196/1500\n",
      "42/42 [==============================] - 0s 739us/step - loss: 0.2943 - accuracy: 0.8843\n",
      "Epoch 197/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.3020 - accuracy: 0.8810\n",
      "Epoch 198/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.3090 - accuracy: 0.8787\n",
      "Epoch 199/1500\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.2825 - accuracy: 0.8892\n",
      "Epoch 200/1500\n",
      "42/42 [==============================] - 0s 740us/step - loss: 0.2969 - accuracy: 0.8821\n",
      "Epoch 201/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.2985 - accuracy: 0.8765\n",
      "Epoch 202/1500\n",
      "42/42 [==============================] - 0s 725us/step - loss: 0.3086 - accuracy: 0.8780\n",
      "Epoch 203/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.2930 - accuracy: 0.8810\n",
      "Epoch 204/1500\n",
      "42/42 [==============================] - 0s 732us/step - loss: 0.2833 - accuracy: 0.8896\n",
      "Epoch 205/1500\n",
      "42/42 [==============================] - 0s 731us/step - loss: 0.2970 - accuracy: 0.8825\n",
      "Epoch 206/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2976 - accuracy: 0.8843\n",
      "Epoch 207/1500\n",
      "42/42 [==============================] - 0s 742us/step - loss: 0.2913 - accuracy: 0.8836\n",
      "Epoch 208/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.2994 - accuracy: 0.8716\n",
      "Epoch 209/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8907\n",
      "Epoch 210/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2984 - accuracy: 0.8757\n",
      "Epoch 211/1500\n",
      "42/42 [==============================] - 0s 730us/step - loss: 0.2943 - accuracy: 0.8776\n",
      "Epoch 212/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2821 - accuracy: 0.8843\n",
      "Epoch 213/1500\n",
      "42/42 [==============================] - 0s 763us/step - loss: 0.2899 - accuracy: 0.8840\n",
      "Epoch 214/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2808 - accuracy: 0.8907\n",
      "Epoch 215/1500\n",
      "42/42 [==============================] - 0s 733us/step - loss: 0.2882 - accuracy: 0.8791\n",
      "Epoch 216/1500\n",
      "42/42 [==============================] - 0s 765us/step - loss: 0.2767 - accuracy: 0.8922\n",
      "Epoch 217/1500\n",
      "42/42 [==============================] - 0s 742us/step - loss: 0.2702 - accuracy: 0.8937\n",
      "Epoch 218/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.2873 - accuracy: 0.8885\n",
      "Epoch 219/1500\n",
      "42/42 [==============================] - 0s 733us/step - loss: 0.2821 - accuracy: 0.8858\n",
      "Epoch 220/1500\n",
      "42/42 [==============================] - 0s 763us/step - loss: 0.2846 - accuracy: 0.8870\n",
      "Epoch 221/1500\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.2790 - accuracy: 0.8843\n",
      "Epoch 222/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.2823 - accuracy: 0.8900\n",
      "Epoch 223/1500\n",
      "42/42 [==============================] - 0s 733us/step - loss: 0.2754 - accuracy: 0.8949\n",
      "Epoch 224/1500\n",
      "42/42 [==============================] - 0s 768us/step - loss: 0.2779 - accuracy: 0.8903\n",
      "Epoch 225/1500\n",
      "42/42 [==============================] - 0s 837us/step - loss: 0.2845 - accuracy: 0.8945\n",
      "Epoch 226/1500\n",
      "42/42 [==============================] - 0s 742us/step - loss: 0.2777 - accuracy: 0.8870\n",
      "Epoch 227/1500\n",
      "42/42 [==============================] - 0s 707us/step - loss: 0.2749 - accuracy: 0.8930\n",
      "Epoch 228/1500\n",
      "42/42 [==============================] - 0s 738us/step - loss: 0.2818 - accuracy: 0.8915\n",
      "Epoch 229/1500\n",
      "42/42 [==============================] - 0s 742us/step - loss: 0.2859 - accuracy: 0.8881\n",
      "Epoch 230/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.2795 - accuracy: 0.8798\n",
      "Epoch 231/1500\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.2692 - accuracy: 0.8870\n",
      "Epoch 232/1500\n",
      "42/42 [==============================] - 0s 737us/step - loss: 0.2718 - accuracy: 0.8949\n",
      "Epoch 233/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2856 - accuracy: 0.8847\n",
      "Epoch 234/1500\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.2735 - accuracy: 0.8885\n",
      "Epoch 235/1500\n",
      "42/42 [==============================] - 0s 804us/step - loss: 0.2710 - accuracy: 0.8888\n",
      "Epoch 236/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2662 - accuracy: 0.8941\n",
      "Epoch 237/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.2732 - accuracy: 0.8945\n",
      "Epoch 238/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.8866\n",
      "Epoch 239/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2659 - accuracy: 0.8903\n",
      "Epoch 240/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.2675 - accuracy: 0.8926\n",
      "Epoch 241/1500\n",
      "42/42 [==============================] - 0s 764us/step - loss: 0.2506 - accuracy: 0.9009\n",
      "Epoch 242/1500\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.2638 - accuracy: 0.8952\n",
      "Epoch 243/1500\n",
      "42/42 [==============================] - 0s 738us/step - loss: 0.2662 - accuracy: 0.8907\n",
      "Epoch 244/1500\n",
      "42/42 [==============================] - 0s 767us/step - loss: 0.2785 - accuracy: 0.8832\n",
      "Epoch 245/1500\n",
      "42/42 [==============================] - 0s 785us/step - loss: 0.2726 - accuracy: 0.8885\n",
      "Epoch 246/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.2708 - accuracy: 0.8877\n",
      "Epoch 247/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.2729 - accuracy: 0.8873\n",
      "Epoch 248/1500\n",
      "42/42 [==============================] - 0s 720us/step - loss: 0.2691 - accuracy: 0.8907\n",
      "Epoch 249/1500\n",
      "42/42 [==============================] - 0s 742us/step - loss: 0.2562 - accuracy: 0.9061\n",
      "Epoch 250/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.2729 - accuracy: 0.8971\n",
      "Epoch 251/1500\n",
      "42/42 [==============================] - 0s 740us/step - loss: 0.2718 - accuracy: 0.8903\n",
      "Epoch 252/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.2647 - accuracy: 0.8952\n",
      "Epoch 253/1500\n",
      "42/42 [==============================] - 0s 737us/step - loss: 0.2633 - accuracy: 0.8971\n",
      "Epoch 254/1500\n",
      "42/42 [==============================] - 0s 742us/step - loss: 0.2534 - accuracy: 0.8997\n",
      "Epoch 255/1500\n",
      "42/42 [==============================] - 0s 762us/step - loss: 0.2723 - accuracy: 0.8881\n",
      "Epoch 256/1500\n",
      "42/42 [==============================] - 0s 740us/step - loss: 0.2766 - accuracy: 0.8934\n",
      "Epoch 257/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.2738 - accuracy: 0.8952\n",
      "Epoch 258/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2639 - accuracy: 0.8945\n",
      "Epoch 259/1500\n",
      "42/42 [==============================] - 0s 742us/step - loss: 0.2636 - accuracy: 0.8922\n",
      "Epoch 260/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.2725 - accuracy: 0.8967\n",
      "Epoch 261/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.2616 - accuracy: 0.8934\n",
      "Epoch 262/1500\n",
      "42/42 [==============================] - 0s 790us/step - loss: 0.2482 - accuracy: 0.8975\n",
      "Epoch 263/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2510 - accuracy: 0.8979\n",
      "Epoch 264/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.8949\n",
      "Epoch 265/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2687 - accuracy: 0.8911\n",
      "Epoch 266/1500\n",
      "42/42 [==============================] - 0s 740us/step - loss: 0.2552 - accuracy: 0.9016\n",
      "Epoch 267/1500\n",
      "42/42 [==============================] - 0s 745us/step - loss: 0.2577 - accuracy: 0.9016\n",
      "Epoch 268/1500\n",
      "42/42 [==============================] - 0s 768us/step - loss: 0.2583 - accuracy: 0.8956\n",
      "Epoch 269/1500\n",
      "42/42 [==============================] - 0s 765us/step - loss: 0.2498 - accuracy: 0.9016\n",
      "Epoch 270/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2700 - accuracy: 0.8892\n",
      "Epoch 271/1500\n",
      "42/42 [==============================] - 0s 828us/step - loss: 0.2720 - accuracy: 0.8881\n",
      "Epoch 272/1500\n",
      "42/42 [==============================] - 0s 834us/step - loss: 0.2425 - accuracy: 0.9024\n",
      "Epoch 273/1500\n",
      "42/42 [==============================] - 0s 810us/step - loss: 0.2561 - accuracy: 0.8892\n",
      "Epoch 274/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.2560 - accuracy: 0.8956\n",
      "Epoch 275/1500\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.2624 - accuracy: 0.8949\n",
      "Epoch 276/1500\n",
      "42/42 [==============================] - 0s 777us/step - loss: 0.2520 - accuracy: 0.8960\n",
      "Epoch 277/1500\n",
      "42/42 [==============================] - 0s 727us/step - loss: 0.2362 - accuracy: 0.9009\n",
      "Epoch 278/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2385 - accuracy: 0.9039\n",
      "Epoch 279/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.2620 - accuracy: 0.8975\n",
      "Epoch 280/1500\n",
      "42/42 [==============================] - 0s 760us/step - loss: 0.2473 - accuracy: 0.9005\n",
      "Epoch 281/1500\n",
      "42/42 [==============================] - 0s 773us/step - loss: 0.2406 - accuracy: 0.9035\n",
      "Epoch 282/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.2388 - accuracy: 0.9151\n",
      "Epoch 283/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2668 - accuracy: 0.8937\n",
      "Epoch 284/1500\n",
      "42/42 [==============================] - 0s 770us/step - loss: 0.2602 - accuracy: 0.8960\n",
      "Epoch 285/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.2426 - accuracy: 0.9024\n",
      "Epoch 286/1500\n",
      "42/42 [==============================] - 0s 737us/step - loss: 0.2453 - accuracy: 0.9009\n",
      "Epoch 287/1500\n",
      "42/42 [==============================] - 0s 785us/step - loss: 0.2405 - accuracy: 0.9087\n",
      "Epoch 288/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.9050\n",
      "Epoch 289/1500\n",
      "42/42 [==============================] - 0s 759us/step - loss: 0.2525 - accuracy: 0.9024\n",
      "Epoch 290/1500\n",
      "42/42 [==============================] - 0s 802us/step - loss: 0.2509 - accuracy: 0.9001\n",
      "Epoch 291/1500\n",
      "42/42 [==============================] - 0s 889us/step - loss: 0.2498 - accuracy: 0.9005\n",
      "Epoch 292/1500\n",
      "42/42 [==============================] - 0s 902us/step - loss: 0.2389 - accuracy: 0.9080\n",
      "Epoch 293/1500\n",
      "42/42 [==============================] - 0s 875us/step - loss: 0.2363 - accuracy: 0.9065\n",
      "Epoch 294/1500\n",
      "42/42 [==============================] - 0s 882us/step - loss: 0.2532 - accuracy: 0.8982\n",
      "Epoch 295/1500\n",
      "42/42 [==============================] - 0s 878us/step - loss: 0.2410 - accuracy: 0.9039\n",
      "Epoch 296/1500\n",
      "42/42 [==============================] - 0s 810us/step - loss: 0.2468 - accuracy: 0.8997\n",
      "Epoch 297/1500\n",
      "42/42 [==============================] - 0s 801us/step - loss: 0.2438 - accuracy: 0.9084\n",
      "Epoch 298/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.2483 - accuracy: 0.9027\n",
      "Epoch 299/1500\n",
      "42/42 [==============================] - 0s 757us/step - loss: 0.2518 - accuracy: 0.9009\n",
      "Epoch 300/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.2467 - accuracy: 0.8990\n",
      "Epoch 301/1500\n",
      "42/42 [==============================] - 0s 769us/step - loss: 0.2602 - accuracy: 0.8926\n",
      "Epoch 302/1500\n",
      "42/42 [==============================] - 0s 731us/step - loss: 0.2303 - accuracy: 0.9046\n",
      "Epoch 303/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2413 - accuracy: 0.9065\n",
      "Epoch 304/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2426 - accuracy: 0.8997\n",
      "Epoch 305/1500\n",
      "42/42 [==============================] - 0s 764us/step - loss: 0.2509 - accuracy: 0.9076\n",
      "Epoch 306/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2312 - accuracy: 0.9136\n",
      "Epoch 307/1500\n",
      "42/42 [==============================] - 0s 727us/step - loss: 0.2426 - accuracy: 0.9054\n",
      "Epoch 308/1500\n",
      "42/42 [==============================] - 0s 745us/step - loss: 0.2537 - accuracy: 0.9001\n",
      "Epoch 309/1500\n",
      "42/42 [==============================] - 0s 746us/step - loss: 0.2376 - accuracy: 0.9035\n",
      "Epoch 310/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.9042\n",
      "Epoch 311/1500\n",
      "42/42 [==============================] - 0s 767us/step - loss: 0.2465 - accuracy: 0.9035\n",
      "Epoch 312/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2441 - accuracy: 0.9042\n",
      "Epoch 313/1500\n",
      "42/42 [==============================] - 0s 723us/step - loss: 0.2393 - accuracy: 0.9069\n",
      "Epoch 314/1500\n",
      "42/42 [==============================] - 0s 780us/step - loss: 0.2391 - accuracy: 0.9042\n",
      "Epoch 315/1500\n",
      "42/42 [==============================] - 0s 763us/step - loss: 0.2357 - accuracy: 0.9084\n",
      "Epoch 316/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2328 - accuracy: 0.9065\n",
      "Epoch 317/1500\n",
      "42/42 [==============================] - 0s 734us/step - loss: 0.2560 - accuracy: 0.9012\n",
      "Epoch 318/1500\n",
      "42/42 [==============================] - 0s 760us/step - loss: 0.2403 - accuracy: 0.8964\n",
      "Epoch 319/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2528 - accuracy: 0.8956\n",
      "Epoch 320/1500\n",
      "42/42 [==============================] - 0s 760us/step - loss: 0.2512 - accuracy: 0.9012\n",
      "Epoch 321/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.2269 - accuracy: 0.9140\n",
      "Epoch 322/1500\n",
      "42/42 [==============================] - 0s 720us/step - loss: 0.2327 - accuracy: 0.9080\n",
      "Epoch 323/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.2328 - accuracy: 0.9046\n",
      "Epoch 324/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.2419 - accuracy: 0.9012\n",
      "Epoch 325/1500\n",
      "42/42 [==============================] - 0s 754us/step - loss: 0.2355 - accuracy: 0.9076\n",
      "Epoch 326/1500\n",
      "42/42 [==============================] - 0s 755us/step - loss: 0.2400 - accuracy: 0.9084\n",
      "Epoch 327/1500\n",
      "42/42 [==============================] - 0s 755us/step - loss: 0.2344 - accuracy: 0.9087\n",
      "Epoch 328/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2440 - accuracy: 0.9050\n",
      "Epoch 329/1500\n",
      "42/42 [==============================] - 0s 761us/step - loss: 0.2327 - accuracy: 0.9057\n",
      "Epoch 330/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2412 - accuracy: 0.9020\n",
      "Epoch 331/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.9042\n",
      "Epoch 332/1500\n",
      "42/42 [==============================] - 0s 790us/step - loss: 0.2224 - accuracy: 0.9136\n",
      "Epoch 333/1500\n",
      "42/42 [==============================] - 0s 738us/step - loss: 0.2265 - accuracy: 0.9133\n",
      "Epoch 334/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.2389 - accuracy: 0.9035\n",
      "Epoch 335/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.2233 - accuracy: 0.9069\n",
      "Epoch 336/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2391 - accuracy: 0.9091\n",
      "Epoch 337/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2208 - accuracy: 0.9140\n",
      "Epoch 338/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2360 - accuracy: 0.9099\n",
      "Epoch 339/1500\n",
      "42/42 [==============================] - 0s 720us/step - loss: 0.2368 - accuracy: 0.9035\n",
      "Epoch 340/1500\n",
      "42/42 [==============================] - 0s 782us/step - loss: 0.2314 - accuracy: 0.9069\n",
      "Epoch 341/1500\n",
      "42/42 [==============================] - 0s 782us/step - loss: 0.2172 - accuracy: 0.9159\n",
      "Epoch 342/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.2400 - accuracy: 0.9031\n",
      "Epoch 343/1500\n",
      "42/42 [==============================] - 0s 772us/step - loss: 0.2262 - accuracy: 0.9080\n",
      "Epoch 344/1500\n",
      "42/42 [==============================] - 0s 783us/step - loss: 0.2234 - accuracy: 0.9121\n",
      "Epoch 345/1500\n",
      "42/42 [==============================] - 0s 756us/step - loss: 0.2217 - accuracy: 0.9118\n",
      "Epoch 346/1500\n",
      "42/42 [==============================] - 0s 767us/step - loss: 0.2318 - accuracy: 0.9065\n",
      "Epoch 347/1500\n",
      "42/42 [==============================] - 0s 764us/step - loss: 0.2282 - accuracy: 0.9095\n",
      "Epoch 348/1500\n",
      "42/42 [==============================] - 0s 745us/step - loss: 0.2347 - accuracy: 0.9099\n",
      "Epoch 349/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.2147 - accuracy: 0.9136\n",
      "Epoch 350/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.2313 - accuracy: 0.9133\n",
      "Epoch 351/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9215\n",
      "Epoch 352/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2483 - accuracy: 0.8975\n",
      "Epoch 353/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2283 - accuracy: 0.9125\n",
      "Epoch 354/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.2242 - accuracy: 0.9133\n",
      "Epoch 355/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2202 - accuracy: 0.9099\n",
      "Epoch 356/1500\n",
      "42/42 [==============================] - 0s 769us/step - loss: 0.2174 - accuracy: 0.9200\n",
      "Epoch 357/1500\n",
      "42/42 [==============================] - 0s 733us/step - loss: 0.2415 - accuracy: 0.8994\n",
      "Epoch 358/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.2162 - accuracy: 0.9125\n",
      "Epoch 359/1500\n",
      "42/42 [==============================] - 0s 761us/step - loss: 0.2224 - accuracy: 0.9042\n",
      "Epoch 360/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.2219 - accuracy: 0.9110\n",
      "Epoch 361/1500\n",
      "42/42 [==============================] - 0s 744us/step - loss: 0.2118 - accuracy: 0.9170\n",
      "Epoch 362/1500\n",
      "42/42 [==============================] - 0s 738us/step - loss: 0.2227 - accuracy: 0.9148\n",
      "Epoch 363/1500\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.2129 - accuracy: 0.9211\n",
      "Epoch 364/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2433 - accuracy: 0.9065\n",
      "Epoch 365/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.2310 - accuracy: 0.9084\n",
      "Epoch 366/1500\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.2358 - accuracy: 0.9072\n",
      "Epoch 367/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2094 - accuracy: 0.9196\n",
      "Epoch 368/1500\n",
      "42/42 [==============================] - 0s 745us/step - loss: 0.2145 - accuracy: 0.9166\n",
      "Epoch 369/1500\n",
      "42/42 [==============================] - 0s 741us/step - loss: 0.2162 - accuracy: 0.9178\n",
      "Epoch 370/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2169 - accuracy: 0.9114\n",
      "Epoch 371/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.2193 - accuracy: 0.9103\n",
      "Epoch 372/1500\n",
      "42/42 [==============================] - 0s 774us/step - loss: 0.2059 - accuracy: 0.9238\n",
      "Epoch 373/1500\n",
      "42/42 [==============================] - 0s 743us/step - loss: 0.2197 - accuracy: 0.9114\n",
      "Epoch 374/1500\n",
      "42/42 [==============================] - 0s 727us/step - loss: 0.2164 - accuracy: 0.9151\n",
      "Epoch 375/1500\n",
      "42/42 [==============================] - 0s 755us/step - loss: 0.2174 - accuracy: 0.9110\n",
      "Epoch 376/1500\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.2228 - accuracy: 0.9091\n",
      "Epoch 377/1500\n",
      "42/42 [==============================] - 0s 753us/step - loss: 0.2254 - accuracy: 0.9121\n",
      "Epoch 378/1500\n",
      "42/42 [==============================] - 0s 762us/step - loss: 0.2188 - accuracy: 0.9084\n",
      "Epoch 379/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.2135 - accuracy: 0.9159\n",
      "Epoch 380/1500\n",
      "42/42 [==============================] - 0s 746us/step - loss: 0.2184 - accuracy: 0.9133\n",
      "Epoch 381/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.2136 - accuracy: 0.9144\n",
      "Epoch 382/1500\n",
      "42/42 [==============================] - 0s 730us/step - loss: 0.2057 - accuracy: 0.9226\n",
      "Epoch 383/1500\n",
      "42/42 [==============================] - 0s 767us/step - loss: 0.2140 - accuracy: 0.9170\n",
      "Epoch 384/1500\n",
      "42/42 [==============================] - 0s 756us/step - loss: 0.2089 - accuracy: 0.9151\n",
      "Epoch 385/1500\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.2081 - accuracy: 0.9219\n",
      "Epoch 386/1500\n",
      "42/42 [==============================] - 0s 780us/step - loss: 0.2243 - accuracy: 0.9087\n",
      "Epoch 387/1500\n",
      "42/42 [==============================] - 0s 769us/step - loss: 0.2271 - accuracy: 0.9140\n",
      "Epoch 388/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9057\n",
      "Epoch 389/1500\n",
      "42/42 [==============================] - 0s 759us/step - loss: 0.2156 - accuracy: 0.9155\n",
      "Epoch 390/1500\n",
      "42/42 [==============================] - 0s 761us/step - loss: 0.2116 - accuracy: 0.9178\n",
      "Epoch 391/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.2127 - accuracy: 0.9226\n",
      "Epoch 392/1500\n",
      "42/42 [==============================] - 0s 768us/step - loss: 0.2099 - accuracy: 0.9159\n",
      "Epoch 393/1500\n",
      "42/42 [==============================] - 0s 756us/step - loss: 0.2142 - accuracy: 0.9061\n",
      "Epoch 394/1500\n",
      "42/42 [==============================] - 0s 751us/step - loss: 0.2091 - accuracy: 0.9208\n",
      "Epoch 395/1500\n",
      "42/42 [==============================] - 0s 788us/step - loss: 0.2150 - accuracy: 0.9170\n",
      "Epoch 396/1500\n",
      "42/42 [==============================] - 0s 767us/step - loss: 0.2108 - accuracy: 0.9204\n",
      "Epoch 397/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2136 - accuracy: 0.9159\n",
      "Epoch 398/1500\n",
      "42/42 [==============================] - 0s 769us/step - loss: 0.2133 - accuracy: 0.9140\n",
      "Epoch 399/1500\n",
      "42/42 [==============================] - 0s 768us/step - loss: 0.2021 - accuracy: 0.9226\n",
      "Epoch 400/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.2095 - accuracy: 0.9204\n",
      "Epoch 401/1500\n",
      "42/42 [==============================] - 0s 746us/step - loss: 0.2168 - accuracy: 0.9193\n",
      "Epoch 402/1500\n",
      "42/42 [==============================] - 0s 752us/step - loss: 0.1961 - accuracy: 0.9226\n",
      "Epoch 403/1500\n",
      "42/42 [==============================] - 0s 750us/step - loss: 0.2208 - accuracy: 0.9110\n",
      "Epoch 404/1500\n",
      "42/42 [==============================] - 0s 761us/step - loss: 0.2079 - accuracy: 0.9215\n",
      "Epoch 405/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9238\n",
      "Epoch 406/1500\n",
      "42/42 [==============================] - 0s 793us/step - loss: 0.2173 - accuracy: 0.9174\n",
      "Epoch 407/1500\n",
      "42/42 [==============================] - 0s 794us/step - loss: 0.2116 - accuracy: 0.9189\n",
      "Epoch 408/1500\n",
      "42/42 [==============================] - 0s 756us/step - loss: 0.2111 - accuracy: 0.9174\n",
      "Epoch 409/1500\n",
      "42/42 [==============================] - 0s 749us/step - loss: 0.2099 - accuracy: 0.9163\n",
      "Epoch 410/1500\n",
      "42/42 [==============================] - 0s 760us/step - loss: 0.2075 - accuracy: 0.9208\n",
      "Epoch 411/1500\n",
      "42/42 [==============================] - 0s 746us/step - loss: 0.2110 - accuracy: 0.9133\n",
      "Epoch 412/1500\n",
      "42/42 [==============================] - 0s 777us/step - loss: 0.2250 - accuracy: 0.9129\n",
      "Epoch 413/1500\n",
      "42/42 [==============================] - 0s 794us/step - loss: 0.2088 - accuracy: 0.9200\n",
      "Epoch 414/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.2122 - accuracy: 0.9181\n",
      "Epoch 415/1500\n",
      "42/42 [==============================] - 0s 745us/step - loss: 0.2076 - accuracy: 0.9215\n",
      "Epoch 416/1500\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.2153 - accuracy: 0.9144\n",
      "Epoch 417/1500\n",
      "42/42 [==============================] - 0s 779us/step - loss: 0.2168 - accuracy: 0.9174\n",
      "Epoch 418/1500\n",
      "42/42 [==============================] - 0s 831us/step - loss: 0.1942 - accuracy: 0.9200\n",
      "Epoch 419/1500\n",
      "42/42 [==============================] - 0s 800us/step - loss: 0.1990 - accuracy: 0.9223\n",
      "Epoch 420/1500\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.2068 - accuracy: 0.9211\n",
      "Epoch 421/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9230\n",
      "Epoch 422/1500\n",
      "42/42 [==============================] - 0s 832us/step - loss: 0.2140 - accuracy: 0.9159\n",
      "Epoch 423/1500\n",
      "42/42 [==============================] - 0s 798us/step - loss: 0.2004 - accuracy: 0.9196\n",
      "Epoch 424/1500\n",
      "42/42 [==============================] - 0s 856us/step - loss: 0.1866 - accuracy: 0.9294\n",
      "Epoch 425/1500\n",
      "42/42 [==============================] - 0s 789us/step - loss: 0.1895 - accuracy: 0.9275\n",
      "Epoch 426/1500\n",
      "42/42 [==============================] - 0s 823us/step - loss: 0.1942 - accuracy: 0.9238\n",
      "Epoch 427/1500\n",
      "42/42 [==============================] - 0s 817us/step - loss: 0.2024 - accuracy: 0.9196\n",
      "Epoch 428/1500\n",
      "42/42 [==============================] - 0s 793us/step - loss: 0.2023 - accuracy: 0.9166\n",
      "Epoch 429/1500\n",
      "42/42 [==============================] - 0s 840us/step - loss: 0.2033 - accuracy: 0.9181\n",
      "Epoch 430/1500\n",
      "42/42 [==============================] - 0s 796us/step - loss: 0.1941 - accuracy: 0.9256\n",
      "Epoch 431/1500\n",
      "42/42 [==============================] - 0s 789us/step - loss: 0.2003 - accuracy: 0.9215\n",
      "Epoch 432/1500\n",
      "42/42 [==============================] - 0s 784us/step - loss: 0.1930 - accuracy: 0.9238\n",
      "Epoch 433/1500\n",
      "42/42 [==============================] - 0s 799us/step - loss: 0.1925 - accuracy: 0.9193\n",
      "Epoch 434/1500\n",
      "42/42 [==============================] - 0s 790us/step - loss: 0.1993 - accuracy: 0.9260\n",
      "Epoch 435/1500\n",
      "42/42 [==============================] - 0s 797us/step - loss: 0.1962 - accuracy: 0.9219\n",
      "Epoch 436/1500\n",
      "42/42 [==============================] - 0s 793us/step - loss: 0.1960 - accuracy: 0.9283\n",
      "Epoch 437/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2048 - accuracy: 0.9159\n",
      "Epoch 438/1500\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.1994 - accuracy: 0.9238\n",
      "Epoch 439/1500\n",
      "42/42 [==============================] - 0s 785us/step - loss: 0.2002 - accuracy: 0.9211\n",
      "Epoch 440/1500\n",
      "42/42 [==============================] - 0s 769us/step - loss: 0.2241 - accuracy: 0.9091\n",
      "Epoch 441/1500\n",
      "42/42 [==============================] - 0s 763us/step - loss: 0.1842 - accuracy: 0.9264\n",
      "Epoch 442/1500\n",
      "42/42 [==============================] - 0s 756us/step - loss: 0.1953 - accuracy: 0.9208\n",
      "Epoch 443/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.2038 - accuracy: 0.9219\n",
      "Epoch 444/1500\n",
      "42/42 [==============================] - 0s 762us/step - loss: 0.2096 - accuracy: 0.9129\n",
      "Epoch 445/1500\n",
      "42/42 [==============================] - 0s 725us/step - loss: 0.2052 - accuracy: 0.9219\n",
      "Epoch 446/1500\n",
      "42/42 [==============================] - 0s 760us/step - loss: 0.1856 - accuracy: 0.9268\n",
      "Epoch 447/1500\n",
      "42/42 [==============================] - 0s 792us/step - loss: 0.1773 - accuracy: 0.9339\n",
      "Epoch 448/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.2024 - accuracy: 0.9215\n",
      "Epoch 449/1500\n",
      "42/42 [==============================] - 0s 759us/step - loss: 0.1871 - accuracy: 0.9271\n",
      "Epoch 450/1500\n",
      "42/42 [==============================] - 0s 757us/step - loss: 0.2048 - accuracy: 0.9196\n",
      "Epoch 451/1500\n",
      "42/42 [==============================] - 0s 759us/step - loss: 0.1902 - accuracy: 0.9290\n",
      "Epoch 452/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9302\n",
      "Epoch 453/1500\n",
      "42/42 [==============================] - 0s 850us/step - loss: 0.1974 - accuracy: 0.9238\n",
      "Epoch 454/1500\n",
      "42/42 [==============================] - 0s 752us/step - loss: 0.2107 - accuracy: 0.9208\n",
      "Epoch 455/1500\n",
      "42/42 [==============================] - 0s 785us/step - loss: 0.1955 - accuracy: 0.9185\n",
      "Epoch 456/1500\n",
      "42/42 [==============================] - 0s 837us/step - loss: 0.1815 - accuracy: 0.9335\n",
      "Epoch 457/1500\n",
      "42/42 [==============================] - 0s 778us/step - loss: 0.2015 - accuracy: 0.9148\n",
      "Epoch 458/1500\n",
      "42/42 [==============================] - 0s 774us/step - loss: 0.1950 - accuracy: 0.9249\n",
      "Epoch 459/1500\n",
      "42/42 [==============================] - 0s 765us/step - loss: 0.1930 - accuracy: 0.9189\n",
      "Epoch 460/1500\n",
      "42/42 [==============================] - 0s 765us/step - loss: 0.2120 - accuracy: 0.9211\n",
      "Epoch 461/1500\n",
      "42/42 [==============================] - 0s 878us/step - loss: 0.2146 - accuracy: 0.9148\n",
      "Epoch 462/1500\n",
      "42/42 [==============================] - 0s 795us/step - loss: 0.2015 - accuracy: 0.9241\n",
      "Epoch 463/1500\n",
      "42/42 [==============================] - 0s 868us/step - loss: 0.1933 - accuracy: 0.9241\n",
      "Epoch 464/1500\n",
      "42/42 [==============================] - 0s 797us/step - loss: 0.2013 - accuracy: 0.9193\n",
      "Epoch 465/1500\n",
      "42/42 [==============================] - 0s 817us/step - loss: 0.2039 - accuracy: 0.9219\n",
      "Epoch 466/1500\n",
      "42/42 [==============================] - 0s 774us/step - loss: 0.1956 - accuracy: 0.9219\n",
      "Epoch 467/1500\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9174\n",
      "Epoch 468/1500\n",
      "42/42 [==============================] - 0s 764us/step - loss: 0.2015 - accuracy: 0.9170\n",
      "Epoch 469/1500\n",
      "42/42 [==============================] - 0s 772us/step - loss: 0.1898 - accuracy: 0.9249\n",
      "Epoch 470/1500\n",
      "42/42 [==============================] - 0s 775us/step - loss: 0.1884 - accuracy: 0.9215\n",
      "Epoch 471/1500\n",
      "42/42 [==============================] - 0s 808us/step - loss: 0.1995 - accuracy: 0.9245\n",
      "Epoch 472/1500\n",
      "42/42 [==============================] - 0s 844us/step - loss: 0.1921 - accuracy: 0.9234\n",
      "Epoch 473/1500\n",
      "42/42 [==============================] - 0s 805us/step - loss: 0.1979 - accuracy: 0.9223\n",
      "Epoch 474/1500\n",
      "42/42 [==============================] - 0s 758us/step - loss: 0.1920 - accuracy: 0.9208\n",
      "Epoch 475/1500\n",
      "42/42 [==============================] - 0s 747us/step - loss: 0.1827 - accuracy: 0.9275\n",
      "Epoch 476/1500\n",
      "42/42 [==============================] - 0s 764us/step - loss: 0.1917 - accuracy: 0.9230\n",
      "Epoch 477/1500\n",
      " 1/42 [..............................] - ETA: 0s - loss: 0.2470 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 447.\n",
      "42/42 [==============================] - 0s 837us/step - loss: 0.1892 - accuracy: 0.9275\n",
      "Epoch 477: early stopping\n",
      "9/9 [==============================] - 0s 715us/step - loss: 0.6264 - accuracy: 0.7537\n",
      "9/9 [==============================] - 0s 568us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.6263511776924133, Accuracy: 0.753731369972229, Precision: 0.742591583669464, Recall: 0.7260151663405088, F1 Score: 0.713229859571323\n",
      "Confusion Matrix:\n",
      " [[131   2  27]\n",
      " [ 28  45   0]\n",
      " [  9   0  26]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '044A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "746\n",
      "Length of y_train_val:\n",
      "746\n",
      "Length of groups_train_val:\n",
      "746\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    150\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     21\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({2: 1111, 0: 1050, 1: 1014})\n",
      "Epoch 1/1500\n",
      "50/50 [==============================] - 0s 907us/step - loss: 1.0757 - accuracy: 0.5427\n",
      "Epoch 2/1500\n",
      "50/50 [==============================] - 0s 836us/step - loss: 0.8527 - accuracy: 0.6365\n",
      "Epoch 3/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.7808 - accuracy: 0.6813\n",
      "Epoch 4/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.7188 - accuracy: 0.7087\n",
      "Epoch 5/1500\n",
      "50/50 [==============================] - 0s 719us/step - loss: 0.6952 - accuracy: 0.7077\n",
      "Epoch 6/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.6451 - accuracy: 0.7326\n",
      "Epoch 7/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.6120 - accuracy: 0.7496\n",
      "Epoch 8/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.7461\n",
      "Epoch 9/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.6040 - accuracy: 0.7506\n",
      "Epoch 10/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.5768 - accuracy: 0.7635\n",
      "Epoch 11/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.5676 - accuracy: 0.7584\n",
      "Epoch 12/1500\n",
      "50/50 [==============================] - 0s 760us/step - loss: 0.5661 - accuracy: 0.7704\n",
      "Epoch 13/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.5520 - accuracy: 0.7739\n",
      "Epoch 14/1500\n",
      "50/50 [==============================] - 0s 832us/step - loss: 0.5393 - accuracy: 0.7798\n",
      "Epoch 15/1500\n",
      "50/50 [==============================] - 0s 775us/step - loss: 0.5221 - accuracy: 0.7871\n",
      "Epoch 16/1500\n",
      "50/50 [==============================] - 0s 765us/step - loss: 0.5081 - accuracy: 0.7890\n",
      "Epoch 17/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.5290 - accuracy: 0.7836\n",
      "Epoch 18/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.5122 - accuracy: 0.7890\n",
      "Epoch 19/1500\n",
      "50/50 [==============================] - 0s 770us/step - loss: 0.5118 - accuracy: 0.7928\n",
      "Epoch 20/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.4871 - accuracy: 0.8000\n",
      "Epoch 21/1500\n",
      "50/50 [==============================] - 0s 805us/step - loss: 0.4968 - accuracy: 0.8028\n",
      "Epoch 22/1500\n",
      "50/50 [==============================] - 0s 832us/step - loss: 0.4746 - accuracy: 0.8082\n",
      "Epoch 23/1500\n",
      "50/50 [==============================] - 0s 834us/step - loss: 0.4753 - accuracy: 0.8028\n",
      "Epoch 24/1500\n",
      "50/50 [==============================] - 0s 907us/step - loss: 0.4761 - accuracy: 0.7969\n",
      "Epoch 25/1500\n",
      "50/50 [==============================] - 0s 912us/step - loss: 0.4808 - accuracy: 0.8000\n",
      "Epoch 26/1500\n",
      "50/50 [==============================] - 0s 816us/step - loss: 0.4728 - accuracy: 0.8079\n",
      "Epoch 27/1500\n",
      "50/50 [==============================] - 0s 856us/step - loss: 0.4637 - accuracy: 0.8101\n",
      "Epoch 28/1500\n",
      "50/50 [==============================] - 0s 827us/step - loss: 0.4577 - accuracy: 0.8063\n",
      "Epoch 29/1500\n",
      "50/50 [==============================] - 0s 866us/step - loss: 0.4566 - accuracy: 0.8047\n",
      "Epoch 30/1500\n",
      "50/50 [==============================] - 0s 982us/step - loss: 0.4498 - accuracy: 0.8091\n",
      "Epoch 31/1500\n",
      "50/50 [==============================] - 0s 925us/step - loss: 0.4351 - accuracy: 0.8217\n",
      "Epoch 32/1500\n",
      "50/50 [==============================] - 0s 879us/step - loss: 0.4445 - accuracy: 0.8139\n",
      "Epoch 33/1500\n",
      "50/50 [==============================] - 0s 761us/step - loss: 0.4361 - accuracy: 0.8252\n",
      "Epoch 34/1500\n",
      "50/50 [==============================] - 0s 761us/step - loss: 0.4377 - accuracy: 0.8195\n",
      "Epoch 35/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.4193 - accuracy: 0.8246\n",
      "Epoch 36/1500\n",
      "50/50 [==============================] - 0s 728us/step - loss: 0.4248 - accuracy: 0.8255\n",
      "Epoch 37/1500\n",
      "50/50 [==============================] - 0s 713us/step - loss: 0.4269 - accuracy: 0.8268\n",
      "Epoch 38/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.4286 - accuracy: 0.8176\n",
      "Epoch 39/1500\n",
      "50/50 [==============================] - 0s 708us/step - loss: 0.4253 - accuracy: 0.8243\n",
      "Epoch 40/1500\n",
      "50/50 [==============================] - 0s 738us/step - loss: 0.4123 - accuracy: 0.8312\n",
      "Epoch 41/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.4184 - accuracy: 0.8268\n",
      "Epoch 42/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.4080 - accuracy: 0.8230\n",
      "Epoch 43/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.4287 - accuracy: 0.8255\n",
      "Epoch 44/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.4050 - accuracy: 0.8331\n",
      "Epoch 45/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.4114 - accuracy: 0.8261\n",
      "Epoch 46/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.4127 - accuracy: 0.8296\n",
      "Epoch 47/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.4023 - accuracy: 0.8287\n",
      "Epoch 48/1500\n",
      "50/50 [==============================] - 0s 730us/step - loss: 0.4087 - accuracy: 0.8334\n",
      "Epoch 49/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.3842 - accuracy: 0.8463\n",
      "Epoch 50/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.3906 - accuracy: 0.8387\n",
      "Epoch 51/1500\n",
      "50/50 [==============================] - 0s 725us/step - loss: 0.4006 - accuracy: 0.8283\n",
      "Epoch 52/1500\n",
      "50/50 [==============================] - 0s 735us/step - loss: 0.3929 - accuracy: 0.8391\n",
      "Epoch 53/1500\n",
      "50/50 [==============================] - 0s 738us/step - loss: 0.3879 - accuracy: 0.8416\n",
      "Epoch 54/1500\n",
      "50/50 [==============================] - 0s 766us/step - loss: 0.3814 - accuracy: 0.8438\n",
      "Epoch 55/1500\n",
      "50/50 [==============================] - 0s 730us/step - loss: 0.3827 - accuracy: 0.8441\n",
      "Epoch 56/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8416\n",
      "Epoch 57/1500\n",
      "50/50 [==============================] - 0s 764us/step - loss: 0.3714 - accuracy: 0.8482\n",
      "Epoch 58/1500\n",
      "50/50 [==============================] - 0s 738us/step - loss: 0.3900 - accuracy: 0.8372\n",
      "Epoch 59/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3800 - accuracy: 0.8381\n",
      "Epoch 60/1500\n",
      "50/50 [==============================] - 0s 766us/step - loss: 0.3667 - accuracy: 0.8548\n",
      "Epoch 61/1500\n",
      "50/50 [==============================] - 0s 764us/step - loss: 0.3779 - accuracy: 0.8441\n",
      "Epoch 62/1500\n",
      "50/50 [==============================] - 0s 769us/step - loss: 0.3607 - accuracy: 0.8523\n",
      "Epoch 63/1500\n",
      "50/50 [==============================] - 0s 840us/step - loss: 0.3712 - accuracy: 0.8472\n",
      "Epoch 64/1500\n",
      "50/50 [==============================] - 0s 884us/step - loss: 0.3678 - accuracy: 0.8498\n",
      "Epoch 65/1500\n",
      "50/50 [==============================] - 0s 845us/step - loss: 0.3641 - accuracy: 0.8450\n",
      "Epoch 66/1500\n",
      "50/50 [==============================] - 0s 790us/step - loss: 0.3581 - accuracy: 0.8504\n",
      "Epoch 67/1500\n",
      "50/50 [==============================] - 0s 771us/step - loss: 0.3588 - accuracy: 0.8444\n",
      "Epoch 68/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.3452 - accuracy: 0.8580\n",
      "Epoch 69/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.3553 - accuracy: 0.8554\n",
      "Epoch 70/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.3561 - accuracy: 0.8545\n",
      "Epoch 71/1500\n",
      "50/50 [==============================] - 0s 763us/step - loss: 0.3434 - accuracy: 0.8545\n",
      "Epoch 72/1500\n",
      "50/50 [==============================] - 0s 728us/step - loss: 0.3564 - accuracy: 0.8620\n",
      "Epoch 73/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.3473 - accuracy: 0.8617\n",
      "Epoch 74/1500\n",
      "50/50 [==============================] - 0s 722us/step - loss: 0.3352 - accuracy: 0.8611\n",
      "Epoch 75/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3413 - accuracy: 0.8576\n",
      "Epoch 76/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.3457 - accuracy: 0.8589\n",
      "Epoch 77/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3438 - accuracy: 0.8576\n",
      "Epoch 78/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3465 - accuracy: 0.8576\n",
      "Epoch 79/1500\n",
      "50/50 [==============================] - 0s 732us/step - loss: 0.3450 - accuracy: 0.8561\n",
      "Epoch 80/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3486 - accuracy: 0.8482\n",
      "Epoch 81/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.3377 - accuracy: 0.8608\n",
      "Epoch 82/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.3419 - accuracy: 0.8598\n",
      "Epoch 83/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.3444 - accuracy: 0.8655\n",
      "Epoch 84/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.3367 - accuracy: 0.8580\n",
      "Epoch 85/1500\n",
      "50/50 [==============================] - 0s 719us/step - loss: 0.3383 - accuracy: 0.8605\n",
      "Epoch 86/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.3318 - accuracy: 0.8620\n",
      "Epoch 87/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3312 - accuracy: 0.8602\n",
      "Epoch 88/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.3245 - accuracy: 0.8709\n",
      "Epoch 89/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.3259 - accuracy: 0.8680\n",
      "Epoch 90/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.3271 - accuracy: 0.8712\n",
      "Epoch 91/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.3276 - accuracy: 0.8605\n",
      "Epoch 92/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.3262 - accuracy: 0.8715\n",
      "Epoch 93/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8671\n",
      "Epoch 94/1500\n",
      "50/50 [==============================] - 0s 771us/step - loss: 0.3269 - accuracy: 0.8605\n",
      "Epoch 95/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.3238 - accuracy: 0.8680\n",
      "Epoch 96/1500\n",
      "50/50 [==============================] - 0s 717us/step - loss: 0.3291 - accuracy: 0.8661\n",
      "Epoch 97/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.3250 - accuracy: 0.8646\n",
      "Epoch 98/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.3205 - accuracy: 0.8674\n",
      "Epoch 99/1500\n",
      "50/50 [==============================] - 0s 761us/step - loss: 0.3022 - accuracy: 0.8797\n",
      "Epoch 100/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.3174 - accuracy: 0.8734\n",
      "Epoch 101/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.3258 - accuracy: 0.8639\n",
      "Epoch 102/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.3106 - accuracy: 0.8759\n",
      "Epoch 103/1500\n",
      "50/50 [==============================] - 0s 771us/step - loss: 0.3135 - accuracy: 0.8699\n",
      "Epoch 104/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.3089 - accuracy: 0.8746\n",
      "Epoch 105/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.3258 - accuracy: 0.8728\n",
      "Epoch 106/1500\n",
      "50/50 [==============================] - 0s 767us/step - loss: 0.2982 - accuracy: 0.8746\n",
      "Epoch 107/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2996 - accuracy: 0.8787\n",
      "Epoch 108/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.3028 - accuracy: 0.8762\n",
      "Epoch 109/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.3018 - accuracy: 0.8775\n",
      "Epoch 110/1500\n",
      "50/50 [==============================] - 0s 730us/step - loss: 0.3144 - accuracy: 0.8709\n",
      "Epoch 111/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.3161 - accuracy: 0.8743\n",
      "Epoch 112/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.3084 - accuracy: 0.8699\n",
      "Epoch 113/1500\n",
      "50/50 [==============================] - 0s 724us/step - loss: 0.2946 - accuracy: 0.8816\n",
      "Epoch 114/1500\n",
      "50/50 [==============================] - 0s 738us/step - loss: 0.3095 - accuracy: 0.8734\n",
      "Epoch 115/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.3002 - accuracy: 0.8854\n",
      "Epoch 116/1500\n",
      "50/50 [==============================] - 0s 785us/step - loss: 0.2939 - accuracy: 0.8835\n",
      "Epoch 117/1500\n",
      "50/50 [==============================] - 0s 813us/step - loss: 0.3015 - accuracy: 0.8759\n",
      "Epoch 118/1500\n",
      "50/50 [==============================] - 0s 824us/step - loss: 0.3013 - accuracy: 0.8831\n",
      "Epoch 119/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.3003 - accuracy: 0.8750\n",
      "Epoch 120/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.3059 - accuracy: 0.8737\n",
      "Epoch 121/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.2989 - accuracy: 0.8746\n",
      "Epoch 122/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.3004 - accuracy: 0.8746\n",
      "Epoch 123/1500\n",
      "50/50 [==============================] - 0s 786us/step - loss: 0.2961 - accuracy: 0.8781\n",
      "Epoch 124/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3028 - accuracy: 0.8775\n",
      "Epoch 125/1500\n",
      "50/50 [==============================] - 0s 807us/step - loss: 0.2930 - accuracy: 0.8800\n",
      "Epoch 126/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2981 - accuracy: 0.8778\n",
      "Epoch 127/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2805 - accuracy: 0.8838\n",
      "Epoch 128/1500\n",
      "50/50 [==============================] - 0s 760us/step - loss: 0.2994 - accuracy: 0.8816\n",
      "Epoch 129/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.2797 - accuracy: 0.8879\n",
      "Epoch 130/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.2977 - accuracy: 0.8809\n",
      "Epoch 131/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.2850 - accuracy: 0.8803\n",
      "Epoch 132/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2908 - accuracy: 0.8838\n",
      "Epoch 133/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.2934 - accuracy: 0.8769\n",
      "Epoch 134/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2892 - accuracy: 0.8819\n",
      "Epoch 135/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2865 - accuracy: 0.8797\n",
      "Epoch 136/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2756 - accuracy: 0.8964\n",
      "Epoch 137/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2855 - accuracy: 0.8775\n",
      "Epoch 138/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2895 - accuracy: 0.8819\n",
      "Epoch 139/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2759 - accuracy: 0.8904\n",
      "Epoch 140/1500\n",
      "50/50 [==============================] - 0s 768us/step - loss: 0.2719 - accuracy: 0.8926\n",
      "Epoch 141/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2786 - accuracy: 0.8917\n",
      "Epoch 142/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2898 - accuracy: 0.8844\n",
      "Epoch 143/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2724 - accuracy: 0.8910\n",
      "Epoch 144/1500\n",
      "50/50 [==============================] - 0s 736us/step - loss: 0.2843 - accuracy: 0.8913\n",
      "Epoch 145/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.2803 - accuracy: 0.8872\n",
      "Epoch 146/1500\n",
      "50/50 [==============================] - 0s 756us/step - loss: 0.2897 - accuracy: 0.8869\n",
      "Epoch 147/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2880 - accuracy: 0.8844\n",
      "Epoch 148/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2720 - accuracy: 0.8876\n",
      "Epoch 149/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2777 - accuracy: 0.8935\n",
      "Epoch 150/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2662 - accuracy: 0.8945\n",
      "Epoch 151/1500\n",
      "50/50 [==============================] - 0s 747us/step - loss: 0.2763 - accuracy: 0.8926\n",
      "Epoch 152/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8913\n",
      "Epoch 153/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2590 - accuracy: 0.8951\n",
      "Epoch 154/1500\n",
      "50/50 [==============================] - 0s 766us/step - loss: 0.2533 - accuracy: 0.9014\n",
      "Epoch 155/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2667 - accuracy: 0.8939\n",
      "Epoch 156/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2648 - accuracy: 0.8961\n",
      "Epoch 157/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.2581 - accuracy: 0.8992\n",
      "Epoch 158/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.2792 - accuracy: 0.8885\n",
      "Epoch 159/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2695 - accuracy: 0.8891\n",
      "Epoch 160/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.2587 - accuracy: 0.8929\n",
      "Epoch 161/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.2719 - accuracy: 0.8923\n",
      "Epoch 162/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.2523 - accuracy: 0.9011\n",
      "Epoch 163/1500\n",
      "50/50 [==============================] - 0s 742us/step - loss: 0.2658 - accuracy: 0.8929\n",
      "Epoch 164/1500\n",
      "50/50 [==============================] - 0s 747us/step - loss: 0.2694 - accuracy: 0.8857\n",
      "Epoch 165/1500\n",
      "50/50 [==============================] - 0s 773us/step - loss: 0.2511 - accuracy: 0.8961\n",
      "Epoch 166/1500\n",
      "50/50 [==============================] - 0s 737us/step - loss: 0.2579 - accuracy: 0.8989\n",
      "Epoch 167/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2641 - accuracy: 0.8942\n",
      "Epoch 168/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.2662 - accuracy: 0.8973\n",
      "Epoch 169/1500\n",
      "50/50 [==============================] - 0s 761us/step - loss: 0.2458 - accuracy: 0.8986\n",
      "Epoch 170/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2602 - accuracy: 0.9011\n",
      "Epoch 171/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2641 - accuracy: 0.8970\n",
      "Epoch 172/1500\n",
      "50/50 [==============================] - 0s 739us/step - loss: 0.2615 - accuracy: 0.8976\n",
      "Epoch 173/1500\n",
      "50/50 [==============================] - 0s 764us/step - loss: 0.2547 - accuracy: 0.8961\n",
      "Epoch 174/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2553 - accuracy: 0.8935\n",
      "Epoch 175/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2628 - accuracy: 0.8973\n",
      "Epoch 176/1500\n",
      "50/50 [==============================] - 0s 762us/step - loss: 0.2561 - accuracy: 0.8970\n",
      "Epoch 177/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2596 - accuracy: 0.8967\n",
      "Epoch 178/1500\n",
      "50/50 [==============================] - 0s 820us/step - loss: 0.2527 - accuracy: 0.9020\n",
      "Epoch 179/1500\n",
      "50/50 [==============================] - 0s 789us/step - loss: 0.2523 - accuracy: 0.8998\n",
      "Epoch 180/1500\n",
      "50/50 [==============================] - 0s 772us/step - loss: 0.2431 - accuracy: 0.9046\n",
      "Epoch 181/1500\n",
      "50/50 [==============================] - 0s 791us/step - loss: 0.2606 - accuracy: 0.8923\n",
      "Epoch 182/1500\n",
      "50/50 [==============================] - 0s 762us/step - loss: 0.2475 - accuracy: 0.9011\n",
      "Epoch 183/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2385 - accuracy: 0.9087\n",
      "Epoch 184/1500\n",
      "50/50 [==============================] - 0s 725us/step - loss: 0.2608 - accuracy: 0.8976\n",
      "Epoch 185/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.2534 - accuracy: 0.8995\n",
      "Epoch 186/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2410 - accuracy: 0.9049\n",
      "Epoch 187/1500\n",
      "50/50 [==============================] - 0s 779us/step - loss: 0.2550 - accuracy: 0.9014\n",
      "Epoch 188/1500\n",
      "50/50 [==============================] - 0s 783us/step - loss: 0.2419 - accuracy: 0.9043\n",
      "Epoch 189/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2421 - accuracy: 0.9074\n",
      "Epoch 190/1500\n",
      "50/50 [==============================] - 0s 771us/step - loss: 0.2524 - accuracy: 0.8945\n",
      "Epoch 191/1500\n",
      "50/50 [==============================] - 0s 763us/step - loss: 0.2427 - accuracy: 0.9058\n",
      "Epoch 192/1500\n",
      "50/50 [==============================] - 0s 760us/step - loss: 0.2505 - accuracy: 0.9055\n",
      "Epoch 193/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.2498 - accuracy: 0.9017\n",
      "Epoch 194/1500\n",
      "50/50 [==============================] - 0s 763us/step - loss: 0.2350 - accuracy: 0.9112\n",
      "Epoch 195/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.2664 - accuracy: 0.8945\n",
      "Epoch 196/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2326 - accuracy: 0.9061\n",
      "Epoch 197/1500\n",
      "50/50 [==============================] - 0s 772us/step - loss: 0.2407 - accuracy: 0.9017\n",
      "Epoch 198/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.2413 - accuracy: 0.9011\n",
      "Epoch 199/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.2431 - accuracy: 0.8983\n",
      "Epoch 200/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9061\n",
      "Epoch 201/1500\n",
      "50/50 [==============================] - 0s 757us/step - loss: 0.2386 - accuracy: 0.9005\n",
      "Epoch 202/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2327 - accuracy: 0.9087\n",
      "Epoch 203/1500\n",
      "50/50 [==============================] - 0s 756us/step - loss: 0.2283 - accuracy: 0.9115\n",
      "Epoch 204/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2408 - accuracy: 0.9074\n",
      "Epoch 205/1500\n",
      "50/50 [==============================] - 0s 746us/step - loss: 0.2475 - accuracy: 0.9017\n",
      "Epoch 206/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2241 - accuracy: 0.9128\n",
      "Epoch 207/1500\n",
      "50/50 [==============================] - 0s 759us/step - loss: 0.2398 - accuracy: 0.8980\n",
      "Epoch 208/1500\n",
      "50/50 [==============================] - 0s 725us/step - loss: 0.2406 - accuracy: 0.9055\n",
      "Epoch 209/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.2279 - accuracy: 0.9153\n",
      "Epoch 210/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.2318 - accuracy: 0.9068\n",
      "Epoch 211/1500\n",
      "50/50 [==============================] - 0s 748us/step - loss: 0.2332 - accuracy: 0.8967\n",
      "Epoch 212/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2326 - accuracy: 0.9065\n",
      "Epoch 213/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2240 - accuracy: 0.9077\n",
      "Epoch 214/1500\n",
      "50/50 [==============================] - 0s 729us/step - loss: 0.2306 - accuracy: 0.9046\n",
      "Epoch 215/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2392 - accuracy: 0.9061\n",
      "Epoch 216/1500\n",
      "50/50 [==============================] - 0s 840us/step - loss: 0.2190 - accuracy: 0.9143\n",
      "Epoch 217/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2391 - accuracy: 0.9065\n",
      "Epoch 218/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2232 - accuracy: 0.9134\n",
      "Epoch 219/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2314 - accuracy: 0.9124\n",
      "Epoch 220/1500\n",
      "50/50 [==============================] - 0s 744us/step - loss: 0.2438 - accuracy: 0.9027\n",
      "Epoch 221/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.2162 - accuracy: 0.9121\n",
      "Epoch 222/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9049\n",
      "Epoch 223/1500\n",
      "50/50 [==============================] - 0s 745us/step - loss: 0.2210 - accuracy: 0.9128\n",
      "Epoch 224/1500\n",
      "50/50 [==============================] - 0s 741us/step - loss: 0.2192 - accuracy: 0.9096\n",
      "Epoch 225/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2247 - accuracy: 0.9131\n",
      "Epoch 226/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2384 - accuracy: 0.9052\n",
      "Epoch 227/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2105 - accuracy: 0.9191\n",
      "Epoch 228/1500\n",
      "50/50 [==============================] - 0s 747us/step - loss: 0.2299 - accuracy: 0.9083\n",
      "Epoch 229/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.2162 - accuracy: 0.9118\n",
      "Epoch 230/1500\n",
      "50/50 [==============================] - 0s 756us/step - loss: 0.2267 - accuracy: 0.9065\n",
      "Epoch 231/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2402 - accuracy: 0.9043\n",
      "Epoch 232/1500\n",
      "50/50 [==============================] - 0s 740us/step - loss: 0.2331 - accuracy: 0.9043\n",
      "Epoch 233/1500\n",
      "50/50 [==============================] - 0s 749us/step - loss: 0.2257 - accuracy: 0.9131\n",
      "Epoch 234/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2133 - accuracy: 0.9206\n",
      "Epoch 235/1500\n",
      "50/50 [==============================] - 0s 755us/step - loss: 0.2206 - accuracy: 0.9118\n",
      "Epoch 236/1500\n",
      "50/50 [==============================] - 0s 731us/step - loss: 0.2132 - accuracy: 0.9165\n",
      "Epoch 237/1500\n",
      "50/50 [==============================] - 0s 743us/step - loss: 0.2102 - accuracy: 0.9231\n",
      "Epoch 238/1500\n",
      "50/50 [==============================] - 0s 765us/step - loss: 0.2244 - accuracy: 0.9109\n",
      "Epoch 239/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2132 - accuracy: 0.9134\n",
      "Epoch 240/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2218 - accuracy: 0.9106\n",
      "Epoch 241/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2152 - accuracy: 0.9137\n",
      "Epoch 242/1500\n",
      "50/50 [==============================] - 0s 786us/step - loss: 0.2199 - accuracy: 0.9112\n",
      "Epoch 243/1500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9065\n",
      "Epoch 244/1500\n",
      "50/50 [==============================] - 0s 747us/step - loss: 0.2194 - accuracy: 0.9153\n",
      "Epoch 245/1500\n",
      "50/50 [==============================] - 0s 758us/step - loss: 0.2254 - accuracy: 0.9128\n",
      "Epoch 246/1500\n",
      "50/50 [==============================] - 0s 734us/step - loss: 0.2211 - accuracy: 0.9146\n",
      "Epoch 247/1500\n",
      "50/50 [==============================] - 0s 754us/step - loss: 0.2176 - accuracy: 0.9121\n",
      "Epoch 248/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2153 - accuracy: 0.9118\n",
      "Epoch 249/1500\n",
      "50/50 [==============================] - 0s 759us/step - loss: 0.2160 - accuracy: 0.9099\n",
      "Epoch 250/1500\n",
      "50/50 [==============================] - 0s 750us/step - loss: 0.2157 - accuracy: 0.9150\n",
      "Epoch 251/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2164 - accuracy: 0.9146\n",
      "Epoch 252/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2218 - accuracy: 0.9077\n",
      "Epoch 253/1500\n",
      "50/50 [==============================] - 0s 751us/step - loss: 0.2137 - accuracy: 0.9143\n",
      "Epoch 254/1500\n",
      "50/50 [==============================] - 0s 762us/step - loss: 0.2199 - accuracy: 0.9112\n",
      "Epoch 255/1500\n",
      "50/50 [==============================] - 0s 753us/step - loss: 0.2230 - accuracy: 0.9134\n",
      "Epoch 256/1500\n",
      "50/50 [==============================] - 0s 752us/step - loss: 0.2145 - accuracy: 0.9140\n",
      "Epoch 257/1500\n",
      " 1/50 [..............................] - ETA: 0s - loss: 0.2015 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 227.\n",
      "50/50 [==============================] - 0s 788us/step - loss: 0.2127 - accuracy: 0.9184\n",
      "Epoch 257: early stopping\n",
      "6/6 [==============================] - 0s 792us/step - loss: 1.1231 - accuracy: 0.6702\n",
      "6/6 [==============================] - 0s 595us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.62 (18/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 459, Predictions: 459, Actuals: 459, Gender: 459\n",
      "Final Test Results - Loss: 1.1231064796447754, Accuracy: 0.6701570749282837, Precision: 0.5016705516705516, Recall: 0.6416794674859191, F1 Score: 0.5297176135411429\n",
      "Confusion Matrix:\n",
      " [[105  28  22]\n",
      " [  6  15   0]\n",
      " [  6   1   8]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({1: 1116, 0: 1063, 2: 797})\n",
      "Epoch 1/1500\n",
      "47/47 [==============================] - 0s 898us/step - loss: 1.1232 - accuracy: 0.5111\n",
      "Epoch 2/1500\n",
      "47/47 [==============================] - 0s 855us/step - loss: 0.8426 - accuracy: 0.6458\n",
      "Epoch 3/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.7991 - accuracy: 0.6653\n",
      "Epoch 4/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.7456 - accuracy: 0.6878\n",
      "Epoch 5/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.7256 - accuracy: 0.6983\n",
      "Epoch 6/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.6920 - accuracy: 0.7114\n",
      "Epoch 7/1500\n",
      "47/47 [==============================] - 0s 692us/step - loss: 0.6629 - accuracy: 0.7177\n",
      "Epoch 8/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.7379\n",
      "Epoch 9/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.6397 - accuracy: 0.7295\n",
      "Epoch 10/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.6086 - accuracy: 0.7500\n",
      "Epoch 11/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.5877 - accuracy: 0.7517\n",
      "Epoch 12/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.5856 - accuracy: 0.7550\n",
      "Epoch 13/1500\n",
      "47/47 [==============================] - 0s 665us/step - loss: 0.5888 - accuracy: 0.7571\n",
      "Epoch 14/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.5707 - accuracy: 0.7577\n",
      "Epoch 15/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.5472 - accuracy: 0.7742\n",
      "Epoch 16/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.5409 - accuracy: 0.7765\n",
      "Epoch 17/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.5378 - accuracy: 0.7782\n",
      "Epoch 18/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.5183 - accuracy: 0.7779\n",
      "Epoch 19/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.5225 - accuracy: 0.7829\n",
      "Epoch 20/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.5175 - accuracy: 0.7846\n",
      "Epoch 21/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.5181 - accuracy: 0.7806\n",
      "Epoch 22/1500\n",
      "47/47 [==============================] - 0s 709us/step - loss: 0.5028 - accuracy: 0.7886\n",
      "Epoch 23/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.4964 - accuracy: 0.7923\n",
      "Epoch 24/1500\n",
      "47/47 [==============================] - 0s 827us/step - loss: 0.4965 - accuracy: 0.7917\n",
      "Epoch 25/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.4930 - accuracy: 0.7866\n",
      "Epoch 26/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.4908 - accuracy: 0.7883\n",
      "Epoch 27/1500\n",
      "47/47 [==============================] - 0s 697us/step - loss: 0.4722 - accuracy: 0.8024\n",
      "Epoch 28/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.4746 - accuracy: 0.8041\n",
      "Epoch 29/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.4713 - accuracy: 0.8014\n",
      "Epoch 30/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.4778 - accuracy: 0.8054\n",
      "Epoch 31/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.4703 - accuracy: 0.7977\n",
      "Epoch 32/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.4602 - accuracy: 0.8075\n",
      "Epoch 33/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.4476 - accuracy: 0.8118\n",
      "Epoch 34/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.4443 - accuracy: 0.8196\n",
      "Epoch 35/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.4573 - accuracy: 0.8105\n",
      "Epoch 36/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.4669 - accuracy: 0.8028\n",
      "Epoch 37/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.4559 - accuracy: 0.8085\n",
      "Epoch 38/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.4442 - accuracy: 0.8189\n",
      "Epoch 39/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.4412 - accuracy: 0.8122\n",
      "Epoch 40/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.4310 - accuracy: 0.8196\n",
      "Epoch 41/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.4241 - accuracy: 0.8219\n",
      "Epoch 42/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.4247 - accuracy: 0.8259\n",
      "Epoch 43/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.4290 - accuracy: 0.8206\n",
      "Epoch 44/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.4365 - accuracy: 0.8199\n",
      "Epoch 45/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.4164 - accuracy: 0.8246\n",
      "Epoch 46/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.4078 - accuracy: 0.8276\n",
      "Epoch 47/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.4211 - accuracy: 0.8266\n",
      "Epoch 48/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.4061 - accuracy: 0.8317\n",
      "Epoch 49/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.4076 - accuracy: 0.8333\n",
      "Epoch 50/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.4314 - accuracy: 0.8236\n",
      "Epoch 51/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.3945 - accuracy: 0.8337\n",
      "Epoch 52/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.4194 - accuracy: 0.8263\n",
      "Epoch 53/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.4103 - accuracy: 0.8290\n",
      "Epoch 54/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.3928 - accuracy: 0.8337\n",
      "Epoch 55/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.3967 - accuracy: 0.8357\n",
      "Epoch 56/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8421\n",
      "Epoch 57/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.3951 - accuracy: 0.8327\n",
      "Epoch 58/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.3964 - accuracy: 0.8350\n",
      "Epoch 59/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.3980 - accuracy: 0.8330\n",
      "Epoch 60/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.3798 - accuracy: 0.8411\n",
      "Epoch 61/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.3739 - accuracy: 0.8438\n",
      "Epoch 62/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.3866 - accuracy: 0.8394\n",
      "Epoch 63/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.4033 - accuracy: 0.8387\n",
      "Epoch 64/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.3918 - accuracy: 0.8414\n",
      "Epoch 65/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.3739 - accuracy: 0.8421\n",
      "Epoch 66/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.3671 - accuracy: 0.8441\n",
      "Epoch 67/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.3601 - accuracy: 0.8501\n",
      "Epoch 68/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.3645 - accuracy: 0.8491\n",
      "Epoch 69/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.3727 - accuracy: 0.8505\n",
      "Epoch 70/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.3831 - accuracy: 0.8417\n",
      "Epoch 71/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.3614 - accuracy: 0.8491\n",
      "Epoch 72/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.3603 - accuracy: 0.8454\n",
      "Epoch 73/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.3554 - accuracy: 0.8542\n",
      "Epoch 74/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.3496 - accuracy: 0.8602\n",
      "Epoch 75/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.3633 - accuracy: 0.8522\n",
      "Epoch 76/1500\n",
      "47/47 [==============================] - 0s 750us/step - loss: 0.3675 - accuracy: 0.8522\n",
      "Epoch 77/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.3638 - accuracy: 0.8481\n",
      "Epoch 78/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.3544 - accuracy: 0.8562\n",
      "Epoch 79/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.3677 - accuracy: 0.8414\n",
      "Epoch 80/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.3562 - accuracy: 0.8515\n",
      "Epoch 81/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.3674 - accuracy: 0.8468\n",
      "Epoch 82/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.3591 - accuracy: 0.8505\n",
      "Epoch 83/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.3589 - accuracy: 0.8542\n",
      "Epoch 84/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.3460 - accuracy: 0.8515\n",
      "Epoch 85/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.3522 - accuracy: 0.8579\n",
      "Epoch 86/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.3385 - accuracy: 0.8579\n",
      "Epoch 87/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.3536 - accuracy: 0.8501\n",
      "Epoch 88/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.3567 - accuracy: 0.8501\n",
      "Epoch 89/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.3406 - accuracy: 0.8676\n",
      "Epoch 90/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.3362 - accuracy: 0.8582\n",
      "Epoch 91/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.3444 - accuracy: 0.8649\n",
      "Epoch 92/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.3429 - accuracy: 0.8595\n",
      "Epoch 93/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8622\n",
      "Epoch 94/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.3422 - accuracy: 0.8565\n",
      "Epoch 95/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.3478 - accuracy: 0.8569\n",
      "Epoch 96/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.3331 - accuracy: 0.8646\n",
      "Epoch 97/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.3394 - accuracy: 0.8619\n",
      "Epoch 98/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.3205 - accuracy: 0.8690\n",
      "Epoch 99/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.3313 - accuracy: 0.8606\n",
      "Epoch 100/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.3274 - accuracy: 0.8653\n",
      "Epoch 101/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.3224 - accuracy: 0.8636\n",
      "Epoch 102/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.3267 - accuracy: 0.8676\n",
      "Epoch 103/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.3321 - accuracy: 0.8683\n",
      "Epoch 104/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.3315 - accuracy: 0.8690\n",
      "Epoch 105/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.3285 - accuracy: 0.8626\n",
      "Epoch 106/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.3283 - accuracy: 0.8622\n",
      "Epoch 107/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.3270 - accuracy: 0.8639\n",
      "Epoch 108/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.3288 - accuracy: 0.8653\n",
      "Epoch 109/1500\n",
      "47/47 [==============================] - 0s 706us/step - loss: 0.3286 - accuracy: 0.8723\n",
      "Epoch 110/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.3226 - accuracy: 0.8626\n",
      "Epoch 111/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.3090 - accuracy: 0.8780\n",
      "Epoch 112/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.3218 - accuracy: 0.8642\n",
      "Epoch 113/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.3156 - accuracy: 0.8720\n",
      "Epoch 114/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.3192 - accuracy: 0.8552\n",
      "Epoch 115/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.3319 - accuracy: 0.8592\n",
      "Epoch 116/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.3024 - accuracy: 0.8740\n",
      "Epoch 117/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.3122 - accuracy: 0.8710\n",
      "Epoch 118/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.3116 - accuracy: 0.8703\n",
      "Epoch 119/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.3096 - accuracy: 0.8740\n",
      "Epoch 120/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.3161 - accuracy: 0.8636\n",
      "Epoch 121/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.3161 - accuracy: 0.8737\n",
      "Epoch 122/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.3067 - accuracy: 0.8784\n",
      "Epoch 123/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.2977 - accuracy: 0.8841\n",
      "Epoch 124/1500\n",
      "47/47 [==============================] - 0s 714us/step - loss: 0.3129 - accuracy: 0.8740\n",
      "Epoch 125/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2927 - accuracy: 0.8851\n",
      "Epoch 126/1500\n",
      "47/47 [==============================] - 0s 750us/step - loss: 0.3010 - accuracy: 0.8810\n",
      "Epoch 127/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.3019 - accuracy: 0.8767\n",
      "Epoch 128/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2948 - accuracy: 0.8821\n",
      "Epoch 129/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.3000 - accuracy: 0.8780\n",
      "Epoch 130/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2967 - accuracy: 0.8821\n",
      "Epoch 131/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.3067 - accuracy: 0.8757\n",
      "Epoch 132/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2916 - accuracy: 0.8854\n",
      "Epoch 133/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.3037 - accuracy: 0.8770\n",
      "Epoch 134/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3002 - accuracy: 0.8790\n",
      "Epoch 135/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.2851 - accuracy: 0.8861\n",
      "Epoch 136/1500\n",
      "47/47 [==============================] - 0s 750us/step - loss: 0.2994 - accuracy: 0.8723\n",
      "Epoch 137/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2944 - accuracy: 0.8804\n",
      "Epoch 138/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2966 - accuracy: 0.8837\n",
      "Epoch 139/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2936 - accuracy: 0.8804\n",
      "Epoch 140/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2885 - accuracy: 0.8868\n",
      "Epoch 141/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.2868 - accuracy: 0.8861\n",
      "Epoch 142/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2695 - accuracy: 0.8878\n",
      "Epoch 143/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2966 - accuracy: 0.8794\n",
      "Epoch 144/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2826 - accuracy: 0.8878\n",
      "Epoch 145/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2948 - accuracy: 0.8824\n",
      "Epoch 146/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.2917 - accuracy: 0.8777\n",
      "Epoch 147/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.2951 - accuracy: 0.8760\n",
      "Epoch 148/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2835 - accuracy: 0.8891\n",
      "Epoch 149/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2871 - accuracy: 0.8787\n",
      "Epoch 150/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.2803 - accuracy: 0.8864\n",
      "Epoch 151/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2850 - accuracy: 0.8861\n",
      "Epoch 152/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.2887 - accuracy: 0.8743\n",
      "Epoch 153/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8928\n",
      "Epoch 154/1500\n",
      "47/47 [==============================] - 0s 802us/step - loss: 0.2768 - accuracy: 0.8878\n",
      "Epoch 155/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2745 - accuracy: 0.8894\n",
      "Epoch 156/1500\n",
      "47/47 [==============================] - 0s 711us/step - loss: 0.2846 - accuracy: 0.8871\n",
      "Epoch 157/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.2632 - accuracy: 0.8985\n",
      "Epoch 158/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2804 - accuracy: 0.8854\n",
      "Epoch 159/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2789 - accuracy: 0.8851\n",
      "Epoch 160/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.2736 - accuracy: 0.8928\n",
      "Epoch 161/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2747 - accuracy: 0.8871\n",
      "Epoch 162/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.2750 - accuracy: 0.8874\n",
      "Epoch 163/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2661 - accuracy: 0.8931\n",
      "Epoch 164/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.2710 - accuracy: 0.8894\n",
      "Epoch 165/1500\n",
      "47/47 [==============================] - 0s 715us/step - loss: 0.2662 - accuracy: 0.8918\n",
      "Epoch 166/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2762 - accuracy: 0.8884\n",
      "Epoch 167/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.2731 - accuracy: 0.8942\n",
      "Epoch 168/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2601 - accuracy: 0.8968\n",
      "Epoch 169/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.2820 - accuracy: 0.8898\n",
      "Epoch 170/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2711 - accuracy: 0.8938\n",
      "Epoch 171/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2595 - accuracy: 0.9022\n",
      "Epoch 172/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2620 - accuracy: 0.8935\n",
      "Epoch 173/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2567 - accuracy: 0.8985\n",
      "Epoch 174/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2693 - accuracy: 0.8921\n",
      "Epoch 175/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.2555 - accuracy: 0.9032\n",
      "Epoch 176/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2604 - accuracy: 0.8928\n",
      "Epoch 177/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2616 - accuracy: 0.8955\n",
      "Epoch 178/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.8928\n",
      "Epoch 179/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.2545 - accuracy: 0.8918\n",
      "Epoch 180/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2536 - accuracy: 0.8968\n",
      "Epoch 181/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.2584 - accuracy: 0.8945\n",
      "Epoch 182/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.2467 - accuracy: 0.9009\n",
      "Epoch 183/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.2545 - accuracy: 0.9015\n",
      "Epoch 184/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2705 - accuracy: 0.8915\n",
      "Epoch 185/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2685 - accuracy: 0.8884\n",
      "Epoch 186/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2562 - accuracy: 0.8935\n",
      "Epoch 187/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2677 - accuracy: 0.8995\n",
      "Epoch 188/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.2556 - accuracy: 0.8995\n",
      "Epoch 189/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2517 - accuracy: 0.8999\n",
      "Epoch 190/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2395 - accuracy: 0.9052\n",
      "Epoch 191/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.2551 - accuracy: 0.9019\n",
      "Epoch 192/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.2428 - accuracy: 0.9026\n",
      "Epoch 193/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.2565 - accuracy: 0.9029\n",
      "Epoch 194/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2564 - accuracy: 0.8962\n",
      "Epoch 195/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.2356 - accuracy: 0.9029\n",
      "Epoch 196/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2523 - accuracy: 0.8955\n",
      "Epoch 197/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2490 - accuracy: 0.8955\n",
      "Epoch 198/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.2388 - accuracy: 0.9089\n",
      "Epoch 199/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2466 - accuracy: 0.9012\n",
      "Epoch 200/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.2415 - accuracy: 0.9052\n",
      "Epoch 201/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9042\n",
      "Epoch 202/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2418 - accuracy: 0.9046\n",
      "Epoch 203/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.2447 - accuracy: 0.9012\n",
      "Epoch 204/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.2472 - accuracy: 0.9042\n",
      "Epoch 205/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2393 - accuracy: 0.9069\n",
      "Epoch 206/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2511 - accuracy: 0.8965\n",
      "Epoch 207/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2378 - accuracy: 0.9089\n",
      "Epoch 208/1500\n",
      "47/47 [==============================] - 0s 798us/step - loss: 0.2361 - accuracy: 0.9096\n",
      "Epoch 209/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.2332 - accuracy: 0.9076\n",
      "Epoch 210/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.2453 - accuracy: 0.9042\n",
      "Epoch 211/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2408 - accuracy: 0.9005\n",
      "Epoch 212/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2365 - accuracy: 0.9036\n",
      "Epoch 213/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.2408 - accuracy: 0.9002\n",
      "Epoch 214/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2343 - accuracy: 0.9039\n",
      "Epoch 215/1500\n",
      "47/47 [==============================] - 0s 708us/step - loss: 0.2307 - accuracy: 0.9083\n",
      "Epoch 216/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.2305 - accuracy: 0.9113\n",
      "Epoch 217/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2404 - accuracy: 0.9062\n",
      "Epoch 218/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.2450 - accuracy: 0.9069\n",
      "Epoch 219/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2338 - accuracy: 0.9073\n",
      "Epoch 220/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.2370 - accuracy: 0.9042\n",
      "Epoch 221/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2248 - accuracy: 0.9099\n",
      "Epoch 222/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2271 - accuracy: 0.9093\n",
      "Epoch 223/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.9140\n",
      "Epoch 224/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.2200 - accuracy: 0.9089\n",
      "Epoch 225/1500\n",
      "47/47 [==============================] - 0s 836us/step - loss: 0.2355 - accuracy: 0.9042\n",
      "Epoch 226/1500\n",
      "47/47 [==============================] - 0s 867us/step - loss: 0.2361 - accuracy: 0.9032\n",
      "Epoch 227/1500\n",
      "47/47 [==============================] - 0s 896us/step - loss: 0.2442 - accuracy: 0.9009\n",
      "Epoch 228/1500\n",
      "47/47 [==============================] - 0s 882us/step - loss: 0.2209 - accuracy: 0.9140\n",
      "Epoch 229/1500\n",
      "47/47 [==============================] - 0s 839us/step - loss: 0.2258 - accuracy: 0.9103\n",
      "Epoch 230/1500\n",
      "47/47 [==============================] - 0s 814us/step - loss: 0.2292 - accuracy: 0.9126\n",
      "Epoch 231/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.2401 - accuracy: 0.9069\n",
      "Epoch 232/1500\n",
      "47/47 [==============================] - 0s 813us/step - loss: 0.2370 - accuracy: 0.8995\n",
      "Epoch 233/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.2279 - accuracy: 0.9062\n",
      "Epoch 234/1500\n",
      "47/47 [==============================] - 0s 784us/step - loss: 0.2189 - accuracy: 0.9103\n",
      "Epoch 235/1500\n",
      "47/47 [==============================] - 0s 796us/step - loss: 0.2243 - accuracy: 0.9140\n",
      "Epoch 236/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.2357 - accuracy: 0.9093\n",
      "Epoch 237/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.2357 - accuracy: 0.9086\n",
      "Epoch 238/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2361 - accuracy: 0.9066\n",
      "Epoch 239/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2376 - accuracy: 0.9123\n",
      "Epoch 240/1500\n",
      "47/47 [==============================] - 0s 750us/step - loss: 0.2199 - accuracy: 0.9180\n",
      "Epoch 241/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.2279 - accuracy: 0.9073\n",
      "Epoch 242/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2317 - accuracy: 0.9073\n",
      "Epoch 243/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2192 - accuracy: 0.9173\n",
      "Epoch 244/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9120\n",
      "Epoch 245/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.2229 - accuracy: 0.9039\n",
      "Epoch 246/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.2270 - accuracy: 0.9133\n",
      "Epoch 247/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.2087 - accuracy: 0.9157\n",
      "Epoch 248/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.2216 - accuracy: 0.9133\n",
      "Epoch 249/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2254 - accuracy: 0.9147\n",
      "Epoch 250/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2149 - accuracy: 0.9147\n",
      "Epoch 251/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2065 - accuracy: 0.9173\n",
      "Epoch 252/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2213 - accuracy: 0.9153\n",
      "Epoch 253/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2212 - accuracy: 0.9207\n",
      "Epoch 254/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.2104 - accuracy: 0.9163\n",
      "Epoch 255/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2012 - accuracy: 0.9163\n",
      "Epoch 256/1500\n",
      "47/47 [==============================] - 0s 802us/step - loss: 0.2149 - accuracy: 0.9153\n",
      "Epoch 257/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.2041 - accuracy: 0.9210\n",
      "Epoch 258/1500\n",
      "47/47 [==============================] - 0s 793us/step - loss: 0.2206 - accuracy: 0.9133\n",
      "Epoch 259/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2033 - accuracy: 0.9214\n",
      "Epoch 260/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.2062 - accuracy: 0.9150\n",
      "Epoch 261/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2066 - accuracy: 0.9177\n",
      "Epoch 262/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2178 - accuracy: 0.9147\n",
      "Epoch 263/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.9143\n",
      "Epoch 264/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.2241 - accuracy: 0.9076\n",
      "Epoch 265/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.2142 - accuracy: 0.9136\n",
      "Epoch 266/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2106 - accuracy: 0.9200\n",
      "Epoch 267/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.2090 - accuracy: 0.9197\n",
      "Epoch 268/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.2149 - accuracy: 0.9183\n",
      "Epoch 269/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.2076 - accuracy: 0.9194\n",
      "Epoch 270/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.2203 - accuracy: 0.9113\n",
      "Epoch 271/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2182 - accuracy: 0.9147\n",
      "Epoch 272/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2095 - accuracy: 0.9187\n",
      "Epoch 273/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1965 - accuracy: 0.9251\n",
      "Epoch 274/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2080 - accuracy: 0.9224\n",
      "Epoch 275/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.1989 - accuracy: 0.9271\n",
      "Epoch 276/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2029 - accuracy: 0.9200\n",
      "Epoch 277/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2081 - accuracy: 0.9187\n",
      "Epoch 278/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.1961 - accuracy: 0.9227\n",
      "Epoch 279/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.2080 - accuracy: 0.9190\n",
      "Epoch 280/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.2164 - accuracy: 0.9177\n",
      "Epoch 281/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9190\n",
      "Epoch 282/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.1931 - accuracy: 0.9237\n",
      "Epoch 283/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.2063 - accuracy: 0.9214\n",
      "Epoch 284/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.1881 - accuracy: 0.9274\n",
      "Epoch 285/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2006 - accuracy: 0.9234\n",
      "Epoch 286/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2011 - accuracy: 0.9241\n",
      "Epoch 287/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.1979 - accuracy: 0.9220\n",
      "Epoch 288/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.2030 - accuracy: 0.9264\n",
      "Epoch 289/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.1891 - accuracy: 0.9318\n",
      "Epoch 290/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.1967 - accuracy: 0.9231\n",
      "Epoch 291/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.1903 - accuracy: 0.9291\n",
      "Epoch 292/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.1942 - accuracy: 0.9274\n",
      "Epoch 293/1500\n",
      "47/47 [==============================] - 0s 784us/step - loss: 0.2036 - accuracy: 0.9224\n",
      "Epoch 294/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.2045 - accuracy: 0.9224\n",
      "Epoch 295/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1885 - accuracy: 0.9247\n",
      "Epoch 296/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1968 - accuracy: 0.9264\n",
      "Epoch 297/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2025 - accuracy: 0.9194\n",
      "Epoch 298/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1912 - accuracy: 0.9284\n",
      "Epoch 299/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9194\n",
      "Epoch 300/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.1921 - accuracy: 0.9251\n",
      "Epoch 301/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.1917 - accuracy: 0.9284\n",
      "Epoch 302/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.1873 - accuracy: 0.9298\n",
      "Epoch 303/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1997 - accuracy: 0.9214\n",
      "Epoch 304/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2011 - accuracy: 0.9200\n",
      "Epoch 305/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.1853 - accuracy: 0.9281\n",
      "Epoch 306/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1888 - accuracy: 0.9257\n",
      "Epoch 307/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1965 - accuracy: 0.9177\n",
      "Epoch 308/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.1973 - accuracy: 0.9237\n",
      "Epoch 309/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.2013 - accuracy: 0.9254\n",
      "Epoch 310/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.1844 - accuracy: 0.9304\n",
      "Epoch 311/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1835 - accuracy: 0.9298\n",
      "Epoch 312/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.1965 - accuracy: 0.9264\n",
      "Epoch 313/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2037 - accuracy: 0.9200\n",
      "Epoch 314/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.1955 - accuracy: 0.9217\n",
      "Epoch 315/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.2036 - accuracy: 0.9204\n",
      "Epoch 316/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1859 - accuracy: 0.9257\n",
      "Epoch 317/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.1876 - accuracy: 0.9257\n",
      "Epoch 318/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1889 - accuracy: 0.9254\n",
      "Epoch 319/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1809 - accuracy: 0.9274\n",
      "Epoch 320/1500\n",
      "47/47 [==============================] - 0s 805us/step - loss: 0.1859 - accuracy: 0.9325\n",
      "Epoch 321/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1879 - accuracy: 0.9291\n",
      "Epoch 322/1500\n",
      "47/47 [==============================] - 0s 783us/step - loss: 0.1785 - accuracy: 0.9368\n",
      "Epoch 323/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.1907 - accuracy: 0.9254\n",
      "Epoch 324/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.1903 - accuracy: 0.9291\n",
      "Epoch 325/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.1957 - accuracy: 0.9254\n",
      "Epoch 326/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1773 - accuracy: 0.9318\n",
      "Epoch 327/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.1885 - accuracy: 0.9325\n",
      "Epoch 328/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1782 - accuracy: 0.9382\n",
      "Epoch 329/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1824 - accuracy: 0.9304\n",
      "Epoch 330/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1891 - accuracy: 0.9257\n",
      "Epoch 331/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1771 - accuracy: 0.9331\n",
      "Epoch 332/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1847 - accuracy: 0.9271\n",
      "Epoch 333/1500\n",
      "47/47 [==============================] - 0s 812us/step - loss: 0.1698 - accuracy: 0.9328\n",
      "Epoch 334/1500\n",
      "47/47 [==============================] - 0s 784us/step - loss: 0.1666 - accuracy: 0.9351\n",
      "Epoch 335/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.1841 - accuracy: 0.9284\n",
      "Epoch 336/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.1728 - accuracy: 0.9368\n",
      "Epoch 337/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.1832 - accuracy: 0.9278\n",
      "Epoch 338/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.1823 - accuracy: 0.9365\n",
      "Epoch 339/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.1871 - accuracy: 0.9274\n",
      "Epoch 340/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1956 - accuracy: 0.9284\n",
      "Epoch 341/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.1759 - accuracy: 0.9301\n",
      "Epoch 342/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1856 - accuracy: 0.9224\n",
      "Epoch 343/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.1809 - accuracy: 0.9294\n",
      "Epoch 344/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.1819 - accuracy: 0.9315\n",
      "Epoch 345/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.1820 - accuracy: 0.9254\n",
      "Epoch 346/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1779 - accuracy: 0.9385\n",
      "Epoch 347/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.1708 - accuracy: 0.9335\n",
      "Epoch 348/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1837 - accuracy: 0.9318\n",
      "Epoch 349/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1774 - accuracy: 0.9271\n",
      "Epoch 350/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1754 - accuracy: 0.9341\n",
      "Epoch 351/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.1749 - accuracy: 0.9331\n",
      "Epoch 352/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1726 - accuracy: 0.9288\n",
      "Epoch 353/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1765 - accuracy: 0.9294\n",
      "Epoch 354/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.1716 - accuracy: 0.9351\n",
      "Epoch 355/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.1687 - accuracy: 0.9335\n",
      "Epoch 356/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.1705 - accuracy: 0.9395\n",
      "Epoch 357/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.1807 - accuracy: 0.9331\n",
      "Epoch 358/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.1779 - accuracy: 0.9338\n",
      "Epoch 359/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.1750 - accuracy: 0.9318\n",
      "Epoch 360/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.1765 - accuracy: 0.9298\n",
      "Epoch 361/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.1895 - accuracy: 0.9294\n",
      "Epoch 362/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.1801 - accuracy: 0.9321\n",
      "Epoch 363/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.9362\n",
      "Epoch 364/1500\n",
      "47/47 [==============================] - 0s 815us/step - loss: 0.1606 - accuracy: 0.9402\n",
      "Epoch 365/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1749 - accuracy: 0.9365\n",
      "Epoch 366/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1706 - accuracy: 0.9335\n",
      "Epoch 367/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1695 - accuracy: 0.9355\n",
      "Epoch 368/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.1749 - accuracy: 0.9351\n",
      "Epoch 369/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1762 - accuracy: 0.9321\n",
      "Epoch 370/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1644 - accuracy: 0.9378\n",
      "Epoch 371/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.1716 - accuracy: 0.9365\n",
      "Epoch 372/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.1710 - accuracy: 0.9351\n",
      "Epoch 373/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.1815 - accuracy: 0.9308\n",
      "Epoch 374/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1671 - accuracy: 0.9382\n",
      "Epoch 375/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.1708 - accuracy: 0.9351\n",
      "Epoch 376/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1669 - accuracy: 0.9368\n",
      "Epoch 377/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1792 - accuracy: 0.9328\n",
      "Epoch 378/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.1678 - accuracy: 0.9365\n",
      "Epoch 379/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.1633 - accuracy: 0.9362\n",
      "Epoch 380/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.1705 - accuracy: 0.9375\n",
      "Epoch 381/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1706 - accuracy: 0.9375\n",
      "Epoch 382/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.1679 - accuracy: 0.9399\n",
      "Epoch 383/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1753 - accuracy: 0.9321\n",
      "Epoch 384/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1630 - accuracy: 0.9405\n",
      "Epoch 385/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.1671 - accuracy: 0.9311\n",
      "Epoch 386/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.1707 - accuracy: 0.9351\n",
      "Epoch 387/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1591 - accuracy: 0.9368\n",
      "Epoch 388/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.1786 - accuracy: 0.9341\n",
      "Epoch 389/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.1711 - accuracy: 0.9358\n",
      "Epoch 390/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1671 - accuracy: 0.9348\n",
      "Epoch 391/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9315\n",
      "Epoch 392/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.1681 - accuracy: 0.9385\n",
      "Epoch 393/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.1714 - accuracy: 0.9315\n",
      "Epoch 394/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.1630 - accuracy: 0.9368\n",
      "Epoch 395/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1779 - accuracy: 0.9331\n",
      "Epoch 396/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.1567 - accuracy: 0.9419\n",
      "Epoch 397/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.1536 - accuracy: 0.9435\n",
      "Epoch 398/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.1740 - accuracy: 0.9341\n",
      "Epoch 399/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1730 - accuracy: 0.9368\n",
      "Epoch 400/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.1631 - accuracy: 0.9382\n",
      "Epoch 401/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.1661 - accuracy: 0.9345\n",
      "Epoch 402/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.1584 - accuracy: 0.9435\n",
      "Epoch 403/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1569 - accuracy: 0.9368\n",
      "Epoch 404/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.1660 - accuracy: 0.9385\n",
      "Epoch 405/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1695 - accuracy: 0.9365\n",
      "Epoch 406/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.1685 - accuracy: 0.9348\n",
      "Epoch 407/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.1598 - accuracy: 0.9405\n",
      "Epoch 408/1500\n",
      "47/47 [==============================] - 0s 832us/step - loss: 0.1622 - accuracy: 0.9388\n",
      "Epoch 409/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.1708 - accuracy: 0.9301\n",
      "Epoch 410/1500\n",
      "47/47 [==============================] - 0s 802us/step - loss: 0.1670 - accuracy: 0.9382\n",
      "Epoch 411/1500\n",
      "47/47 [==============================] - 0s 818us/step - loss: 0.1498 - accuracy: 0.9462\n",
      "Epoch 412/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.1613 - accuracy: 0.9382\n",
      "Epoch 413/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.1606 - accuracy: 0.9402\n",
      "Epoch 414/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.1656 - accuracy: 0.9345\n",
      "Epoch 415/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.1592 - accuracy: 0.9439\n",
      "Epoch 416/1500\n",
      "47/47 [==============================] - 0s 802us/step - loss: 0.1563 - accuracy: 0.9402\n",
      "Epoch 417/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.1591 - accuracy: 0.9385\n",
      "Epoch 418/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.1709 - accuracy: 0.9392\n",
      "Epoch 419/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9362\n",
      "Epoch 420/1500\n",
      "47/47 [==============================] - 0s 819us/step - loss: 0.1838 - accuracy: 0.9304\n",
      "Epoch 421/1500\n",
      "47/47 [==============================] - 0s 820us/step - loss: 0.1635 - accuracy: 0.9365\n",
      "Epoch 422/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.1642 - accuracy: 0.9439\n",
      "Epoch 423/1500\n",
      "47/47 [==============================] - 0s 861us/step - loss: 0.1625 - accuracy: 0.9399\n",
      "Epoch 424/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1647 - accuracy: 0.9358\n",
      "Epoch 425/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.1574 - accuracy: 0.9405\n",
      "Epoch 426/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.1575 - accuracy: 0.9395\n",
      "Epoch 427/1500\n",
      "47/47 [==============================] - 0s 827us/step - loss: 0.1510 - accuracy: 0.9412\n",
      "Epoch 428/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.1577 - accuracy: 0.9412\n",
      "Epoch 429/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.1583 - accuracy: 0.9425\n",
      "Epoch 430/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.1734 - accuracy: 0.9318\n",
      "Epoch 431/1500\n",
      "47/47 [==============================] - 0s 793us/step - loss: 0.1651 - accuracy: 0.9395\n",
      "Epoch 432/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9452\n",
      "Epoch 433/1500\n",
      "47/47 [==============================] - 0s 851us/step - loss: 0.1639 - accuracy: 0.9405\n",
      "Epoch 434/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.1620 - accuracy: 0.9355\n",
      "Epoch 435/1500\n",
      "47/47 [==============================] - 0s 790us/step - loss: 0.1565 - accuracy: 0.9415\n",
      "Epoch 436/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1501 - accuracy: 0.9446\n",
      "Epoch 437/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.1504 - accuracy: 0.9412\n",
      "Epoch 438/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.1439 - accuracy: 0.9459\n",
      "Epoch 439/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.1404 - accuracy: 0.9452\n",
      "Epoch 440/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1625 - accuracy: 0.9409\n",
      "Epoch 441/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.1648 - accuracy: 0.9415\n",
      "Epoch 442/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1454 - accuracy: 0.9439\n",
      "Epoch 443/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.1412 - accuracy: 0.9446\n",
      "Epoch 444/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1521 - accuracy: 0.9429\n",
      "Epoch 445/1500\n",
      "47/47 [==============================] - 0s 795us/step - loss: 0.1596 - accuracy: 0.9365\n",
      "Epoch 446/1500\n",
      "47/47 [==============================] - 0s 786us/step - loss: 0.1405 - accuracy: 0.9469\n",
      "Epoch 447/1500\n",
      "47/47 [==============================] - 0s 790us/step - loss: 0.1556 - accuracy: 0.9368\n",
      "Epoch 448/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1494 - accuracy: 0.9395\n",
      "Epoch 449/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1487 - accuracy: 0.9412\n",
      "Epoch 450/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.1624 - accuracy: 0.9358\n",
      "Epoch 451/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.1696 - accuracy: 0.9365\n",
      "Epoch 452/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1639 - accuracy: 0.9331\n",
      "Epoch 453/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.1650 - accuracy: 0.9338\n",
      "Epoch 454/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.1648 - accuracy: 0.9318\n",
      "Epoch 455/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.1423 - accuracy: 0.9459\n",
      "Epoch 456/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9412\n",
      "Epoch 457/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.1471 - accuracy: 0.9419\n",
      "Epoch 458/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.1616 - accuracy: 0.9385\n",
      "Epoch 459/1500\n",
      "47/47 [==============================] - 0s 790us/step - loss: 0.1367 - accuracy: 0.9479\n",
      "Epoch 460/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.1511 - accuracy: 0.9446\n",
      "Epoch 461/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.1468 - accuracy: 0.9409\n",
      "Epoch 462/1500\n",
      "47/47 [==============================] - 0s 812us/step - loss: 0.1597 - accuracy: 0.9409\n",
      "Epoch 463/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.1598 - accuracy: 0.9392\n",
      "Epoch 464/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1382 - accuracy: 0.9503\n",
      "Epoch 465/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.1496 - accuracy: 0.9409\n",
      "Epoch 466/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.1463 - accuracy: 0.9449\n",
      "Epoch 467/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1503 - accuracy: 0.9456\n",
      "Epoch 468/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9321\n",
      "Epoch 469/1500\n",
      "47/47 [==============================] - 0s 837us/step - loss: 0.1405 - accuracy: 0.9499\n",
      "Epoch 470/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.1587 - accuracy: 0.9412\n",
      "Epoch 471/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.1565 - accuracy: 0.9415\n",
      "Epoch 472/1500\n",
      "47/47 [==============================] - 0s 821us/step - loss: 0.1468 - accuracy: 0.9479\n",
      "Epoch 473/1500\n",
      "47/47 [==============================] - 0s 795us/step - loss: 0.1564 - accuracy: 0.9442\n",
      "Epoch 474/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.1598 - accuracy: 0.9429\n",
      "Epoch 475/1500\n",
      "47/47 [==============================] - 0s 793us/step - loss: 0.1556 - accuracy: 0.9422\n",
      "Epoch 476/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.1593 - accuracy: 0.9385\n",
      "Epoch 477/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.1505 - accuracy: 0.9399\n",
      "Epoch 478/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.1546 - accuracy: 0.9446\n",
      "Epoch 479/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.1468 - accuracy: 0.9439\n",
      "Epoch 480/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1458 - accuracy: 0.9446\n",
      "Epoch 481/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.1586 - accuracy: 0.9385\n",
      "Epoch 482/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.1582 - accuracy: 0.9415\n",
      "Epoch 483/1500\n",
      "47/47 [==============================] - 0s 786us/step - loss: 0.1418 - accuracy: 0.9456\n",
      "Epoch 484/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1494 - accuracy: 0.9422\n",
      "Epoch 485/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.1343 - accuracy: 0.9519\n",
      "Epoch 486/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.1700 - accuracy: 0.9338\n",
      "Epoch 487/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.1565 - accuracy: 0.9362\n",
      "Epoch 488/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1506 - accuracy: 0.9425\n",
      "Epoch 489/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1422 - accuracy: 0.9476\n",
      "Epoch 490/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1385 - accuracy: 0.9479\n",
      "Epoch 491/1500\n",
      "47/47 [==============================] - 0s 783us/step - loss: 0.1361 - accuracy: 0.9530\n",
      "Epoch 492/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9506\n",
      "Epoch 493/1500\n",
      "47/47 [==============================] - 0s 790us/step - loss: 0.1413 - accuracy: 0.9509\n",
      "Epoch 494/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.1366 - accuracy: 0.9493\n",
      "Epoch 495/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.1300 - accuracy: 0.9493\n",
      "Epoch 496/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1407 - accuracy: 0.9466\n",
      "Epoch 497/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.1258 - accuracy: 0.9533\n",
      "Epoch 498/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1418 - accuracy: 0.9466\n",
      "Epoch 499/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1437 - accuracy: 0.9425\n",
      "Epoch 500/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.1538 - accuracy: 0.9449\n",
      "Epoch 501/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.1368 - accuracy: 0.9462\n",
      "Epoch 502/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.1484 - accuracy: 0.9456\n",
      "Epoch 503/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1455 - accuracy: 0.9419\n",
      "Epoch 504/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9479\n",
      "Epoch 505/1500\n",
      "47/47 [==============================] - 0s 932us/step - loss: 0.1555 - accuracy: 0.9405\n",
      "Epoch 506/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.1581 - accuracy: 0.9429\n",
      "Epoch 507/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.1372 - accuracy: 0.9456\n",
      "Epoch 508/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.1388 - accuracy: 0.9459\n",
      "Epoch 509/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.1503 - accuracy: 0.9442\n",
      "Epoch 510/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1438 - accuracy: 0.9466\n",
      "Epoch 511/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1298 - accuracy: 0.9516\n",
      "Epoch 512/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.1499 - accuracy: 0.9442\n",
      "Epoch 513/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.1516 - accuracy: 0.9395\n",
      "Epoch 514/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1483 - accuracy: 0.9446\n",
      "Epoch 515/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.1403 - accuracy: 0.9415\n",
      "Epoch 516/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9462\n",
      "Epoch 517/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1394 - accuracy: 0.9462\n",
      "Epoch 518/1500\n",
      "47/47 [==============================] - 0s 786us/step - loss: 0.1390 - accuracy: 0.9483\n",
      "Epoch 519/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.1593 - accuracy: 0.9412\n",
      "Epoch 520/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.1587 - accuracy: 0.9399\n",
      "Epoch 521/1500\n",
      "47/47 [==============================] - 0s 819us/step - loss: 0.1325 - accuracy: 0.9466\n",
      "Epoch 522/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.1458 - accuracy: 0.9466\n",
      "Epoch 523/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1428 - accuracy: 0.9446\n",
      "Epoch 524/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.1368 - accuracy: 0.9472\n",
      "Epoch 525/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.1402 - accuracy: 0.9466\n",
      "Epoch 526/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.1442 - accuracy: 0.9476\n",
      "Epoch 527/1500\n",
      " 1/47 [..............................] - ETA: 0s - loss: 0.1288 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 497.\n",
      "47/47 [==============================] - 0s 824us/step - loss: 0.1412 - accuracy: 0.9503\n",
      "Epoch 527: early stopping\n",
      "7/7 [==============================] - 0s 842us/step - loss: 0.9185 - accuracy: 0.7581\n",
      "7/7 [==============================] - 0s 729us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 459, Predictions: 459, Actuals: 459, Gender: 459\n",
      "After appending - Cat IDs: 674, Predictions: 674, Actuals: 674, Gender: 674\n",
      "Final Test Results - Loss: 0.9185493588447571, Accuracy: 0.7581395506858826, Precision: 0.7360544217687076, Recall: 0.6725256850736111, F1 Score: 0.6973900074571215\n",
      "Confusion Matrix:\n",
      " [[123   1  27]\n",
      " [  3   4   0]\n",
      " [ 21   0  36]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'101A'}\n",
      "Moved to Test Set:\n",
      "{'101A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A'\n",
      " '097A' '099A' '100A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A' '039A'\n",
      " '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A' '069A'\n",
      " '073A' '088A' '092A' '094A' '097B' '101A' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     490\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     98\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1194, 1: 1083, 2: 711})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "47/47 [==============================] - 0s 943us/step - loss: 1.1451 - accuracy: 0.4963\n",
      "Epoch 2/1500\n",
      "47/47 [==============================] - 0s 907us/step - loss: 0.9183 - accuracy: 0.5981\n",
      "Epoch 3/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.8275 - accuracy: 0.6322\n",
      "Epoch 4/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.7596 - accuracy: 0.6747\n",
      "Epoch 5/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.7344 - accuracy: 0.6924\n",
      "Epoch 6/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.7075 - accuracy: 0.6965\n",
      "Epoch 7/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.6600 - accuracy: 0.7202\n",
      "Epoch 8/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.6461 - accuracy: 0.7349\n",
      "Epoch 9/1500\n",
      "47/47 [==============================] - 0s 884us/step - loss: 0.6403 - accuracy: 0.7249\n",
      "Epoch 10/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.6109 - accuracy: 0.7493\n",
      "Epoch 11/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.5880 - accuracy: 0.7530\n",
      "Epoch 12/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.5898 - accuracy: 0.7527\n",
      "Epoch 13/1500\n",
      "47/47 [==============================] - 0s 716us/step - loss: 0.5647 - accuracy: 0.7687\n",
      "Epoch 14/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.5636 - accuracy: 0.7560\n",
      "Epoch 15/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.5503 - accuracy: 0.7734\n",
      "Epoch 16/1500\n",
      "47/47 [==============================] - 0s 697us/step - loss: 0.5490 - accuracy: 0.7674\n",
      "Epoch 17/1500\n",
      "47/47 [==============================] - 0s 708us/step - loss: 0.5420 - accuracy: 0.7691\n",
      "Epoch 18/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.5300 - accuracy: 0.7811\n",
      "Epoch 19/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.5280 - accuracy: 0.7815\n",
      "Epoch 20/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.5178 - accuracy: 0.7851\n",
      "Epoch 21/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.5072 - accuracy: 0.8015\n",
      "Epoch 22/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.4945 - accuracy: 0.7938\n",
      "Epoch 23/1500\n",
      "47/47 [==============================] - 0s 710us/step - loss: 0.4930 - accuracy: 0.7915\n",
      "Epoch 24/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.5035 - accuracy: 0.7938\n",
      "Epoch 25/1500\n",
      "47/47 [==============================] - 0s 705us/step - loss: 0.4970 - accuracy: 0.7979\n",
      "Epoch 26/1500\n",
      "47/47 [==============================] - 0s 724us/step - loss: 0.4838 - accuracy: 0.8005\n",
      "Epoch 27/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.4741 - accuracy: 0.8072\n",
      "Epoch 28/1500\n",
      "47/47 [==============================] - 0s 709us/step - loss: 0.4754 - accuracy: 0.7982\n",
      "Epoch 29/1500\n",
      "47/47 [==============================] - 0s 722us/step - loss: 0.4785 - accuracy: 0.7975\n",
      "Epoch 30/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.4646 - accuracy: 0.8079\n",
      "Epoch 31/1500\n",
      "47/47 [==============================] - 0s 694us/step - loss: 0.4740 - accuracy: 0.8042\n",
      "Epoch 32/1500\n",
      "47/47 [==============================] - 0s 709us/step - loss: 0.4623 - accuracy: 0.8143\n",
      "Epoch 33/1500\n",
      "47/47 [==============================] - 0s 721us/step - loss: 0.4579 - accuracy: 0.8032\n",
      "Epoch 34/1500\n",
      "47/47 [==============================] - 0s 714us/step - loss: 0.4567 - accuracy: 0.8143\n",
      "Epoch 35/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.4449 - accuracy: 0.8186\n",
      "Epoch 36/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.4399 - accuracy: 0.8166\n",
      "Epoch 37/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.4353 - accuracy: 0.8189\n",
      "Epoch 38/1500\n",
      "47/47 [==============================] - 0s 696us/step - loss: 0.4358 - accuracy: 0.8096\n",
      "Epoch 39/1500\n",
      "47/47 [==============================] - 0s 698us/step - loss: 0.4521 - accuracy: 0.8143\n",
      "Epoch 40/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.4287 - accuracy: 0.8263\n",
      "Epoch 41/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.4400 - accuracy: 0.8226\n",
      "Epoch 42/1500\n",
      "47/47 [==============================] - 0s 712us/step - loss: 0.4297 - accuracy: 0.8233\n",
      "Epoch 43/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.4259 - accuracy: 0.8280\n",
      "Epoch 44/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.4300 - accuracy: 0.8260\n",
      "Epoch 45/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.4246 - accuracy: 0.8250\n",
      "Epoch 46/1500\n",
      "47/47 [==============================] - 0s 685us/step - loss: 0.4305 - accuracy: 0.8283\n",
      "Epoch 47/1500\n",
      "47/47 [==============================] - 0s 724us/step - loss: 0.4195 - accuracy: 0.8290\n",
      "Epoch 48/1500\n",
      "47/47 [==============================] - 0s 715us/step - loss: 0.3967 - accuracy: 0.8370\n",
      "Epoch 49/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.4255 - accuracy: 0.8246\n",
      "Epoch 50/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.4077 - accuracy: 0.8317\n",
      "Epoch 51/1500\n",
      "47/47 [==============================] - 0s 706us/step - loss: 0.4036 - accuracy: 0.8300\n",
      "Epoch 52/1500\n",
      "47/47 [==============================] - 0s 714us/step - loss: 0.4106 - accuracy: 0.8384\n",
      "Epoch 53/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.3971 - accuracy: 0.8367\n",
      "Epoch 54/1500\n",
      "47/47 [==============================] - 0s 707us/step - loss: 0.4102 - accuracy: 0.8280\n",
      "Epoch 55/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.4092 - accuracy: 0.8380\n",
      "Epoch 56/1500\n",
      "47/47 [==============================] - 0s 723us/step - loss: 0.3889 - accuracy: 0.8370\n",
      "Epoch 57/1500\n",
      "47/47 [==============================] - 0s 714us/step - loss: 0.3922 - accuracy: 0.8313\n",
      "Epoch 58/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3875 - accuracy: 0.8471\n",
      "Epoch 59/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3827 - accuracy: 0.8440\n",
      "Epoch 60/1500\n",
      "47/47 [==============================] - 0s 712us/step - loss: 0.3905 - accuracy: 0.8363\n",
      "Epoch 61/1500\n",
      "47/47 [==============================] - 0s 723us/step - loss: 0.3917 - accuracy: 0.8377\n",
      "Epoch 62/1500\n",
      "47/47 [==============================] - 0s 721us/step - loss: 0.3923 - accuracy: 0.8360\n",
      "Epoch 63/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.3843 - accuracy: 0.8400\n",
      "Epoch 64/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.3746 - accuracy: 0.8457\n",
      "Epoch 65/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.3945 - accuracy: 0.8424\n",
      "Epoch 66/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.3542 - accuracy: 0.8558\n",
      "Epoch 67/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.3843 - accuracy: 0.8410\n",
      "Epoch 68/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.3846 - accuracy: 0.8427\n",
      "Epoch 69/1500\n",
      "47/47 [==============================] - 0s 709us/step - loss: 0.3696 - accuracy: 0.8471\n",
      "Epoch 70/1500\n",
      "47/47 [==============================] - 0s 723us/step - loss: 0.3735 - accuracy: 0.8474\n",
      "Epoch 71/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.3735 - accuracy: 0.8471\n",
      "Epoch 72/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.3741 - accuracy: 0.8417\n",
      "Epoch 73/1500\n",
      "47/47 [==============================] - 0s 707us/step - loss: 0.3656 - accuracy: 0.8531\n",
      "Epoch 74/1500\n",
      "47/47 [==============================] - 0s 695us/step - loss: 0.3801 - accuracy: 0.8387\n",
      "Epoch 75/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.3830 - accuracy: 0.8457\n",
      "Epoch 76/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.3623 - accuracy: 0.8554\n",
      "Epoch 77/1500\n",
      "47/47 [==============================] - 0s 708us/step - loss: 0.3684 - accuracy: 0.8544\n",
      "Epoch 78/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.3597 - accuracy: 0.8467\n",
      "Epoch 79/1500\n",
      "47/47 [==============================] - 0s 700us/step - loss: 0.3687 - accuracy: 0.8447\n",
      "Epoch 80/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3558 - accuracy: 0.8517\n",
      "Epoch 81/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.3529 - accuracy: 0.8537\n",
      "Epoch 82/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.3639 - accuracy: 0.8521\n",
      "Epoch 83/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.3540 - accuracy: 0.8604\n",
      "Epoch 84/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.3491 - accuracy: 0.8604\n",
      "Epoch 85/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.3629 - accuracy: 0.8511\n",
      "Epoch 86/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.3377 - accuracy: 0.8648\n",
      "Epoch 87/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.3386 - accuracy: 0.8651\n",
      "Epoch 88/1500\n",
      "47/47 [==============================] - 0s 713us/step - loss: 0.3543 - accuracy: 0.8537\n",
      "Epoch 89/1500\n",
      "47/47 [==============================] - 0s 724us/step - loss: 0.3420 - accuracy: 0.8578\n",
      "Epoch 90/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.3428 - accuracy: 0.8624\n",
      "Epoch 91/1500\n",
      "47/47 [==============================] - 0s 715us/step - loss: 0.3478 - accuracy: 0.8551\n",
      "Epoch 92/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.3462 - accuracy: 0.8604\n",
      "Epoch 93/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.3392 - accuracy: 0.8658\n",
      "Epoch 94/1500\n",
      "47/47 [==============================] - 0s 721us/step - loss: 0.3480 - accuracy: 0.8541\n",
      "Epoch 95/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8564\n",
      "Epoch 96/1500\n",
      "47/47 [==============================] - 0s 711us/step - loss: 0.3396 - accuracy: 0.8614\n",
      "Epoch 97/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3482 - accuracy: 0.8658\n",
      "Epoch 98/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.3303 - accuracy: 0.8712\n",
      "Epoch 99/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.3370 - accuracy: 0.8635\n",
      "Epoch 100/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.3330 - accuracy: 0.8671\n",
      "Epoch 101/1500\n",
      "47/47 [==============================] - 0s 721us/step - loss: 0.3541 - accuracy: 0.8618\n",
      "Epoch 102/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.3294 - accuracy: 0.8691\n",
      "Epoch 103/1500\n",
      "47/47 [==============================] - 0s 722us/step - loss: 0.3358 - accuracy: 0.8675\n",
      "Epoch 104/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.3180 - accuracy: 0.8712\n",
      "Epoch 105/1500\n",
      "47/47 [==============================] - 0s 713us/step - loss: 0.3235 - accuracy: 0.8705\n",
      "Epoch 106/1500\n",
      "47/47 [==============================] - 0s 716us/step - loss: 0.3322 - accuracy: 0.8691\n",
      "Epoch 107/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.3443 - accuracy: 0.8588\n",
      "Epoch 108/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.3255 - accuracy: 0.8665\n",
      "Epoch 109/1500\n",
      "47/47 [==============================] - 0s 715us/step - loss: 0.3230 - accuracy: 0.8668\n",
      "Epoch 110/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.3153 - accuracy: 0.8698\n",
      "Epoch 111/1500\n",
      "47/47 [==============================] - 0s 694us/step - loss: 0.3279 - accuracy: 0.8651\n",
      "Epoch 112/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.3210 - accuracy: 0.8712\n",
      "Epoch 113/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.3067 - accuracy: 0.8765\n",
      "Epoch 114/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3185 - accuracy: 0.8748\n",
      "Epoch 115/1500\n",
      "47/47 [==============================] - 0s 716us/step - loss: 0.3164 - accuracy: 0.8645\n",
      "Epoch 116/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.3073 - accuracy: 0.8785\n",
      "Epoch 117/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.3222 - accuracy: 0.8735\n",
      "Epoch 118/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3133 - accuracy: 0.8742\n",
      "Epoch 119/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.3088 - accuracy: 0.8712\n",
      "Epoch 120/1500\n",
      "47/47 [==============================] - 0s 718us/step - loss: 0.3174 - accuracy: 0.8735\n",
      "Epoch 121/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.3161 - accuracy: 0.8735\n",
      "Epoch 122/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.3034 - accuracy: 0.8745\n",
      "Epoch 123/1500\n",
      "47/47 [==============================] - 0s 723us/step - loss: 0.3175 - accuracy: 0.8758\n",
      "Epoch 124/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.3117 - accuracy: 0.8752\n",
      "Epoch 125/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.3119 - accuracy: 0.8725\n",
      "Epoch 126/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.3195 - accuracy: 0.8691\n",
      "Epoch 127/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.3168 - accuracy: 0.8768\n",
      "Epoch 128/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.2964 - accuracy: 0.8795\n",
      "Epoch 129/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.3093 - accuracy: 0.8722\n",
      "Epoch 130/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.2989 - accuracy: 0.8852\n",
      "Epoch 131/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.3077 - accuracy: 0.8799\n",
      "Epoch 132/1500\n",
      "47/47 [==============================] - 0s 722us/step - loss: 0.3084 - accuracy: 0.8732\n",
      "Epoch 133/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2919 - accuracy: 0.8785\n",
      "Epoch 134/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2929 - accuracy: 0.8799\n",
      "Epoch 135/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.2903 - accuracy: 0.8835\n",
      "Epoch 136/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.3043 - accuracy: 0.8738\n",
      "Epoch 137/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.2886 - accuracy: 0.8859\n",
      "Epoch 138/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.2918 - accuracy: 0.8845\n",
      "Epoch 139/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.2937 - accuracy: 0.8882\n",
      "Epoch 140/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.3068 - accuracy: 0.8701\n",
      "Epoch 141/1500\n",
      "47/47 [==============================] - 0s 724us/step - loss: 0.2899 - accuracy: 0.8819\n",
      "Epoch 142/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.2898 - accuracy: 0.8889\n",
      "Epoch 143/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.2921 - accuracy: 0.8832\n",
      "Epoch 144/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.2920 - accuracy: 0.8765\n",
      "Epoch 145/1500\n",
      "47/47 [==============================] - 0s 722us/step - loss: 0.2927 - accuracy: 0.8835\n",
      "Epoch 146/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.2916 - accuracy: 0.8845\n",
      "Epoch 147/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8732\n",
      "Epoch 148/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.2965 - accuracy: 0.8822\n",
      "Epoch 149/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.3017 - accuracy: 0.8772\n",
      "Epoch 150/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2902 - accuracy: 0.8839\n",
      "Epoch 151/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.2750 - accuracy: 0.8889\n",
      "Epoch 152/1500\n",
      "47/47 [==============================] - 0s 722us/step - loss: 0.2854 - accuracy: 0.8862\n",
      "Epoch 153/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2907 - accuracy: 0.8842\n",
      "Epoch 154/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.2884 - accuracy: 0.8819\n",
      "Epoch 155/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2696 - accuracy: 0.8919\n",
      "Epoch 156/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.2947 - accuracy: 0.8778\n",
      "Epoch 157/1500\n",
      "47/47 [==============================] - 0s 719us/step - loss: 0.2688 - accuracy: 0.8896\n",
      "Epoch 158/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2694 - accuracy: 0.8936\n",
      "Epoch 159/1500\n",
      "47/47 [==============================] - 0s 696us/step - loss: 0.2803 - accuracy: 0.8922\n",
      "Epoch 160/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.2745 - accuracy: 0.8896\n",
      "Epoch 161/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.2787 - accuracy: 0.8882\n",
      "Epoch 162/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.2816 - accuracy: 0.8852\n",
      "Epoch 163/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2678 - accuracy: 0.8886\n",
      "Epoch 164/1500\n",
      "47/47 [==============================] - 0s 723us/step - loss: 0.2778 - accuracy: 0.8926\n",
      "Epoch 165/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2777 - accuracy: 0.8842\n",
      "Epoch 166/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2857 - accuracy: 0.8872\n",
      "Epoch 167/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.2762 - accuracy: 0.8886\n",
      "Epoch 168/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.2705 - accuracy: 0.8922\n",
      "Epoch 169/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.2690 - accuracy: 0.8932\n",
      "Epoch 170/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2772 - accuracy: 0.8835\n",
      "Epoch 171/1500\n",
      "47/47 [==============================] - 0s 726us/step - loss: 0.2692 - accuracy: 0.8929\n",
      "Epoch 172/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2706 - accuracy: 0.8942\n",
      "Epoch 173/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2583 - accuracy: 0.8956\n",
      "Epoch 174/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.2706 - accuracy: 0.8912\n",
      "Epoch 175/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.2579 - accuracy: 0.8922\n",
      "Epoch 176/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2742 - accuracy: 0.8882\n",
      "Epoch 177/1500\n",
      "47/47 [==============================] - 0s 720us/step - loss: 0.2624 - accuracy: 0.8959\n",
      "Epoch 178/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.2639 - accuracy: 0.8919\n",
      "Epoch 179/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2668 - accuracy: 0.8936\n",
      "Epoch 180/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.2639 - accuracy: 0.8902\n",
      "Epoch 181/1500\n",
      "47/47 [==============================] - 0s 726us/step - loss: 0.2702 - accuracy: 0.8929\n",
      "Epoch 182/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.2678 - accuracy: 0.8946\n",
      "Epoch 183/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2700 - accuracy: 0.8969\n",
      "Epoch 184/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2591 - accuracy: 0.9036\n",
      "Epoch 185/1500\n",
      "47/47 [==============================] - 0s 830us/step - loss: 0.2661 - accuracy: 0.8919\n",
      "Epoch 186/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.8952\n",
      "Epoch 187/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2548 - accuracy: 0.8963\n",
      "Epoch 188/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2480 - accuracy: 0.9029\n",
      "Epoch 189/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.2510 - accuracy: 0.9053\n",
      "Epoch 190/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.2642 - accuracy: 0.8932\n",
      "Epoch 191/1500\n",
      "47/47 [==============================] - 0s 721us/step - loss: 0.2529 - accuracy: 0.9043\n",
      "Epoch 192/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2579 - accuracy: 0.8969\n",
      "Epoch 193/1500\n",
      "47/47 [==============================] - 0s 726us/step - loss: 0.2616 - accuracy: 0.8936\n",
      "Epoch 194/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.2549 - accuracy: 0.8986\n",
      "Epoch 195/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.2512 - accuracy: 0.9009\n",
      "Epoch 196/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.2498 - accuracy: 0.8952\n",
      "Epoch 197/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.2516 - accuracy: 0.8989\n",
      "Epoch 198/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.2620 - accuracy: 0.8959\n",
      "Epoch 199/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.2522 - accuracy: 0.9056\n",
      "Epoch 200/1500\n",
      "47/47 [==============================] - 0s 793us/step - loss: 0.2429 - accuracy: 0.8996\n",
      "Epoch 201/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.2602 - accuracy: 0.8952\n",
      "Epoch 202/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.2486 - accuracy: 0.8979\n",
      "Epoch 203/1500\n",
      "47/47 [==============================] - 0s 714us/step - loss: 0.2515 - accuracy: 0.8983\n",
      "Epoch 204/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2528 - accuracy: 0.8989\n",
      "Epoch 205/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.2361 - accuracy: 0.9127\n",
      "Epoch 206/1500\n",
      "47/47 [==============================] - 0s 707us/step - loss: 0.2429 - accuracy: 0.9013\n",
      "Epoch 207/1500\n",
      "47/47 [==============================] - 0s 726us/step - loss: 0.2516 - accuracy: 0.9006\n",
      "Epoch 208/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2428 - accuracy: 0.9029\n",
      "Epoch 209/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.2567 - accuracy: 0.9003\n",
      "Epoch 210/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.2531 - accuracy: 0.9013\n",
      "Epoch 211/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2410 - accuracy: 0.9029\n",
      "Epoch 212/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.2391 - accuracy: 0.9056\n",
      "Epoch 213/1500\n",
      "47/47 [==============================] - 0s 722us/step - loss: 0.2394 - accuracy: 0.9009\n",
      "Epoch 214/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.2525 - accuracy: 0.9029\n",
      "Epoch 215/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.2422 - accuracy: 0.9056\n",
      "Epoch 216/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.2413 - accuracy: 0.9073\n",
      "Epoch 217/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2557 - accuracy: 0.8963\n",
      "Epoch 218/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2493 - accuracy: 0.8959\n",
      "Epoch 219/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9053\n",
      "Epoch 220/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2451 - accuracy: 0.9036\n",
      "Epoch 221/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2453 - accuracy: 0.9023\n",
      "Epoch 222/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2434 - accuracy: 0.9029\n",
      "Epoch 223/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.2356 - accuracy: 0.9016\n",
      "Epoch 224/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.2455 - accuracy: 0.9043\n",
      "Epoch 225/1500\n",
      "47/47 [==============================] - 0s 726us/step - loss: 0.2409 - accuracy: 0.9023\n",
      "Epoch 226/1500\n",
      "47/47 [==============================] - 0s 784us/step - loss: 0.2290 - accuracy: 0.9103\n",
      "Epoch 227/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2332 - accuracy: 0.9066\n",
      "Epoch 228/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.2385 - accuracy: 0.9100\n",
      "Epoch 229/1500\n",
      "47/47 [==============================] - 0s 705us/step - loss: 0.2325 - accuracy: 0.9063\n",
      "Epoch 230/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2362 - accuracy: 0.9083\n",
      "Epoch 231/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2460 - accuracy: 0.9043\n",
      "Epoch 232/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.2313 - accuracy: 0.9050\n",
      "Epoch 233/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.2304 - accuracy: 0.9070\n",
      "Epoch 234/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2360 - accuracy: 0.9086\n",
      "Epoch 235/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2143 - accuracy: 0.9214\n",
      "Epoch 236/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.2327 - accuracy: 0.9093\n",
      "Epoch 237/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.2309 - accuracy: 0.9050\n",
      "Epoch 238/1500\n",
      "47/47 [==============================] - 0s 816us/step - loss: 0.2189 - accuracy: 0.9157\n",
      "Epoch 239/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2258 - accuracy: 0.9150\n",
      "Epoch 240/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2188 - accuracy: 0.9143\n",
      "Epoch 241/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2300 - accuracy: 0.9076\n",
      "Epoch 242/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2317 - accuracy: 0.9076\n",
      "Epoch 243/1500\n",
      "47/47 [==============================] - 0s 700us/step - loss: 0.2243 - accuracy: 0.9100\n",
      "Epoch 244/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2384 - accuracy: 0.9029\n",
      "Epoch 245/1500\n",
      "47/47 [==============================] - 0s 708us/step - loss: 0.2359 - accuracy: 0.9076\n",
      "Epoch 246/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2334 - accuracy: 0.9056\n",
      "Epoch 247/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2347 - accuracy: 0.9070\n",
      "Epoch 248/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2311 - accuracy: 0.9120\n",
      "Epoch 249/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.2345 - accuracy: 0.9070\n",
      "Epoch 250/1500\n",
      "47/47 [==============================] - 0s 815us/step - loss: 0.2241 - accuracy: 0.9143\n",
      "Epoch 251/1500\n",
      "47/47 [==============================] - 0s 804us/step - loss: 0.2123 - accuracy: 0.9133\n",
      "Epoch 252/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.2263 - accuracy: 0.9103\n",
      "Epoch 253/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2417 - accuracy: 0.9083\n",
      "Epoch 254/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2229 - accuracy: 0.9106\n",
      "Epoch 255/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.2319 - accuracy: 0.9090\n",
      "Epoch 256/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.2292 - accuracy: 0.9083\n",
      "Epoch 257/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2215 - accuracy: 0.9133\n",
      "Epoch 258/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.2266 - accuracy: 0.9100\n",
      "Epoch 259/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2167 - accuracy: 0.9187\n",
      "Epoch 260/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2082 - accuracy: 0.9203\n",
      "Epoch 261/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.2294 - accuracy: 0.9093\n",
      "Epoch 262/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2233 - accuracy: 0.9127\n",
      "Epoch 263/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.2161 - accuracy: 0.9100\n",
      "Epoch 264/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.2241 - accuracy: 0.9070\n",
      "Epoch 265/1500\n",
      "47/47 [==============================] - 0s 834us/step - loss: 0.2223 - accuracy: 0.9110\n",
      "Epoch 266/1500\n",
      "47/47 [==============================] - 0s 848us/step - loss: 0.2256 - accuracy: 0.9120\n",
      "Epoch 267/1500\n",
      "47/47 [==============================] - 0s 827us/step - loss: 0.2204 - accuracy: 0.9123\n",
      "Epoch 268/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.2218 - accuracy: 0.9083\n",
      "Epoch 269/1500\n",
      "47/47 [==============================] - 0s 844us/step - loss: 0.2293 - accuracy: 0.9096\n",
      "Epoch 270/1500\n",
      "47/47 [==============================] - 0s 923us/step - loss: 0.2199 - accuracy: 0.9143\n",
      "Epoch 271/1500\n",
      "47/47 [==============================] - 0s 869us/step - loss: 0.2119 - accuracy: 0.9103\n",
      "Epoch 272/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.2201 - accuracy: 0.9183\n",
      "Epoch 273/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2301 - accuracy: 0.9113\n",
      "Epoch 274/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9133\n",
      "Epoch 275/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.2297 - accuracy: 0.9110\n",
      "Epoch 276/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.2044 - accuracy: 0.9210\n",
      "Epoch 277/1500\n",
      "47/47 [==============================] - 0s 726us/step - loss: 0.2212 - accuracy: 0.9160\n",
      "Epoch 278/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2278 - accuracy: 0.9076\n",
      "Epoch 279/1500\n",
      "47/47 [==============================] - 0s 946us/step - loss: 0.2215 - accuracy: 0.9167\n",
      "Epoch 280/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.2107 - accuracy: 0.9157\n",
      "Epoch 281/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2237 - accuracy: 0.9143\n",
      "Epoch 282/1500\n",
      "47/47 [==============================] - 0s 786us/step - loss: 0.2129 - accuracy: 0.9140\n",
      "Epoch 283/1500\n",
      "47/47 [==============================] - 0s 836us/step - loss: 0.2172 - accuracy: 0.9140\n",
      "Epoch 284/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.2038 - accuracy: 0.9150\n",
      "Epoch 285/1500\n",
      "47/47 [==============================] - 0s 849us/step - loss: 0.2290 - accuracy: 0.9096\n",
      "Epoch 286/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.2022 - accuracy: 0.9167\n",
      "Epoch 287/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.2064 - accuracy: 0.9143\n",
      "Epoch 288/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2207 - accuracy: 0.9153\n",
      "Epoch 289/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.2130 - accuracy: 0.9173\n",
      "Epoch 290/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2056 - accuracy: 0.9203\n",
      "Epoch 291/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.2062 - accuracy: 0.9200\n",
      "Epoch 292/1500\n",
      "47/47 [==============================] - 0s 818us/step - loss: 0.2111 - accuracy: 0.9183\n",
      "Epoch 293/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.2127 - accuracy: 0.9150\n",
      "Epoch 294/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.2219 - accuracy: 0.9123\n",
      "Epoch 295/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.2049 - accuracy: 0.9254\n",
      "Epoch 296/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2133 - accuracy: 0.9106\n",
      "Epoch 297/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.2158 - accuracy: 0.9140\n",
      "Epoch 298/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2070 - accuracy: 0.9203\n",
      "Epoch 299/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.2220 - accuracy: 0.9123\n",
      "Epoch 300/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1980 - accuracy: 0.9220\n",
      "Epoch 301/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2153 - accuracy: 0.9086\n",
      "Epoch 302/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.2051 - accuracy: 0.9183\n",
      "Epoch 303/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2071 - accuracy: 0.9190\n",
      "Epoch 304/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2144 - accuracy: 0.9157\n",
      "Epoch 305/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2091 - accuracy: 0.9173\n",
      "Epoch 306/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.1971 - accuracy: 0.9190\n",
      "Epoch 307/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2037 - accuracy: 0.9190\n",
      "Epoch 308/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2074 - accuracy: 0.9160\n",
      "Epoch 309/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.2059 - accuracy: 0.9163\n",
      "Epoch 310/1500\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.2062 - accuracy: 0.9247\n",
      "Epoch 311/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2191 - accuracy: 0.9123\n",
      "Epoch 312/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2007 - accuracy: 0.9190\n",
      "Epoch 313/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.1893 - accuracy: 0.9220\n",
      "Epoch 314/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2065 - accuracy: 0.9133\n",
      "Epoch 315/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2099 - accuracy: 0.9167\n",
      "Epoch 316/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.2006 - accuracy: 0.9224\n",
      "Epoch 317/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.2083 - accuracy: 0.9190\n",
      "Epoch 318/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.1981 - accuracy: 0.9207\n",
      "Epoch 319/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.1966 - accuracy: 0.9214\n",
      "Epoch 320/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9197\n",
      "Epoch 321/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.2081 - accuracy: 0.9180\n",
      "Epoch 322/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2008 - accuracy: 0.9200\n",
      "Epoch 323/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.1999 - accuracy: 0.9193\n",
      "Epoch 324/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.2098 - accuracy: 0.9160\n",
      "Epoch 325/1500\n",
      "47/47 [==============================] - 0s 834us/step - loss: 0.2091 - accuracy: 0.9173\n",
      "Epoch 326/1500\n",
      "47/47 [==============================] - 0s 795us/step - loss: 0.2122 - accuracy: 0.9127\n",
      "Epoch 327/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.2132 - accuracy: 0.9157\n",
      "Epoch 328/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.1943 - accuracy: 0.9240\n",
      "Epoch 329/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.2082 - accuracy: 0.9163\n",
      "Epoch 330/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.2050 - accuracy: 0.9190\n",
      "Epoch 331/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2081 - accuracy: 0.9177\n",
      "Epoch 332/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1977 - accuracy: 0.9200\n",
      "Epoch 333/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1887 - accuracy: 0.9260\n",
      "Epoch 334/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.2107 - accuracy: 0.9157\n",
      "Epoch 335/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1918 - accuracy: 0.9183\n",
      "Epoch 336/1500\n",
      "47/47 [==============================] - 0s 802us/step - loss: 0.1989 - accuracy: 0.9207\n",
      "Epoch 337/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.2122 - accuracy: 0.9167\n",
      "Epoch 338/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1881 - accuracy: 0.9301\n",
      "Epoch 339/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.1874 - accuracy: 0.9311\n",
      "Epoch 340/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1908 - accuracy: 0.9294\n",
      "Epoch 341/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1932 - accuracy: 0.9280\n",
      "Epoch 342/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.1893 - accuracy: 0.9287\n",
      "Epoch 343/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.1850 - accuracy: 0.9324\n",
      "Epoch 344/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.1933 - accuracy: 0.9227\n",
      "Epoch 345/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.2015 - accuracy: 0.9207\n",
      "Epoch 346/1500\n",
      "47/47 [==============================] - 0s 812us/step - loss: 0.2039 - accuracy: 0.9207\n",
      "Epoch 347/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.2030 - accuracy: 0.9177\n",
      "Epoch 348/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.1975 - accuracy: 0.9250\n",
      "Epoch 349/1500\n",
      "47/47 [==============================] - 0s 817us/step - loss: 0.2026 - accuracy: 0.9197\n",
      "Epoch 350/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.1950 - accuracy: 0.9240\n",
      "Epoch 351/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.1981 - accuracy: 0.9210\n",
      "Epoch 352/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.2025 - accuracy: 0.9220\n",
      "Epoch 353/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.1826 - accuracy: 0.9267\n",
      "Epoch 354/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.1967 - accuracy: 0.9260\n",
      "Epoch 355/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.1868 - accuracy: 0.9284\n",
      "Epoch 356/1500\n",
      "47/47 [==============================] - 0s 795us/step - loss: 0.1917 - accuracy: 0.9210\n",
      "Epoch 357/1500\n",
      "47/47 [==============================] - 0s 795us/step - loss: 0.1998 - accuracy: 0.9150\n",
      "Epoch 358/1500\n",
      "47/47 [==============================] - 0s 846us/step - loss: 0.1877 - accuracy: 0.9237\n",
      "Epoch 359/1500\n",
      "47/47 [==============================] - 0s 824us/step - loss: 0.1972 - accuracy: 0.9237\n",
      "Epoch 360/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.1936 - accuracy: 0.9244\n",
      "Epoch 361/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1837 - accuracy: 0.9301\n",
      "Epoch 362/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.1837 - accuracy: 0.9311\n",
      "Epoch 363/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.1885 - accuracy: 0.9297\n",
      "Epoch 364/1500\n",
      "47/47 [==============================] - 0s 836us/step - loss: 0.1919 - accuracy: 0.9284\n",
      "Epoch 365/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.1880 - accuracy: 0.9294\n",
      "Epoch 366/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1913 - accuracy: 0.9240\n",
      "Epoch 367/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.1826 - accuracy: 0.9297\n",
      "Epoch 368/1500\n",
      "47/47 [==============================] - 0s 858us/step - loss: 0.1802 - accuracy: 0.9284\n",
      "Epoch 369/1500\n",
      "47/47 [==============================] - 0s 814us/step - loss: 0.1893 - accuracy: 0.9277\n",
      "Epoch 370/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.1847 - accuracy: 0.9264\n",
      "Epoch 371/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.1945 - accuracy: 0.9240\n",
      "Epoch 372/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1868 - accuracy: 0.9270\n",
      "Epoch 373/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1799 - accuracy: 0.9280\n",
      "Epoch 374/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.1859 - accuracy: 0.9254\n",
      "Epoch 375/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.1967 - accuracy: 0.9277\n",
      "Epoch 376/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.1952 - accuracy: 0.9200\n",
      "Epoch 377/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.1830 - accuracy: 0.9307\n",
      "Epoch 378/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.1815 - accuracy: 0.9334\n",
      "Epoch 379/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.1929 - accuracy: 0.9257\n",
      "Epoch 380/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1914 - accuracy: 0.9210\n",
      "Epoch 381/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.1724 - accuracy: 0.9321\n",
      "Epoch 382/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.1897 - accuracy: 0.9224\n",
      "Epoch 383/1500\n",
      "47/47 [==============================] - 0s 819us/step - loss: 0.1853 - accuracy: 0.9257\n",
      "Epoch 384/1500\n",
      "47/47 [==============================] - 0s 833us/step - loss: 0.1846 - accuracy: 0.9260\n",
      "Epoch 385/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.1930 - accuracy: 0.9234\n",
      "Epoch 386/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.1933 - accuracy: 0.9200\n",
      "Epoch 387/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.1871 - accuracy: 0.9287\n",
      "Epoch 388/1500\n",
      "47/47 [==============================] - 0s 814us/step - loss: 0.1893 - accuracy: 0.9207\n",
      "Epoch 389/1500\n",
      "47/47 [==============================] - 0s 842us/step - loss: 0.1904 - accuracy: 0.9230\n",
      "Epoch 390/1500\n",
      "47/47 [==============================] - 0s 827us/step - loss: 0.1924 - accuracy: 0.9230\n",
      "Epoch 391/1500\n",
      "47/47 [==============================] - 0s 805us/step - loss: 0.1982 - accuracy: 0.9247\n",
      "Epoch 392/1500\n",
      "47/47 [==============================] - 0s 750us/step - loss: 0.1851 - accuracy: 0.9277\n",
      "Epoch 393/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.1882 - accuracy: 0.9224\n",
      "Epoch 394/1500\n",
      "47/47 [==============================] - 0s 786us/step - loss: 0.1826 - accuracy: 0.9284\n",
      "Epoch 395/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.1847 - accuracy: 0.9294\n",
      "Epoch 396/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.1983 - accuracy: 0.9247\n",
      "Epoch 397/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.1973 - accuracy: 0.9244\n",
      "Epoch 398/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.9240\n",
      "Epoch 399/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.1797 - accuracy: 0.9264\n",
      "Epoch 400/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.1871 - accuracy: 0.9301\n",
      "Epoch 401/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2017 - accuracy: 0.9220\n",
      "Epoch 402/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.1723 - accuracy: 0.9327\n",
      "Epoch 403/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.1776 - accuracy: 0.9260\n",
      "Epoch 404/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.1764 - accuracy: 0.9290\n",
      "Epoch 405/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.2013 - accuracy: 0.9197\n",
      "Epoch 406/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.1895 - accuracy: 0.9257\n",
      "Epoch 407/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.1746 - accuracy: 0.9304\n",
      "Epoch 408/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.1737 - accuracy: 0.9317\n",
      "Epoch 409/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.1865 - accuracy: 0.9274\n",
      "Epoch 410/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.1748 - accuracy: 0.9257\n",
      "Epoch 411/1500\n",
      " 1/47 [..............................] - ETA: 0s - loss: 0.1226 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 381.\n",
      "47/47 [==============================] - 0s 790us/step - loss: 0.1914 - accuracy: 0.9257\n",
      "Epoch 411: early stopping\n",
      "6/6 [==============================] - 0s 750us/step - loss: 1.1309 - accuracy: 0.6685\n",
      "6/6 [==============================] - 0s 617us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 674, Predictions: 674, Actuals: 674, Gender: 674\n",
      "After appending - Cat IDs: 855, Predictions: 855, Actuals: 855, Gender: 855\n",
      "Final Test Results - Loss: 1.1309243440628052, Accuracy: 0.6685082912445068, Precision: 0.7695370370370371, Recall: 0.7042812430136375, F1 Score: 0.6881887052341598\n",
      "Confusion Matrix:\n",
      " [[91  1  6]\n",
      " [ 1 11  0]\n",
      " [52  0 19]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6571315464509369\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.9497328400611877\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7126340717077255\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6874633985364401\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6861253904784191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 855, Predictions: 855, Actuals: 855, Gender: 855\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (82/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, adult,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, adult, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [senior, senior, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A  [kitten, adult, kitten, kitten, kitten, adult,...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [kitten, adult, kitten, adult, kitten, adult, ...        kitten           kitten                   True\n",
       "76    070A              [adult, senior, adult, adult, senior]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                    [adult, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "81    075A              [adult, adult, senior, senior, adult]         adult            adult                   True\n",
       "80    074A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "18    016A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "42    036A  [adult, adult, senior, senior, adult, adult, a...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "40    034A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "52    047A  [kitten, adult, adult, kitten, adult, adult, k...         adult           kitten                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "6     005A  [senior, adult, senior, senior, senior, senior...        senior            adult                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "39    033A  [kitten, adult, kitten, kitten, adult, kitten,...        kitten            adult                  False\n",
       "102   108A         [adult, adult, adult, adult, adult, adult]         adult           senior                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "101   106A  [adult, senior, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "90    095A  [senior, adult, senior, senior, senior, senior...        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "30    025C             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, senior, adult, s...         adult           senior                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [kitten, adult]         adult           senior                  False\n",
       "69    063A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    11\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          564            450  79.787234\n",
      "1           kitten          113             75  66.371681\n",
      "2           senior          178             89  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgvElEQVR4nO3dd3QU5dvG8e8mpJCEEgIBQu9FpJfQpFepUv2JBaRJEwRE6VJEpXeQJk2KShcQpCgt0puEUEMLvUkKIWXfP3Iyb5YkEFJIwl6fczhnd2Z25p7NDnvtM888YzKbzWZERERERKyETXIXICIiIiLyOikAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqpEnuAkSsUUBAAOvWrWPfvn1cvnyZR48e4eDgQNasWSlXrhzvvfceBQsWTO4yE42fnx/NmjUznh8+fNh43LRpU27evAnAnDlzKF++fJzXGxQURMOGDQkICACgSJEiLF++PJGqlvh60d87OWzatImRI0caz/v378/777+ffAW9gtDQULZv38727du5ePEi9+/fx2w2kzFjRgoXLkydOnVo2LAhadLo61zkVeiIEXnNjh49ytdff839+/ctpoeEhODv78/Fixf55ZdfaNOmDV988YW+2F5g+/btRvgF8PHx4d9//+Wtt95KxqokpdmwYYPF87Vr16aKAOzr68vw4cM5c+ZMtHm3b9/m9u3b7Nmzh+XLlzN58mSyZcuWDFWKpE76ZhV5jU6ePEnv3r0JDg4GwNbWlooVK5I3b16CgoI4dOgQN27cwGw2s3r1ah48eMB3332XzFWnXOvXr482be3atQrAYrh69SpHjx61mHbp0iWOHz9O6dKlk6eoOLh+/TodO3bkyZMnANjY2FCuXDkKFChAcHAwJ0+e5OLFiwCcP3+ePn36sHz5cuzs7JKzbJFUQwFY5DUJDg5m6NChRvjNkSMHEydOtOjqEBYWxvz585k3bx4Af/75J2vXrqVly5bJUnNK5uvry4kTJwBInz49//33HwDbtm2jX79+ODs7J2d5kkJEbf2N+jlZu3Ztig3AoaGhfPnll0b4zZYtGxMnTqRIkSIWy/3yyy98//33QESo//3332nRosXrLlckVVIAFnlN/vjjD/z8/ICI1pzx48dH6+dra2tLt27duHz5Mn/++ScAixYtokWLFvz999/0798fAA8PD9avX4/JZLJ4fZs2bbh8+TIAU6ZMoVq1akBE+F65ciVbtmzh2rVr2NvbU6hQId577z0aNGhgsZ7Dhw/TvXt3AOrVq0fjxo2ZNGkSt27dImvWrMycOZMcOXJw7949FixYwIEDB7hz5w5hYWFkzJiR4sWL07FjR0qWLJkE7+L/i9r626ZNG7y8vPj3338JDAxk69attGrVKtbXnj17lqVLl3L06FEePXpEpkyZKFCgAO3bt6dKlSrRlvf392f58uXs2rWL69evY2dnh4eHB/Xr16dNmzY4OTkZy44cOZJNmzYB0KVLF7p162bMi/reZs+enY0bNxrzIvs+u7m5MW/ePEaOHIm3tzfp06fnyy+/pE6dOjx79ozly5ezfft2rl27RnBwMM7OzuTLl49WrVrx7rvvxrv2Tp06cfLkSQD69u1Lhw4dLNazYsUKJk6cCEC1atWYMmVKrO/v8549e8aiRYvYuHEjDx48IGfOnDRr1oz27dsbXXyGDBnCH3/8AUDbtm358ssvLdaxe/duBgwYAECBAgVYtWrVS7cbGhpq/C0g4m/zxRdfABE/LgcMGEC6dOlifG1AQAALFy5k+/bt3Lt3Dw8PD1q3bk27du3w9PQkLCws2t8QIj5bCxcu5OjRowQEBODu7k7lypXp2LEjWbNmjdP79eeff3Lu3Dkg4v+KSZMmUbhw4WjLtWnThosXL/L48WPy589PgQIFjHlxPY4Bbt68yerVq9mzZw+3bt0iTZo0FCxYkMaNG9OsWbNo3bCi9tPfsGEDHh4eFu9xTJ//jRs38s033wDQoUMH3n//fWbOnMn+/fsJDg6mWLFidOnShQoVKsTpPRJJKAVgkdfk77//Nh5XqFAhxi+0SB988IERgP38/Lhw4QJVq1bFzc2N+/fv4+fnx4kTJyxasLy9vY3wmyVLFipXrgxEfJH36tWLU6dOGcsGBwdz9OhRjh49ipeXFyNGjIgWpiHi1OqXX35JSEgIENFP2cPDg4cPH9K1a1euXr1qsfz9+/fZs2cP+/fvZ9q0aVSqVOkV36W4CQ0N5ffffzeeN23alGzZsvHvv/8CEa17sQXgTZs2MXr0aMLCwoxpkf0p9+/fT69evfjkk0+Mebdu3eKzzz7j2rVrxrSnT5/i4+ODj48PO3bsYM6cORYhOCGePn1Kr169jB9L9+/fp3DhwoSHhzNkyBB27dplsfyTJ084efIkJ0+e5Pr16xaB+1Vqb9asmRGAt23bFi0Ab9++3XjcpEmTV9qnvn37cvDgQeP5pUuXmDJlCidOnOCHH37AZDLRvHlzIwDv2LGDAQMGYGPz/wMVxWf7+/bt4969ewCUKVOGd955h5IlS3Ly5EmCg4P5/fffad++fbTX+fv706VLF86fP29M8/X1ZcKECVy4cCHW7W3dupURI0ZYfLZu3LjBr7/+yvbt25k+fTrFixd/ad1R99XT0/OF/1d89dVXL11fbMcxwP79+xk8eDD+/v4Wrzl+/DjHjx9n69atTJo0CRcXl5duJ678/Pzo0KEDDx8+NKYdPXqUnj17MmzYMJo2bZpo2xKJjYZBE3lNon6ZvuzUa7FixSz68nl7e5MmTRqLL/6tW7davGbz5s3G43fffRdbW1sAJk6caITftGnT0rRpU959910cHByAiEC4du3aGOvw9fXFZDLRtGlT6tatS6NGjTCZTPz0009G+M2RIwft27fnvffeI3PmzEBEV46VK1e+cB8TYs+ePTx48ACICDY5c+akfv36pE2bFohohfP29o72ukuXLjF27FgjoBQqVIg2bdrg6elpLDNjxgx8fHyM50OGDDECpIuLC02aNKF58+ZGF4szZ84we/bsRNu3gIAA/Pz8qF69Oi1btqRSpUrkypWLvXv3GuHX2dmZ5s2b0759e4tw9PPPP2M2m+NVe/369Y0Qf+bMGa5fv26s59atW8ZnKH369LzzzjuvtE8HDx6kWLFitGnThqJFixrTd+3aZbTkV6hQwWiRvH//PkeOHDGWCw4OZs+ePUDEWZJGjRrFabtRzxJEHjvNmzc3pq1bty7G102bNs3ieK1SpQrvvfceHh4erFu3ziLgRrpy5YrFD6u33nrLYn8fP37M119/bXSBepGzZ88aj0uVKvXS5V8mtuPYz8+Pr7/+2gi/WbNmpWXLltSuXdto9T169CjDhg1LcA1R7dy5k4cPH1KlShVatmyJu7s7AOHh4Xz33XfGqDAiSUktwCKvSdTWDjc3txcumyZNGtKnT2+MFPHo0SMAmjVrxuLFi4GIVqIBAwaQJk0awsLC2LZtm/H6yCGo7t27Z7SU2tnZsXDhQgoVKgRA69at+fTTTwkPD2fZsmW89957MdbSp0+faK1kuXLlokGDBly9epWpU6eSKVMmABo1akSXLl2AiJavpBI12ES2Fjk7O1O3bl3jlPSaNWsYMmSIxetWrFhhtILVrFmT7777zviiHzNmDOvWrcPZ2ZmDBw9SpEgRTpw4YfQzdnZ2ZtmyZeTMmdPYbufOnbG1teXff/8lPDzcosUyIWrVqsX48eMtptnb29OiRQvOnz9P9+7djRb+p0+fUq9ePYKCgggICODRo0e4urq+cu1OTk7UrVvX6DO7bds2OnXqBEScko8M1vXr18fe3v6V9qdevXqMHTsWGxsbwsPDGTZsmNHau2bNGlq0aGEEtDlz5hjbjzwdvm/fPgIDAwGoVKmS8UPrRe7du8e+ffuAiB9+9erVM2qZOHEigYGBXLhwgZMnT1p01wkKCrI4uxC1O0hAQABdunQxuidEtXLlSiPcNmzYkNGjR2MymQgPD6d///7s2bOHGzdusHPnzpcG+KgjxEQeW5FCQ0MtfrBFFVOXjEgxHceLFi0yRlEpXrw4s2bNMlp6jx07Rvfu3QkLC2PPnj0cPnz4lYYofJkBAwYY9Tx8+JAOHTpw+/ZtgoODWbt2LT169Ei0bYnERC3AIq9JaGio8ThqK11soi4T+ThPnjyUKVMGiGhROnDgABDRwhb5pVm6dGly584NwJEjR4wWqdKlSxvhF+Dtt98mb968QMSV8pGn3J/XoEGDaNNat27N2LFjWbp0KZkyZeLx48fs3bvXIjjEpaUrPu7cuWPsd9q0aalbt64xL2rr3rZt24zQFCnqeLRt27a16NvYs2dP1q1bx+7du/nwww+jLf/OO+8YARIi3s9ly5bx999/s3DhwkQLvxDze+7p6cnQoUNZvHgxlStXJjg4mOPHj7N06VKLz0rk+x6f2p9//yJFdseBV+/+ANCxY0djGzY2Nnz00UfGPB8fH+NHSZMmTYzldu7caRwzUbsExPX0+KZNm4zPfu3atY3WbScnJyMMA9HOfnh7exvvYbp06SxCo7Ozs0XtUUXt4tGqVSujS5GNjY1F3+x//vnnpbVHnp0BYmxtjo+YPlNR39devXpZdHMoU6YM9evXN57v3r07UeqAiAaAtm3bGs9dXV1p06aN8Tzyh5tIUlILsMhrkiFDBu7evQtg9EuMzbNnz3j8+LHxPGPGjMbj5s2bc+zYMSCiG0T16tUtuj9EvQHBrVu3jMeHDh16YQvO5cuXLS5mAXB0dMTV1TXG5U+fPs369es5cuRItL7AEHE6Myls3LjRCAW2trbGhVGRTCYTZrOZgIAA/vjjD4sRNO7cuWM8zp49u8XrXF1do+3ri5YHLE7nx0VcfvjEti2I+HuuWbMGLy8vfHx8YgxHke97fGovVaoUefPmxdfXlwsXLnD58mXSpk3L6dOnAcibNy8lSpSI0z5EFfmDLFLkDy+ICHiPHz8mc+bMZMuWDU9PT/bv38/jx4/5559/KFeuHHv37gUiAmlcu19EHf3hzJkzFi2KUY+/7du3079/fyP8RR6jENG95/kLwPLlyxfj9qIea5FnQWIS2U//RbJmzcqlS5eAiP7pUdnY2PDxxx8bzy9cuGC0dMcmpuP40aNHFv1+Y/o8FC1alC1btgBY9CN/kbgc97ly5Yr2gzHq+/r8GOkiSUEBWOQ1KVy4sPHlGrV/Y0xOnjxpEW6ifjnVrVuX8ePHExAQwN9//82TJ0/466+/gOitW1G/jBwcHF54IUtkK1xUsQ0ltmLFCiZNmoTZbMbR0ZEaNWpQunRpsmXLxtdff/3CfUsIs9lsEWz8/f0tWt6e96Ih5F61ZS0+LXHPB96Y3uOYxPS+nzhxgt69exMYGIjJZKJ06dKULVuWkiVLMmbMGIvg9rxXqb158+ZMnToViGgFjnpxX3xafyFivx0dHWOtJ7K/OkT8gNu/f7+x/aCgIIKCgoCI7gtRW0djc/ToUYsfZZcvX441eD59+pTNmzcbLZJR/2av8iMu6rIZM2a02Keo4nJjm7feessIwM/fRc/GxobevXsbzzdu3PjSABzT5ykudUR9L2K6SBaiv0dx+Yw/e/Ys2rSo1zzEti2RxKQALPKaVK9e3fiiOnbsGKdOneLtt9+OcdmlS5caj7Nly2bRdcHR0ZH69euzdu1agoKCmDVrlnGqv27dusaFYBAxGkSkMmXKMGPGDIvthIWFxfpFDcQ4qP5///3H9OnTMZvN2NnZsXr1aqPlOPJLO6kcOXLklfoWnzlzBh8fH2P8VHd3d6Mly9fX16Il8urVq/z222/kz5+fIkWKULRoUePiHIi4yOl5s2fPJl26dBQoUIAyZcrg6Oho0bL19OlTi+Uj+3K/TEzv+6RJk4y/8+jRo2nYsKExL2r3mkjxqR0iLqCcOXMmoaGhbNu2zQhPNjY2NG7cOE71P+/8+fOULVvWeB41nDo4OJA+fXrjeY0aNciYMSOPHj1i9+7dxri9EPfuDzHdIOVF1q1bZwTgqMeMn58foaGhFmExtlEg3N3djc/mpEmTLPoVv+w4e16jRo2MvrynTp3iyJEjlCtXLsZl4xLSY/o8ubi44OLiYrQC+/j4RBuCLOrFoLly5TIeR/blhuif8ahnrmITOYRf1B8zUT8TUf8GIklFfYBFXpMmTZoYF++YzWa+/PLLaLc4DQkJYdKkSRYtOp988km004VR+2r+9ttvxuOo3R8AypUrZ7SmHDlyxOIL7dy5c1SvXp127doxZMiQaF9kEHNLzJUrV4wWHFtbW4txVKN2xUiKLhBRr9pv3749hw8fjvFfxYoVjeXWrFljPI4aIlavXm3RWrV69WqWL1/O6NGjWbBgQbTlDxw4YNx5CyKu1F+wYAFTpkyhb9++xnsSNcw9/4Ngx44dcdrP2IakixS1S8yBAwcsLrCMfN/jUztEXHRVvXp1IOJvHfkZrVixokWofhULFy40QrrZbDYu5AQoUaKERTi0s7MzgnZAQIAx+kPu3Llj/cEYlb+/v8X7vGzZshg/I5s2bTLe53PnzhndPIoVK2YEM39/f4vRTP777z9++umnGLcbNeCvWLHC4vP/1VdfUb9+fbp3727R7zY2FSpUsFjf4MGDjSHqotq5cyczZ8586fpia1GN2p1k5syZFrcVP378uEU/8Nq1axuPox7zUT/jt2/fthhuMTZPnjyx+Az4+/tbHKeR1zmIJCW1AIu8Jo6OjowdO5aePXsSGhrK3bt3+eSTTyhfvjwFChQgMDAQLy8viz5/77zzTozj2ZYoUYICBQpw8eJF44s2T5480YZXy549O7Vq1WLnzp2EhITQqVMnateujbOzM3/++SfPnj3j4sWL5M+f3+IU9YtEvQL/6dOndOzYkUqVKuHt7W3xJZ3YF8E9efLEYgzcqBe/Pa9BgwZG14itW7fSt29f0qZNS/v27dm0aROhoaEcPHiQ999/nwoVKnDjxg3jtDtAu3btgIiLxaKOG9uxY0dq1KiBo6OjRZBp3LixEXyjttbv37+fcePGUaRIEf7666+Xnqp+kcyZMxsXKg4ePJj69etz//59i/Gl4f/f9/jUHql58+bRxhuOb/cHAC8vLzp06ED58uU5ffq0ETYBi4uhom7/559/jtf2t27davyYy5kzZ6z9tLNly0bp0qWN/vRr1qyhRIkSODk50bRpU3799Vcg4oYyhw8fJkuWLOzfvz9an9xI77//Pps3byYsLIzt27dz5coVypQpw+XLl43P4qNHjxg4cOBL98FkMvHNN9/QoUMHHj9+zP379/n0008pU6YMhQsXJjg4OMa+969698OPPvqIHTt2EBwczOnTp2nXrh2VK1fmv//+46+//jK6qtSsWdMilBYuXJhDhw4BMGHCBO7cuYPZbGblypVGd5WX+fHHHzl27Bi5c+fmwIEDxmc7bdq0Fj/wRZKKWoBFXqNy5coxY8YMYxi08PBwDh48yIoVK1i/fr3Fl2uLFi34/vvvY229ef5LIrbTw4MHDyZ//vxARDjasmULv/76q3E6vmDBggwaNCjO+5A9e3aL8Onr68uqVas4efIkadKkMYL048ePLU5fJ9SWLVuMcJclS5YXjo9au3Zt47Rv5MVwELGvX3/9tdHi6Ovryy+//GIRfjt27GhxseCYMWOM8WkDAwPZsmULa9euNU4d58+fn759+1psO3J5iGih//bbb9m3b5/Fle6vKnJkCohoifz111/ZtWsXYWFhFn27o16s9Kq1R6pcubLFaWhnZ2dq1qwZr7oLFy5M2bJluXDhAitXrrQIv82aNaNOnTrRXlOgQAGLi+1epftF1D7iL/qRBJYjI2zfvt14X3r16mUcMwB79+5l7dq13L592yKIRz0zU7hwYQYOHGjRqrxq1Soj/JpMJr788kuLu7W9SPbs2Vm2bJlx4wyz2czRo0dZuXIla9eutQi/tra2NG7c+JXHoy5YsCCjRo0ygvOtW7dYu3YtO3bsMFrsy5Urx8iRIy1e98EHHxj7+eDBA6ZMmcLUqVP577//4vRDJW/evOTIkYNDhw7x22+/Wdwhc8iQIfE+0yDyKhSARV6z8uXLs379egYOHIinpydubm6kSZPGuKVt69atWbZsGUOHDo2x716kxo0bG/NtbW1j/eLJmDEjS5YsoUePHhQpUgQnJyecnJwoWLAgn332GfPnz7c4pR4Xo0aNokePHuTNmxd7e3syZMhAtWrVmD9/PrVq1QIivrB37tz5Sut9kaj9OmvXrv3CC2XSpUtncUvjqENdNW/enEWLFlGvXj3c3NywtbUlffr0VKpUiQkTJtCzZ0+LdXl4eLB06VI6depEvnz5cHBwwMHBgQIFCtC1a1cWL15MhgwZjOXTpk3L/PnzadSoERkzZsTR0ZESJUowZsyYGMNmXLVp04bvvvuO4sWL4+TkRNq0aSlRogSjR4+2WG/U0/+vWnskW1tb3nrrLeN53bp143yG4Hn29vbMmDGDLl264OHhgb29Pfnz5+err7564Q0WonZ3KF++PNmyZXvpts6fP2/RrehlAbhu3brGj6GgoCDj5jIuLi4sXLiQ9u3b4+7ujr29PYULF+bbb7/lgw8+MF7//HvSunVrFixYQN26dcmcOTN2dnZkzZqVd955h3nz5tG6deuX7kNU2bNnZ9GiRYwbN446deqQPXt27O3tcXBwIFu2bFStWpW+ffuyceNGRo0aFeuILS9Sp04dVqxYwYcffki+fPlwdHTE2dmZUqVKMWTIEGbOnBnt4tlq1aoxefJkSpYsaYwwUb9+fZYtWxanUUIyZcrEokWLePfdd0mfPj2Ojo6UK1eO2bNnW/RtF0lKJnNcx+URERGrcPXqVdq3b2/0DZ47d26sF2ElhUePHtGmTRujb/PIkSMT1AXjVS1YsID06dOTIUMGChcubHGx5KZNm4wW0erVqzN58uTXVldqtnHjRr755hsgor/0jz/+mMwVibVTH2AREeHmzZusXr2asLAwtm7daoTfAgUKvJbwGxQUxOzZs7G1tTVulQsR4zO/rCU3sW3YsMEY0SFdunTUqVMHZ2dnbt26ZVyUBxEtoSKSOqXYAHz79m3atWvHhAkTLPrjXbt2jUmTJnHs2DFsbW2pW7cuvXv3tjhFExgYyPTp09m5cyeBgYGUKVOGL774wuJXvIiI/D+TyWQx/B5EjMgQl4u2EoODgwOrV6+2GNLNZDLxxRdfxLv7RXx1796d4cOHYzabefLkicXoI5FKliwZ52HZRCTlSZEB+NatW/Tu3dviLjUQcRV49+7dcXNzY+TIkTx8+JBp06bh5+fH9OnTjeWGDBnC6dOn6dOnD87OzsybN4/u3buzevXqaFc7i4hIxIWFuXLl4s6dOzg6OlKkSBE6der0wrsHJiYbGxvefvttvL29sbOzI1++fHTo0MFi+K3XpVGjRmTPnp3Vq1fz77//cu/ePUJDQ3FyciJfvnzUrl2btm3bYm9v/9prE5HEkaL6AIeHh/P7778zZcoUIOIq8jlz5hj/AS9atIgFCxawadMm46Kdffv28fnnnzN//nxKly7NyZMn6dSpE1OnTqVq1aoAPHz4kGbNmvHJJ5/w6aefJseuiYiIiEgKkaJGgTh//jzjxo3j3XffNTrLR3XgwAHKlCljccW6p6cnzs7OxviaBw4cIG3atHh6ehrLuLq6UrZs2QSNwSkiIiIib4YUFYCzZcvG2rVrY+3z5evrS+7cuS2m2dra4uHhYdzq09fXlxw5ckS77WSuXLlivB2oiIiIiFiXFNUHOEOGDDGOSRnJ398/xjvdODk5GbdwjMsyr8rHx8d47YvGZRURERGR5BMSEoLJZHrpLbVTVAB+maj3Vn9e5B154rJMfER2lY4cGkhEREREUqdUFYBdXFwIDAyMNj0gIMC4daKLiwsPHjyIcZnn72YTV0WKFOHUqVOYzWYKFiwYr3WIiIiISNK6cOHCC+8UGilVBeA8efJY3OceICwsDD8/P+P2q3ny5MHLy4vw8HCLFt9r164leBxgk8mEk5NTgtYhIiIiIkkjLuEXUthFcC/j6enJ0aNHjTsEAXh5eREYGGiM+uDp6UlAQAAHDhwwlnn48CHHjh2zGBlCRERERKxTqgrArVu3xsHBgZ49e7Jr1y7WrVvHsGHDqFKlCqVKlQIi7jFerlw5hg0bxrp169i1axc9evQgXbp0tG7dOpn3QERERESSW6rqAuHq6sqcOXOYNGkSQ4cOxdnZmTp16tC3b1+L5caPH8/kyZOZOnUq4eHhlCpVinHjxukucCIiIiKSsu4El5KdOnUKgLfffjuZKxERERGRmMQ1r6WqLhAiIiIiIgmlACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWJU0yV2ACMDhw4fp3r17rPO7du1K165d2bNnD/PmzePChQtkzJiROnXq8Nlnn+Hk5PTC9R87doyZM2dy/vx5XFxcqFWrFp999hnOzs6JvSsiIiKSwpnMZrM5uYtIDU6dOgXA22+/ncyVvJn8/f25fPlytOmzZ8/m33//ZcmSJVy6dIkvv/yScuXK8f777xMSEsKCBQuwt7dnwYIFpEkT8++5ixcv8uGHH1K6dGk6dOjAnTt3mD59OiVLlmTy5MlJvWsiIiLymsQ1r6kFWFIEFxeXaB/Wv/76i4MHD/Ldd9+RJ08evvrqK/Lly8f06dOxs7MDoEyZMrRo0YKNGzfSsmXLGNe9detWTCYTEyZMMFqKw8LCGDduHDdv3iR79uxJu3MiIiKSoqgPsKRIT58+Zfz48VSrVo26desCcPnyZTw9PY3wC+Dm5ka+fPnYu3dvrOsKDg4mTZo0ODo6GtMyZMgAwOPHj5NoD0RERCSlUgCWFGnlypXcvXuX/v37G9MyZszIzZs3LZYLDQ3l1q1b3LhxI9Z1NWvWDIDJkyfz6NEjLl68yLx58yhYsCCFChVKmh0QERGRFEsBWFKckJAQVqxYQf369cmVK5cxvVmzZuzatYuffvqJhw8fcuvWLUaNGoW/vz9BQUGxrq9gwYL07t2bVatWUbduXdq1a0dgYCBTpkzB1tb2deySiIiIpCAKwJLi7Nixg/v37/Phhx9aTO/atSsff/wxc+bMoV69erRo0QJnZ2dq1Khh0b3heT/99BPfffcdrVq1Yvbs2YwbNw4nJyd69OjB/fv3k3p3REREJIXRRXCS4uzYsYP8+fNTuHBhi+lp0qShd+/edO3alRs3bpAlSxbSpUtHly5djD69zwsNDWX+/Pk0atSIQYMGGdPLlStHixYtWLp0KX379k3K3REREZEURi3AkqKEhoZy4MAB6tWrF23e4cOHOXDgAA4ODuTPn5906dIRGhrKhQsXKFKkSIzre/ToEU+fPqVUqVIW0zNlykSePHm4dOlSkuyHiIiIpFwKwJKiXLhwIcbAChEtw2PGjCE0NNSYtmHDBp48eULNmjVjXJ+rqysZMmTg2LFjFtMfPXrE1atXyZEjR6LWLyIiIimfukBIinLhwgUA8ufPH21eq1atWLduHSNHjqRZs2acO3eOGTNmUK9ePcqVK2csd/bsWezt7cmfPz+2trZ07dqV8ePH4+zsTN26dXn06BE//fQTNjY2fPDBB69t30RERCRlUACWFCXyorR06dJFm1ewYEEmT57MzJkz6devH5kzZ6ZTp0506tTJYrmBAweSPXt2fvzxRwDatWtHunTpWLZsGRs3biRjxoyULl2a8ePHqwVYRETECulWyHGkWyGLiIiIpGxv9K2Q165dy4oVK/Dz8yNbtmy0bduWNm3aYDKZALh27RqTJk3i2LFj2NraUrduXXr37o2Li0syVy4iIiIiyS3VBeB169YxduxY2rVrR40aNTh27Bjjx4/n2bNndOjQgSdPntC9e3fc3NwYOXIkDx8+ZNq0afj5+TF9+vTkLl9EREREklmqC8AbNmygdOnSDBw4EICKFSty5coVVq9eTYcOHfj11195/Pgxy5cvJ2PGjAC4u7vz+eefc/z4cUqXLp18xYuIiIhIskt1w6AFBwfj7OxsMS1Dhgw8fvwYgAMHDlCmTBkj/AJ4enri7OzMvn37XmepIiIiIpICpboA/P777+Pl5cXmzZvx9/fnwIED/P777zRu3BgAX19fcufObfEaW1tbPDw8uHLlSnKULCIiIiIpSKrrAtGgQQOOHDnC8OHDjWmVK1emf//+APj7+0drIQZwcnIiICAgQds2m80EBgYmaB0iIiIikjTMZrMxKMKLpLoA3L9/f44fP06fPn146623uHDhAj/++CODBg1iwoQJhIeHx/paG5uENXiHhITg7e2doHWkFHZ2dqRJk+r+/FYjNDSUkJCQ5C5DREQk1bG3t3/pMqkqAZ04cYL9+/czdOhQWrRoAUC5cuXIkSMHffv2Ze/evbi4uMTYShsQEIC7u3uCtm9nZ0fBggUTtI6UwGQy4eDgiI3Ny38hSfIIDzcTHPwUDdMtIiISd5F3lH2ZVBWAb968CUCpUqUsppctWxaAixcvkidPHq5du2YxPywsDD8/P2rVqpWg7ZtMJpycnBK0jpRkpdc57vynLh0pjXt6J9p7FiZt2rTJXYqIiEiqEpfuD5DKAnDevHkBOHbsGPny5TOmnzhxAoCcOXPi6enJkiVLePjwIa6urgB4eXkRGBiIp6fna685JbvzXyB+DxPWL1pEREQktUlVAbho0aLUrl2byZMn899//1GiRAkuXbrEjz/+SLFixahZsyblypVj1apV9OzZky5duvD48WOmTZtGlSpVorUci4iIiIj1MZlTWSfDkJAQFixYwObNm7l79y7ZsmWjZs2adOnSxeiecOHCBSZNmsSJEydwdnamRo0a9O3bN8bRIeIqrveWTk2mbTuuFuAUyMPVmT71Syd3GSIiIqlOXPNaqmoBhogL0bp370737t1jXaZgwYLMmjXrNVYlIiIiIqlFqrsRhoiIiIhIQigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiJvgFOnTtGtWzeqVatG/fr1GTFiBA8ePDDm37lzh6FDh1KnTh1q1KhBjx49OHv27CttY+LEiZQvXz6xSxcRee0UgEVEUjlvb2+6d++Ok5MTEyZMoHfv3nh5eTFgwAAAAgIC6NKlCz4+Pnz99deMGTOGgIAAevbsyb179+K0jaNHj7Jy5cqk3A0RkdcmTXIXICIiCTNt2jSKFCnCxIkTsbGJaNdwdnZm4sSJ3Lhxgy1btvD48WN+/fVXMmfODECxYsX48MMPOXz4MA0bNnzh+gMDA/nmm29wd3fn9u3bSb4/IiJJTS3AIiKp2KNHjzhy5AitW7c2wi9A7dq1+f3338mRIwc7duygTp06RvgFyJw5M1u2bHlp+AWYOnUqbm5uNG3aNEn2QUTkdVMAFhFJxS5cuEB4eDiurq4MHTqUd955h+rVqzN8+HCePHlCaGgoly5dIk+ePMyePZsGDRpQqVIlunXrxsWLF1+6fi8vL37//XdGjBiByWR6DXskIpL0FIBFRFKxhw8fAjBq1CgcHByYMGECn3/+OXv27KFv3748fvyYsLAwfv75Zw4fPsywYcMYN24cDx8+pGvXrty9ezfWdfv7+zN69Gi6d+9Onjx5XtcuiYgkOfUBFhFJxUJCQgAoWrQow4YNA6BixYqkS5eOIUOGcODAAWPZ6dOn4+TkBEDx4sVp2bIlq1evpmfPnjGue+LEiWTNmpX//e9/SbwXIiKvlwKwiEgqFhloq1evbjG9SpUqAPj5+QFQrlw5Y1mAbNmykS9fPnx8fGJc7549e9i2bRtLliwhPDyc8PBwzGYzAKGhodjY2Fj0ORYRSU0UgEVEUrHcuXMD8OzZM4vpoaGhAKRPnx5XV9do8yOXcXBwiHG9O3bsIDg4mHbt2kWb5+npSZMmTRg5cmQCqxcRSR4KwCIiqVi+fPnw8PBg27ZttGvXzrhQ7a+//gKgdOnSVK1alV27dvHo0SMyZswIgK+vL1euXKF58+Yxrrdr1660bdvWYtratWtZu3YtS5YsMdYjIpIaJSgAX79+ndu3b/Pw4UPSpElDxowZyZ8/P+nTp0+s+kRE5AVMJhN9+vTh66+/ZvDgwbRo0YLLly8za9YsateuTdGiRencuTO7d++mZ8+edOnShZCQEGbNmkXWrFlp0aKFsa5Tp07h6upKzpw58fDwwMPDw2Jbe/bsASL6D4uIpGavHIBPnz7N2rVr8fLyivXq4dy5c1O9enWaNm1K/vz5E1ykiIjErm7dujg4ODBv3jz69etH+vTpadWqFZ999hkAOXPmZOHChUyfPp3hw4djY2NDpUqV+OKLL3B2djbW07FjR3VtEBGrYDJHXtXwEsePH2fatGmcPn0agJe9LPI0XOnSpenbt2+qbzE4deoUAG+//XYyV5J4pm07jt/DgOQuQ57j4epMn/qlk7sMERGRVCeueS1OLcBjx45lw4YNhIeHA5A3b17efvttChUqRJYsWYwWhP/++4+7d+9y/vx5zp49y6VLlzh27BgdO3akcePGjBgxIiH7JCIiIiKSYHEKwOvWrcPd3Z333nuPunXrxnlA9Pv37/Pnn3+yZs0a405CIiIiIiLJKU4B+IcffqBGjRqvPOajm5sb7dq1o127dnh5ecWrQBERERGRxBSnAFyrVq0Eb8jT0zPB6xARERERSagEjwPs7+/P7Nmz2bt3L/fv38fd3Z2GDRvSsWNH7OzsEqNGEREREZFEk+AAPGrUKHbt2mU8v3btGvPnzycoKIjPP/88oasXEREREUlUCQrAISEh/PXXX9SuXZsPP/yQjBkz4u/vz/r16/njjz8UgEVEREQkxYnTVW1jx47l3r170aYHBwcTHh5O/vz5eeutt8iZMydFixblrbfeIjg4ONGLFRFJbuFxGzpdkoH+NiISV3EeBm3Lli20bduWTz75xLjVsYuLC4UKFWLBggUsX76cdOnSERgYSEBAADVq1EjSwkVEkoONycRKr3Pc+S8wuUuRKNzTO9Hes3BylyEiqUScAvA333zD3LlzWbp0KWvXruWjjz7i/fffx9HRkW+++YYhQ4Zw+fJlgoKCAChVqhQDBw5M0sJFRJLLnf8CdRdFEZFULE4BuHHjxtSvX581a9awcOFCZs2axapVq+jcuTMtW7Zk1apV3Lx5kwcPHuDu7o67u3tS1y0iIiIiEi9xvrNFmjRpaNu2LevWreOzzz7j2bNn/PDDD7Ru3Zo//vgDDw8PSpQoofArIiIiIinaq93aDXB0dKRTp06sX7+eDz/8kLt37zJ8+HD+97//sW/fvqSoUUREREQk0cQ5AN+/f5/ff/+dpUuX8scff2Aymejduzfr1q2jZcuWXL58mX79+tG1a1dOnjyZlDWLiIiIiMRbnPoAHz58mP79+xsXuQG4uroyd+5c8ubNy9dff82HH37I7Nmz2b59O507d6ZatWpMmjQpyQoXEREREYmPOLUAT5s2jTRp0lC1alUaNGhAjRo1SJMmDbNmzTKWyZkzJ2PHjmXZsmVUrlyZvXv3JlnRIiIiIiLxFacWYF9fX6ZNm0bp0qWNaU+ePKFz587Rli1cuDBTp07l+PHjiVWjiIiIiEiiiVMAzpYtG6NHj6ZKlSq4uLgQFBTE8ePHyZ49e6yviRqWRURERERSijgF4E6dOjFixAhWrlyJyWTCbDZjZ2dn0QVCRERERCQ1iFMAbtiwIfny5eOvv/4ybnZRv359cubMmdT1iYiIiIgkqjgFYIAiRYpQpEiRpKxFRERERCTJxWkUiP79+3Pw4MF4b+TMmTMMHTo03q9/3qlTp+jWrRvVqlWjfv36jBgxggcPHhjzr127Rr9+/ahZsyZ16tRh3Lhx+Pv7J9r2RURERCT1ilML8J49e9izZw85c+akTp061KxZk2LFimFjE3N+Dg0N5cSJExw8eJA9e/Zw4cIFAMaMGZPggr29venevTsVK1ZkwoQJ3L17lxkzZnDt2jUWLlzIkydP6N69O25ubowcOZKHDx8ybdo0/Pz8mD59eoK3LyIiIiKpW5wC8Lx58/j+++85f/48ixcvZvHixdjZ2ZEvXz6yZMmCs7MzJpOJwMBAbt26xdWrVwkODgbAbDZTtGhR+vfvnygFT5s2jSJFijBx4kQjgDs7OzNx4kRu3LjBtm3bePz4McuXLydjxowAuLu78/nnn3P8+HGNTiEiIiJi5eIUgEuVKsWyZcvYsWMHS5cuxdvbm2fPnuHj48O5c+csljWbzQCYTCYqVqxIq1atqFmzJiaTKcHFPnr0iCNHjjBy5EiL1ufatWtTu3ZtAA4cOECZMmWM8Avg6emJs7Mz+/btUwAWERERsXJxvgjOxsaGevXqUa9ePfz8/Ni/fz8nTpzg7t27Rv/bTJkykTNnTkqXLk2FChXImjVrohZ74cIFwsPDcXV1ZejQofz999+YzWZq1arFwIEDSZcuHb6+vtSrV8/idba2tnh4eHDlypUEbd9sNhMYGJigdaQEJpOJtGnTJncZ8hJBQUHGD0pJGXTspHw6bkSsm9lsjlOja5wDcFQeHh60bt2a1q1bx+fl8fbw4UMARo0aRZUqVZgwYQJXr15l5syZ3Lhxg/nz5+Pv74+zs3O01zo5OREQEJCg7YeEhODt7Z2gdaQEadOmpXjx4sldhrzE5cuXCQoKSu4yJAodOymfjhsRsbe3f+ky8QrAySUkJASAokWLMmzYMAAqVqxIunTpGDJkCP/88w/h4eGxvj62i/biys7OjoIFCyZoHSlBYnRHkaSXL18+tWSlMDp2Uj4dNyLWLXLghZdJVQHYyckJgOrVq1tMr1KlCgBnz57FxcUlxm4KAQEBuLu7J2j7JpPJqEEkqelUu8ir03EjYt3i2lCRsCbR1yx37twAPHv2zGJ6aGgoAI6OjuTJk4dr165ZzA8LC8PPz4+8efO+ljpFREREJOVKVQE4X758eHh4sG3bNotTXH/99RcApUuXxtPTk6NHjxr9hQG8vLwIDAzE09PztdcsIiIiIilLqgrAJpOJPn36cOrUKQYPHsw///zDypUrmTRpErVr16Zo0aK0bt0aBwcHevbsya5du1i3bh3Dhg2jSpUqlCpVKrl3QURERESSWbz6AJ8+fZoSJUokdi1xUrduXRwcHJg3bx79+vUjffr0tGrVis8++wwAV1dX5syZw6RJkxg6dCjOzs7UqVOHvn37Jku9IiIiIpKyxCsAd+zYkXz58vHuu+/SuHFjsmTJkth1vVD16tWjXQgXVcGCBZk1a9ZrrEhEREREUot4d4Hw9fVl5syZNGnShF69evHHH38Ytz8WEREREUmp4tUC/PHHH7Njxw6uX7+O2Wzm4MGDHDx4ECcnJ+rVq8e7776rWw6LiIiISIoUrwDcq1cvevXqhY+PD3/++Sc7duzg2rVrBAQEsH79etavX4+HhwdNmjShSZMmZMuWLbHrFhERERGJlwTdCKNIkSIUKVKEnj17cu7cOVavXs369esB8PPz48cff2T+/Pm0atWK/v37J/hObCIiIiKJJTg4mHfeeYewsDCL6WnTpmXPnj0AnDlzhilTpuDt7Y2zszNNmzala9eu2NnZvXDdXl5ezJo1i4sXL+Lm5kabNm3o0KGD7iiZQiT4TnBPnjxhx44dbN++nSNHjmAymTCbzcY4vWFhYfzyyy+kT5+ebt26JbhgERERkcRw8eJFwsLCGD16NDlz5jSmRzbYXb9+nR49elCyZEnGjRuHr68vs2bN4vHjxwwePDjW9Z46dYq+fftSr149unfvzvHjx5k2bRphYWF88sknSb1bEgfxCsCBgYHs3r2bbdu2cfDgQeNObGazGRsbGypVqkSzZs0wmUxMnz4dPz8/tm7dqgAsIiIiKca5c+ewtbWlTp062NvbR5u/ePFinJ2dmThxInZ2dlSrVg1HR0d++OEHOnXqFGsXz7lz51KkSBFGjx4NQJUqVQgNDWXRokW0b98eR0fHJN0vebl4BeB69eoREhICYLT0enh40LRp02h9ft3d3fn000+5c+dOIpQrIiIikjh8fHzImzdvjOEXIroxVK1a1aK7Q506dfjuu+84cOAALVu2jPaaZ8+eceTIkWiNfnXq1GHJkiUcP35cd6ZNAeIVgJ89ewaAvb09tWvXpnnz5pQvXz7GZT08PABIly5dPEsUERERSXyRLcA9e/bkxIkT2NvbGzfPsrW15ebNm+TOndviNa6urjg7O3PlypUY13njxg1CQkKivS5XrlwAXLlyRQE4BYhXAC5WrBjNmjWjYcOGuLi4vHDZtGnTMnPmTHLkyBGvAkVEREQSm9ls5sKFC5jNZlq0aMGnn37KmTNnmDdvHpcvX2bcuHEAMeYcZ2dnAgICYlyvv7+/sUxUTk5OALG+Tl6veAXgJUuWABF9gUNCQoxTA1euXCFz5swWf3RnZ2cqVqyYCKWKiIiIJA6z2czEiRNxdXWlQIECAJQtWxY3NzeGDRvG4cOHX/j62EZzCA8Pf+HrNCJWyhDvv8L69etp0qQJp06dMqYtW7aMRo0asWHDhkQpTkRERCQp2NjYUL58eSP8RqpWrRoQ0ZUBYm6xDQgIiPUMeOT0wMDAaK+JOl+SV7wC8L59+xgzZgz+/v5cuHDBmO7r60tQUBBjxozh4MGDiVakiIiISGK6e/cua9eu5datWxbTg4ODAcicOTPu7u5cv37dYv6DBw8ICAggX758Ma43Z86c2Nracu3aNYvpkc/z5s2bSHsgCRGvALx8+XIAsmfPbvHL6YMPPiBXrlyYzWaWLl2aOBWKiIiIJLKwsDDGjh3Lb7/9ZjF927Zt2NraUqZMGSpVqsSePXuMi/8Bdu7cia2tLRUqVIhxvQ4ODpQpU4Zdu3YZI2VFvs7FxYUSJUokzQ7JK4lXH+CLFy9iMpkYPnw45cqVM6bXrFmTDBky0LVrV86fP59oRYqIiIgkpmzZstG0aVOWLl2Kg4MDJUuW5Pjx4yxatIi2bduSJ08ePv74Y7Zt20afPn344IMPuHLlCrNmzaJly5bGkK/Pnj3Dx8cHd3d3smbNCsCnn35Kjx49+Oqrr2jWrBknT55k6dKl9OrVS2MApxDxagGOvMLR1dU12rzI4c6ePHmSgLJEREREktbXX39N586d2bx5M3379mXz5s1069aNfv36ARHdFWbMmMHTp08ZNGgQP//8M//73/8YMGCAsY579+7RsWNH1q1bZ0yrUKECP/zwA1euXGHAgAFs3bqVzz//nI8//vh176LEIl4twFmzZuX69eusWbPG4kNgNptZuXKlsYyIiIhISmVvb0/nzp3p3LlzrMuUKVOGn376Kdb5Hh4eMY4YUatWLWrVqpUYZUoSiFcArlmzJkuXLmX16tV4eXlRqFAhQkNDOXfuHDdv3sRkMlGjRo3ErlVEREREJMHiFYA7derE7t27uXbtGlevXuXq1avGPLPZTK5cufj0008TrUgRERERkcQSrz7ALi4uLFq0iBYtWuDi4oLZbMZsNuPs7EyLFi1YuHChxrkTERERkRQpXi3AABkyZGDIkCEMHjyYR48eYTabcXV1jfXOKCIiIiIiKUGC78dnMplwdXUlU6ZMRvgNDw9n//79CS5ORERERCSxxasF2Gw2s3DhQv7++2/+++8/i/teh4aG8ujRI0JDQ/nnn38SrVARERERkcQQrwC8atUq5syZg8lksrjLCWBMU1cIEREREUmJ4tUF4vfffwcgbdq05MqVC5PJxFtvvUW+fPmM8Dto0KBELVRERERSr/DnGswk5bDGv028WoCvX7+OyWTi+++/x9XVlQ4dOtCtWzcqV67M5MmT+fnnn/H19U3kUkVERCS1sjGZWOl1jjv/BSZ3KRKFe3on2nsWTu4yXrt4BeDg4GAAcufOTfbs2XFycuL06dNUrlyZli1b8vPPP7Nv3z769++fqMWKiIhI6nXnv0D8HgYkdxki8esCkSlTJgB8fHwwmUwUKlSIffv2ARGtwwB37txJpBJFRERERBJPvAJwqVKlMJvNDBs2jGvXrlGmTBnOnDlD27ZtGTx4MPD/IVlEREREJCWJVwDu3Lkz6dOnJyQkhCxZstCgQQNMJhO+vr4EBQVhMpmoW7duYtcqIiIiIpJg8QrA+fLlY+nSpXTp0gVHR0cKFizIiBEjyJo1K+nTp6d58+Z069YtsWsVEREREUmweF0Et2/fPkqWLEnnzp2NaY0bN6Zx48aJVpiIiIiISFKIVwvw8OHDadiwIX///Xdi1yMiIiIikqTiFYCfPn1KSEgIefPmTeRyRERERESSVrwCcJ06dQDYtWtXohYjIiIiIpLU4tUHuHDhwuzdu5eZM2eyZs0a8ufPj4uLC2nS/P/qTCYTw4cPT7RCRUREREQSQ7wC8NSpUzGZTADcvHmTmzdvxricArCIiIiIpDTxCsAAZrP5hfMjA7KIiIiISEoSrwC8YcOGxK5DREREROS1iFcAzp49e2LXISIiIiLyWsQrAB89ejROy5UtWzY+qxcRERERSTLxCsDdunV7aR9fk8nEP//8E6+iRERERESSSpJdBCciIiIikhLFKwB36dLF4rnZbObZs2fcunWLXbt2UbRoUTp16pQoBYqIiIiIJKZ4BeCuXbvGOu/PP/9k8ODBPHnyJN5FiYiIiIgklXjdCvlFateuDcCKFSsSe9UiIiIiIgmW6AH40KFDmM1mLl68mNirFhERERFJsHh1gejevXu0aeHh4fj7+3Pp0iUAMmXKlLDKRERERESSQLwC8JEjR2IdBi1ydIgmTZrEvyoRERERkSSSqMOg2dnZkSVLFho0aEDnzp0TVFhcDRw4kLNnz7Jx40Zj2rVr15g0aRLHjh3D1taWunXr0rt3b1xcXF5LTSIiIiKScsUrAB86dCix64iXzZs3s2vXLotbMz958oTu3bvj5ubGyJEjefjwIdOmTcPPz4/p06cnY7UiIiIikhLEuwU4JiEhIdjZ2SXmKmN19+5dJkyYQNasWS2m//rrrzx+/Jjly5eTMWNGANzd3fn88885fvw4pUuXfi31iYiIiEjKFO9RIHx8fOjRowdnz541pk2bNo3OnTtz/vz5RCnuRUaPHk2lSpWoUKGCxfQDBw5QpkwZI/wCeHp64uzszL59+5K8LhERERFJ2eIVgC9dukS3bt04fPiwRdj19fXlxIkTdO3aFV9f38SqMZp169Zx9uxZBg0aFG2er68vuXPntphma2uLh4cHV65cSbKaRERERCR1iFcXiIULFxIQEIC9vb3FaBDFihXj6NGjBAQE8NNPPzFy5MjEqtNw8+ZNJk+ezPDhwy1aeSP5+/vj7OwcbbqTkxMBAQEJ2rbZbCYwMDBB60gJTCYTadOmTe4y5CWCgoJivNhUko+OnZRPx03KpGMn5XtTjh2z2RzrSGVRxSsAHz9+HJPJxNChQ2nUqJExvUePHhQsWJAhQ4Zw7Nix+Kz6hcxmM6NGjaJKlSrUqVMnxmXCw8Njfb2NTcLu+xESEoK3t3eC1pESpE2bluLFiyd3GfISly9fJigoKLnLkCh07KR8Om5SJh07Kd+bdOzY29u/dJl4BeAHDx4AUKJEiWjzihQpAsC9e/fis+oXWr16NefPn2flypWEhoYC/z8cW2hoKDY2Nri4uMTYShsQEIC7u3uCtm9nZ0fBggUTtI6UIC6/jCT55cuX7434Nf4m0bGT8um4SZl07KR8b8qxc+HChTgtF68AnCFDBu7fv8+hQ4fIlSuXxbz9+/cDkC5duvis+oV27NjBo0ePaNiwYbR5np6edOnShTx58nDt2jWLeWFhYfj5+VGrVq0Ebd9kMuHk5JSgdYjElU4Xirw6HTci8fOmHDtx/bEVrwBcvnx5tm7dysSJE/H29qZIkSKEhoZy5swZtm/fjslkijY6Q2IYPHhwtNbdefPm4e3tzaRJk8iSJQs2NjYsWbKEhw8f4urqCoCXlxeBgYF4enomek0iIiIikrrEKwB37tyZv//+m6CgINavX28xz2w2kzZtWj799NNEKTCqvHnzRpuWIUMG7OzsjL5FrVu3ZtWqVfTs2ZMuXbrw+PFjpk2bRpUqVShVqlSi1yQiIiIiqUu8rgrLkycP06dPJ3fu3JjNZot/uXPnZvr06TGG1dfB1dWVOXPmkDFjRoYOHcqsWbOoU6cO48aNS5Z6RERERCRlifed4EqWLMmvv/6Kj48P165dw2w2kytXLooUKfJaO7vHNNRawYIFmTVr1murQURERERSjwTdCjkwMJD8+fMbIz9cuXKFwMDAGMfhFRERERFJCeI9MO769etp0qQJp06dMqYtW7aMRo0asWHDhkQpTkREREQkscUrAO/bt48xY8bg7+9vMd6ar68vQUFBjBkzhoMHDyZakSIiIiIiiSVeAXj58uUAZM+enQIFChjTP/jgA3LlyoXZbGbp0qWJU6GIiIiISCKKVx/gixcvYjKZGD58OOXKlTOm16xZkwwZMtC1a1fOnz+faEWKiIiIiCSWeLUA+/v7Axg3mogq8g5wT548SUBZIiIiIiJJI14BOGvWrACsWbPGYrrZbGblypUWy4iIiIiIpCTx6gJRs2ZNli5dyurVq/Hy8qJQoUKEhoZy7tw5bt68iclkokaNGoldq4iIiIhIgsUrAHfq1Indu3dz7do1rl69ytWrV415kTfESIpbIYuIiIiIJFS8ukC4uLiwaNEiWrRogYuLi3EbZGdnZ1q0aMHChQtxcXFJ7FpFRERERBIs3neCy5AhA0OGDGHw4ME8evQIs9mMq6vra70NsoiIiIjIq4r3neAimUwmXF1dyZQpEyaTiaCgINauXctHH32UGPWJiIiIiCSqeLcAP8/b25s1a9awbds2goKCEmu1IiIiIiKJKkEBODAwkC1btrBu3Tp8fHyM6WazWV0hRERERCRFilcA/vfff1m7di3bt283WnvNZjMAtra21KhRg1atWiVelSIiIiIiiSTOATggIIAtW7awdu1a4zbHkaE3kslkYtOmTWTOnDlxqxQRERERSSRxCsCjRo3izz//5OnTpxah18nJidq1a5MtWzbmz58PoPArIiIiIilanALwxo0bMZlMmM1m0qRJg6enJ40aNaJGjRo4ODhw4MCBpK5TRERERCRRvNIwaCaTCXd3d0qUKEHx4sVxcHBIqrpERERERJJEnFqAS5cuzfHjxwG4efMmc+fOZe7cuRQvXpyGDRvqrm8iIiIikmrEKQDPmzePq1evsm7dOjZv3sz9+/cBOHPmDGfOnLFYNiwsDFtb28SvVEREREQkEcS5C0Tu3Lnp06cPv//+O+PHj6datWpGv+Co4/42bNiQKVOmcPHixSQrWkREREQkvl55HGBbW1tq1qxJzZo1uXfvHhs2bGDjxo1cv34dgMePH/Pzzz+zYsUK/vnnn0QvWEREREQkIV7pIrjnZc6cmU6dOrF27Vpmz55Nw4YNsbOzM1qFRURERERSmgTdCjmq8uXLU758eQYNGsTmzZvZsGFDYq1aRERERCTRJFoAjuTi4kLbtm1p27ZtYq9aRERERCTBEtQFQkREREQktVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJV0iR3Aa8qPDycNWvW8Ouvv3Ljxg0yZcrEO++8Q7du3XBxcQHg2rVrTJo0iWPHjmFra0vdunXp3bu3MV9ERERErFeqC8BLlixh9uzZfPjhh1SoUIGrV68yZ84cLl68yMyZM/H396d79+64ubkxcuRIHj58yLRp0/Dz82P69OnJXb6IiIiIJLNUFYDDw8NZvHgx7733Hr169QKgUqVKZMiQgcGDB+Pt7c0///zD48ePWb58ORkzZgTA3d2dzz//nOPHj1O6dOnk2wERERERSXapqg9wQEAAjRs3pkGDBhbT8+bNC8D169c5cOAAZcqUMcIvgKenJ87Ozuzbt+81VisiIiIiKVGqagFOly4dAwcOjDZ99+7dAOTPnx9fX1/q1atnMd/W1hYPDw+uXLnyOsoUERERkRQsVQXgmJw+fZrFixdTvXp1ChYsiL+/P87OztGWc3JyIiAgIEHbMpvNBAYGJmgdKYHJZCJt2rTJXYa8RFBQEGazObnLkCh07KR8Om5SJh07Kd+bcuyYzWZMJtNLl0vVAfj48eP069cPDw8PRowYAUT0E46NjU3CenyEhITg7e2doHWkBGnTpqV48eLJXYa8xOXLlwkKCkruMiQKHTspn46blEnHTsr3Jh079vb2L10m1Qbgbdu28c0335A7d26mT59u9Pl1cXGJsZU2ICAAd3f3BG3Tzs6OggULJmgdKUFcfhlJ8suXL98b8Wv8TaJjJ+XTcZMy6dhJ+d6UY+fChQtxWi5VBuClS5cybdo0ypUrx4QJEyzG982TJw/Xrl2zWD4sLAw/Pz9q1aqVoO2aTCacnJwStA6RuNLpQpFXp+NGJH7elGMnrj+2UtUoEAC//fYbU6dOpW7dukyfPj3azS08PT05evQoDx8+NKZ5eXkRGBiIp6fn6y5XRERERFKYVNUCfO/ePSZNmoSHhwft2rXj7NmzFvNz5sxJ69atWbVqFT179qRLly48fvyYadOmUaVKFUqVKpVMlYuIiIhISpGqAvC+ffsIDg7Gz8+Pzp07R5s/YsQImjZtypw5c5g0aRJDhw7F2dmZOnXq0Ldv39dfsIiIiIikOKkqADdv3pzmzZu/dLmCBQsya9as11CRiIiIiKQ2qa4PsIiIiIhIQigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlXe6ADs5eXFRx99RNWqVWnWrBlLly7FbDYnd1kiIiIikoze2AB86tQp+vbtS548eRg/fjwNGzZk2rRpLF68OLlLExEREZFklCa5C0gqc+fOpUiRIowePRqAKlWqEBoayqJFi2jfvj2Ojo7JXKGIiIiIJIc3sgX42bNnHDlyhFq1allMr1OnDgEBARw/fjx5ChMRERGRZPdGBuAbN24QEhJC7ty5LabnypULgCtXriRHWSIiIiKSAryRXSD8/f0BcHZ2tpju5OQEQEBAwCutz8fHh2fPngFw8uTJRKgw+ZlMJipmCicso7qCpDS2NuGcOnVKF2ymUDp2UiYdNymfjp2U6U07dkJCQjCZTC9d7o0MwOHh4S+cb2Pz6g3fkW9mXN7U1MLZwS65S5AXeJM+a28aHTspl46blE3HTsr1phw7JpPJegOwi4sLAIGBgRbTI1t+I+fHVZEiRRKnMBERERFJdm9kH+CcOXNia2vLtWvXLKZHPs+bN28yVCUiIiIiKcEbGYAdHBwoU6YMu3btsujTsnPnTlxcXChRokQyViciIiIiyemNDMAAn376KadPn+arr75i3759zJ49m6VLl9KxY0eNASwiIiJixUzmN+Wyvxjs2rWLuXPncuXKFdzd3WnTpg0dOnRI7rJEREREJBm90QFYREREROR5b2wXCBERERGRmCgAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWCxehoJUN50MX3G9bkXEWumACypkp+fH+XLl2fjxo3xfs2TJ08YPnw4x44dS6oyRZJE06ZNGTlyZIzz5s6dS/ny5Y3nx48f5/PPP7dYZv78+SxdujQpSxSxKvH5TpLkpQAsVsvHx4fNmzcTHh6e3KWIJJoWLVqwaNEi4/m6deu4fPmyxTJz5swhKCjodZcm8sbKnDkzixYtolq1asldisRRmuQuQEREEk/WrFnJmjVrcpchYlXs7e15++23k7sMeQVqAZZk9/TpU2bMmEHLli2pXLkyNWrUoEePHvj4+BjL7Ny5k/fff5+qVavywQcfcO7cOYt1bNy4kfLly+Pn52cxPbZTxYcPH6Z79+4AdO/ena5duyb+jom8JuvXr6dChQrMnz/fogvEyJEj2bRpEzdv3jROz0bOmzdvnkVXiQsXLtC3b19q1KhBjRo1GDBgANevXzfmHz58mPLly3Pw4EF69uxJ1apVadCgAdOmTSMsLOz17rDIK/D29uazzz6jRo0avPPOO/To0YNTp04Z848dO0bXrl2pWrUqtWvXZsSIETx8+NCYv3HjRipVqsTp06fp2LEjVapUoUmTJhbdiGLqAnH16lW+/PJLGjRoQLVq1ejWrRvHjx+P9pply5bRqlUrqlatyoYNG5L2zRCDArAkuxEjRrBhwwY++eQTZsyYQb9+/bh06RJDhw7FbDbz999/M2jQIAoWLMiECROoV68ew4YNS9A2ixYtyqBBgwAYNGgQX331VWLsishrt23bNsaOHUvnzp3p3LmzxbzOnTtTtWpV3NzcjNOzkd0jmjdvbjy+cuUKn376KQ8ePGDkyJEMGzaMGzduGNOiGjZsGGXKlGHKlCk0aNCAJUuWsG7duteyryKvyt/fn969e5MxY0Z++OEHvv32W4KCgujVqxf+/v4cPXqUzz77DEdHR7777ju++OILjhw5Qrdu3Xj69KmxnvDwcL766ivq16/P1KlTKV26NFOnTuXAgQMxbvfSpUt8+OGH3Lx5k4EDBzJmzBhMJhPdu3fnyJEjFsvOmzePjz/+mFGjRlGpUqUkfT/k/6kLhCSrkJAQAgMDGThwIPXq1QOgXLly+Pv7M2XKFO7fv8/8+fN56623GD16NACVK1cGYMaMGfHerouLC/ny5QMgX7585M+fP4F7IvL67dmzh+HDh/PJJ5/QrVu3aPNz5syJq6urxelZV1dXANzd3Y1p8+bNw9HRkVmzZuHi4gJAhQoVaN68OUuXLrW4iK5FixZG0K5QoQJ//fUXe/fupVWrVkm6ryLxcfnyZR49ekT79u0pVaoUAHnz5mXNmjUEBAQwY8YM8uTJw+TJk7G1tQXg7bffpm3btmzYsIG2bdsCEaOmdO7cmRYtWgBQqlQpdu3axZ49e4zvpKjmzZuHnZ0dc+bMwdnZGYBq1arRrl07pk6dypIlS4xl69atS7NmzZLybZAYqAVYkpWdnR3Tp0+nXr163Llzh8OHD/Pbb7+xd+9eICIge3t7U716dYvXRYZlEWvl7e3NV199hbu7u9GdJ74OHTpE2bJlcXR0JDQ0lNDQUJydnSlTpgz//POPxbLP93N0d3fXBXWSYhUoUABXV1f69evHt99+y65du3Bzc6NPnz5kyJCB06dPU61aNcxms/HZz5EjB3nz5o322S9ZsqTx2N7enowZM8b62T9y5AjVq1c3wi9AmjRpqF+/Pt7e3gQGBhrTCxcunMh7LXGhFmBJdgcOHGDixIn4+vri7OxMoUKFcHJyAuDOnTuYzWYyZsxo8ZrMmTMnQ6UiKcfFixepVq0ae/fuZfXq1bRv3z7e63r06BHbt29n+/bt0eZFthhHcnR0tHhuMpk0koqkWE5OTsybN48FCxawfft21qxZg4ODA++++y4dO3YkPDycxYsXs3jx4mivdXBwsHj+/GffxsYm1vG0Hz9+jJubW7Tpbm5umM1mAgICLGqU108BWJLV9evXGTBgADVq1GDKlCnkyJEDk8nEL7/8wv79+8mQIQM2NjbR+iE+fvzY4rnJZAKI9kUc9Ve2yJukSpUqTJkyha+//ppZs2ZRs2ZNsmXLFq91pUuXjooVK9KhQ4do8yJPC4ukVnnz5mX06NGEhYXx77//snnzZn799Vfc3d0xmUz873//o0GDBtFe93zgfRUZMmTg/v370aZHTsuQIQP37t2L9/ol4dQFQpKVt7c3wcHBfPLJJ+TMmdMIsvv37wciThmVLFmSnTt3WvzS/vvvvy3WE3ma6fbt28Y0X1/faEE5Kn2xS2qWKVMmAPr374+NjQ3fffddjMvZ2ET/b/75aWXLluXy5csULlyY4sWLU7x4cYoVK8by5cvZvXt3otcu8rr8+eef1K1bl3v37mFra0vJkiX56quvSJcuHffv36do0aL4+voan/vixYuTP39+5s6dG+1itVdRtmxZ9uzZY9HSGxYWxh9//EHx4sWxt7dPjN2TBFAAlmRVtGhRbG1tmT59Ol5eXuzZs4eBAwcafYCfPn1Kz549uXTpEgMHDmT//v2sWLGCuXPnWqynfPnyODg4MGXKFPbt28e2bdvo378/GTJkiHXb6dKlA2Dfvn3RhlUTSS0yZ85Mz5492bt3L1u3bo02P126dDx48IB9+/YZLU7p0qXjxIkTHD16FLPZTJcuXbh27Rr9+vVj9+7dHDhwgC+//JJt27ZRqFCh171LIommdOnShIeHM2DAAHbv3s2hQ4cYO3Ys/v7+1KlTh549e+Ll5cXQoUPZu3cvf//9N3369OHQoUMULVo03tvt0qULwcHBdO/enT///JO//vqL3r17c+PGDXr27JmIeyjxpQAsySpXrlyMHTuW27dv079/f7799lsg4nauJpOJY8eOUaZMGaZNm8adO3cYOHAga9asYfjw4RbrSZcuHePHjycsLIwBAwYwZ84cunTpQvHixWPddv78+WnQoAGrV69m6NChSbqfIkmpVatWvPXWW0ycODHaWY+mTZuSPXt2+vfvz6ZNmwDo2LEj3t7e9OnTh9u3b1OoUCHmz5+PyWRixIgRDBo0iHv37jFhwgRq166dHLskkigyZ87M9OnTcXFxYfTo0fTt2xcfHx9++OEHypcvj6enJ9OnT+f27dsMGjSI4cOHY2try6xZsxJ0Y4sCBQowf/58XF1dGTVqlPGdNXfuXA11lkKYzLH14BYREREReQOpBVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauSJrkLEBF5E3Tp0oVjx44BETefGDFiRDJXFN2FCxf47bffOHjwIPfu3ePZs2e4urpSrFgxmjVrRo0aNZK7RBGR10I3whARSaArV67QqlUr47mjoyNbt27FxcUlGauy9NNPPzFnzhxCQ0NjXaZRo0Z888032Njo5KCIvNn0v5yISAKtX7/e4vnTp0/ZvHlzMlUT3erVq5kxYwahoaFkzZqVwYMH88svv7By5Ur69u2Ls7MzAFu2bOHnn39O5mpFRJKeWoBFRBIgNDSUd999l/v37+Ph4cHt27cJCwujcOHCKSJM3rt3j6ZNmxISEkLWrFlZsmQJbm5uFsvs27ePzz//HIAsWbKwefNmTCZTcpQrIvJaqA+wiEgC7N27l/v37wPQrFkzTp8+zd69ezl37hynT5+mRIkS0V7j5+fHjBkz8PLyIiQkhDJlyvDFF1/w7bffcvToUcqWLcuPP/5oLO/r68vcuXM5dOgQgYGBZM+enUaNGvHhhx/i4ODwwvo2bdpESEgIAJ07d44WfgGqVq1K37598fDwoHjx4kb43bhxI9988w0AkyZNYvHixZw5cwZXV1eWLl2Km5sbISEhrFy5kq1bt3Lt2jUAChQoQIsWLWjWrJlFkO7atStHjx4F4PDhw8b0w4cP0717dyCiL3W3bt0sli9cuDDff/89U6dO5dChQ5hMJipXrkzv3r3x8PB44f6LiMREAVhEJAGidn9o0KABuXLlYu/evQCsWbMmWgC+efMmH3/8MQ8fPjSm7d+/nzNnzsTYZ/jff/+lR48eBAQEGNOuXLnCnDlzOHjwILNmzSJNmtj/K48MnACenp6xLtehQ4cX7CWMGDGCJ0+eAODm5oabmxuBgYF07dqVs2fPWix76tQpTp06xb59+xg3bhy2trYvXPfLPHz4kI4dO/Lo0SNj2vbt2zl69CiLFy8mW7ZsCVq/iFgf9QEWEYmnu3fvsn//fgCKFy9Orly5qFGjhtGndvv27fj7+1u8ZsaMGUb4bdSoEStWrGD27NlkypSJ69evWyxrNpsZNWoUAQEBZMyYkfHjx/Pbb78xcOBAbGxsOHr0KKtWrXphjbdv3zYeZ8mSxWLevXv3uH37drR/z549i7aekJAQJk2axM8//8wXX3wBwJQpU4zwW79+fZYtW8bChQupVKkSADt37mTp0qUvfhPj4O7du6RPn54ZM2awYsUKGjVqBMD9+/eZPn16gtcvItZHAVhEJJ42btxIWFgYAA0bNgQiRoCoVasWAEFBQWzdutVYPjw83Ggdzpo1KyNGjKBQoUJUqFCBsWPHRlv/+fPnuXjxIgBNmjShePHiODo6UrNmTcqWLQvA77///sIao47o8PwIEB999BHvvvtutH8nT56Mtp66devyzjvvULhwYcqUKUNAQICx7QIFCjB69GiKFi1KyZIlmTBhgtHV4mUBPa6GDRuGp6cnhQoVYsSIEWTPnh2APXv2GH8DEZG4UgAWEYkHs9nMhg0bjOcuLi7s37+f/fv3W5ySX7t2rfH44cOHRleG4sWLW3RdKFSokNFyHOnq1avG42XLllmE1Mg+tBcvXoyxxTZS1qxZjcd+fn6vupuGAgUKRKstODgYgPLly1t0c0ibNi0lS5YEIlpvo3ZdiA+TyWTRlSRNmjQUL14cgMDAwASvX0Ssj/oAi4jEw5EjRyy6LIwaNSrG5Xx8fPj333956623sLOzM6bHZQCeuPSdDQsL47///iNz5swxzq9YsaLR6rx3717y589vzIs6VNvIkSPZtGlTrNt5vn/yy2p72f6FhYUZ64gM0i9aV2hoaKzvn0asEJFXpRZgEZF4eH7s3xeJbAVOnz496dKlA8Db29uiS8LZs2ctLnQDyJUrl/G4R48eHD582Pi3bNkytm7dyuHDh2MNvxDRN9fR0RGAxYsXx9oK/Py2n/f8hXY5cuTA3t4eiBjFITw83JgXFBTEqVOngIgW6IwZMwIYyz+/vVu3br1w2xDxgyNSWFgYPj4+QEQwj1y/iEhcKQCLiLyiJ0+esHPnTgAyZMjAgQMHLMLp4cOH2bp1q9HCuW3bNiPwNWjQAIi4OO2bb77hwoULeHl5MWTIkGjbKVCgAIULFwYiukD88ccfXL9+nc2bN/Pxxx/TsGFDBg4c+MJaM2fOTL9+/QB4/PgxHTt25JdffsHX1xdfX1+2bt1Kt27d2LVr1yu9B87OztSpUweI6IYxfPhwzp49y6lTp/jyyy+NoeHatm1rvCbqRXgrVqwgPDwcHx8fFi9e/NLtfffdd+zZs4cLFy7w3XffcePGDQBq1qypO9eJyCtTFwgRkVe0ZcsW47R948aNLU7NR8qcOTM1atRg586dBAYGsnXrVlq1akWnTp3YtWsX9+/fZ8uWLWzZsgWAbNmykTZtWoKCgoxT+iaTif79+9OnTx/++++/aCE5Q4YMxpi5L9KqVStCQkKYOnUq9+/f5/vvv49xOVtbW5o3b270r32ZgQMHcu7cOS5evMjWrVstLvgDqF27tsXwag0aNGDjxo0AzJs3j/nz52M2m3n77bdf2j/ZbDYbQT5SlixZ6NWrV5xqFRGJSj+bRUReUdTuD82bN491uVatWhmPI7tBuLu7s2DBAmrVqoWzszPOzs7Url2b+fPnG10EonYVKFeuHD/99BP16tXDzc0NOzs7smbNStOmTfnpp58oWLBgnGpu3749v/zyCx07dqRIkSJkyJABOzs7MmfOTMWKFenVqxcbN25k8ODBODk5xWmd6dOnZ+nSpXz++ecUK1YMJycnHB0dKVGiBEOHDuX777+36Cvs6enJ6NGjKVCgAPb29mTPnp0uXbowefLkl24r8j1LmzYtLi4u1K9fn0WLFr2w+4eISGx0K2QRkdfIy8sLe3t73N3dyZYtm9G3Njw8nOrVqxMcHEz9+vX59ttvk7nS5BfbneNERBJKXSBERF6jVatWsWfPHgBatGjBxx9/zLNnz9i0aZPRrSKuXRBERCR+FIBFRF6jdu3asW/fPsLDw1m3bh3r1q2zmJ81a1aaNWuWPMWJiFgJ9QEWEXmNPD09mTVrFtWrV8fNzQ1bW1vs7e3JmTMnrVq14qeffiJ9+vTJXaaIyBtNfYBFRERExKqoBVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsyv8BpnaAODzrb3wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    228      165     72.37\n",
      "1          M    337      261     77.45\n",
      "2          X    290      188     64.83\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({2: 1065, 0: 1026, 1: 971})\n",
      "Epoch 1/1500\n",
      "48/48 [==============================] - 0s 959us/step - loss: 0.9919 - accuracy: 0.5536\n",
      "Epoch 2/1500\n",
      "48/48 [==============================] - 0s 855us/step - loss: 0.8452 - accuracy: 0.6346\n",
      "Epoch 3/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.7250 - accuracy: 0.6914\n",
      "Epoch 4/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.6976 - accuracy: 0.7093\n",
      "Epoch 5/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 0.6706 - accuracy: 0.7257\n",
      "Epoch 6/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.6461 - accuracy: 0.7319\n",
      "Epoch 7/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.6197 - accuracy: 0.7391\n",
      "Epoch 8/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.6182 - accuracy: 0.7446\n",
      "Epoch 9/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 0.5956 - accuracy: 0.7462\n",
      "Epoch 10/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.5831 - accuracy: 0.7603\n",
      "Epoch 11/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 0.5747 - accuracy: 0.7639\n",
      "Epoch 12/1500\n",
      "48/48 [==============================] - 0s 759us/step - loss: 0.5704 - accuracy: 0.7691\n",
      "Epoch 13/1500\n",
      "48/48 [==============================] - 0s 768us/step - loss: 0.5441 - accuracy: 0.7763\n",
      "Epoch 14/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.5380 - accuracy: 0.7812\n",
      "Epoch 15/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 0.5217 - accuracy: 0.7936\n",
      "Epoch 16/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.5181 - accuracy: 0.7858\n",
      "Epoch 17/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.5257 - accuracy: 0.7796\n",
      "Epoch 18/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 0.5010 - accuracy: 0.7962\n",
      "Epoch 19/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 0.5104 - accuracy: 0.7871\n",
      "Epoch 20/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.4971 - accuracy: 0.7926\n",
      "Epoch 21/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 0.4852 - accuracy: 0.7982\n",
      "Epoch 22/1500\n",
      "48/48 [==============================] - 0s 760us/step - loss: 0.4823 - accuracy: 0.7959\n",
      "Epoch 23/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 0.4844 - accuracy: 0.8067\n",
      "Epoch 24/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.4665 - accuracy: 0.8168\n",
      "Epoch 25/1500\n",
      "48/48 [==============================] - 0s 802us/step - loss: 0.4639 - accuracy: 0.8210\n",
      "Epoch 26/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 0.4755 - accuracy: 0.8076\n",
      "Epoch 27/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 0.4833 - accuracy: 0.8057\n",
      "Epoch 28/1500\n",
      "48/48 [==============================] - 0s 764us/step - loss: 0.4626 - accuracy: 0.8152\n",
      "Epoch 29/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 0.4494 - accuracy: 0.8174\n",
      "Epoch 30/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.4483 - accuracy: 0.8165\n",
      "Epoch 31/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 0.4476 - accuracy: 0.8253\n",
      "Epoch 32/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.4512 - accuracy: 0.8145\n",
      "Epoch 33/1500\n",
      "48/48 [==============================] - 0s 793us/step - loss: 0.4350 - accuracy: 0.8236\n",
      "Epoch 34/1500\n",
      "48/48 [==============================] - 0s 897us/step - loss: 0.4348 - accuracy: 0.8240\n",
      "Epoch 35/1500\n",
      "48/48 [==============================] - 0s 776us/step - loss: 0.4268 - accuracy: 0.8253\n",
      "Epoch 36/1500\n",
      "48/48 [==============================] - 0s 786us/step - loss: 0.4232 - accuracy: 0.8266\n",
      "Epoch 37/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.4348 - accuracy: 0.8191\n",
      "Epoch 38/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 0.4193 - accuracy: 0.8276\n",
      "Epoch 39/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.4280 - accuracy: 0.8223\n",
      "Epoch 40/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 0.4028 - accuracy: 0.8390\n",
      "Epoch 41/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.4142 - accuracy: 0.8383\n",
      "Epoch 42/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.4081 - accuracy: 0.8331\n",
      "Epoch 43/1500\n",
      "48/48 [==============================] - 0s 762us/step - loss: 0.4103 - accuracy: 0.8318\n",
      "Epoch 44/1500\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.4104 - accuracy: 0.8298\n",
      "Epoch 45/1500\n",
      "48/48 [==============================] - 0s 881us/step - loss: 0.4082 - accuracy: 0.8357\n",
      "Epoch 46/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.4001 - accuracy: 0.8403\n",
      "Epoch 47/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.3938 - accuracy: 0.8390\n",
      "Epoch 48/1500\n",
      "48/48 [==============================] - 0s 920us/step - loss: 0.4021 - accuracy: 0.8436\n",
      "Epoch 49/1500\n",
      "48/48 [==============================] - 0s 913us/step - loss: 0.3927 - accuracy: 0.8449\n",
      "Epoch 50/1500\n",
      "48/48 [==============================] - 0s 957us/step - loss: 0.4027 - accuracy: 0.8429\n",
      "Epoch 51/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8380\n",
      "Epoch 52/1500\n",
      "48/48 [==============================] - 0s 898us/step - loss: 0.3855 - accuracy: 0.8423\n",
      "Epoch 53/1500\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.3853 - accuracy: 0.8475\n",
      "Epoch 54/1500\n",
      "48/48 [==============================] - 0s 779us/step - loss: 0.3780 - accuracy: 0.8465\n",
      "Epoch 55/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 0.3865 - accuracy: 0.8426\n",
      "Epoch 56/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.3841 - accuracy: 0.8462\n",
      "Epoch 57/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.3783 - accuracy: 0.8459\n",
      "Epoch 58/1500\n",
      "48/48 [==============================] - 0s 800us/step - loss: 0.3730 - accuracy: 0.8508\n",
      "Epoch 59/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.3766 - accuracy: 0.8501\n",
      "Epoch 60/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.3718 - accuracy: 0.8543\n",
      "Epoch 61/1500\n",
      "48/48 [==============================] - 0s 781us/step - loss: 0.3651 - accuracy: 0.8547\n",
      "Epoch 62/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8521\n",
      "Epoch 63/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3722 - accuracy: 0.8488\n",
      "Epoch 64/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.8511\n",
      "Epoch 65/1500\n",
      "48/48 [==============================] - 0s 940us/step - loss: 0.3720 - accuracy: 0.8521\n",
      "Epoch 66/1500\n",
      "48/48 [==============================] - 0s 915us/step - loss: 0.3532 - accuracy: 0.8602\n",
      "Epoch 67/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3558 - accuracy: 0.8651\n",
      "Epoch 68/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8511\n",
      "Epoch 69/1500\n",
      "48/48 [==============================] - 0s 865us/step - loss: 0.3566 - accuracy: 0.8579\n",
      "Epoch 70/1500\n",
      "48/48 [==============================] - 0s 998us/step - loss: 0.3521 - accuracy: 0.8602\n",
      "Epoch 71/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3597 - accuracy: 0.8622\n",
      "Epoch 72/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8583\n",
      "Epoch 73/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8710\n",
      "Epoch 74/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3600 - accuracy: 0.8563\n",
      "Epoch 75/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3403 - accuracy: 0.8586\n",
      "Epoch 76/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3582 - accuracy: 0.8547\n",
      "Epoch 77/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8612\n",
      "Epoch 78/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.8602\n",
      "Epoch 79/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3533 - accuracy: 0.8619\n",
      "Epoch 80/1500\n",
      "48/48 [==============================] - 0s 936us/step - loss: 0.3410 - accuracy: 0.8619\n",
      "Epoch 81/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8596\n",
      "Epoch 82/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3419 - accuracy: 0.8638\n",
      "Epoch 83/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8668\n",
      "Epoch 84/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8762\n",
      "Epoch 85/1500\n",
      "48/48 [==============================] - 0s 866us/step - loss: 0.3254 - accuracy: 0.8723\n",
      "Epoch 86/1500\n",
      "48/48 [==============================] - 0s 908us/step - loss: 0.3291 - accuracy: 0.8697\n",
      "Epoch 87/1500\n",
      "48/48 [==============================] - 0s 953us/step - loss: 0.3306 - accuracy: 0.8697\n",
      "Epoch 88/1500\n",
      "48/48 [==============================] - 0s 920us/step - loss: 0.3166 - accuracy: 0.8707\n",
      "Epoch 89/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8713\n",
      "Epoch 90/1500\n",
      "48/48 [==============================] - 0s 827us/step - loss: 0.3302 - accuracy: 0.8730\n",
      "Epoch 91/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.3278 - accuracy: 0.8681\n",
      "Epoch 92/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8739\n",
      "Epoch 93/1500\n",
      "48/48 [==============================] - 0s 875us/step - loss: 0.3219 - accuracy: 0.8736\n",
      "Epoch 94/1500\n",
      "48/48 [==============================] - 0s 909us/step - loss: 0.3256 - accuracy: 0.8690\n",
      "Epoch 95/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.3128 - accuracy: 0.8779\n",
      "Epoch 96/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8775\n",
      "Epoch 97/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8792\n",
      "Epoch 98/1500\n",
      "48/48 [==============================] - 0s 928us/step - loss: 0.3287 - accuracy: 0.8645\n",
      "Epoch 99/1500\n",
      "48/48 [==============================] - 0s 895us/step - loss: 0.3238 - accuracy: 0.8720\n",
      "Epoch 100/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8752\n",
      "Epoch 101/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3174 - accuracy: 0.8743\n",
      "Epoch 102/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3155 - accuracy: 0.8759\n",
      "Epoch 103/1500\n",
      "48/48 [==============================] - 0s 935us/step - loss: 0.3087 - accuracy: 0.8779\n",
      "Epoch 104/1500\n",
      "48/48 [==============================] - 0s 854us/step - loss: 0.3119 - accuracy: 0.8867\n",
      "Epoch 105/1500\n",
      "48/48 [==============================] - 0s 834us/step - loss: 0.3147 - accuracy: 0.8795\n",
      "Epoch 106/1500\n",
      "48/48 [==============================] - 0s 987us/step - loss: 0.3148 - accuracy: 0.8785\n",
      "Epoch 107/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3087 - accuracy: 0.8779\n",
      "Epoch 108/1500\n",
      "48/48 [==============================] - 0s 967us/step - loss: 0.3112 - accuracy: 0.8720\n",
      "Epoch 109/1500\n",
      "48/48 [==============================] - 0s 857us/step - loss: 0.2935 - accuracy: 0.8837\n",
      "Epoch 110/1500\n",
      "48/48 [==============================] - 0s 856us/step - loss: 0.2936 - accuracy: 0.8824\n",
      "Epoch 111/1500\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.2978 - accuracy: 0.8821\n",
      "Epoch 112/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.3031 - accuracy: 0.8841\n",
      "Epoch 113/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 0.2981 - accuracy: 0.8863\n",
      "Epoch 114/1500\n",
      "48/48 [==============================] - 0s 825us/step - loss: 0.2959 - accuracy: 0.8834\n",
      "Epoch 115/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 0.2973 - accuracy: 0.8857\n",
      "Epoch 116/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 0.3028 - accuracy: 0.8837\n",
      "Epoch 117/1500\n",
      "48/48 [==============================] - 0s 813us/step - loss: 0.2805 - accuracy: 0.8965\n",
      "Epoch 118/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.3022 - accuracy: 0.8831\n",
      "Epoch 119/1500\n",
      "48/48 [==============================] - 0s 775us/step - loss: 0.3046 - accuracy: 0.8762\n",
      "Epoch 120/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.2967 - accuracy: 0.8811\n",
      "Epoch 121/1500\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.2895 - accuracy: 0.8850\n",
      "Epoch 122/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.2921 - accuracy: 0.8850\n",
      "Epoch 123/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.2923 - accuracy: 0.8824\n",
      "Epoch 124/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 0.2898 - accuracy: 0.8863\n",
      "Epoch 125/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.2865 - accuracy: 0.8828\n",
      "Epoch 126/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 0.2808 - accuracy: 0.8909\n",
      "Epoch 127/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 0.3063 - accuracy: 0.8831\n",
      "Epoch 128/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 0.2864 - accuracy: 0.8906\n",
      "Epoch 129/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.2835 - accuracy: 0.8831\n",
      "Epoch 130/1500\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.2870 - accuracy: 0.8886\n",
      "Epoch 131/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.2813 - accuracy: 0.8860\n",
      "Epoch 132/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.2953 - accuracy: 0.8847\n",
      "Epoch 133/1500\n",
      "48/48 [==============================] - 0s 759us/step - loss: 0.2773 - accuracy: 0.8880\n",
      "Epoch 134/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 0.2848 - accuracy: 0.8922\n",
      "Epoch 135/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.2857 - accuracy: 0.8847\n",
      "Epoch 136/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.2740 - accuracy: 0.8890\n",
      "Epoch 137/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 0.2790 - accuracy: 0.8948\n",
      "Epoch 138/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 0.2869 - accuracy: 0.8867\n",
      "Epoch 139/1500\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.2777 - accuracy: 0.8965\n",
      "Epoch 140/1500\n",
      "48/48 [==============================] - 0s 857us/step - loss: 0.2846 - accuracy: 0.8870\n",
      "Epoch 141/1500\n",
      "48/48 [==============================] - 0s 881us/step - loss: 0.2738 - accuracy: 0.8935\n",
      "Epoch 142/1500\n",
      "48/48 [==============================] - 0s 830us/step - loss: 0.2851 - accuracy: 0.8847\n",
      "Epoch 143/1500\n",
      "48/48 [==============================] - 0s 822us/step - loss: 0.2748 - accuracy: 0.8994\n",
      "Epoch 144/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 0.2787 - accuracy: 0.8893\n",
      "Epoch 145/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 0.2564 - accuracy: 0.8984\n",
      "Epoch 146/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.2716 - accuracy: 0.8945\n",
      "Epoch 147/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.2795 - accuracy: 0.8919\n",
      "Epoch 148/1500\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.2700 - accuracy: 0.8922\n",
      "Epoch 149/1500\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.2706 - accuracy: 0.8961\n",
      "Epoch 150/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 0.2660 - accuracy: 0.8912\n",
      "Epoch 151/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.2658 - accuracy: 0.8981\n",
      "Epoch 152/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 0.2699 - accuracy: 0.8893\n",
      "Epoch 153/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 0.2887 - accuracy: 0.8965\n",
      "Epoch 154/1500\n",
      "48/48 [==============================] - 0s 812us/step - loss: 0.2577 - accuracy: 0.9004\n",
      "Epoch 155/1500\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.2588 - accuracy: 0.9020\n",
      "Epoch 156/1500\n",
      "48/48 [==============================] - 0s 813us/step - loss: 0.2751 - accuracy: 0.8909\n",
      "Epoch 157/1500\n",
      "48/48 [==============================] - 0s 776us/step - loss: 0.2559 - accuracy: 0.9020\n",
      "Epoch 158/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 0.2614 - accuracy: 0.8961\n",
      "Epoch 159/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.2614 - accuracy: 0.8978\n",
      "Epoch 160/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 0.2635 - accuracy: 0.9001\n",
      "Epoch 161/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 0.2623 - accuracy: 0.9024\n",
      "Epoch 162/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 0.2612 - accuracy: 0.8991\n",
      "Epoch 163/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.2786 - accuracy: 0.8929\n",
      "Epoch 164/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.2626 - accuracy: 0.8939\n",
      "Epoch 165/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 0.2655 - accuracy: 0.8975\n",
      "Epoch 166/1500\n",
      "48/48 [==============================] - 0s 778us/step - loss: 0.2489 - accuracy: 0.8984\n",
      "Epoch 167/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 0.2662 - accuracy: 0.8991\n",
      "Epoch 168/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.2661 - accuracy: 0.8948\n",
      "Epoch 169/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.2501 - accuracy: 0.9014\n",
      "Epoch 170/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.2532 - accuracy: 0.9001\n",
      "Epoch 171/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.2532 - accuracy: 0.9033\n",
      "Epoch 172/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 0.2470 - accuracy: 0.8988\n",
      "Epoch 173/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.2650 - accuracy: 0.9040\n",
      "Epoch 174/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.2457 - accuracy: 0.9050\n",
      "Epoch 175/1500\n",
      "48/48 [==============================] - 0s 780us/step - loss: 0.2630 - accuracy: 0.8971\n",
      "Epoch 176/1500\n",
      "48/48 [==============================] - 0s 821us/step - loss: 0.2499 - accuracy: 0.8981\n",
      "Epoch 177/1500\n",
      "48/48 [==============================] - 0s 837us/step - loss: 0.2550 - accuracy: 0.9066\n",
      "Epoch 178/1500\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.2514 - accuracy: 0.9010\n",
      "Epoch 179/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 0.2593 - accuracy: 0.9017\n",
      "Epoch 180/1500\n",
      "48/48 [==============================] - 0s 781us/step - loss: 0.2477 - accuracy: 0.9089\n",
      "Epoch 181/1500\n",
      "48/48 [==============================] - 0s 831us/step - loss: 0.2463 - accuracy: 0.9063\n",
      "Epoch 182/1500\n",
      "48/48 [==============================] - 0s 841us/step - loss: 0.2433 - accuracy: 0.9014\n",
      "Epoch 183/1500\n",
      "48/48 [==============================] - 0s 825us/step - loss: 0.2668 - accuracy: 0.8991\n",
      "Epoch 184/1500\n",
      "48/48 [==============================] - 0s 791us/step - loss: 0.2483 - accuracy: 0.8978\n",
      "Epoch 185/1500\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.2493 - accuracy: 0.8981\n",
      "Epoch 186/1500\n",
      "48/48 [==============================] - 0s 854us/step - loss: 0.2456 - accuracy: 0.9050\n",
      "Epoch 187/1500\n",
      "48/48 [==============================] - 0s 790us/step - loss: 0.2425 - accuracy: 0.9086\n",
      "Epoch 188/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 0.2440 - accuracy: 0.9059\n",
      "Epoch 189/1500\n",
      "48/48 [==============================] - 0s 824us/step - loss: 0.2548 - accuracy: 0.8997\n",
      "Epoch 190/1500\n",
      "48/48 [==============================] - 0s 831us/step - loss: 0.2434 - accuracy: 0.9046\n",
      "Epoch 191/1500\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.2508 - accuracy: 0.8981\n",
      "Epoch 192/1500\n",
      "48/48 [==============================] - 0s 827us/step - loss: 0.2367 - accuracy: 0.9030\n",
      "Epoch 193/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.2466 - accuracy: 0.9014\n",
      "Epoch 194/1500\n",
      "48/48 [==============================] - 0s 800us/step - loss: 0.2360 - accuracy: 0.9121\n",
      "Epoch 195/1500\n",
      "48/48 [==============================] - 0s 802us/step - loss: 0.2747 - accuracy: 0.8932\n",
      "Epoch 196/1500\n",
      "48/48 [==============================] - 0s 818us/step - loss: 0.2463 - accuracy: 0.9027\n",
      "Epoch 197/1500\n",
      "48/48 [==============================] - 0s 847us/step - loss: 0.2499 - accuracy: 0.9037\n",
      "Epoch 198/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.2463 - accuracy: 0.8991\n",
      "Epoch 199/1500\n",
      "48/48 [==============================] - 0s 791us/step - loss: 0.2473 - accuracy: 0.9066\n",
      "Epoch 200/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.2503 - accuracy: 0.9004\n",
      "Epoch 201/1500\n",
      "48/48 [==============================] - 0s 800us/step - loss: 0.2382 - accuracy: 0.9092\n",
      "Epoch 202/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.2476 - accuracy: 0.9050\n",
      "Epoch 203/1500\n",
      "48/48 [==============================] - 0s 849us/step - loss: 0.2582 - accuracy: 0.9027\n",
      "Epoch 204/1500\n",
      "48/48 [==============================] - 0s 835us/step - loss: 0.2377 - accuracy: 0.9073\n",
      "Epoch 205/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.2345 - accuracy: 0.9069\n",
      "Epoch 206/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 0.2410 - accuracy: 0.9056\n",
      "Epoch 207/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 0.2398 - accuracy: 0.9043\n",
      "Epoch 208/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.2354 - accuracy: 0.9059\n",
      "Epoch 209/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 0.2345 - accuracy: 0.9164\n",
      "Epoch 210/1500\n",
      "48/48 [==============================] - 0s 829us/step - loss: 0.2415 - accuracy: 0.9033\n",
      "Epoch 211/1500\n",
      "48/48 [==============================] - 0s 744us/step - loss: 0.2289 - accuracy: 0.9144\n",
      "Epoch 212/1500\n",
      "48/48 [==============================] - 0s 878us/step - loss: 0.2228 - accuracy: 0.9108\n",
      "Epoch 213/1500\n",
      "48/48 [==============================] - 0s 909us/step - loss: 0.2403 - accuracy: 0.9050\n",
      "Epoch 214/1500\n",
      "48/48 [==============================] - 0s 898us/step - loss: 0.2329 - accuracy: 0.9059\n",
      "Epoch 215/1500\n",
      "48/48 [==============================] - 0s 968us/step - loss: 0.2256 - accuracy: 0.9131\n",
      "Epoch 216/1500\n",
      "48/48 [==============================] - 0s 947us/step - loss: 0.2351 - accuracy: 0.9069\n",
      "Epoch 217/1500\n",
      "48/48 [==============================] - 0s 932us/step - loss: 0.2221 - accuracy: 0.9108\n",
      "Epoch 218/1500\n",
      "48/48 [==============================] - 0s 840us/step - loss: 0.2283 - accuracy: 0.9089\n",
      "Epoch 219/1500\n",
      "48/48 [==============================] - 0s 765us/step - loss: 0.2305 - accuracy: 0.9115\n",
      "Epoch 220/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.2268 - accuracy: 0.9118\n",
      "Epoch 221/1500\n",
      "48/48 [==============================] - 0s 818us/step - loss: 0.2381 - accuracy: 0.9073\n",
      "Epoch 222/1500\n",
      "48/48 [==============================] - 0s 821us/step - loss: 0.2122 - accuracy: 0.9167\n",
      "Epoch 223/1500\n",
      "48/48 [==============================] - 0s 842us/step - loss: 0.2284 - accuracy: 0.9121\n",
      "Epoch 224/1500\n",
      "48/48 [==============================] - 0s 820us/step - loss: 0.2251 - accuracy: 0.9148\n",
      "Epoch 225/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 0.2201 - accuracy: 0.9135\n",
      "Epoch 226/1500\n",
      "48/48 [==============================] - 0s 744us/step - loss: 0.2271 - accuracy: 0.9141\n",
      "Epoch 227/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 0.2309 - accuracy: 0.9076\n",
      "Epoch 228/1500\n",
      "48/48 [==============================] - 0s 758us/step - loss: 0.2137 - accuracy: 0.9121\n",
      "Epoch 229/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.2400 - accuracy: 0.9053\n",
      "Epoch 230/1500\n",
      "48/48 [==============================] - 0s 804us/step - loss: 0.2201 - accuracy: 0.9164\n",
      "Epoch 231/1500\n",
      "48/48 [==============================] - 0s 845us/step - loss: 0.2224 - accuracy: 0.9138\n",
      "Epoch 232/1500\n",
      "48/48 [==============================] - 0s 857us/step - loss: 0.2145 - accuracy: 0.9144\n",
      "Epoch 233/1500\n",
      "48/48 [==============================] - 0s 792us/step - loss: 0.2164 - accuracy: 0.9118\n",
      "Epoch 234/1500\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.2225 - accuracy: 0.9092\n",
      "Epoch 235/1500\n",
      "48/48 [==============================] - 0s 849us/step - loss: 0.2212 - accuracy: 0.9164\n",
      "Epoch 236/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 0.2274 - accuracy: 0.9141\n",
      "Epoch 237/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.2382 - accuracy: 0.9092\n",
      "Epoch 238/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 0.2156 - accuracy: 0.9170\n",
      "Epoch 239/1500\n",
      "48/48 [==============================] - 0s 820us/step - loss: 0.2188 - accuracy: 0.9167\n",
      "Epoch 240/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.2065 - accuracy: 0.9193\n",
      "Epoch 241/1500\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.2175 - accuracy: 0.9177\n",
      "Epoch 242/1500\n",
      "48/48 [==============================] - 0s 808us/step - loss: 0.2252 - accuracy: 0.9157\n",
      "Epoch 243/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 0.2064 - accuracy: 0.9200\n",
      "Epoch 244/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 0.2149 - accuracy: 0.9148\n",
      "Epoch 245/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 0.2165 - accuracy: 0.9138\n",
      "Epoch 246/1500\n",
      "48/48 [==============================] - 0s 780us/step - loss: 0.2287 - accuracy: 0.9121\n",
      "Epoch 247/1500\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.2223 - accuracy: 0.9115\n",
      "Epoch 248/1500\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.2089 - accuracy: 0.9197\n",
      "Epoch 249/1500\n",
      "48/48 [==============================] - 0s 825us/step - loss: 0.2306 - accuracy: 0.9144\n",
      "Epoch 250/1500\n",
      "48/48 [==============================] - 0s 845us/step - loss: 0.2229 - accuracy: 0.9170\n",
      "Epoch 251/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.2167 - accuracy: 0.9170\n",
      "Epoch 252/1500\n",
      "48/48 [==============================] - 0s 818us/step - loss: 0.2177 - accuracy: 0.9164\n",
      "Epoch 253/1500\n",
      "48/48 [==============================] - 0s 821us/step - loss: 0.2134 - accuracy: 0.9144\n",
      "Epoch 254/1500\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.2289 - accuracy: 0.9079\n",
      "Epoch 255/1500\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.2082 - accuracy: 0.9226\n",
      "Epoch 256/1500\n",
      "48/48 [==============================] - 0s 822us/step - loss: 0.2198 - accuracy: 0.9121\n",
      "Epoch 257/1500\n",
      "48/48 [==============================] - 0s 837us/step - loss: 0.2090 - accuracy: 0.9197\n",
      "Epoch 258/1500\n",
      "48/48 [==============================] - 0s 874us/step - loss: 0.2139 - accuracy: 0.9157\n",
      "Epoch 259/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.2163 - accuracy: 0.9174\n",
      "Epoch 260/1500\n",
      "48/48 [==============================] - 0s 778us/step - loss: 0.2235 - accuracy: 0.9206\n",
      "Epoch 261/1500\n",
      "48/48 [==============================] - 0s 831us/step - loss: 0.2156 - accuracy: 0.9180\n",
      "Epoch 262/1500\n",
      "48/48 [==============================] - 0s 854us/step - loss: 0.2165 - accuracy: 0.9161\n",
      "Epoch 263/1500\n",
      "48/48 [==============================] - 0s 822us/step - loss: 0.2200 - accuracy: 0.9141\n",
      "Epoch 264/1500\n",
      "48/48 [==============================] - 0s 924us/step - loss: 0.2001 - accuracy: 0.9187\n",
      "Epoch 265/1500\n",
      "48/48 [==============================] - 0s 923us/step - loss: 0.2133 - accuracy: 0.9187\n",
      "Epoch 266/1500\n",
      "48/48 [==============================] - 0s 880us/step - loss: 0.2076 - accuracy: 0.9187\n",
      "Epoch 267/1500\n",
      "48/48 [==============================] - 0s 934us/step - loss: 0.2054 - accuracy: 0.9206\n",
      "Epoch 268/1500\n",
      "48/48 [==============================] - 0s 905us/step - loss: 0.2168 - accuracy: 0.9180\n",
      "Epoch 269/1500\n",
      "48/48 [==============================] - 0s 861us/step - loss: 0.2099 - accuracy: 0.9177\n",
      "Epoch 270/1500\n",
      "48/48 [==============================] - 0s 811us/step - loss: 0.2088 - accuracy: 0.9285\n",
      "Epoch 271/1500\n",
      "48/48 [==============================] - 0s 829us/step - loss: 0.2006 - accuracy: 0.9226\n",
      "Epoch 272/1500\n",
      "48/48 [==============================] - 0s 828us/step - loss: 0.2080 - accuracy: 0.9197\n",
      "Epoch 273/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.2010 - accuracy: 0.9236\n",
      "Epoch 274/1500\n",
      "48/48 [==============================] - 0s 859us/step - loss: 0.2145 - accuracy: 0.9105\n",
      "Epoch 275/1500\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.2136 - accuracy: 0.9206\n",
      "Epoch 276/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.2002 - accuracy: 0.9203\n",
      "Epoch 277/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.2159 - accuracy: 0.9177\n",
      "Epoch 278/1500\n",
      "48/48 [==============================] - 0s 808us/step - loss: 0.1978 - accuracy: 0.9236\n",
      "Epoch 279/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.2002 - accuracy: 0.9193\n",
      "Epoch 280/1500\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.2144 - accuracy: 0.9213\n",
      "Epoch 281/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.1996 - accuracy: 0.9239\n",
      "Epoch 282/1500\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.2037 - accuracy: 0.9219\n",
      "Epoch 283/1500\n",
      "48/48 [==============================] - 0s 817us/step - loss: 0.2086 - accuracy: 0.9154\n",
      "Epoch 284/1500\n",
      "48/48 [==============================] - 0s 830us/step - loss: 0.2045 - accuracy: 0.9200\n",
      "Epoch 285/1500\n",
      "48/48 [==============================] - 0s 827us/step - loss: 0.2012 - accuracy: 0.9203\n",
      "Epoch 286/1500\n",
      "48/48 [==============================] - 0s 815us/step - loss: 0.2021 - accuracy: 0.9233\n",
      "Epoch 287/1500\n",
      "48/48 [==============================] - 0s 792us/step - loss: 0.2060 - accuracy: 0.9249\n",
      "Epoch 288/1500\n",
      "48/48 [==============================] - 0s 872us/step - loss: 0.2115 - accuracy: 0.9161\n",
      "Epoch 289/1500\n",
      "48/48 [==============================] - 0s 976us/step - loss: 0.1986 - accuracy: 0.9216\n",
      "Epoch 290/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9265\n",
      "Epoch 291/1500\n",
      "48/48 [==============================] - 0s 932us/step - loss: 0.2087 - accuracy: 0.9213\n",
      "Epoch 292/1500\n",
      "48/48 [==============================] - 0s 870us/step - loss: 0.2050 - accuracy: 0.9200\n",
      "Epoch 293/1500\n",
      "48/48 [==============================] - 0s 768us/step - loss: 0.2056 - accuracy: 0.9226\n",
      "Epoch 294/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.1905 - accuracy: 0.9265\n",
      "Epoch 295/1500\n",
      "48/48 [==============================] - 0s 796us/step - loss: 0.1992 - accuracy: 0.9193\n",
      "Epoch 296/1500\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.1978 - accuracy: 0.9239\n",
      "Epoch 297/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.2063 - accuracy: 0.9213\n",
      "Epoch 298/1500\n",
      "48/48 [==============================] - 0s 830us/step - loss: 0.1842 - accuracy: 0.9275\n",
      "Epoch 299/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.1995 - accuracy: 0.9272\n",
      "Epoch 300/1500\n",
      "48/48 [==============================] - 0s 759us/step - loss: 0.1944 - accuracy: 0.9223\n",
      "Epoch 301/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.2012 - accuracy: 0.9242\n",
      "Epoch 302/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.2077 - accuracy: 0.9193\n",
      "Epoch 303/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 0.1950 - accuracy: 0.9262\n",
      "Epoch 304/1500\n",
      "48/48 [==============================] - 0s 768us/step - loss: 0.1970 - accuracy: 0.9246\n",
      "Epoch 305/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.2044 - accuracy: 0.9164\n",
      "Epoch 306/1500\n",
      "48/48 [==============================] - 0s 762us/step - loss: 0.1972 - accuracy: 0.9213\n",
      "Epoch 307/1500\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.1917 - accuracy: 0.9216\n",
      "Epoch 308/1500\n",
      "48/48 [==============================] - 0s 933us/step - loss: 0.1966 - accuracy: 0.9236\n",
      "Epoch 309/1500\n",
      "48/48 [==============================] - 0s 875us/step - loss: 0.2014 - accuracy: 0.9210\n",
      "Epoch 310/1500\n",
      "48/48 [==============================] - 0s 861us/step - loss: 0.1887 - accuracy: 0.9262\n",
      "Epoch 311/1500\n",
      "48/48 [==============================] - 0s 821us/step - loss: 0.2094 - accuracy: 0.9193\n",
      "Epoch 312/1500\n",
      "48/48 [==============================] - 0s 961us/step - loss: 0.1911 - accuracy: 0.9210\n",
      "Epoch 313/1500\n",
      "48/48 [==============================] - 0s 898us/step - loss: 0.1844 - accuracy: 0.9275\n",
      "Epoch 314/1500\n",
      "48/48 [==============================] - 0s 883us/step - loss: 0.2013 - accuracy: 0.9239\n",
      "Epoch 315/1500\n",
      "48/48 [==============================] - 0s 896us/step - loss: 0.1951 - accuracy: 0.9213\n",
      "Epoch 316/1500\n",
      "48/48 [==============================] - 0s 786us/step - loss: 0.1989 - accuracy: 0.9203\n",
      "Epoch 317/1500\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.1938 - accuracy: 0.9288\n",
      "Epoch 318/1500\n",
      "48/48 [==============================] - 0s 824us/step - loss: 0.1957 - accuracy: 0.9259\n",
      "Epoch 319/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.1990 - accuracy: 0.9190\n",
      "Epoch 320/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.1903 - accuracy: 0.9265\n",
      "Epoch 321/1500\n",
      "48/48 [==============================] - 0s 891us/step - loss: 0.1892 - accuracy: 0.9295\n",
      "Epoch 322/1500\n",
      "48/48 [==============================] - 0s 850us/step - loss: 0.2018 - accuracy: 0.9203\n",
      "Epoch 323/1500\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.1978 - accuracy: 0.9246\n",
      "Epoch 324/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.1764 - accuracy: 0.9314\n",
      "Epoch 325/1500\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.1932 - accuracy: 0.9229\n",
      "Epoch 326/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 0.1870 - accuracy: 0.9288\n",
      "Epoch 327/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.1913 - accuracy: 0.9275\n",
      "Epoch 328/1500\n",
      "48/48 [==============================] - 0s 840us/step - loss: 0.1895 - accuracy: 0.9233\n",
      "Epoch 329/1500\n",
      "48/48 [==============================] - 0s 872us/step - loss: 0.1920 - accuracy: 0.9210\n",
      "Epoch 330/1500\n",
      "48/48 [==============================] - 0s 892us/step - loss: 0.1972 - accuracy: 0.9226\n",
      "Epoch 331/1500\n",
      "48/48 [==============================] - 0s 823us/step - loss: 0.1951 - accuracy: 0.9229\n",
      "Epoch 332/1500\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.1878 - accuracy: 0.9291\n",
      "Epoch 333/1500\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.1987 - accuracy: 0.9255\n",
      "Epoch 334/1500\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.1943 - accuracy: 0.9200\n",
      "Epoch 335/1500\n",
      "48/48 [==============================] - 0s 760us/step - loss: 0.1864 - accuracy: 0.9301\n",
      "Epoch 336/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 0.2001 - accuracy: 0.9213\n",
      "Epoch 337/1500\n",
      "48/48 [==============================] - 0s 819us/step - loss: 0.1945 - accuracy: 0.9226\n",
      "Epoch 338/1500\n",
      "48/48 [==============================] - 0s 865us/step - loss: 0.1945 - accuracy: 0.9262\n",
      "Epoch 339/1500\n",
      "48/48 [==============================] - 0s 937us/step - loss: 0.1866 - accuracy: 0.9252\n",
      "Epoch 340/1500\n",
      "48/48 [==============================] - 0s 802us/step - loss: 0.1949 - accuracy: 0.9268\n",
      "Epoch 341/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 0.1880 - accuracy: 0.9282\n",
      "Epoch 342/1500\n",
      "48/48 [==============================] - 0s 790us/step - loss: 0.1935 - accuracy: 0.9262\n",
      "Epoch 343/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.1856 - accuracy: 0.9282\n",
      "Epoch 344/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.1896 - accuracy: 0.9311\n",
      "Epoch 345/1500\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.1901 - accuracy: 0.9272\n",
      "Epoch 346/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 0.1947 - accuracy: 0.9252\n",
      "Epoch 347/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 0.1967 - accuracy: 0.9233\n",
      "Epoch 348/1500\n",
      "48/48 [==============================] - 0s 803us/step - loss: 0.1893 - accuracy: 0.9282\n",
      "Epoch 349/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.1847 - accuracy: 0.9275\n",
      "Epoch 350/1500\n",
      "48/48 [==============================] - 0s 781us/step - loss: 0.1813 - accuracy: 0.9324\n",
      "Epoch 351/1500\n",
      "48/48 [==============================] - 0s 779us/step - loss: 0.1748 - accuracy: 0.9301\n",
      "Epoch 352/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 0.1773 - accuracy: 0.9331\n",
      "Epoch 353/1500\n",
      "48/48 [==============================] - 0s 760us/step - loss: 0.1931 - accuracy: 0.9236\n",
      "Epoch 354/1500\n",
      "48/48 [==============================] - 0s 764us/step - loss: 0.1857 - accuracy: 0.9295\n",
      "Epoch 355/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 0.1883 - accuracy: 0.9291\n",
      "Epoch 356/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.1908 - accuracy: 0.9229\n",
      "Epoch 357/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.1867 - accuracy: 0.9262\n",
      "Epoch 358/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 0.1727 - accuracy: 0.9353\n",
      "Epoch 359/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 0.1700 - accuracy: 0.9340\n",
      "Epoch 360/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 0.1828 - accuracy: 0.9288\n",
      "Epoch 361/1500\n",
      "48/48 [==============================] - 0s 765us/step - loss: 0.1867 - accuracy: 0.9334\n",
      "Epoch 362/1500\n",
      "48/48 [==============================] - 0s 765us/step - loss: 0.1876 - accuracy: 0.9255\n",
      "Epoch 363/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 0.1745 - accuracy: 0.9370\n",
      "Epoch 364/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 0.1767 - accuracy: 0.9337\n",
      "Epoch 365/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 0.1779 - accuracy: 0.9311\n",
      "Epoch 366/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.1830 - accuracy: 0.9304\n",
      "Epoch 367/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.9298\n",
      "Epoch 368/1500\n",
      "48/48 [==============================] - 0s 933us/step - loss: 0.1821 - accuracy: 0.9321\n",
      "Epoch 369/1500\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.1758 - accuracy: 0.9327\n",
      "Epoch 370/1500\n",
      "48/48 [==============================] - 0s 876us/step - loss: 0.1759 - accuracy: 0.9275\n",
      "Epoch 371/1500\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.1817 - accuracy: 0.9259\n",
      "Epoch 372/1500\n",
      "48/48 [==============================] - 0s 826us/step - loss: 0.1653 - accuracy: 0.9383\n",
      "Epoch 373/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 0.1692 - accuracy: 0.9396\n",
      "Epoch 374/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.1730 - accuracy: 0.9331\n",
      "Epoch 375/1500\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.1605 - accuracy: 0.9432\n",
      "Epoch 376/1500\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.1655 - accuracy: 0.9363\n",
      "Epoch 377/1500\n",
      "48/48 [==============================] - 0s 830us/step - loss: 0.1822 - accuracy: 0.9321\n",
      "Epoch 378/1500\n",
      "48/48 [==============================] - 0s 874us/step - loss: 0.1774 - accuracy: 0.9265\n",
      "Epoch 379/1500\n",
      "48/48 [==============================] - 0s 874us/step - loss: 0.1941 - accuracy: 0.9275\n",
      "Epoch 380/1500\n",
      "48/48 [==============================] - 0s 889us/step - loss: 0.1878 - accuracy: 0.9291\n",
      "Epoch 381/1500\n",
      "48/48 [==============================] - 0s 823us/step - loss: 0.1848 - accuracy: 0.9295\n",
      "Epoch 382/1500\n",
      "48/48 [==============================] - 0s 857us/step - loss: 0.1773 - accuracy: 0.9337\n",
      "Epoch 383/1500\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.1853 - accuracy: 0.9246\n",
      "Epoch 384/1500\n",
      "48/48 [==============================] - 0s 913us/step - loss: 0.1719 - accuracy: 0.9331\n",
      "Epoch 385/1500\n",
      "48/48 [==============================] - 0s 896us/step - loss: 0.1912 - accuracy: 0.9278\n",
      "Epoch 386/1500\n",
      "48/48 [==============================] - 0s 793us/step - loss: 0.1845 - accuracy: 0.9308\n",
      "Epoch 387/1500\n",
      "48/48 [==============================] - 0s 805us/step - loss: 0.1777 - accuracy: 0.9314\n",
      "Epoch 388/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.1891 - accuracy: 0.9210\n",
      "Epoch 389/1500\n",
      "48/48 [==============================] - 0s 815us/step - loss: 0.1610 - accuracy: 0.9376\n",
      "Epoch 390/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.1736 - accuracy: 0.9334\n",
      "Epoch 391/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.1783 - accuracy: 0.9331\n",
      "Epoch 392/1500\n",
      "48/48 [==============================] - 0s 777us/step - loss: 0.1817 - accuracy: 0.9317\n",
      "Epoch 393/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 0.1651 - accuracy: 0.9396\n",
      "Epoch 394/1500\n",
      "48/48 [==============================] - 0s 811us/step - loss: 0.1807 - accuracy: 0.9291\n",
      "Epoch 395/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.1847 - accuracy: 0.9262\n",
      "Epoch 396/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.1824 - accuracy: 0.9249\n",
      "Epoch 397/1500\n",
      "48/48 [==============================] - 0s 811us/step - loss: 0.1790 - accuracy: 0.9331\n",
      "Epoch 398/1500\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.1832 - accuracy: 0.9268\n",
      "Epoch 399/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.1728 - accuracy: 0.9314\n",
      "Epoch 400/1500\n",
      "48/48 [==============================] - 0s 843us/step - loss: 0.1844 - accuracy: 0.9229\n",
      "Epoch 401/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.1666 - accuracy: 0.9334\n",
      "Epoch 402/1500\n",
      "48/48 [==============================] - 0s 777us/step - loss: 0.1810 - accuracy: 0.9321\n",
      "Epoch 403/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.1623 - accuracy: 0.9396\n",
      "Epoch 404/1500\n",
      "48/48 [==============================] - 0s 775us/step - loss: 0.1745 - accuracy: 0.9331\n",
      "Epoch 405/1500\n",
      " 1/48 [..............................] - ETA: 0s - loss: 0.3000 - accuracy: 0.8906Restoring model weights from the end of the best epoch: 375.\n",
      "48/48 [==============================] - 0s 856us/step - loss: 0.1758 - accuracy: 0.9272\n",
      "Epoch 405: early stopping\n",
      "7/7 [==============================] - 0s 768us/step - loss: 0.9108 - accuracy: 0.7064\n",
      "7/7 [==============================] - 0s 899us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 0.9108279943466187, Accuracy: 0.7064220309257507, Precision: 0.6529863058901172, Recall: 0.6939826839826839, F1 Score: 0.6525848990547322\n",
      "Confusion Matrix:\n",
      " [[117   3  45]\n",
      " [  3  25   0]\n",
      " [ 12   1  12]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'007A'}\n",
      "Moved to Test Set:\n",
      "{'007A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '007A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "728\n",
      "Length of y_train_val:\n",
      "728\n",
      "Length of groups_train_val:\n",
      "728\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     463\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     125\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1125, 1: 944, 2: 881})\n",
      "Epoch 1/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 1.1204 - accuracy: 0.4993\n",
      "Epoch 2/1500\n",
      "47/47 [==============================] - 0s 933us/step - loss: 0.8865 - accuracy: 0.5936\n",
      "Epoch 3/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.8030 - accuracy: 0.6468\n",
      "Epoch 4/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.7752 - accuracy: 0.6542\n",
      "Epoch 5/1500\n",
      "47/47 [==============================] - 0s 796us/step - loss: 0.7469 - accuracy: 0.6793\n",
      "Epoch 6/1500\n",
      "47/47 [==============================] - 0s 815us/step - loss: 0.7177 - accuracy: 0.7020\n",
      "Epoch 7/1500\n",
      "47/47 [==============================] - 0s 832us/step - loss: 0.6849 - accuracy: 0.7027\n",
      "Epoch 8/1500\n",
      "47/47 [==============================] - 0s 835us/step - loss: 0.6767 - accuracy: 0.7102\n",
      "Epoch 9/1500\n",
      "47/47 [==============================] - 0s 818us/step - loss: 0.6654 - accuracy: 0.7102\n",
      "Epoch 10/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.6292 - accuracy: 0.7258\n",
      "Epoch 11/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.6176 - accuracy: 0.7305\n",
      "Epoch 12/1500\n",
      "47/47 [==============================] - 0s 805us/step - loss: 0.5969 - accuracy: 0.7434\n",
      "Epoch 13/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.5982 - accuracy: 0.7363\n",
      "Epoch 14/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.5880 - accuracy: 0.7478\n",
      "Epoch 15/1500\n",
      "47/47 [==============================] - 0s 774us/step - loss: 0.5847 - accuracy: 0.7441\n",
      "Epoch 16/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.5741 - accuracy: 0.7563\n",
      "Epoch 17/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.5481 - accuracy: 0.7634\n",
      "Epoch 18/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.5427 - accuracy: 0.7675\n",
      "Epoch 19/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.5484 - accuracy: 0.7614\n",
      "Epoch 20/1500\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5387 - accuracy: 0.7753\n",
      "Epoch 21/1500\n",
      "47/47 [==============================] - 0s 750us/step - loss: 0.5279 - accuracy: 0.7756\n",
      "Epoch 22/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.5301 - accuracy: 0.7817\n",
      "Epoch 23/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.5188 - accuracy: 0.7868\n",
      "Epoch 24/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.5081 - accuracy: 0.7831\n",
      "Epoch 25/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.4860 - accuracy: 0.7915\n",
      "Epoch 26/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.4966 - accuracy: 0.7915\n",
      "Epoch 27/1500\n",
      "47/47 [==============================] - 0s 732us/step - loss: 0.4828 - accuracy: 0.7969\n",
      "Epoch 28/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.4893 - accuracy: 0.7956\n",
      "Epoch 29/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.4811 - accuracy: 0.7871\n",
      "Epoch 30/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.4771 - accuracy: 0.7959\n",
      "Epoch 31/1500\n",
      "47/47 [==============================] - 0s 724us/step - loss: 0.4783 - accuracy: 0.7953\n",
      "Epoch 32/1500\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.4600 - accuracy: 0.8061\n",
      "Epoch 33/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.4761 - accuracy: 0.7885\n",
      "Epoch 34/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.4597 - accuracy: 0.8088\n",
      "Epoch 35/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.4506 - accuracy: 0.8078\n",
      "Epoch 36/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.4395 - accuracy: 0.8088\n",
      "Epoch 37/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.4470 - accuracy: 0.8088\n",
      "Epoch 38/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.4462 - accuracy: 0.8075\n",
      "Epoch 39/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.4307 - accuracy: 0.8224\n",
      "Epoch 40/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.4436 - accuracy: 0.8105\n",
      "Epoch 41/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.4352 - accuracy: 0.8149\n",
      "Epoch 42/1500\n",
      "47/47 [==============================] - 0s 824us/step - loss: 0.4487 - accuracy: 0.8068\n",
      "Epoch 43/1500\n",
      "47/47 [==============================] - 0s 850us/step - loss: 0.4212 - accuracy: 0.8258\n",
      "Epoch 44/1500\n",
      "47/47 [==============================] - 0s 804us/step - loss: 0.4169 - accuracy: 0.8234\n",
      "Epoch 45/1500\n",
      "47/47 [==============================] - 0s 815us/step - loss: 0.4286 - accuracy: 0.8258\n",
      "Epoch 46/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.4087 - accuracy: 0.8264\n",
      "Epoch 47/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.4166 - accuracy: 0.8264\n",
      "Epoch 48/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.4075 - accuracy: 0.8234\n",
      "Epoch 49/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.4106 - accuracy: 0.8251\n",
      "Epoch 50/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.4059 - accuracy: 0.8339\n",
      "Epoch 51/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.3987 - accuracy: 0.8339\n",
      "Epoch 52/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.4097 - accuracy: 0.8339\n",
      "Epoch 53/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.4054 - accuracy: 0.8251\n",
      "Epoch 54/1500\n",
      "47/47 [==============================] - 0s 815us/step - loss: 0.3858 - accuracy: 0.8359\n",
      "Epoch 55/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.4005 - accuracy: 0.8312\n",
      "Epoch 56/1500\n",
      "47/47 [==============================] - 0s 853us/step - loss: 0.3904 - accuracy: 0.8339\n",
      "Epoch 57/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.3854 - accuracy: 0.8417\n",
      "Epoch 58/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.3857 - accuracy: 0.8427\n",
      "Epoch 59/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.3917 - accuracy: 0.8441\n",
      "Epoch 60/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.3970 - accuracy: 0.8356\n",
      "Epoch 61/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.3790 - accuracy: 0.8502\n",
      "Epoch 62/1500\n",
      "47/47 [==============================] - 0s 820us/step - loss: 0.3754 - accuracy: 0.8444\n",
      "Epoch 63/1500\n",
      "47/47 [==============================] - 0s 817us/step - loss: 0.3790 - accuracy: 0.8420\n",
      "Epoch 64/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.3769 - accuracy: 0.8407\n",
      "Epoch 65/1500\n",
      "47/47 [==============================] - 0s 865us/step - loss: 0.3728 - accuracy: 0.8441\n",
      "Epoch 66/1500\n",
      "47/47 [==============================] - 0s 856us/step - loss: 0.3714 - accuracy: 0.8481\n",
      "Epoch 67/1500\n",
      "47/47 [==============================] - 0s 882us/step - loss: 0.3705 - accuracy: 0.8451\n",
      "Epoch 68/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.3665 - accuracy: 0.8481\n",
      "Epoch 69/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.3753 - accuracy: 0.8468\n",
      "Epoch 70/1500\n",
      "47/47 [==============================] - 0s 827us/step - loss: 0.3640 - accuracy: 0.8492\n",
      "Epoch 71/1500\n",
      "47/47 [==============================] - 0s 804us/step - loss: 0.3694 - accuracy: 0.8478\n",
      "Epoch 72/1500\n",
      "47/47 [==============================] - 0s 831us/step - loss: 0.3626 - accuracy: 0.8458\n",
      "Epoch 73/1500\n",
      "47/47 [==============================] - 0s 849us/step - loss: 0.3606 - accuracy: 0.8519\n",
      "Epoch 74/1500\n",
      "47/47 [==============================] - 0s 854us/step - loss: 0.3728 - accuracy: 0.8451\n",
      "Epoch 75/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.3683 - accuracy: 0.8505\n",
      "Epoch 76/1500\n",
      "47/47 [==============================] - 0s 839us/step - loss: 0.3537 - accuracy: 0.8549\n",
      "Epoch 77/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.3615 - accuracy: 0.8505\n",
      "Epoch 78/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.3458 - accuracy: 0.8553\n",
      "Epoch 79/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.3563 - accuracy: 0.8546\n",
      "Epoch 80/1500\n",
      "47/47 [==============================] - 0s 849us/step - loss: 0.3443 - accuracy: 0.8566\n",
      "Epoch 81/1500\n",
      "47/47 [==============================] - 0s 781us/step - loss: 0.3567 - accuracy: 0.8546\n",
      "Epoch 82/1500\n",
      "47/47 [==============================] - 0s 784us/step - loss: 0.3563 - accuracy: 0.8569\n",
      "Epoch 83/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.3461 - accuracy: 0.8563\n",
      "Epoch 84/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.3401 - accuracy: 0.8617\n",
      "Epoch 85/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.3498 - accuracy: 0.8566\n",
      "Epoch 86/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.3423 - accuracy: 0.8529\n",
      "Epoch 87/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.3518 - accuracy: 0.8508\n",
      "Epoch 88/1500\n",
      "47/47 [==============================] - 0s 843us/step - loss: 0.3294 - accuracy: 0.8607\n",
      "Epoch 89/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.3414 - accuracy: 0.8620\n",
      "Epoch 90/1500\n",
      "47/47 [==============================] - 0s 820us/step - loss: 0.3439 - accuracy: 0.8597\n",
      "Epoch 91/1500\n",
      "47/47 [==============================] - 0s 848us/step - loss: 0.3430 - accuracy: 0.8627\n",
      "Epoch 92/1500\n",
      "47/47 [==============================] - 0s 819us/step - loss: 0.3294 - accuracy: 0.8637\n",
      "Epoch 93/1500\n",
      "47/47 [==============================] - 0s 831us/step - loss: 0.3204 - accuracy: 0.8664\n",
      "Epoch 94/1500\n",
      "47/47 [==============================] - 0s 866us/step - loss: 0.3495 - accuracy: 0.8556\n",
      "Epoch 95/1500\n",
      "47/47 [==============================] - 0s 866us/step - loss: 0.3498 - accuracy: 0.8559\n",
      "Epoch 96/1500\n",
      "47/47 [==============================] - 0s 812us/step - loss: 0.3419 - accuracy: 0.8600\n",
      "Epoch 97/1500\n",
      "47/47 [==============================] - 0s 804us/step - loss: 0.3327 - accuracy: 0.8637\n",
      "Epoch 98/1500\n",
      "47/47 [==============================] - 0s 840us/step - loss: 0.3389 - accuracy: 0.8580\n",
      "Epoch 99/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.3232 - accuracy: 0.8715\n",
      "Epoch 100/1500\n",
      "47/47 [==============================] - 0s 854us/step - loss: 0.3259 - accuracy: 0.8719\n",
      "Epoch 101/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.3254 - accuracy: 0.8715\n",
      "Epoch 102/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.3300 - accuracy: 0.8678\n",
      "Epoch 103/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.3323 - accuracy: 0.8671\n",
      "Epoch 104/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.3141 - accuracy: 0.8725\n",
      "Epoch 105/1500\n",
      "47/47 [==============================] - 0s 798us/step - loss: 0.3097 - accuracy: 0.8722\n",
      "Epoch 106/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.3237 - accuracy: 0.8729\n",
      "Epoch 107/1500\n",
      "47/47 [==============================] - 0s 886us/step - loss: 0.3284 - accuracy: 0.8644\n",
      "Epoch 108/1500\n",
      "47/47 [==============================] - 0s 855us/step - loss: 0.3090 - accuracy: 0.8722\n",
      "Epoch 109/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.3079 - accuracy: 0.8695\n",
      "Epoch 110/1500\n",
      "47/47 [==============================] - 0s 819us/step - loss: 0.3176 - accuracy: 0.8658\n",
      "Epoch 111/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.3095 - accuracy: 0.8742\n",
      "Epoch 112/1500\n",
      "47/47 [==============================] - 0s 863us/step - loss: 0.3020 - accuracy: 0.8800\n",
      "Epoch 113/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.3118 - accuracy: 0.8736\n",
      "Epoch 114/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.2949 - accuracy: 0.8851\n",
      "Epoch 115/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.3167 - accuracy: 0.8627\n",
      "Epoch 116/1500\n",
      "47/47 [==============================] - 0s 868us/step - loss: 0.3062 - accuracy: 0.8824\n",
      "Epoch 117/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.3008 - accuracy: 0.8756\n",
      "Epoch 118/1500\n",
      "47/47 [==============================] - 0s 830us/step - loss: 0.3061 - accuracy: 0.8810\n",
      "Epoch 119/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8729\n",
      "Epoch 120/1500\n",
      "47/47 [==============================] - 0s 858us/step - loss: 0.3163 - accuracy: 0.8719\n",
      "Epoch 121/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.3104 - accuracy: 0.8810\n",
      "Epoch 122/1500\n",
      "47/47 [==============================] - 0s 794us/step - loss: 0.3258 - accuracy: 0.8708\n",
      "Epoch 123/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.3073 - accuracy: 0.8742\n",
      "Epoch 124/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.3018 - accuracy: 0.8769\n",
      "Epoch 125/1500\n",
      "47/47 [==============================] - 0s 740us/step - loss: 0.2941 - accuracy: 0.8766\n",
      "Epoch 126/1500\n",
      "47/47 [==============================] - 0s 717us/step - loss: 0.2953 - accuracy: 0.8810\n",
      "Epoch 127/1500\n",
      "47/47 [==============================] - 0s 765us/step - loss: 0.2780 - accuracy: 0.8936\n",
      "Epoch 128/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.3126 - accuracy: 0.8715\n",
      "Epoch 129/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2889 - accuracy: 0.8847\n",
      "Epoch 130/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.3039 - accuracy: 0.8776\n",
      "Epoch 131/1500\n",
      "47/47 [==============================] - 0s 818us/step - loss: 0.2985 - accuracy: 0.8868\n",
      "Epoch 132/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.2998 - accuracy: 0.8800\n",
      "Epoch 133/1500\n",
      "47/47 [==============================] - 0s 751us/step - loss: 0.2862 - accuracy: 0.8878\n",
      "Epoch 134/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2912 - accuracy: 0.8797\n",
      "Epoch 135/1500\n",
      "47/47 [==============================] - 0s 759us/step - loss: 0.3048 - accuracy: 0.8749\n",
      "Epoch 136/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2858 - accuracy: 0.8847\n",
      "Epoch 137/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2807 - accuracy: 0.8892\n",
      "Epoch 138/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.2945 - accuracy: 0.8881\n",
      "Epoch 139/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.3002 - accuracy: 0.8773\n",
      "Epoch 140/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.3026 - accuracy: 0.8851\n",
      "Epoch 141/1500\n",
      "47/47 [==============================] - 0s 722us/step - loss: 0.2814 - accuracy: 0.8888\n",
      "Epoch 142/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2876 - accuracy: 0.8847\n",
      "Epoch 143/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2796 - accuracy: 0.8925\n",
      "Epoch 144/1500\n",
      "47/47 [==============================] - 0s 725us/step - loss: 0.2829 - accuracy: 0.8847\n",
      "Epoch 145/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2909 - accuracy: 0.8851\n",
      "Epoch 146/1500\n",
      "47/47 [==============================] - 0s 734us/step - loss: 0.2735 - accuracy: 0.8888\n",
      "Epoch 147/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.2836 - accuracy: 0.8908\n",
      "Epoch 148/1500\n",
      "47/47 [==============================] - 0s 886us/step - loss: 0.2810 - accuracy: 0.8831\n",
      "Epoch 149/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.2678 - accuracy: 0.8919\n",
      "Epoch 150/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.2885 - accuracy: 0.8847\n",
      "Epoch 151/1500\n",
      "47/47 [==============================] - 0s 861us/step - loss: 0.2902 - accuracy: 0.8793\n",
      "Epoch 152/1500\n",
      "47/47 [==============================] - 0s 847us/step - loss: 0.2722 - accuracy: 0.8895\n",
      "Epoch 153/1500\n",
      "47/47 [==============================] - 0s 843us/step - loss: 0.2697 - accuracy: 0.8912\n",
      "Epoch 154/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.2962 - accuracy: 0.8834\n",
      "Epoch 155/1500\n",
      "47/47 [==============================] - 0s 826us/step - loss: 0.2679 - accuracy: 0.8953\n",
      "Epoch 156/1500\n",
      "47/47 [==============================] - 0s 819us/step - loss: 0.2796 - accuracy: 0.8908\n",
      "Epoch 157/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.2690 - accuracy: 0.8875\n",
      "Epoch 158/1500\n",
      "47/47 [==============================] - 0s 866us/step - loss: 0.2661 - accuracy: 0.8976\n",
      "Epoch 159/1500\n",
      "47/47 [==============================] - 0s 811us/step - loss: 0.2762 - accuracy: 0.8946\n",
      "Epoch 160/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.2699 - accuracy: 0.8885\n",
      "Epoch 161/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.2590 - accuracy: 0.9044\n",
      "Epoch 162/1500\n",
      "47/47 [==============================] - 0s 816us/step - loss: 0.2847 - accuracy: 0.8824\n",
      "Epoch 163/1500\n",
      "47/47 [==============================] - 0s 851us/step - loss: 0.2709 - accuracy: 0.8949\n",
      "Epoch 164/1500\n",
      "47/47 [==============================] - 0s 771us/step - loss: 0.2702 - accuracy: 0.8844\n",
      "Epoch 165/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2698 - accuracy: 0.8939\n",
      "Epoch 166/1500\n",
      "47/47 [==============================] - 0s 824us/step - loss: 0.2697 - accuracy: 0.8902\n",
      "Epoch 167/1500\n",
      "47/47 [==============================] - 0s 796us/step - loss: 0.2585 - accuracy: 0.8980\n",
      "Epoch 168/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.2680 - accuracy: 0.8983\n",
      "Epoch 169/1500\n",
      "47/47 [==============================] - 0s 853us/step - loss: 0.2631 - accuracy: 0.8986\n",
      "Epoch 170/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2715 - accuracy: 0.8844\n",
      "Epoch 171/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2556 - accuracy: 0.9031\n",
      "Epoch 172/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.2729 - accuracy: 0.8922\n",
      "Epoch 173/1500\n",
      "47/47 [==============================] - 0s 832us/step - loss: 0.2691 - accuracy: 0.8895\n",
      "Epoch 174/1500\n",
      "47/47 [==============================] - 0s 845us/step - loss: 0.2597 - accuracy: 0.9064\n",
      "Epoch 175/1500\n",
      "47/47 [==============================] - 0s 841us/step - loss: 0.2742 - accuracy: 0.8902\n",
      "Epoch 176/1500\n",
      "47/47 [==============================] - 0s 861us/step - loss: 0.2642 - accuracy: 0.8959\n",
      "Epoch 177/1500\n",
      "47/47 [==============================] - 0s 813us/step - loss: 0.2606 - accuracy: 0.8966\n",
      "Epoch 178/1500\n",
      "47/47 [==============================] - 0s 830us/step - loss: 0.2628 - accuracy: 0.8959\n",
      "Epoch 179/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2602 - accuracy: 0.8919\n",
      "Epoch 180/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.2611 - accuracy: 0.8990\n",
      "Epoch 181/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.2490 - accuracy: 0.9017\n",
      "Epoch 182/1500\n",
      "47/47 [==============================] - 0s 839us/step - loss: 0.2504 - accuracy: 0.8959\n",
      "Epoch 183/1500\n",
      "47/47 [==============================] - 0s 816us/step - loss: 0.2761 - accuracy: 0.8885\n",
      "Epoch 184/1500\n",
      "47/47 [==============================] - 0s 816us/step - loss: 0.2559 - accuracy: 0.9014\n",
      "Epoch 185/1500\n",
      "47/47 [==============================] - 0s 840us/step - loss: 0.2655 - accuracy: 0.8908\n",
      "Epoch 186/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.2510 - accuracy: 0.9010\n",
      "Epoch 187/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.2540 - accuracy: 0.9003\n",
      "Epoch 188/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2709 - accuracy: 0.8861\n",
      "Epoch 189/1500\n",
      "47/47 [==============================] - 0s 786us/step - loss: 0.2545 - accuracy: 0.9037\n",
      "Epoch 190/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.2433 - accuracy: 0.9031\n",
      "Epoch 191/1500\n",
      "47/47 [==============================] - 0s 802us/step - loss: 0.2811 - accuracy: 0.8831\n",
      "Epoch 192/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.2532 - accuracy: 0.8973\n",
      "Epoch 193/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.2529 - accuracy: 0.9017\n",
      "Epoch 194/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2426 - accuracy: 0.9044\n",
      "Epoch 195/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.2532 - accuracy: 0.8990\n",
      "Epoch 196/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2511 - accuracy: 0.9034\n",
      "Epoch 197/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2425 - accuracy: 0.9081\n",
      "Epoch 198/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2404 - accuracy: 0.9071\n",
      "Epoch 199/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2482 - accuracy: 0.8976\n",
      "Epoch 200/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.2357 - accuracy: 0.9044\n",
      "Epoch 201/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2518 - accuracy: 0.8973\n",
      "Epoch 202/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2584 - accuracy: 0.8983\n",
      "Epoch 203/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.2425 - accuracy: 0.9003\n",
      "Epoch 204/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2442 - accuracy: 0.9058\n",
      "Epoch 205/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.2301 - accuracy: 0.9092\n",
      "Epoch 206/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.2291 - accuracy: 0.9078\n",
      "Epoch 207/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.2366 - accuracy: 0.9105\n",
      "Epoch 208/1500\n",
      "47/47 [==============================] - 0s 728us/step - loss: 0.2434 - accuracy: 0.9007\n",
      "Epoch 209/1500\n",
      "47/47 [==============================] - 0s 715us/step - loss: 0.2393 - accuracy: 0.9061\n",
      "Epoch 210/1500\n",
      "47/47 [==============================] - 0s 714us/step - loss: 0.2430 - accuracy: 0.9020\n",
      "Epoch 211/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2410 - accuracy: 0.8997\n",
      "Epoch 212/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2217 - accuracy: 0.9149\n",
      "Epoch 213/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.2271 - accuracy: 0.9095\n",
      "Epoch 214/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2366 - accuracy: 0.9098\n",
      "Epoch 215/1500\n",
      "47/47 [==============================] - 0s 726us/step - loss: 0.2388 - accuracy: 0.9088\n",
      "Epoch 216/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2567 - accuracy: 0.8990\n",
      "Epoch 217/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.2325 - accuracy: 0.9088\n",
      "Epoch 218/1500\n",
      "47/47 [==============================] - 0s 733us/step - loss: 0.2418 - accuracy: 0.9071\n",
      "Epoch 219/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.2396 - accuracy: 0.9044\n",
      "Epoch 220/1500\n",
      "47/47 [==============================] - 0s 889us/step - loss: 0.2330 - accuracy: 0.9108\n",
      "Epoch 221/1500\n",
      "47/47 [==============================] - 0s 820us/step - loss: 0.2239 - accuracy: 0.9122\n",
      "Epoch 222/1500\n",
      "47/47 [==============================] - 0s 884us/step - loss: 0.2440 - accuracy: 0.9024\n",
      "Epoch 223/1500\n",
      "47/47 [==============================] - 0s 851us/step - loss: 0.2345 - accuracy: 0.9132\n",
      "Epoch 224/1500\n",
      "47/47 [==============================] - 0s 817us/step - loss: 0.2408 - accuracy: 0.9020\n",
      "Epoch 225/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.2332 - accuracy: 0.9047\n",
      "Epoch 226/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2334 - accuracy: 0.9112\n",
      "Epoch 227/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2288 - accuracy: 0.9041\n",
      "Epoch 228/1500\n",
      "47/47 [==============================] - 0s 775us/step - loss: 0.2351 - accuracy: 0.8973\n",
      "Epoch 229/1500\n",
      "47/47 [==============================] - 0s 828us/step - loss: 0.2203 - accuracy: 0.9108\n",
      "Epoch 230/1500\n",
      "47/47 [==============================] - 0s 856us/step - loss: 0.2276 - accuracy: 0.9136\n",
      "Epoch 231/1500\n",
      "47/47 [==============================] - 0s 849us/step - loss: 0.2218 - accuracy: 0.9159\n",
      "Epoch 232/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2307 - accuracy: 0.9078\n",
      "Epoch 233/1500\n",
      "47/47 [==============================] - 0s 816us/step - loss: 0.2419 - accuracy: 0.9047\n",
      "Epoch 234/1500\n",
      "47/47 [==============================] - 0s 808us/step - loss: 0.2365 - accuracy: 0.9054\n",
      "Epoch 235/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.2317 - accuracy: 0.9108\n",
      "Epoch 236/1500\n",
      "47/47 [==============================] - 0s 823us/step - loss: 0.2111 - accuracy: 0.9173\n",
      "Epoch 237/1500\n",
      "47/47 [==============================] - 0s 803us/step - loss: 0.2303 - accuracy: 0.9081\n",
      "Epoch 238/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2315 - accuracy: 0.9112\n",
      "Epoch 239/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2453 - accuracy: 0.9068\n",
      "Epoch 240/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.2265 - accuracy: 0.9044\n",
      "Epoch 241/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2233 - accuracy: 0.9139\n",
      "Epoch 242/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.2254 - accuracy: 0.9092\n",
      "Epoch 243/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.2234 - accuracy: 0.9095\n",
      "Epoch 244/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2195 - accuracy: 0.9142\n",
      "Epoch 245/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9068\n",
      "Epoch 246/1500\n",
      "47/47 [==============================] - 0s 960us/step - loss: 0.2278 - accuracy: 0.9119\n",
      "Epoch 247/1500\n",
      "47/47 [==============================] - 0s 871us/step - loss: 0.2230 - accuracy: 0.9092\n",
      "Epoch 248/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.2256 - accuracy: 0.9105\n",
      "Epoch 249/1500\n",
      "47/47 [==============================] - 0s 823us/step - loss: 0.2284 - accuracy: 0.9068\n",
      "Epoch 250/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.2400 - accuracy: 0.9105\n",
      "Epoch 251/1500\n",
      "47/47 [==============================] - 0s 784us/step - loss: 0.2278 - accuracy: 0.9136\n",
      "Epoch 252/1500\n",
      "47/47 [==============================] - 0s 783us/step - loss: 0.2293 - accuracy: 0.9112\n",
      "Epoch 253/1500\n",
      "47/47 [==============================] - 0s 838us/step - loss: 0.2122 - accuracy: 0.9129\n",
      "Epoch 254/1500\n",
      "47/47 [==============================] - 0s 840us/step - loss: 0.2200 - accuracy: 0.9115\n",
      "Epoch 255/1500\n",
      "47/47 [==============================] - 0s 822us/step - loss: 0.2144 - accuracy: 0.9149\n",
      "Epoch 256/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2122 - accuracy: 0.9163\n",
      "Epoch 257/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.2228 - accuracy: 0.9102\n",
      "Epoch 258/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.2225 - accuracy: 0.9112\n",
      "Epoch 259/1500\n",
      "47/47 [==============================] - 0s 812us/step - loss: 0.2090 - accuracy: 0.9197\n",
      "Epoch 260/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.2202 - accuracy: 0.9119\n",
      "Epoch 261/1500\n",
      "47/47 [==============================] - 0s 810us/step - loss: 0.2242 - accuracy: 0.9163\n",
      "Epoch 262/1500\n",
      "47/47 [==============================] - 0s 764us/step - loss: 0.2126 - accuracy: 0.9156\n",
      "Epoch 263/1500\n",
      "47/47 [==============================] - 0s 792us/step - loss: 0.2207 - accuracy: 0.9136\n",
      "Epoch 264/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.2184 - accuracy: 0.9139\n",
      "Epoch 265/1500\n",
      "47/47 [==============================] - 0s 813us/step - loss: 0.2084 - accuracy: 0.9166\n",
      "Epoch 266/1500\n",
      "47/47 [==============================] - 0s 813us/step - loss: 0.2051 - accuracy: 0.9142\n",
      "Epoch 267/1500\n",
      "47/47 [==============================] - 0s 777us/step - loss: 0.2122 - accuracy: 0.9163\n",
      "Epoch 268/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.2152 - accuracy: 0.9136\n",
      "Epoch 269/1500\n",
      "47/47 [==============================] - 0s 760us/step - loss: 0.2138 - accuracy: 0.9166\n",
      "Epoch 270/1500\n",
      "47/47 [==============================] - 0s 791us/step - loss: 0.2151 - accuracy: 0.9125\n",
      "Epoch 271/1500\n",
      "47/47 [==============================] - 0s 741us/step - loss: 0.2209 - accuracy: 0.9136\n",
      "Epoch 272/1500\n",
      "47/47 [==============================] - 0s 736us/step - loss: 0.2069 - accuracy: 0.9197\n",
      "Epoch 273/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.2073 - accuracy: 0.9241\n",
      "Epoch 274/1500\n",
      "47/47 [==============================] - 0s 753us/step - loss: 0.2018 - accuracy: 0.9217\n",
      "Epoch 275/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2064 - accuracy: 0.9193\n",
      "Epoch 276/1500\n",
      "47/47 [==============================] - 0s 811us/step - loss: 0.2133 - accuracy: 0.9149\n",
      "Epoch 277/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2082 - accuracy: 0.9193\n",
      "Epoch 278/1500\n",
      "47/47 [==============================] - 0s 745us/step - loss: 0.1988 - accuracy: 0.9197\n",
      "Epoch 279/1500\n",
      "47/47 [==============================] - 0s 727us/step - loss: 0.2143 - accuracy: 0.9115\n",
      "Epoch 280/1500\n",
      "47/47 [==============================] - 0s 776us/step - loss: 0.2127 - accuracy: 0.9176\n",
      "Epoch 281/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.2100 - accuracy: 0.9169\n",
      "Epoch 282/1500\n",
      "47/47 [==============================] - 0s 796us/step - loss: 0.1982 - accuracy: 0.9251\n",
      "Epoch 283/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.2158 - accuracy: 0.9136\n",
      "Epoch 284/1500\n",
      "47/47 [==============================] - 0s 724us/step - loss: 0.2155 - accuracy: 0.9163\n",
      "Epoch 285/1500\n",
      "47/47 [==============================] - 0s 729us/step - loss: 0.2072 - accuracy: 0.9186\n",
      "Epoch 286/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.2085 - accuracy: 0.9214\n",
      "Epoch 287/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.2130 - accuracy: 0.9220\n",
      "Epoch 288/1500\n",
      "47/47 [==============================] - 0s 730us/step - loss: 0.2197 - accuracy: 0.9169\n",
      "Epoch 289/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.2190 - accuracy: 0.9132\n",
      "Epoch 290/1500\n",
      "47/47 [==============================] - 0s 768us/step - loss: 0.1992 - accuracy: 0.9254\n",
      "Epoch 291/1500\n",
      "47/47 [==============================] - 0s 789us/step - loss: 0.1958 - accuracy: 0.9278\n",
      "Epoch 292/1500\n",
      "47/47 [==============================] - 0s 784us/step - loss: 0.2065 - accuracy: 0.9176\n",
      "Epoch 293/1500\n",
      "47/47 [==============================] - 0s 855us/step - loss: 0.2184 - accuracy: 0.9180\n",
      "Epoch 294/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.2171 - accuracy: 0.9122\n",
      "Epoch 295/1500\n",
      "47/47 [==============================] - 0s 843us/step - loss: 0.1991 - accuracy: 0.9224\n",
      "Epoch 296/1500\n",
      "47/47 [==============================] - 0s 801us/step - loss: 0.2030 - accuracy: 0.9234\n",
      "Epoch 297/1500\n",
      "47/47 [==============================] - 0s 779us/step - loss: 0.1934 - accuracy: 0.9244\n",
      "Epoch 298/1500\n",
      "47/47 [==============================] - 0s 773us/step - loss: 0.2040 - accuracy: 0.9241\n",
      "Epoch 299/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.2015 - accuracy: 0.9254\n",
      "Epoch 300/1500\n",
      "47/47 [==============================] - 0s 815us/step - loss: 0.2014 - accuracy: 0.9193\n",
      "Epoch 301/1500\n",
      "47/47 [==============================] - 0s 790us/step - loss: 0.2020 - accuracy: 0.9193\n",
      "Epoch 302/1500\n",
      "47/47 [==============================] - 0s 785us/step - loss: 0.1889 - accuracy: 0.9302\n",
      "Epoch 303/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.1927 - accuracy: 0.9271\n",
      "Epoch 304/1500\n",
      "47/47 [==============================] - 0s 767us/step - loss: 0.2047 - accuracy: 0.9254\n",
      "Epoch 305/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.1975 - accuracy: 0.9258\n",
      "Epoch 306/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.2069 - accuracy: 0.9186\n",
      "Epoch 307/1500\n",
      "47/47 [==============================] - 0s 807us/step - loss: 0.1989 - accuracy: 0.9210\n",
      "Epoch 308/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.1991 - accuracy: 0.9214\n",
      "Epoch 309/1500\n",
      "47/47 [==============================] - 0s 766us/step - loss: 0.2223 - accuracy: 0.9108\n",
      "Epoch 310/1500\n",
      "47/47 [==============================] - 0s 738us/step - loss: 0.2028 - accuracy: 0.9203\n",
      "Epoch 311/1500\n",
      "47/47 [==============================] - 0s 758us/step - loss: 0.2146 - accuracy: 0.9173\n",
      "Epoch 312/1500\n",
      "47/47 [==============================] - 0s 772us/step - loss: 0.1892 - accuracy: 0.9224\n",
      "Epoch 313/1500\n",
      "47/47 [==============================] - 0s 763us/step - loss: 0.2005 - accuracy: 0.9217\n",
      "Epoch 314/1500\n",
      "47/47 [==============================] - 0s 752us/step - loss: 0.1928 - accuracy: 0.9302\n",
      "Epoch 315/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2020 - accuracy: 0.9207\n",
      "Epoch 316/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.2002 - accuracy: 0.9190\n",
      "Epoch 317/1500\n",
      "47/47 [==============================] - 0s 761us/step - loss: 0.1931 - accuracy: 0.9231\n",
      "Epoch 318/1500\n",
      "47/47 [==============================] - 0s 754us/step - loss: 0.2004 - accuracy: 0.9254\n",
      "Epoch 319/1500\n",
      "47/47 [==============================] - 0s 783us/step - loss: 0.1952 - accuracy: 0.9264\n",
      "Epoch 320/1500\n",
      "47/47 [==============================] - 0s 780us/step - loss: 0.1938 - accuracy: 0.9278\n",
      "Epoch 321/1500\n",
      "47/47 [==============================] - 0s 797us/step - loss: 0.1922 - accuracy: 0.9285\n",
      "Epoch 322/1500\n",
      "47/47 [==============================] - 0s 806us/step - loss: 0.1999 - accuracy: 0.9254\n",
      "Epoch 323/1500\n",
      "47/47 [==============================] - 0s 893us/step - loss: 0.1957 - accuracy: 0.9166\n",
      "Epoch 324/1500\n",
      "47/47 [==============================] - 0s 889us/step - loss: 0.1960 - accuracy: 0.9210\n",
      "Epoch 325/1500\n",
      "47/47 [==============================] - 0s 788us/step - loss: 0.1959 - accuracy: 0.9234\n",
      "Epoch 326/1500\n",
      "47/47 [==============================] - 0s 809us/step - loss: 0.1770 - accuracy: 0.9308\n",
      "Epoch 327/1500\n",
      "47/47 [==============================] - 0s 778us/step - loss: 0.1922 - accuracy: 0.9281\n",
      "Epoch 328/1500\n",
      "47/47 [==============================] - 0s 800us/step - loss: 0.1876 - accuracy: 0.9281\n",
      "Epoch 329/1500\n",
      "47/47 [==============================] - 0s 814us/step - loss: 0.1860 - accuracy: 0.9258\n",
      "Epoch 330/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.1877 - accuracy: 0.9251\n",
      "Epoch 331/1500\n",
      "47/47 [==============================] - 0s 787us/step - loss: 0.1882 - accuracy: 0.9258\n",
      "Epoch 332/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1877 - accuracy: 0.9288\n",
      "Epoch 333/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.1914 - accuracy: 0.9231\n",
      "Epoch 334/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.1848 - accuracy: 0.9292\n",
      "Epoch 335/1500\n",
      "47/47 [==============================] - 0s 724us/step - loss: 0.1901 - accuracy: 0.9305\n",
      "Epoch 336/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.1915 - accuracy: 0.9278\n",
      "Epoch 337/1500\n",
      "47/47 [==============================] - 0s 769us/step - loss: 0.1805 - accuracy: 0.9302\n",
      "Epoch 338/1500\n",
      "47/47 [==============================] - 0s 748us/step - loss: 0.1979 - accuracy: 0.9227\n",
      "Epoch 339/1500\n",
      "47/47 [==============================] - 0s 746us/step - loss: 0.1829 - accuracy: 0.9295\n",
      "Epoch 340/1500\n",
      "47/47 [==============================] - 0s 744us/step - loss: 0.1823 - accuracy: 0.9268\n",
      "Epoch 341/1500\n",
      "47/47 [==============================] - 0s 747us/step - loss: 0.1899 - accuracy: 0.9268\n",
      "Epoch 342/1500\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9254\n",
      "Epoch 343/1500\n",
      "47/47 [==============================] - 0s 799us/step - loss: 0.1822 - accuracy: 0.9251\n",
      "Epoch 344/1500\n",
      "47/47 [==============================] - 0s 755us/step - loss: 0.1859 - accuracy: 0.9251\n",
      "Epoch 345/1500\n",
      "47/47 [==============================] - 0s 770us/step - loss: 0.1935 - accuracy: 0.9247\n",
      "Epoch 346/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.1962 - accuracy: 0.9264\n",
      "Epoch 347/1500\n",
      "47/47 [==============================] - 0s 739us/step - loss: 0.1946 - accuracy: 0.9183\n",
      "Epoch 348/1500\n",
      "47/47 [==============================] - 0s 757us/step - loss: 0.1971 - accuracy: 0.9237\n",
      "Epoch 349/1500\n",
      "47/47 [==============================] - 0s 762us/step - loss: 0.1821 - accuracy: 0.9281\n",
      "Epoch 350/1500\n",
      "47/47 [==============================] - 0s 749us/step - loss: 0.1787 - accuracy: 0.9281\n",
      "Epoch 351/1500\n",
      "47/47 [==============================] - 0s 756us/step - loss: 0.1896 - accuracy: 0.9275\n",
      "Epoch 352/1500\n",
      "47/47 [==============================] - 0s 737us/step - loss: 0.1958 - accuracy: 0.9254\n",
      "Epoch 353/1500\n",
      "47/47 [==============================] - 0s 742us/step - loss: 0.1816 - accuracy: 0.9302\n",
      "Epoch 354/1500\n",
      "47/47 [==============================] - 0s 743us/step - loss: 0.1826 - accuracy: 0.9275\n",
      "Epoch 355/1500\n",
      "47/47 [==============================] - 0s 782us/step - loss: 0.2025 - accuracy: 0.9200\n",
      "Epoch 356/1500\n",
      " 1/47 [..............................] - ETA: 0s - loss: 0.1431 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 326.\n",
      "47/47 [==============================] - 0s 796us/step - loss: 0.1785 - accuracy: 0.9244\n",
      "Epoch 356: early stopping\n",
      "7/7 [==============================] - 0s 843us/step - loss: 0.8390 - accuracy: 0.7512\n",
      "7/7 [==============================] - 0s 648us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 427, Predictions: 427, Actuals: 427, Gender: 427\n",
      "Final Test Results - Loss: 0.8390310406684875, Accuracy: 0.7511961460113525, Precision: 0.7250153808293344, Recall: 0.7414965986394558, F1 Score: 0.731208114402294\n",
      "Confusion Matrix:\n",
      " [[100   8  17]\n",
      " [  2  32   1]\n",
      " [ 24   0  25]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'045A'}\n",
      "Moved to Test Set:\n",
      "{'045A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '045A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "752\n",
      "Length of y_train_val:\n",
      "752\n",
      "Length of groups_train_val:\n",
      "752\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    146\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1136, 1: 994, 2: 896})\n",
      "Epoch 1/1500\n",
      "48/48 [==============================] - 0s 892us/step - loss: 1.0670 - accuracy: 0.5109\n",
      "Epoch 2/1500\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.8309 - accuracy: 0.6269\n",
      "Epoch 3/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.7903 - accuracy: 0.6537\n",
      "Epoch 4/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.7580 - accuracy: 0.6616\n",
      "Epoch 5/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.7238 - accuracy: 0.6794\n",
      "Epoch 6/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 0.7189 - accuracy: 0.6864\n",
      "Epoch 7/1500\n",
      "48/48 [==============================] - 0s 842us/step - loss: 0.6956 - accuracy: 0.6933\n",
      "Epoch 8/1500\n",
      "48/48 [==============================] - 0s 764us/step - loss: 0.6801 - accuracy: 0.6970\n",
      "Epoch 9/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.6628 - accuracy: 0.7032\n",
      "Epoch 10/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 0.6284 - accuracy: 0.7204\n",
      "Epoch 11/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 0.6364 - accuracy: 0.7168\n",
      "Epoch 12/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 0.6120 - accuracy: 0.7300\n",
      "Epoch 13/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 0.6216 - accuracy: 0.7260\n",
      "Epoch 14/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.5875 - accuracy: 0.7479\n",
      "Epoch 15/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.6008 - accuracy: 0.7422\n",
      "Epoch 16/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.5860 - accuracy: 0.7432\n",
      "Epoch 17/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.5972 - accuracy: 0.7250\n",
      "Epoch 18/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 0.5674 - accuracy: 0.7485\n",
      "Epoch 19/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 0.5721 - accuracy: 0.7373\n",
      "Epoch 20/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 0.5782 - accuracy: 0.7508\n",
      "Epoch 21/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 0.5680 - accuracy: 0.7558\n",
      "Epoch 22/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 0.5492 - accuracy: 0.7571\n",
      "Epoch 23/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.5473 - accuracy: 0.7588\n",
      "Epoch 24/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 0.5389 - accuracy: 0.7660\n",
      "Epoch 25/1500\n",
      "48/48 [==============================] - 0s 744us/step - loss: 0.5350 - accuracy: 0.7690\n",
      "Epoch 26/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 0.5328 - accuracy: 0.7647\n",
      "Epoch 27/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.5234 - accuracy: 0.7710\n",
      "Epoch 28/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.5410 - accuracy: 0.7624\n",
      "Epoch 29/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 0.5250 - accuracy: 0.7561\n",
      "Epoch 30/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 0.5109 - accuracy: 0.7730\n",
      "Epoch 31/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 0.5063 - accuracy: 0.7766\n",
      "Epoch 32/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.5149 - accuracy: 0.7759\n",
      "Epoch 33/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.5054 - accuracy: 0.7839\n",
      "Epoch 34/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 0.5113 - accuracy: 0.7680\n",
      "Epoch 35/1500\n",
      "48/48 [==============================] - 0s 775us/step - loss: 0.4966 - accuracy: 0.7849\n",
      "Epoch 36/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 0.5130 - accuracy: 0.7862\n",
      "Epoch 37/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 0.4878 - accuracy: 0.7779\n",
      "Epoch 38/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.4823 - accuracy: 0.7852\n",
      "Epoch 39/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 0.4889 - accuracy: 0.7849\n",
      "Epoch 40/1500\n",
      "48/48 [==============================] - 0s 744us/step - loss: 0.4860 - accuracy: 0.7842\n",
      "Epoch 41/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 0.4764 - accuracy: 0.7888\n",
      "Epoch 42/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 0.4858 - accuracy: 0.7865\n",
      "Epoch 43/1500\n",
      "48/48 [==============================] - 0s 744us/step - loss: 0.4884 - accuracy: 0.7908\n",
      "Epoch 44/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 0.4801 - accuracy: 0.7885\n",
      "Epoch 45/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 0.4730 - accuracy: 0.7855\n",
      "Epoch 46/1500\n",
      "48/48 [==============================] - 0s 791us/step - loss: 0.4613 - accuracy: 0.7938\n",
      "Epoch 47/1500\n",
      "48/48 [==============================] - 0s 835us/step - loss: 0.4527 - accuracy: 0.8083\n",
      "Epoch 48/1500\n",
      "48/48 [==============================] - 0s 873us/step - loss: 0.4716 - accuracy: 0.7895\n",
      "Epoch 49/1500\n",
      "48/48 [==============================] - 0s 843us/step - loss: 0.4581 - accuracy: 0.8044\n",
      "Epoch 50/1500\n",
      "48/48 [==============================] - 0s 702us/step - loss: 0.4684 - accuracy: 0.7918\n",
      "Epoch 51/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 0.4512 - accuracy: 0.8024\n",
      "Epoch 52/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 0.4550 - accuracy: 0.8080\n",
      "Epoch 53/1500\n",
      "48/48 [==============================] - 0s 785us/step - loss: 0.4510 - accuracy: 0.8011\n",
      "Epoch 54/1500\n",
      "48/48 [==============================] - 0s 817us/step - loss: 0.4560 - accuracy: 0.7941\n",
      "Epoch 55/1500\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.4464 - accuracy: 0.8116\n",
      "Epoch 56/1500\n",
      "48/48 [==============================] - 0s 850us/step - loss: 0.4427 - accuracy: 0.8037\n",
      "Epoch 57/1500\n",
      "48/48 [==============================] - 0s 800us/step - loss: 0.4428 - accuracy: 0.8060\n",
      "Epoch 58/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 0.4432 - accuracy: 0.8073\n",
      "Epoch 59/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 0.4496 - accuracy: 0.8054\n",
      "Epoch 60/1500\n",
      "48/48 [==============================] - 0s 846us/step - loss: 0.4541 - accuracy: 0.7964\n",
      "Epoch 61/1500\n",
      "48/48 [==============================] - 0s 804us/step - loss: 0.4367 - accuracy: 0.8166\n",
      "Epoch 62/1500\n",
      "48/48 [==============================] - 0s 845us/step - loss: 0.4283 - accuracy: 0.8159\n",
      "Epoch 63/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.4353 - accuracy: 0.8070\n",
      "Epoch 64/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.4375 - accuracy: 0.8143\n",
      "Epoch 65/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 0.4383 - accuracy: 0.8073\n",
      "Epoch 66/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 0.4471 - accuracy: 0.8017\n",
      "Epoch 67/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.4472 - accuracy: 0.8093\n",
      "Epoch 68/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.4269 - accuracy: 0.8146\n",
      "Epoch 69/1500\n",
      "48/48 [==============================] - 0s 869us/step - loss: 0.4273 - accuracy: 0.8186\n",
      "Epoch 70/1500\n",
      "48/48 [==============================] - 0s 876us/step - loss: 0.4201 - accuracy: 0.8212\n",
      "Epoch 71/1500\n",
      "48/48 [==============================] - 0s 776us/step - loss: 0.4203 - accuracy: 0.8245\n",
      "Epoch 72/1500\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.4219 - accuracy: 0.8182\n",
      "Epoch 73/1500\n",
      "48/48 [==============================] - 0s 758us/step - loss: 0.4430 - accuracy: 0.8093\n",
      "Epoch 74/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 0.4217 - accuracy: 0.8189\n",
      "Epoch 75/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 0.4267 - accuracy: 0.8116\n",
      "Epoch 76/1500\n",
      "48/48 [==============================] - 0s 768us/step - loss: 0.4130 - accuracy: 0.8225\n",
      "Epoch 77/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.4149 - accuracy: 0.8291\n",
      "Epoch 78/1500\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.4092 - accuracy: 0.8146\n",
      "Epoch 79/1500\n",
      "48/48 [==============================] - 0s 901us/step - loss: 0.4020 - accuracy: 0.8291\n",
      "Epoch 80/1500\n",
      "48/48 [==============================] - 0s 934us/step - loss: 0.4138 - accuracy: 0.8163\n",
      "Epoch 81/1500\n",
      "48/48 [==============================] - 0s 920us/step - loss: 0.4051 - accuracy: 0.8288\n",
      "Epoch 82/1500\n",
      "48/48 [==============================] - 0s 922us/step - loss: 0.4194 - accuracy: 0.8103\n",
      "Epoch 83/1500\n",
      "48/48 [==============================] - 0s 928us/step - loss: 0.4128 - accuracy: 0.8225\n",
      "Epoch 84/1500\n",
      "48/48 [==============================] - 0s 880us/step - loss: 0.4103 - accuracy: 0.8249\n",
      "Epoch 85/1500\n",
      "48/48 [==============================] - 0s 954us/step - loss: 0.3957 - accuracy: 0.8305\n",
      "Epoch 86/1500\n",
      "48/48 [==============================] - 0s 936us/step - loss: 0.4072 - accuracy: 0.8199\n",
      "Epoch 87/1500\n",
      "48/48 [==============================] - 0s 911us/step - loss: 0.3964 - accuracy: 0.8328\n",
      "Epoch 88/1500\n",
      "48/48 [==============================] - 0s 896us/step - loss: 0.4014 - accuracy: 0.8298\n",
      "Epoch 89/1500\n",
      "48/48 [==============================] - 0s 930us/step - loss: 0.3950 - accuracy: 0.8391\n",
      "Epoch 90/1500\n",
      "48/48 [==============================] - 0s 934us/step - loss: 0.3992 - accuracy: 0.8315\n",
      "Epoch 91/1500\n",
      "48/48 [==============================] - 0s 905us/step - loss: 0.4002 - accuracy: 0.8245\n",
      "Epoch 92/1500\n",
      "48/48 [==============================] - 0s 929us/step - loss: 0.3903 - accuracy: 0.8414\n",
      "Epoch 93/1500\n",
      "48/48 [==============================] - 0s 928us/step - loss: 0.3978 - accuracy: 0.8338\n",
      "Epoch 94/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 0.3883 - accuracy: 0.8288\n",
      "Epoch 95/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 0.3926 - accuracy: 0.8318\n",
      "Epoch 96/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 0.3850 - accuracy: 0.8404\n",
      "Epoch 97/1500\n",
      "48/48 [==============================] - 0s 840us/step - loss: 0.3865 - accuracy: 0.8338\n",
      "Epoch 98/1500\n",
      "48/48 [==============================] - 0s 865us/step - loss: 0.3892 - accuracy: 0.8318\n",
      "Epoch 99/1500\n",
      "48/48 [==============================] - 0s 912us/step - loss: 0.3971 - accuracy: 0.8288\n",
      "Epoch 100/1500\n",
      "48/48 [==============================] - 0s 890us/step - loss: 0.3855 - accuracy: 0.8334\n",
      "Epoch 101/1500\n",
      "48/48 [==============================] - 0s 881us/step - loss: 0.3880 - accuracy: 0.8331\n",
      "Epoch 102/1500\n",
      "48/48 [==============================] - 0s 863us/step - loss: 0.3891 - accuracy: 0.8434\n",
      "Epoch 103/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 0.3763 - accuracy: 0.8437\n",
      "Epoch 104/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 0.3922 - accuracy: 0.8308\n",
      "Epoch 105/1500\n",
      "48/48 [==============================] - 0s 900us/step - loss: 0.3768 - accuracy: 0.8384\n",
      "Epoch 106/1500\n",
      "48/48 [==============================] - 0s 962us/step - loss: 0.3704 - accuracy: 0.8473\n",
      "Epoch 107/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8341\n",
      "Epoch 108/1500\n",
      "48/48 [==============================] - 0s 982us/step - loss: 0.3741 - accuracy: 0.8473\n",
      "Epoch 109/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3825 - accuracy: 0.8450\n",
      "Epoch 110/1500\n",
      "48/48 [==============================] - 0s 962us/step - loss: 0.3776 - accuracy: 0.8427\n",
      "Epoch 111/1500\n",
      "48/48 [==============================] - 0s 959us/step - loss: 0.3708 - accuracy: 0.8401\n",
      "Epoch 112/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8490\n",
      "Epoch 113/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8434\n",
      "Epoch 114/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8529\n",
      "Epoch 115/1500\n",
      "48/48 [==============================] - 0s 946us/step - loss: 0.3699 - accuracy: 0.8407\n",
      "Epoch 116/1500\n",
      "48/48 [==============================] - 0s 909us/step - loss: 0.3650 - accuracy: 0.8473\n",
      "Epoch 117/1500\n",
      "48/48 [==============================] - 0s 929us/step - loss: 0.3674 - accuracy: 0.8503\n",
      "Epoch 118/1500\n",
      "48/48 [==============================] - 0s 923us/step - loss: 0.3685 - accuracy: 0.8430\n",
      "Epoch 119/1500\n",
      "48/48 [==============================] - 0s 912us/step - loss: 0.3614 - accuracy: 0.8473\n",
      "Epoch 120/1500\n",
      "48/48 [==============================] - 0s 921us/step - loss: 0.3719 - accuracy: 0.8447\n",
      "Epoch 121/1500\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.3688 - accuracy: 0.8473\n",
      "Epoch 122/1500\n",
      "48/48 [==============================] - 0s 770us/step - loss: 0.3498 - accuracy: 0.8483\n",
      "Epoch 123/1500\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.3440 - accuracy: 0.8582\n",
      "Epoch 124/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 0.3613 - accuracy: 0.8437\n",
      "Epoch 125/1500\n",
      "48/48 [==============================] - 0s 792us/step - loss: 0.3497 - accuracy: 0.8625\n",
      "Epoch 126/1500\n",
      "48/48 [==============================] - 0s 819us/step - loss: 0.3497 - accuracy: 0.8483\n",
      "Epoch 127/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 0.3555 - accuracy: 0.8559\n",
      "Epoch 128/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.3421 - accuracy: 0.8592\n",
      "Epoch 129/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 0.3541 - accuracy: 0.8503\n",
      "Epoch 130/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 0.3699 - accuracy: 0.8427\n",
      "Epoch 131/1500\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.3502 - accuracy: 0.8533\n",
      "Epoch 132/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 0.3613 - accuracy: 0.8556\n",
      "Epoch 133/1500\n",
      "48/48 [==============================] - 0s 768us/step - loss: 0.3476 - accuracy: 0.8536\n",
      "Epoch 134/1500\n",
      "48/48 [==============================] - 0s 835us/step - loss: 0.3511 - accuracy: 0.8539\n",
      "Epoch 135/1500\n",
      "48/48 [==============================] - 0s 840us/step - loss: 0.3450 - accuracy: 0.8559\n",
      "Epoch 136/1500\n",
      "48/48 [==============================] - 0s 804us/step - loss: 0.3586 - accuracy: 0.8483\n",
      "Epoch 137/1500\n",
      "48/48 [==============================] - 0s 781us/step - loss: 0.3346 - accuracy: 0.8642\n",
      "Epoch 138/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.3440 - accuracy: 0.8582\n",
      "Epoch 139/1500\n",
      "48/48 [==============================] - 0s 793us/step - loss: 0.3585 - accuracy: 0.8543\n",
      "Epoch 140/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 0.3355 - accuracy: 0.8619\n",
      "Epoch 141/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 0.3484 - accuracy: 0.8556\n",
      "Epoch 142/1500\n",
      "48/48 [==============================] - 0s 778us/step - loss: 0.3565 - accuracy: 0.8519\n",
      "Epoch 143/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.3383 - accuracy: 0.8576\n",
      "Epoch 144/1500\n",
      "48/48 [==============================] - 0s 852us/step - loss: 0.3435 - accuracy: 0.8605\n",
      "Epoch 145/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.3285 - accuracy: 0.8579\n",
      "Epoch 146/1500\n",
      "48/48 [==============================] - 0s 845us/step - loss: 0.3421 - accuracy: 0.8566\n",
      "Epoch 147/1500\n",
      "48/48 [==============================] - 0s 808us/step - loss: 0.3404 - accuracy: 0.8589\n",
      "Epoch 148/1500\n",
      "48/48 [==============================] - 0s 834us/step - loss: 0.3453 - accuracy: 0.8546\n",
      "Epoch 149/1500\n",
      "48/48 [==============================] - 0s 871us/step - loss: 0.3286 - accuracy: 0.8642\n",
      "Epoch 150/1500\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.3265 - accuracy: 0.8629\n",
      "Epoch 151/1500\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.3267 - accuracy: 0.8662\n",
      "Epoch 152/1500\n",
      "48/48 [==============================] - 0s 817us/step - loss: 0.3349 - accuracy: 0.8648\n",
      "Epoch 153/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8615\n",
      "Epoch 154/1500\n",
      "48/48 [==============================] - 0s 865us/step - loss: 0.3310 - accuracy: 0.8655\n",
      "Epoch 155/1500\n",
      "48/48 [==============================] - 0s 858us/step - loss: 0.3281 - accuracy: 0.8695\n",
      "Epoch 156/1500\n",
      "48/48 [==============================] - 0s 822us/step - loss: 0.3283 - accuracy: 0.8668\n",
      "Epoch 157/1500\n",
      "48/48 [==============================] - 0s 919us/step - loss: 0.3181 - accuracy: 0.8632\n",
      "Epoch 158/1500\n",
      "48/48 [==============================] - 0s 911us/step - loss: 0.3258 - accuracy: 0.8734\n",
      "Epoch 159/1500\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.3303 - accuracy: 0.8691\n",
      "Epoch 160/1500\n",
      "48/48 [==============================] - 0s 829us/step - loss: 0.3238 - accuracy: 0.8622\n",
      "Epoch 161/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.3257 - accuracy: 0.8612\n",
      "Epoch 162/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.3298 - accuracy: 0.8642\n",
      "Epoch 163/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.3357 - accuracy: 0.8668\n",
      "Epoch 164/1500\n",
      "48/48 [==============================] - 0s 838us/step - loss: 0.3256 - accuracy: 0.8658\n",
      "Epoch 165/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.3243 - accuracy: 0.8675\n",
      "Epoch 166/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.3145 - accuracy: 0.8695\n",
      "Epoch 167/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 0.3144 - accuracy: 0.8635\n",
      "Epoch 168/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.3075 - accuracy: 0.8724\n",
      "Epoch 169/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.3289 - accuracy: 0.8629\n",
      "Epoch 170/1500\n",
      "48/48 [==============================] - 0s 794us/step - loss: 0.3174 - accuracy: 0.8724\n",
      "Epoch 171/1500\n",
      "48/48 [==============================] - 0s 851us/step - loss: 0.3072 - accuracy: 0.8721\n",
      "Epoch 172/1500\n",
      "48/48 [==============================] - 0s 808us/step - loss: 0.3076 - accuracy: 0.8738\n",
      "Epoch 173/1500\n",
      "48/48 [==============================] - 0s 793us/step - loss: 0.3125 - accuracy: 0.8718\n",
      "Epoch 174/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 0.2999 - accuracy: 0.8830\n",
      "Epoch 175/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.3151 - accuracy: 0.8767\n",
      "Epoch 176/1500\n",
      "48/48 [==============================] - 0s 867us/step - loss: 0.3123 - accuracy: 0.8731\n",
      "Epoch 177/1500\n",
      "48/48 [==============================] - 0s 916us/step - loss: 0.3145 - accuracy: 0.8714\n",
      "Epoch 178/1500\n",
      "48/48 [==============================] - 0s 985us/step - loss: 0.3114 - accuracy: 0.8708\n",
      "Epoch 179/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3154 - accuracy: 0.8761\n",
      "Epoch 180/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.8787\n",
      "Epoch 181/1500\n",
      "48/48 [==============================] - 0s 977us/step - loss: 0.3119 - accuracy: 0.8751\n",
      "Epoch 182/1500\n",
      "48/48 [==============================] - 0s 846us/step - loss: 0.3106 - accuracy: 0.8761\n",
      "Epoch 183/1500\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.3074 - accuracy: 0.8738\n",
      "Epoch 184/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.3103 - accuracy: 0.8748\n",
      "Epoch 185/1500\n",
      "48/48 [==============================] - 0s 893us/step - loss: 0.3015 - accuracy: 0.8738\n",
      "Epoch 186/1500\n",
      "48/48 [==============================] - 0s 925us/step - loss: 0.3157 - accuracy: 0.8678\n",
      "Epoch 187/1500\n",
      "48/48 [==============================] - 0s 928us/step - loss: 0.3150 - accuracy: 0.8681\n",
      "Epoch 188/1500\n",
      "48/48 [==============================] - 0s 918us/step - loss: 0.3014 - accuracy: 0.8774\n",
      "Epoch 189/1500\n",
      "48/48 [==============================] - 0s 905us/step - loss: 0.2989 - accuracy: 0.8767\n",
      "Epoch 190/1500\n",
      "48/48 [==============================] - 0s 862us/step - loss: 0.3176 - accuracy: 0.8658\n",
      "Epoch 191/1500\n",
      "48/48 [==============================] - 0s 776us/step - loss: 0.3001 - accuracy: 0.8817\n",
      "Epoch 192/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 0.3101 - accuracy: 0.8744\n",
      "Epoch 193/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 0.2931 - accuracy: 0.8764\n",
      "Epoch 194/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 0.3002 - accuracy: 0.8724\n",
      "Epoch 195/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.2892 - accuracy: 0.8800\n",
      "Epoch 196/1500\n",
      "48/48 [==============================] - 0s 839us/step - loss: 0.3033 - accuracy: 0.8761\n",
      "Epoch 197/1500\n",
      "48/48 [==============================] - 0s 834us/step - loss: 0.2994 - accuracy: 0.8794\n",
      "Epoch 198/1500\n",
      "48/48 [==============================] - 0s 844us/step - loss: 0.3129 - accuracy: 0.8685\n",
      "Epoch 199/1500\n",
      "48/48 [==============================] - 0s 856us/step - loss: 0.2978 - accuracy: 0.8731\n",
      "Epoch 200/1500\n",
      "48/48 [==============================] - 0s 861us/step - loss: 0.2887 - accuracy: 0.8771\n",
      "Epoch 201/1500\n",
      "48/48 [==============================] - 0s 800us/step - loss: 0.2939 - accuracy: 0.8777\n",
      "Epoch 202/1500\n",
      "48/48 [==============================] - 0s 824us/step - loss: 0.2848 - accuracy: 0.8843\n",
      "Epoch 203/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.2917 - accuracy: 0.8754\n",
      "Epoch 204/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.2938 - accuracy: 0.8761\n",
      "Epoch 205/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 0.2766 - accuracy: 0.8886\n",
      "Epoch 206/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.2806 - accuracy: 0.8860\n",
      "Epoch 207/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 0.2943 - accuracy: 0.8748\n",
      "Epoch 208/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 0.2957 - accuracy: 0.8748\n",
      "Epoch 209/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 0.2803 - accuracy: 0.8824\n",
      "Epoch 210/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 0.3013 - accuracy: 0.8754\n",
      "Epoch 211/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 0.2837 - accuracy: 0.8840\n",
      "Epoch 212/1500\n",
      "48/48 [==============================] - 0s 802us/step - loss: 0.2835 - accuracy: 0.8817\n",
      "Epoch 213/1500\n",
      "48/48 [==============================] - 0s 887us/step - loss: 0.2772 - accuracy: 0.8880\n",
      "Epoch 214/1500\n",
      "48/48 [==============================] - 0s 877us/step - loss: 0.2865 - accuracy: 0.8814\n",
      "Epoch 215/1500\n",
      "48/48 [==============================] - 0s 839us/step - loss: 0.2918 - accuracy: 0.8820\n",
      "Epoch 216/1500\n",
      "48/48 [==============================] - 0s 850us/step - loss: 0.2946 - accuracy: 0.8880\n",
      "Epoch 217/1500\n",
      "48/48 [==============================] - 0s 785us/step - loss: 0.2901 - accuracy: 0.8794\n",
      "Epoch 218/1500\n",
      "48/48 [==============================] - 0s 833us/step - loss: 0.2869 - accuracy: 0.8817\n",
      "Epoch 219/1500\n",
      "48/48 [==============================] - 0s 872us/step - loss: 0.2855 - accuracy: 0.8824\n",
      "Epoch 220/1500\n",
      "48/48 [==============================] - 0s 803us/step - loss: 0.2896 - accuracy: 0.8876\n",
      "Epoch 221/1500\n",
      "48/48 [==============================] - 0s 889us/step - loss: 0.2848 - accuracy: 0.8827\n",
      "Epoch 222/1500\n",
      "48/48 [==============================] - 0s 890us/step - loss: 0.2783 - accuracy: 0.8896\n",
      "Epoch 223/1500\n",
      "48/48 [==============================] - 0s 846us/step - loss: 0.2960 - accuracy: 0.8734\n",
      "Epoch 224/1500\n",
      "48/48 [==============================] - 0s 872us/step - loss: 0.2887 - accuracy: 0.8787\n",
      "Epoch 225/1500\n",
      "48/48 [==============================] - 0s 821us/step - loss: 0.2975 - accuracy: 0.8790\n",
      "Epoch 226/1500\n",
      "48/48 [==============================] - 0s 759us/step - loss: 0.2754 - accuracy: 0.8890\n",
      "Epoch 227/1500\n",
      "48/48 [==============================] - 0s 775us/step - loss: 0.2669 - accuracy: 0.8939\n",
      "Epoch 228/1500\n",
      "48/48 [==============================] - 0s 735us/step - loss: 0.2779 - accuracy: 0.8853\n",
      "Epoch 229/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 0.2893 - accuracy: 0.8830\n",
      "Epoch 230/1500\n",
      "48/48 [==============================] - 0s 758us/step - loss: 0.2807 - accuracy: 0.8850\n",
      "Epoch 231/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.2857 - accuracy: 0.8860\n",
      "Epoch 232/1500\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.2743 - accuracy: 0.8909\n",
      "Epoch 233/1500\n",
      "48/48 [==============================] - 0s 839us/step - loss: 0.2892 - accuracy: 0.8807\n",
      "Epoch 234/1500\n",
      "48/48 [==============================] - 0s 823us/step - loss: 0.2772 - accuracy: 0.8860\n",
      "Epoch 235/1500\n",
      "48/48 [==============================] - 0s 791us/step - loss: 0.2759 - accuracy: 0.8870\n",
      "Epoch 236/1500\n",
      "48/48 [==============================] - 0s 767us/step - loss: 0.2790 - accuracy: 0.8896\n",
      "Epoch 237/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.2748 - accuracy: 0.8817\n",
      "Epoch 238/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 0.2801 - accuracy: 0.8833\n",
      "Epoch 239/1500\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.2684 - accuracy: 0.8896\n",
      "Epoch 240/1500\n",
      "48/48 [==============================] - 0s 804us/step - loss: 0.2614 - accuracy: 0.9005\n",
      "Epoch 241/1500\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.2783 - accuracy: 0.8906\n",
      "Epoch 242/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.2827 - accuracy: 0.8837\n",
      "Epoch 243/1500\n",
      "48/48 [==============================] - 0s 811us/step - loss: 0.2744 - accuracy: 0.8853\n",
      "Epoch 244/1500\n",
      "48/48 [==============================] - 0s 821us/step - loss: 0.2760 - accuracy: 0.8900\n",
      "Epoch 245/1500\n",
      "48/48 [==============================] - 0s 814us/step - loss: 0.2867 - accuracy: 0.8857\n",
      "Epoch 246/1500\n",
      "48/48 [==============================] - 0s 808us/step - loss: 0.2892 - accuracy: 0.8837\n",
      "Epoch 247/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 0.2665 - accuracy: 0.8916\n",
      "Epoch 248/1500\n",
      "48/48 [==============================] - 0s 794us/step - loss: 0.2698 - accuracy: 0.8959\n",
      "Epoch 249/1500\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.2688 - accuracy: 0.8906\n",
      "Epoch 250/1500\n",
      "48/48 [==============================] - 0s 804us/step - loss: 0.2772 - accuracy: 0.8883\n",
      "Epoch 251/1500\n",
      "48/48 [==============================] - 0s 770us/step - loss: 0.2672 - accuracy: 0.8933\n",
      "Epoch 252/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 0.2742 - accuracy: 0.8853\n",
      "Epoch 253/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 0.2716 - accuracy: 0.8883\n",
      "Epoch 254/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 0.2715 - accuracy: 0.8857\n",
      "Epoch 255/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.2816 - accuracy: 0.8870\n",
      "Epoch 256/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 0.2584 - accuracy: 0.8939\n",
      "Epoch 257/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 0.2678 - accuracy: 0.8946\n",
      "Epoch 258/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 0.2613 - accuracy: 0.8886\n",
      "Epoch 259/1500\n",
      "48/48 [==============================] - 0s 803us/step - loss: 0.2565 - accuracy: 0.8942\n",
      "Epoch 260/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.2677 - accuracy: 0.8916\n",
      "Epoch 261/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 0.2682 - accuracy: 0.8906\n",
      "Epoch 262/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 0.2632 - accuracy: 0.9002\n",
      "Epoch 263/1500\n",
      "48/48 [==============================] - 0s 785us/step - loss: 0.2692 - accuracy: 0.8913\n",
      "Epoch 264/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.2595 - accuracy: 0.8952\n",
      "Epoch 265/1500\n",
      "48/48 [==============================] - 0s 839us/step - loss: 0.2662 - accuracy: 0.8942\n",
      "Epoch 266/1500\n",
      "48/48 [==============================] - 0s 790us/step - loss: 0.2619 - accuracy: 0.8949\n",
      "Epoch 267/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.2609 - accuracy: 0.8995\n",
      "Epoch 268/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 0.2631 - accuracy: 0.8959\n",
      "Epoch 269/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.2594 - accuracy: 0.8949\n",
      "Epoch 270/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.2526 - accuracy: 0.8995\n",
      "Epoch 271/1500\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.2808 - accuracy: 0.8873\n",
      "Epoch 272/1500\n",
      "48/48 [==============================] - 0s 805us/step - loss: 0.2612 - accuracy: 0.8926\n",
      "Epoch 273/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.2667 - accuracy: 0.8939\n",
      "Epoch 274/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.2566 - accuracy: 0.8936\n",
      "Epoch 275/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 0.2536 - accuracy: 0.8929\n",
      "Epoch 276/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 0.2524 - accuracy: 0.8966\n",
      "Epoch 277/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 0.2572 - accuracy: 0.8969\n",
      "Epoch 278/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.2523 - accuracy: 0.9005\n",
      "Epoch 279/1500\n",
      "48/48 [==============================] - 0s 735us/step - loss: 0.2620 - accuracy: 0.8939\n",
      "Epoch 280/1500\n",
      "48/48 [==============================] - 0s 808us/step - loss: 0.2413 - accuracy: 0.9035\n",
      "Epoch 281/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 0.2510 - accuracy: 0.8989\n",
      "Epoch 282/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 0.2602 - accuracy: 0.8952\n",
      "Epoch 283/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 0.2656 - accuracy: 0.8959\n",
      "Epoch 284/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 0.2550 - accuracy: 0.8972\n",
      "Epoch 285/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 0.2517 - accuracy: 0.8976\n",
      "Epoch 286/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 0.2521 - accuracy: 0.9002\n",
      "Epoch 287/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 0.2621 - accuracy: 0.8985\n",
      "Epoch 288/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 0.2613 - accuracy: 0.8979\n",
      "Epoch 289/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 0.2495 - accuracy: 0.8995\n",
      "Epoch 290/1500\n",
      "48/48 [==============================] - 0s 806us/step - loss: 0.2483 - accuracy: 0.8942\n",
      "Epoch 291/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.9019\n",
      "Epoch 292/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.9055\n",
      "Epoch 293/1500\n",
      "48/48 [==============================] - 0s 880us/step - loss: 0.2412 - accuracy: 0.9012\n",
      "Epoch 294/1500\n",
      "48/48 [==============================] - 0s 935us/step - loss: 0.2541 - accuracy: 0.8952\n",
      "Epoch 295/1500\n",
      "48/48 [==============================] - 0s 847us/step - loss: 0.2449 - accuracy: 0.9009\n",
      "Epoch 296/1500\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.2699 - accuracy: 0.8916\n",
      "Epoch 297/1500\n",
      "48/48 [==============================] - 0s 913us/step - loss: 0.2402 - accuracy: 0.9061\n",
      "Epoch 298/1500\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2350 - accuracy: 0.9068\n",
      "Epoch 299/1500\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9038\n",
      "Epoch 300/1500\n",
      "48/48 [==============================] - 0s 959us/step - loss: 0.2387 - accuracy: 0.9098\n",
      "Epoch 301/1500\n",
      "48/48 [==============================] - 0s 792us/step - loss: 0.2516 - accuracy: 0.8949\n",
      "Epoch 302/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 0.2359 - accuracy: 0.9075\n",
      "Epoch 303/1500\n",
      "48/48 [==============================] - 0s 793us/step - loss: 0.2465 - accuracy: 0.8989\n",
      "Epoch 304/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.2333 - accuracy: 0.9137\n",
      "Epoch 305/1500\n",
      "48/48 [==============================] - 0s 811us/step - loss: 0.2450 - accuracy: 0.9019\n",
      "Epoch 306/1500\n",
      "48/48 [==============================] - 0s 832us/step - loss: 0.2564 - accuracy: 0.8916\n",
      "Epoch 307/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.2427 - accuracy: 0.9045\n",
      "Epoch 308/1500\n",
      "48/48 [==============================] - 0s 836us/step - loss: 0.2463 - accuracy: 0.9012\n",
      "Epoch 309/1500\n",
      "48/48 [==============================] - 0s 816us/step - loss: 0.2416 - accuracy: 0.9009\n",
      "Epoch 310/1500\n",
      "48/48 [==============================] - 0s 821us/step - loss: 0.2460 - accuracy: 0.9038\n",
      "Epoch 311/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.2422 - accuracy: 0.8995\n",
      "Epoch 312/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 0.2538 - accuracy: 0.8989\n",
      "Epoch 313/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.2450 - accuracy: 0.9002\n",
      "Epoch 314/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 0.2389 - accuracy: 0.9012\n",
      "Epoch 315/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 0.2486 - accuracy: 0.8989\n",
      "Epoch 316/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 0.2401 - accuracy: 0.9052\n",
      "Epoch 317/1500\n",
      "48/48 [==============================] - 0s 799us/step - loss: 0.2577 - accuracy: 0.8946\n",
      "Epoch 318/1500\n",
      "48/48 [==============================] - 0s 882us/step - loss: 0.2324 - accuracy: 0.9088\n",
      "Epoch 319/1500\n",
      "48/48 [==============================] - 0s 916us/step - loss: 0.2389 - accuracy: 0.9042\n",
      "Epoch 320/1500\n",
      "48/48 [==============================] - 0s 991us/step - loss: 0.2486 - accuracy: 0.9022\n",
      "Epoch 321/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.2462 - accuracy: 0.9065\n",
      "Epoch 322/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.9032\n",
      "Epoch 323/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.9098\n",
      "Epoch 324/1500\n",
      "48/48 [==============================] - 0s 966us/step - loss: 0.2446 - accuracy: 0.8979\n",
      "Epoch 325/1500\n",
      "48/48 [==============================] - 0s 921us/step - loss: 0.2419 - accuracy: 0.9015\n",
      "Epoch 326/1500\n",
      "48/48 [==============================] - 0s 949us/step - loss: 0.2488 - accuracy: 0.9009\n",
      "Epoch 327/1500\n",
      "48/48 [==============================] - 0s 967us/step - loss: 0.2426 - accuracy: 0.9015\n",
      "Epoch 328/1500\n",
      "48/48 [==============================] - 0s 931us/step - loss: 0.2365 - accuracy: 0.8995\n",
      "Epoch 329/1500\n",
      "48/48 [==============================] - 0s 906us/step - loss: 0.2463 - accuracy: 0.9012\n",
      "Epoch 330/1500\n",
      "48/48 [==============================] - 0s 930us/step - loss: 0.2516 - accuracy: 0.9022\n",
      "Epoch 331/1500\n",
      "48/48 [==============================] - 0s 868us/step - loss: 0.2351 - accuracy: 0.9061\n",
      "Epoch 332/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 0.2460 - accuracy: 0.9005\n",
      "Epoch 333/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 0.2364 - accuracy: 0.9071\n",
      "Epoch 334/1500\n",
      "29/48 [=================>............] - ETA: 0s - loss: 0.2314 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 304.\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.9032\n",
      "Epoch 334: early stopping\n",
      "6/6 [==============================] - 0s 794us/step - loss: 0.3511 - accuracy: 0.8378\n",
      "6/6 [==============================] - 0s 659us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.90 (27/30)\n",
      "Before appending - Cat IDs: 427, Predictions: 427, Actuals: 427, Gender: 427\n",
      "After appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "Final Test Results - Loss: 0.35109204053878784, Accuracy: 0.837837815284729, Precision: 0.8242452536570184, Recall: 0.7911487758945386, F1 Score: 0.8046304285303423\n",
      "Confusion Matrix:\n",
      " [[107   5   6]\n",
      " [  5  20   0]\n",
      " [ 14   0  28]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1075, 1: 934, 2: 752})\n",
      "Epoch 1/1500\n",
      "44/44 [==============================] - 0s 871us/step - loss: 1.2896 - accuracy: 0.4332\n",
      "Epoch 2/1500\n",
      "44/44 [==============================] - 0s 828us/step - loss: 0.9669 - accuracy: 0.5809\n",
      "Epoch 3/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.8678 - accuracy: 0.6183\n",
      "Epoch 4/1500\n",
      "44/44 [==============================] - 0s 737us/step - loss: 0.8332 - accuracy: 0.6393\n",
      "Epoch 5/1500\n",
      "44/44 [==============================] - 0s 692us/step - loss: 0.7693 - accuracy: 0.6751\n",
      "Epoch 6/1500\n",
      "44/44 [==============================] - 0s 709us/step - loss: 0.7703 - accuracy: 0.6784\n",
      "Epoch 7/1500\n",
      "44/44 [==============================] - 0s 753us/step - loss: 0.7091 - accuracy: 0.6994\n",
      "Epoch 8/1500\n",
      "44/44 [==============================] - 0s 729us/step - loss: 0.7031 - accuracy: 0.6972\n",
      "Epoch 9/1500\n",
      "44/44 [==============================] - 0s 732us/step - loss: 0.6576 - accuracy: 0.7110\n",
      "Epoch 10/1500\n",
      "44/44 [==============================] - 0s 724us/step - loss: 0.6383 - accuracy: 0.7305\n",
      "Epoch 11/1500\n",
      "44/44 [==============================] - 0s 737us/step - loss: 0.6292 - accuracy: 0.7269\n",
      "Epoch 12/1500\n",
      "44/44 [==============================] - 0s 743us/step - loss: 0.6218 - accuracy: 0.7331\n",
      "Epoch 13/1500\n",
      "44/44 [==============================] - 0s 736us/step - loss: 0.5920 - accuracy: 0.7436\n",
      "Epoch 14/1500\n",
      "44/44 [==============================] - 0s 742us/step - loss: 0.5774 - accuracy: 0.7610\n",
      "Epoch 15/1500\n",
      "44/44 [==============================] - 0s 731us/step - loss: 0.5652 - accuracy: 0.7548\n",
      "Epoch 16/1500\n",
      "44/44 [==============================] - 0s 730us/step - loss: 0.5752 - accuracy: 0.7534\n",
      "Epoch 17/1500\n",
      "44/44 [==============================] - 0s 768us/step - loss: 0.5664 - accuracy: 0.7617\n",
      "Epoch 18/1500\n",
      "44/44 [==============================] - 0s 788us/step - loss: 0.5434 - accuracy: 0.7722\n",
      "Epoch 19/1500\n",
      "44/44 [==============================] - 0s 727us/step - loss: 0.5449 - accuracy: 0.7602\n",
      "Epoch 20/1500\n",
      "44/44 [==============================] - 0s 739us/step - loss: 0.5287 - accuracy: 0.7733\n",
      "Epoch 21/1500\n",
      "44/44 [==============================] - 0s 737us/step - loss: 0.5089 - accuracy: 0.7859\n",
      "Epoch 22/1500\n",
      "44/44 [==============================] - 0s 776us/step - loss: 0.5079 - accuracy: 0.7838\n",
      "Epoch 23/1500\n",
      "44/44 [==============================] - 0s 733us/step - loss: 0.5172 - accuracy: 0.7863\n",
      "Epoch 24/1500\n",
      "44/44 [==============================] - 0s 758us/step - loss: 0.5045 - accuracy: 0.7943\n",
      "Epoch 25/1500\n",
      "44/44 [==============================] - 0s 779us/step - loss: 0.5059 - accuracy: 0.7870\n",
      "Epoch 26/1500\n",
      "44/44 [==============================] - 0s 764us/step - loss: 0.5029 - accuracy: 0.7921\n",
      "Epoch 27/1500\n",
      "44/44 [==============================] - 0s 731us/step - loss: 0.4823 - accuracy: 0.7979\n",
      "Epoch 28/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4835 - accuracy: 0.7939\n",
      "Epoch 29/1500\n",
      "44/44 [==============================] - 0s 885us/step - loss: 0.4794 - accuracy: 0.7965\n",
      "Epoch 30/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.4747 - accuracy: 0.8022\n",
      "Epoch 31/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.4717 - accuracy: 0.7979\n",
      "Epoch 32/1500\n",
      "44/44 [==============================] - 0s 736us/step - loss: 0.4695 - accuracy: 0.8048\n",
      "Epoch 33/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.4537 - accuracy: 0.8146\n",
      "Epoch 34/1500\n",
      "44/44 [==============================] - 0s 777us/step - loss: 0.4595 - accuracy: 0.8019\n",
      "Epoch 35/1500\n",
      "44/44 [==============================] - 0s 797us/step - loss: 0.4695 - accuracy: 0.8012\n",
      "Epoch 36/1500\n",
      "44/44 [==============================] - 0s 756us/step - loss: 0.4574 - accuracy: 0.8084\n",
      "Epoch 37/1500\n",
      "44/44 [==============================] - 0s 761us/step - loss: 0.4647 - accuracy: 0.8001\n",
      "Epoch 38/1500\n",
      "44/44 [==============================] - 0s 810us/step - loss: 0.4412 - accuracy: 0.8222\n",
      "Epoch 39/1500\n",
      "44/44 [==============================] - 0s 788us/step - loss: 0.4362 - accuracy: 0.8204\n",
      "Epoch 40/1500\n",
      "44/44 [==============================] - 0s 773us/step - loss: 0.4533 - accuracy: 0.8055\n",
      "Epoch 41/1500\n",
      "44/44 [==============================] - 0s 797us/step - loss: 0.4273 - accuracy: 0.8207\n",
      "Epoch 42/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.4451 - accuracy: 0.8077\n",
      "Epoch 43/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.4345 - accuracy: 0.8175\n",
      "Epoch 44/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.4296 - accuracy: 0.8138\n",
      "Epoch 45/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.4305 - accuracy: 0.8149\n",
      "Epoch 46/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.4263 - accuracy: 0.8218\n",
      "Epoch 47/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.4224 - accuracy: 0.8185\n",
      "Epoch 48/1500\n",
      "44/44 [==============================] - 0s 872us/step - loss: 0.4210 - accuracy: 0.8261\n",
      "Epoch 49/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.4127 - accuracy: 0.8276\n",
      "Epoch 50/1500\n",
      "44/44 [==============================] - 0s 847us/step - loss: 0.4132 - accuracy: 0.8258\n",
      "Epoch 51/1500\n",
      "44/44 [==============================] - 0s 871us/step - loss: 0.4056 - accuracy: 0.8240\n",
      "Epoch 52/1500\n",
      "44/44 [==============================] - 0s 830us/step - loss: 0.4233 - accuracy: 0.8196\n",
      "Epoch 53/1500\n",
      "44/44 [==============================] - 0s 888us/step - loss: 0.4206 - accuracy: 0.8254\n",
      "Epoch 54/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.4082 - accuracy: 0.8290\n",
      "Epoch 55/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.3982 - accuracy: 0.8272\n",
      "Epoch 56/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.3973 - accuracy: 0.8338\n",
      "Epoch 57/1500\n",
      "44/44 [==============================] - 0s 969us/step - loss: 0.4122 - accuracy: 0.8330\n",
      "Epoch 58/1500\n",
      "44/44 [==============================] - 0s 943us/step - loss: 0.3975 - accuracy: 0.8352\n",
      "Epoch 59/1500\n",
      "44/44 [==============================] - 0s 914us/step - loss: 0.3961 - accuracy: 0.8363\n",
      "Epoch 60/1500\n",
      "44/44 [==============================] - 0s 903us/step - loss: 0.3873 - accuracy: 0.8334\n",
      "Epoch 61/1500\n",
      "44/44 [==============================] - 0s 867us/step - loss: 0.4024 - accuracy: 0.8283\n",
      "Epoch 62/1500\n",
      "44/44 [==============================] - 0s 889us/step - loss: 0.3871 - accuracy: 0.8450\n",
      "Epoch 63/1500\n",
      "44/44 [==============================] - 0s 917us/step - loss: 0.3841 - accuracy: 0.8424\n",
      "Epoch 64/1500\n",
      "44/44 [==============================] - 0s 870us/step - loss: 0.3887 - accuracy: 0.8334\n",
      "Epoch 65/1500\n",
      "44/44 [==============================] - 0s 822us/step - loss: 0.4023 - accuracy: 0.8265\n",
      "Epoch 66/1500\n",
      "44/44 [==============================] - 0s 919us/step - loss: 0.3922 - accuracy: 0.8367\n",
      "Epoch 67/1500\n",
      "44/44 [==============================] - 0s 926us/step - loss: 0.3705 - accuracy: 0.8443\n",
      "Epoch 68/1500\n",
      "44/44 [==============================] - 0s 909us/step - loss: 0.3764 - accuracy: 0.8432\n",
      "Epoch 69/1500\n",
      "44/44 [==============================] - 0s 871us/step - loss: 0.3805 - accuracy: 0.8377\n",
      "Epoch 70/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.3701 - accuracy: 0.8468\n",
      "Epoch 71/1500\n",
      "44/44 [==============================] - 0s 859us/step - loss: 0.3830 - accuracy: 0.8392\n",
      "Epoch 72/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.3921 - accuracy: 0.8359\n",
      "Epoch 73/1500\n",
      "44/44 [==============================] - 0s 976us/step - loss: 0.3822 - accuracy: 0.8327\n",
      "Epoch 74/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.3708 - accuracy: 0.8475\n",
      "Epoch 75/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.3804 - accuracy: 0.8439\n",
      "Epoch 76/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.3802 - accuracy: 0.8381\n",
      "Epoch 77/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.3769 - accuracy: 0.8435\n",
      "Epoch 78/1500\n",
      "44/44 [==============================] - 0s 864us/step - loss: 0.3756 - accuracy: 0.8396\n",
      "Epoch 79/1500\n",
      "44/44 [==============================] - 0s 969us/step - loss: 0.3716 - accuracy: 0.8432\n",
      "Epoch 80/1500\n",
      "44/44 [==============================] - 0s 930us/step - loss: 0.3710 - accuracy: 0.8468\n",
      "Epoch 81/1500\n",
      "44/44 [==============================] - 0s 966us/step - loss: 0.3603 - accuracy: 0.8511\n",
      "Epoch 82/1500\n",
      "44/44 [==============================] - 0s 949us/step - loss: 0.3528 - accuracy: 0.8537\n",
      "Epoch 83/1500\n",
      "44/44 [==============================] - 0s 864us/step - loss: 0.3622 - accuracy: 0.8501\n",
      "Epoch 84/1500\n",
      "44/44 [==============================] - 0s 904us/step - loss: 0.3661 - accuracy: 0.8457\n",
      "Epoch 85/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.3632 - accuracy: 0.8486\n",
      "Epoch 86/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.3564 - accuracy: 0.8573\n",
      "Epoch 87/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.3481 - accuracy: 0.8526\n",
      "Epoch 88/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.3477 - accuracy: 0.8530\n",
      "Epoch 89/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.3667 - accuracy: 0.8461\n",
      "Epoch 90/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.3482 - accuracy: 0.8573\n",
      "Epoch 91/1500\n",
      "44/44 [==============================] - 0s 869us/step - loss: 0.3611 - accuracy: 0.8490\n",
      "Epoch 92/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.3477 - accuracy: 0.8522\n",
      "Epoch 93/1500\n",
      "44/44 [==============================] - 0s 871us/step - loss: 0.3453 - accuracy: 0.8587\n",
      "Epoch 94/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.3474 - accuracy: 0.8533\n",
      "Epoch 95/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.3532 - accuracy: 0.8472\n",
      "Epoch 96/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.3488 - accuracy: 0.8504\n",
      "Epoch 97/1500\n",
      "44/44 [==============================] - 0s 873us/step - loss: 0.3537 - accuracy: 0.8537\n",
      "Epoch 98/1500\n",
      "44/44 [==============================] - 0s 870us/step - loss: 0.3445 - accuracy: 0.8551\n",
      "Epoch 99/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.3481 - accuracy: 0.8497\n",
      "Epoch 100/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.3435 - accuracy: 0.8631\n",
      "Epoch 101/1500\n",
      "44/44 [==============================] - 0s 813us/step - loss: 0.3425 - accuracy: 0.8526\n",
      "Epoch 102/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.3336 - accuracy: 0.8627\n",
      "Epoch 103/1500\n",
      "44/44 [==============================] - 0s 894us/step - loss: 0.3356 - accuracy: 0.8595\n",
      "Epoch 104/1500\n",
      "44/44 [==============================] - 0s 879us/step - loss: 0.3470 - accuracy: 0.8573\n",
      "Epoch 105/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.3320 - accuracy: 0.8714\n",
      "Epoch 106/1500\n",
      "44/44 [==============================] - 0s 857us/step - loss: 0.3515 - accuracy: 0.8544\n",
      "Epoch 107/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.3278 - accuracy: 0.8627\n",
      "Epoch 108/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.3289 - accuracy: 0.8667\n",
      "Epoch 109/1500\n",
      "44/44 [==============================] - 0s 809us/step - loss: 0.3377 - accuracy: 0.8591\n",
      "Epoch 110/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.3402 - accuracy: 0.8591\n",
      "Epoch 111/1500\n",
      "44/44 [==============================] - 0s 884us/step - loss: 0.3136 - accuracy: 0.8769\n",
      "Epoch 112/1500\n",
      "44/44 [==============================] - 0s 866us/step - loss: 0.3229 - accuracy: 0.8718\n",
      "Epoch 113/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.3169 - accuracy: 0.8732\n",
      "Epoch 114/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.3186 - accuracy: 0.8664\n",
      "Epoch 115/1500\n",
      "44/44 [==============================] - 0s 816us/step - loss: 0.3258 - accuracy: 0.8656\n",
      "Epoch 116/1500\n",
      "44/44 [==============================] - 0s 863us/step - loss: 0.3312 - accuracy: 0.8638\n",
      "Epoch 117/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.3181 - accuracy: 0.8696\n",
      "Epoch 118/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.3248 - accuracy: 0.8595\n",
      "Epoch 119/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.3367 - accuracy: 0.8595\n",
      "Epoch 120/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.3267 - accuracy: 0.8638\n",
      "Epoch 121/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.3121 - accuracy: 0.8693\n",
      "Epoch 122/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.3163 - accuracy: 0.8656\n",
      "Epoch 123/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.3198 - accuracy: 0.8653\n",
      "Epoch 124/1500\n",
      "44/44 [==============================] - 0s 865us/step - loss: 0.3034 - accuracy: 0.8736\n",
      "Epoch 125/1500\n",
      "44/44 [==============================] - 0s 870us/step - loss: 0.3137 - accuracy: 0.8671\n",
      "Epoch 126/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.3306 - accuracy: 0.8635\n",
      "Epoch 127/1500\n",
      "44/44 [==============================] - 0s 865us/step - loss: 0.3151 - accuracy: 0.8624\n",
      "Epoch 128/1500\n",
      "44/44 [==============================] - 0s 957us/step - loss: 0.3079 - accuracy: 0.8754\n",
      "Epoch 129/1500\n",
      "44/44 [==============================] - 0s 899us/step - loss: 0.3074 - accuracy: 0.8761\n",
      "Epoch 130/1500\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.3189 - accuracy: 0.8656\n",
      "Epoch 131/1500\n",
      "44/44 [==============================] - 0s 972us/step - loss: 0.3145 - accuracy: 0.8674\n",
      "Epoch 132/1500\n",
      "44/44 [==============================] - 0s 928us/step - loss: 0.3225 - accuracy: 0.8645\n",
      "Epoch 133/1500\n",
      "44/44 [==============================] - 0s 932us/step - loss: 0.3055 - accuracy: 0.8783\n",
      "Epoch 134/1500\n",
      "44/44 [==============================] - 0s 967us/step - loss: 0.3211 - accuracy: 0.8678\n",
      "Epoch 135/1500\n",
      "44/44 [==============================] - 0s 872us/step - loss: 0.3094 - accuracy: 0.8671\n",
      "Epoch 136/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.3172 - accuracy: 0.8711\n",
      "Epoch 137/1500\n",
      "44/44 [==============================] - 0s 816us/step - loss: 0.3103 - accuracy: 0.8740\n",
      "Epoch 138/1500\n",
      "44/44 [==============================] - 0s 840us/step - loss: 0.3023 - accuracy: 0.8754\n",
      "Epoch 139/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.3163 - accuracy: 0.8667\n",
      "Epoch 140/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.3092 - accuracy: 0.8707\n",
      "Epoch 141/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.2956 - accuracy: 0.8812\n",
      "Epoch 142/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.3072 - accuracy: 0.8754\n",
      "Epoch 143/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2958 - accuracy: 0.8743\n",
      "Epoch 144/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.3008 - accuracy: 0.8772\n",
      "Epoch 145/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.2943 - accuracy: 0.8790\n",
      "Epoch 146/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.2958 - accuracy: 0.8761\n",
      "Epoch 147/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.3062 - accuracy: 0.8758\n",
      "Epoch 148/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.3046 - accuracy: 0.8711\n",
      "Epoch 149/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.3005 - accuracy: 0.8703\n",
      "Epoch 150/1500\n",
      "44/44 [==============================] - 0s 858us/step - loss: 0.2863 - accuracy: 0.8877\n",
      "Epoch 151/1500\n",
      "44/44 [==============================] - 0s 834us/step - loss: 0.2830 - accuracy: 0.8801\n",
      "Epoch 152/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2917 - accuracy: 0.8790\n",
      "Epoch 153/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2875 - accuracy: 0.8830\n",
      "Epoch 154/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.3096 - accuracy: 0.8671\n",
      "Epoch 155/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.2864 - accuracy: 0.8874\n",
      "Epoch 156/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.2995 - accuracy: 0.8765\n",
      "Epoch 157/1500\n",
      "44/44 [==============================] - 0s 820us/step - loss: 0.2893 - accuracy: 0.8805\n",
      "Epoch 158/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.3054 - accuracy: 0.8721\n",
      "Epoch 159/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2841 - accuracy: 0.8798\n",
      "Epoch 160/1500\n",
      "44/44 [==============================] - 0s 854us/step - loss: 0.2775 - accuracy: 0.8819\n",
      "Epoch 161/1500\n",
      "44/44 [==============================] - 0s 834us/step - loss: 0.2901 - accuracy: 0.8787\n",
      "Epoch 162/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.2909 - accuracy: 0.8794\n",
      "Epoch 163/1500\n",
      "44/44 [==============================] - 0s 813us/step - loss: 0.2825 - accuracy: 0.8787\n",
      "Epoch 164/1500\n",
      "44/44 [==============================] - 0s 866us/step - loss: 0.2945 - accuracy: 0.8801\n",
      "Epoch 165/1500\n",
      "44/44 [==============================] - 0s 886us/step - loss: 0.2930 - accuracy: 0.8812\n",
      "Epoch 166/1500\n",
      "44/44 [==============================] - 0s 903us/step - loss: 0.2771 - accuracy: 0.8848\n",
      "Epoch 167/1500\n",
      "44/44 [==============================] - 0s 924us/step - loss: 0.2749 - accuracy: 0.8790\n",
      "Epoch 168/1500\n",
      "44/44 [==============================] - 0s 874us/step - loss: 0.2973 - accuracy: 0.8765\n",
      "Epoch 169/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2750 - accuracy: 0.8895\n",
      "Epoch 170/1500\n",
      "44/44 [==============================] - 0s 883us/step - loss: 0.2841 - accuracy: 0.8816\n",
      "Epoch 171/1500\n",
      "44/44 [==============================] - 0s 881us/step - loss: 0.2880 - accuracy: 0.8816\n",
      "Epoch 172/1500\n",
      "44/44 [==============================] - 0s 899us/step - loss: 0.2765 - accuracy: 0.8921\n",
      "Epoch 173/1500\n",
      "44/44 [==============================] - 0s 894us/step - loss: 0.2853 - accuracy: 0.8779\n",
      "Epoch 174/1500\n",
      "44/44 [==============================] - 0s 894us/step - loss: 0.2744 - accuracy: 0.8899\n",
      "Epoch 175/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.2894 - accuracy: 0.8855\n",
      "Epoch 176/1500\n",
      "44/44 [==============================] - 0s 865us/step - loss: 0.2931 - accuracy: 0.8845\n",
      "Epoch 177/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2802 - accuracy: 0.8892\n",
      "Epoch 178/1500\n",
      "44/44 [==============================] - 0s 827us/step - loss: 0.2816 - accuracy: 0.8848\n",
      "Epoch 179/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.2853 - accuracy: 0.8798\n",
      "Epoch 180/1500\n",
      "44/44 [==============================] - 0s 821us/step - loss: 0.2903 - accuracy: 0.8812\n",
      "Epoch 181/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2702 - accuracy: 0.8928\n",
      "Epoch 182/1500\n",
      "44/44 [==============================] - 0s 828us/step - loss: 0.2898 - accuracy: 0.8776\n",
      "Epoch 183/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2807 - accuracy: 0.8859\n",
      "Epoch 184/1500\n",
      "44/44 [==============================] - 0s 896us/step - loss: 0.2630 - accuracy: 0.8928\n",
      "Epoch 185/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.2809 - accuracy: 0.8805\n",
      "Epoch 186/1500\n",
      "44/44 [==============================] - 0s 859us/step - loss: 0.2814 - accuracy: 0.8823\n",
      "Epoch 187/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.2672 - accuracy: 0.8913\n",
      "Epoch 188/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.2816 - accuracy: 0.8888\n",
      "Epoch 189/1500\n",
      "44/44 [==============================] - 0s 785us/step - loss: 0.2695 - accuracy: 0.8953\n",
      "Epoch 190/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.2675 - accuracy: 0.8892\n",
      "Epoch 191/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.2693 - accuracy: 0.8870\n",
      "Epoch 192/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.2667 - accuracy: 0.8921\n",
      "Epoch 193/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2687 - accuracy: 0.8895\n",
      "Epoch 194/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.2597 - accuracy: 0.8932\n",
      "Epoch 195/1500\n",
      "44/44 [==============================] - 0s 900us/step - loss: 0.2644 - accuracy: 0.8855\n",
      "Epoch 196/1500\n",
      "44/44 [==============================] - 0s 895us/step - loss: 0.2619 - accuracy: 0.8924\n",
      "Epoch 197/1500\n",
      "44/44 [==============================] - 0s 893us/step - loss: 0.2612 - accuracy: 0.8957\n",
      "Epoch 198/1500\n",
      "44/44 [==============================] - 0s 859us/step - loss: 0.2680 - accuracy: 0.8899\n",
      "Epoch 199/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.2697 - accuracy: 0.8870\n",
      "Epoch 200/1500\n",
      "44/44 [==============================] - 0s 816us/step - loss: 0.2796 - accuracy: 0.8892\n",
      "Epoch 201/1500\n",
      "44/44 [==============================] - 0s 817us/step - loss: 0.2647 - accuracy: 0.8975\n",
      "Epoch 202/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2745 - accuracy: 0.8910\n",
      "Epoch 203/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.2712 - accuracy: 0.8935\n",
      "Epoch 204/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.2687 - accuracy: 0.8892\n",
      "Epoch 205/1500\n",
      "44/44 [==============================] - 0s 894us/step - loss: 0.2605 - accuracy: 0.8975\n",
      "Epoch 206/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.2580 - accuracy: 0.8950\n",
      "Epoch 207/1500\n",
      "44/44 [==============================] - 0s 797us/step - loss: 0.2635 - accuracy: 0.8939\n",
      "Epoch 208/1500\n",
      "44/44 [==============================] - 0s 773us/step - loss: 0.2582 - accuracy: 0.8910\n",
      "Epoch 209/1500\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.2598 - accuracy: 0.8924\n",
      "Epoch 210/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.2456 - accuracy: 0.9062\n",
      "Epoch 211/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.8950\n",
      "Epoch 212/1500\n",
      "44/44 [==============================] - 0s 935us/step - loss: 0.2433 - accuracy: 0.8971\n",
      "Epoch 213/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.2584 - accuracy: 0.8899\n",
      "Epoch 214/1500\n",
      "44/44 [==============================] - 0s 863us/step - loss: 0.2571 - accuracy: 0.8892\n",
      "Epoch 215/1500\n",
      "44/44 [==============================] - 0s 841us/step - loss: 0.2532 - accuracy: 0.8942\n",
      "Epoch 216/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.2674 - accuracy: 0.8924\n",
      "Epoch 217/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.2444 - accuracy: 0.8971\n",
      "Epoch 218/1500\n",
      "44/44 [==============================] - 0s 883us/step - loss: 0.2564 - accuracy: 0.8942\n",
      "Epoch 219/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.2525 - accuracy: 0.8979\n",
      "Epoch 220/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.2694 - accuracy: 0.8982\n",
      "Epoch 221/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.2581 - accuracy: 0.8935\n",
      "Epoch 222/1500\n",
      "44/44 [==============================] - 0s 895us/step - loss: 0.2482 - accuracy: 0.8982\n",
      "Epoch 223/1500\n",
      "44/44 [==============================] - 0s 932us/step - loss: 0.2430 - accuracy: 0.8993\n",
      "Epoch 224/1500\n",
      "44/44 [==============================] - 0s 917us/step - loss: 0.2419 - accuracy: 0.8989\n",
      "Epoch 225/1500\n",
      "44/44 [==============================] - 0s 880us/step - loss: 0.2404 - accuracy: 0.9022\n",
      "Epoch 226/1500\n",
      "44/44 [==============================] - 0s 946us/step - loss: 0.2596 - accuracy: 0.8968\n",
      "Epoch 227/1500\n",
      "44/44 [==============================] - 0s 921us/step - loss: 0.2452 - accuracy: 0.8989\n",
      "Epoch 228/1500\n",
      "44/44 [==============================] - 0s 900us/step - loss: 0.2452 - accuracy: 0.8986\n",
      "Epoch 229/1500\n",
      "44/44 [==============================] - 0s 827us/step - loss: 0.2383 - accuracy: 0.9008\n",
      "Epoch 230/1500\n",
      "44/44 [==============================] - 0s 819us/step - loss: 0.2529 - accuracy: 0.8957\n",
      "Epoch 231/1500\n",
      "44/44 [==============================] - 0s 808us/step - loss: 0.2423 - accuracy: 0.9069\n",
      "Epoch 232/1500\n",
      "44/44 [==============================] - 0s 805us/step - loss: 0.2411 - accuracy: 0.8997\n",
      "Epoch 233/1500\n",
      "44/44 [==============================] - 0s 783us/step - loss: 0.2410 - accuracy: 0.9026\n",
      "Epoch 234/1500\n",
      "44/44 [==============================] - 0s 886us/step - loss: 0.2284 - accuracy: 0.9062\n",
      "Epoch 235/1500\n",
      "44/44 [==============================] - 0s 909us/step - loss: 0.2308 - accuracy: 0.9145\n",
      "Epoch 236/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.2462 - accuracy: 0.9000\n",
      "Epoch 237/1500\n",
      "44/44 [==============================] - 0s 870us/step - loss: 0.2504 - accuracy: 0.8968\n",
      "Epoch 238/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2309 - accuracy: 0.9095\n",
      "Epoch 239/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2421 - accuracy: 0.9015\n",
      "Epoch 240/1500\n",
      "44/44 [==============================] - 0s 854us/step - loss: 0.2360 - accuracy: 0.9022\n",
      "Epoch 241/1500\n",
      "44/44 [==============================] - 0s 852us/step - loss: 0.2423 - accuracy: 0.9073\n",
      "Epoch 242/1500\n",
      "44/44 [==============================] - 0s 869us/step - loss: 0.2506 - accuracy: 0.8935\n",
      "Epoch 243/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.2534 - accuracy: 0.8964\n",
      "Epoch 244/1500\n",
      "44/44 [==============================] - 0s 859us/step - loss: 0.2471 - accuracy: 0.9015\n",
      "Epoch 245/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2455 - accuracy: 0.8928\n",
      "Epoch 246/1500\n",
      "44/44 [==============================] - 0s 840us/step - loss: 0.2454 - accuracy: 0.9044\n",
      "Epoch 247/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2432 - accuracy: 0.9022\n",
      "Epoch 248/1500\n",
      "44/44 [==============================] - 0s 827us/step - loss: 0.2240 - accuracy: 0.9055\n",
      "Epoch 249/1500\n",
      "44/44 [==============================] - 0s 816us/step - loss: 0.2439 - accuracy: 0.9037\n",
      "Epoch 250/1500\n",
      "44/44 [==============================] - 0s 830us/step - loss: 0.2419 - accuracy: 0.8997\n",
      "Epoch 251/1500\n",
      "44/44 [==============================] - 0s 877us/step - loss: 0.2367 - accuracy: 0.9047\n",
      "Epoch 252/1500\n",
      "44/44 [==============================] - 0s 872us/step - loss: 0.2491 - accuracy: 0.9008\n",
      "Epoch 253/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.2520 - accuracy: 0.8957\n",
      "Epoch 254/1500\n",
      "44/44 [==============================] - 0s 864us/step - loss: 0.2461 - accuracy: 0.8986\n",
      "Epoch 255/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2323 - accuracy: 0.9018\n",
      "Epoch 256/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2202 - accuracy: 0.9138\n",
      "Epoch 257/1500\n",
      "44/44 [==============================] - 0s 894us/step - loss: 0.2445 - accuracy: 0.9037\n",
      "Epoch 258/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.2422 - accuracy: 0.9062\n",
      "Epoch 259/1500\n",
      "44/44 [==============================] - 0s 808us/step - loss: 0.2255 - accuracy: 0.9044\n",
      "Epoch 260/1500\n",
      "44/44 [==============================] - 0s 824us/step - loss: 0.2299 - accuracy: 0.9047\n",
      "Epoch 261/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.2211 - accuracy: 0.9080\n",
      "Epoch 262/1500\n",
      "44/44 [==============================] - 0s 802us/step - loss: 0.2447 - accuracy: 0.8946\n",
      "Epoch 263/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.2333 - accuracy: 0.9080\n",
      "Epoch 264/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2320 - accuracy: 0.9080\n",
      "Epoch 265/1500\n",
      "44/44 [==============================] - 0s 857us/step - loss: 0.2174 - accuracy: 0.9138\n",
      "Epoch 266/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.2357 - accuracy: 0.9091\n",
      "Epoch 267/1500\n",
      "44/44 [==============================] - 0s 905us/step - loss: 0.2291 - accuracy: 0.9062\n",
      "Epoch 268/1500\n",
      "44/44 [==============================] - 0s 898us/step - loss: 0.2277 - accuracy: 0.9127\n",
      "Epoch 269/1500\n",
      "44/44 [==============================] - 0s 907us/step - loss: 0.2320 - accuracy: 0.9098\n",
      "Epoch 270/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.2342 - accuracy: 0.9127\n",
      "Epoch 271/1500\n",
      "44/44 [==============================] - 0s 930us/step - loss: 0.2225 - accuracy: 0.9051\n",
      "Epoch 272/1500\n",
      "44/44 [==============================] - 0s 917us/step - loss: 0.2189 - accuracy: 0.9080\n",
      "Epoch 273/1500\n",
      "44/44 [==============================] - 0s 924us/step - loss: 0.2293 - accuracy: 0.9120\n",
      "Epoch 274/1500\n",
      "44/44 [==============================] - 0s 909us/step - loss: 0.2389 - accuracy: 0.9022\n",
      "Epoch 275/1500\n",
      "44/44 [==============================] - 0s 896us/step - loss: 0.2217 - accuracy: 0.9145\n",
      "Epoch 276/1500\n",
      "44/44 [==============================] - 0s 854us/step - loss: 0.2273 - accuracy: 0.9051\n",
      "Epoch 277/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2340 - accuracy: 0.9047\n",
      "Epoch 278/1500\n",
      "44/44 [==============================] - 0s 883us/step - loss: 0.2265 - accuracy: 0.9124\n",
      "Epoch 279/1500\n",
      "44/44 [==============================] - 0s 894us/step - loss: 0.2150 - accuracy: 0.9084\n",
      "Epoch 280/1500\n",
      "44/44 [==============================] - 0s 852us/step - loss: 0.2190 - accuracy: 0.9145\n",
      "Epoch 281/1500\n",
      "44/44 [==============================] - 0s 899us/step - loss: 0.2378 - accuracy: 0.9040\n",
      "Epoch 282/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.2298 - accuracy: 0.9091\n",
      "Epoch 283/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.2093 - accuracy: 0.9120\n",
      "Epoch 284/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.2232 - accuracy: 0.9091\n",
      "Epoch 285/1500\n",
      "44/44 [==============================] - 0s 817us/step - loss: 0.2250 - accuracy: 0.9098\n",
      "Epoch 286/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.2327 - accuracy: 0.9087\n",
      "Epoch 287/1500\n",
      "44/44 [==============================] - 0s 800us/step - loss: 0.2335 - accuracy: 0.9040\n",
      "Epoch 288/1500\n",
      "44/44 [==============================] - 0s 825us/step - loss: 0.2206 - accuracy: 0.9163\n",
      "Epoch 289/1500\n",
      "44/44 [==============================] - 0s 822us/step - loss: 0.2174 - accuracy: 0.9098\n",
      "Epoch 290/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2382 - accuracy: 0.9058\n",
      "Epoch 291/1500\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9131\n",
      "Epoch 292/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.9134\n",
      "Epoch 293/1500\n",
      "44/44 [==============================] - 0s 863us/step - loss: 0.2249 - accuracy: 0.9091\n",
      "Epoch 294/1500\n",
      "44/44 [==============================] - 0s 857us/step - loss: 0.2225 - accuracy: 0.9095\n",
      "Epoch 295/1500\n",
      "44/44 [==============================] - 0s 880us/step - loss: 0.2279 - accuracy: 0.9098\n",
      "Epoch 296/1500\n",
      "44/44 [==============================] - 0s 905us/step - loss: 0.2183 - accuracy: 0.9120\n",
      "Epoch 297/1500\n",
      "44/44 [==============================] - 0s 819us/step - loss: 0.2230 - accuracy: 0.9109\n",
      "Epoch 298/1500\n",
      "44/44 [==============================] - 0s 796us/step - loss: 0.2172 - accuracy: 0.9127\n",
      "Epoch 299/1500\n",
      "44/44 [==============================] - 0s 800us/step - loss: 0.2010 - accuracy: 0.9196\n",
      "Epoch 300/1500\n",
      "44/44 [==============================] - 0s 857us/step - loss: 0.2208 - accuracy: 0.9149\n",
      "Epoch 301/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.2101 - accuracy: 0.9171\n",
      "Epoch 302/1500\n",
      "44/44 [==============================] - 0s 830us/step - loss: 0.2165 - accuracy: 0.9167\n",
      "Epoch 303/1500\n",
      "44/44 [==============================] - 0s 828us/step - loss: 0.2180 - accuracy: 0.9120\n",
      "Epoch 304/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.2219 - accuracy: 0.9098\n",
      "Epoch 305/1500\n",
      "44/44 [==============================] - 0s 804us/step - loss: 0.2045 - accuracy: 0.9178\n",
      "Epoch 306/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.2131 - accuracy: 0.9189\n",
      "Epoch 307/1500\n",
      "44/44 [==============================] - 0s 822us/step - loss: 0.2163 - accuracy: 0.9120\n",
      "Epoch 308/1500\n",
      "44/44 [==============================] - 0s 812us/step - loss: 0.2152 - accuracy: 0.9218\n",
      "Epoch 309/1500\n",
      "44/44 [==============================] - 0s 828us/step - loss: 0.2213 - accuracy: 0.9116\n",
      "Epoch 310/1500\n",
      "44/44 [==============================] - 0s 791us/step - loss: 0.2171 - accuracy: 0.9116\n",
      "Epoch 311/1500\n",
      "44/44 [==============================] - 0s 858us/step - loss: 0.2152 - accuracy: 0.9073\n",
      "Epoch 312/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.2125 - accuracy: 0.9120\n",
      "Epoch 313/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.2079 - accuracy: 0.9218\n",
      "Epoch 314/1500\n",
      "44/44 [==============================] - 0s 816us/step - loss: 0.2075 - accuracy: 0.9203\n",
      "Epoch 315/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.1954 - accuracy: 0.9178\n",
      "Epoch 316/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.2124 - accuracy: 0.9120\n",
      "Epoch 317/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2110 - accuracy: 0.9142\n",
      "Epoch 318/1500\n",
      "44/44 [==============================] - 0s 822us/step - loss: 0.2034 - accuracy: 0.9185\n",
      "Epoch 319/1500\n",
      "44/44 [==============================] - 0s 780us/step - loss: 0.2096 - accuracy: 0.9149\n",
      "Epoch 320/1500\n",
      "44/44 [==============================] - 0s 799us/step - loss: 0.2203 - accuracy: 0.9102\n",
      "Epoch 321/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.2058 - accuracy: 0.9200\n",
      "Epoch 322/1500\n",
      "44/44 [==============================] - 0s 788us/step - loss: 0.2108 - accuracy: 0.9218\n",
      "Epoch 323/1500\n",
      "44/44 [==============================] - 0s 811us/step - loss: 0.2029 - accuracy: 0.9247\n",
      "Epoch 324/1500\n",
      "44/44 [==============================] - 0s 815us/step - loss: 0.2146 - accuracy: 0.9149\n",
      "Epoch 325/1500\n",
      "44/44 [==============================] - 0s 777us/step - loss: 0.2163 - accuracy: 0.9116\n",
      "Epoch 326/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2106 - accuracy: 0.9124\n",
      "Epoch 327/1500\n",
      "44/44 [==============================] - 0s 760us/step - loss: 0.2073 - accuracy: 0.9254\n",
      "Epoch 328/1500\n",
      "44/44 [==============================] - 0s 793us/step - loss: 0.1998 - accuracy: 0.9174\n",
      "Epoch 329/1500\n",
      "44/44 [==============================] - 0s 812us/step - loss: 0.2164 - accuracy: 0.9073\n",
      "Epoch 330/1500\n",
      "44/44 [==============================] - 0s 804us/step - loss: 0.2051 - accuracy: 0.9185\n",
      "Epoch 331/1500\n",
      "44/44 [==============================] - 0s 791us/step - loss: 0.1984 - accuracy: 0.9171\n",
      "Epoch 332/1500\n",
      "44/44 [==============================] - 0s 803us/step - loss: 0.2025 - accuracy: 0.9174\n",
      "Epoch 333/1500\n",
      "44/44 [==============================] - 0s 818us/step - loss: 0.2051 - accuracy: 0.9207\n",
      "Epoch 334/1500\n",
      "44/44 [==============================] - 0s 795us/step - loss: 0.2106 - accuracy: 0.9214\n",
      "Epoch 335/1500\n",
      "44/44 [==============================] - 0s 819us/step - loss: 0.2284 - accuracy: 0.9066\n",
      "Epoch 336/1500\n",
      "44/44 [==============================] - 0s 818us/step - loss: 0.2059 - accuracy: 0.9167\n",
      "Epoch 337/1500\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.1963 - accuracy: 0.9239\n",
      "Epoch 338/1500\n",
      "44/44 [==============================] - 0s 827us/step - loss: 0.1921 - accuracy: 0.9214\n",
      "Epoch 339/1500\n",
      "44/44 [==============================] - 0s 791us/step - loss: 0.2076 - accuracy: 0.9178\n",
      "Epoch 340/1500\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.2062 - accuracy: 0.9178\n",
      "Epoch 341/1500\n",
      "44/44 [==============================] - 0s 815us/step - loss: 0.2135 - accuracy: 0.9134\n",
      "Epoch 342/1500\n",
      "44/44 [==============================] - 0s 824us/step - loss: 0.1984 - accuracy: 0.9207\n",
      "Epoch 343/1500\n",
      "44/44 [==============================] - 0s 803us/step - loss: 0.1957 - accuracy: 0.9203\n",
      "Epoch 344/1500\n",
      "44/44 [==============================] - 0s 773us/step - loss: 0.2069 - accuracy: 0.9185\n",
      "Epoch 345/1500\n",
      "44/44 [==============================] - 0s 809us/step - loss: 0.2019 - accuracy: 0.9167\n",
      "Epoch 346/1500\n",
      "44/44 [==============================] - 0s 804us/step - loss: 0.2009 - accuracy: 0.9239\n",
      "Epoch 347/1500\n",
      "44/44 [==============================] - 0s 802us/step - loss: 0.1914 - accuracy: 0.9261\n",
      "Epoch 348/1500\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.1949 - accuracy: 0.9236\n",
      "Epoch 349/1500\n",
      "44/44 [==============================] - 0s 816us/step - loss: 0.2101 - accuracy: 0.9214\n",
      "Epoch 350/1500\n",
      "44/44 [==============================] - 0s 815us/step - loss: 0.1967 - accuracy: 0.9174\n",
      "Epoch 351/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2044 - accuracy: 0.9149\n",
      "Epoch 352/1500\n",
      "44/44 [==============================] - 0s 804us/step - loss: 0.2011 - accuracy: 0.9239\n",
      "Epoch 353/1500\n",
      "44/44 [==============================] - 0s 812us/step - loss: 0.1953 - accuracy: 0.9221\n",
      "Epoch 354/1500\n",
      "44/44 [==============================] - 0s 802us/step - loss: 0.2046 - accuracy: 0.9210\n",
      "Epoch 355/1500\n",
      "44/44 [==============================] - 0s 806us/step - loss: 0.2014 - accuracy: 0.9185\n",
      "Epoch 356/1500\n",
      "44/44 [==============================] - 0s 816us/step - loss: 0.2159 - accuracy: 0.9095\n",
      "Epoch 357/1500\n",
      "44/44 [==============================] - 0s 794us/step - loss: 0.1974 - accuracy: 0.9232\n",
      "Epoch 358/1500\n",
      "44/44 [==============================] - 0s 799us/step - loss: 0.1854 - accuracy: 0.9290\n",
      "Epoch 359/1500\n",
      "44/44 [==============================] - 0s 812us/step - loss: 0.1843 - accuracy: 0.9243\n",
      "Epoch 360/1500\n",
      "44/44 [==============================] - 0s 778us/step - loss: 0.1897 - accuracy: 0.9268\n",
      "Epoch 361/1500\n",
      "44/44 [==============================] - 0s 791us/step - loss: 0.2013 - accuracy: 0.9203\n",
      "Epoch 362/1500\n",
      "44/44 [==============================] - 0s 786us/step - loss: 0.2069 - accuracy: 0.9152\n",
      "Epoch 363/1500\n",
      "44/44 [==============================] - 0s 759us/step - loss: 0.2099 - accuracy: 0.9192\n",
      "Epoch 364/1500\n",
      "44/44 [==============================] - 0s 763us/step - loss: 0.1948 - accuracy: 0.9290\n",
      "Epoch 365/1500\n",
      "44/44 [==============================] - 0s 759us/step - loss: 0.2154 - accuracy: 0.9145\n",
      "Epoch 366/1500\n",
      "44/44 [==============================] - 0s 761us/step - loss: 0.2002 - accuracy: 0.9247\n",
      "Epoch 367/1500\n",
      "44/44 [==============================] - 0s 795us/step - loss: 0.1840 - accuracy: 0.9305\n",
      "Epoch 368/1500\n",
      "44/44 [==============================] - 0s 774us/step - loss: 0.1829 - accuracy: 0.9308\n",
      "Epoch 369/1500\n",
      "44/44 [==============================] - 0s 825us/step - loss: 0.1964 - accuracy: 0.9236\n",
      "Epoch 370/1500\n",
      "44/44 [==============================] - 0s 801us/step - loss: 0.2157 - accuracy: 0.9145\n",
      "Epoch 371/1500\n",
      "44/44 [==============================] - 0s 758us/step - loss: 0.2046 - accuracy: 0.9171\n",
      "Epoch 372/1500\n",
      "44/44 [==============================] - 0s 736us/step - loss: 0.1878 - accuracy: 0.9279\n",
      "Epoch 373/1500\n",
      "44/44 [==============================] - 0s 765us/step - loss: 0.2018 - accuracy: 0.9163\n",
      "Epoch 374/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.1972 - accuracy: 0.9160\n",
      "Epoch 375/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.1968 - accuracy: 0.9243\n",
      "Epoch 376/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.1911 - accuracy: 0.9218\n",
      "Epoch 377/1500\n",
      "44/44 [==============================] - 0s 785us/step - loss: 0.1936 - accuracy: 0.9196\n",
      "Epoch 378/1500\n",
      "44/44 [==============================] - 0s 814us/step - loss: 0.1793 - accuracy: 0.9330\n",
      "Epoch 379/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.1973 - accuracy: 0.9192\n",
      "Epoch 380/1500\n",
      "44/44 [==============================] - 0s 830us/step - loss: 0.1823 - accuracy: 0.9319\n",
      "Epoch 381/1500\n",
      "44/44 [==============================] - 0s 786us/step - loss: 0.1919 - accuracy: 0.9265\n",
      "Epoch 382/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.1802 - accuracy: 0.9254\n",
      "Epoch 383/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.1923 - accuracy: 0.9272\n",
      "Epoch 384/1500\n",
      "44/44 [==============================] - 0s 788us/step - loss: 0.2034 - accuracy: 0.9185\n",
      "Epoch 385/1500\n",
      "44/44 [==============================] - 0s 812us/step - loss: 0.1951 - accuracy: 0.9236\n",
      "Epoch 386/1500\n",
      "44/44 [==============================] - 0s 821us/step - loss: 0.1824 - accuracy: 0.9330\n",
      "Epoch 387/1500\n",
      "44/44 [==============================] - 0s 803us/step - loss: 0.1809 - accuracy: 0.9308\n",
      "Epoch 388/1500\n",
      "44/44 [==============================] - 0s 806us/step - loss: 0.2027 - accuracy: 0.9192\n",
      "Epoch 389/1500\n",
      "44/44 [==============================] - 0s 815us/step - loss: 0.1879 - accuracy: 0.9294\n",
      "Epoch 390/1500\n",
      "44/44 [==============================] - 0s 817us/step - loss: 0.1925 - accuracy: 0.9210\n",
      "Epoch 391/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.1813 - accuracy: 0.9305\n",
      "Epoch 392/1500\n",
      "44/44 [==============================] - 0s 799us/step - loss: 0.1950 - accuracy: 0.9236\n",
      "Epoch 393/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.1743 - accuracy: 0.9334\n",
      "Epoch 394/1500\n",
      "44/44 [==============================] - 0s 808us/step - loss: 0.1935 - accuracy: 0.9192\n",
      "Epoch 395/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.1898 - accuracy: 0.9239\n",
      "Epoch 396/1500\n",
      "44/44 [==============================] - 0s 817us/step - loss: 0.1764 - accuracy: 0.9341\n",
      "Epoch 397/1500\n",
      "44/44 [==============================] - 0s 779us/step - loss: 0.1925 - accuracy: 0.9229\n",
      "Epoch 398/1500\n",
      "44/44 [==============================] - 0s 801us/step - loss: 0.1861 - accuracy: 0.9229\n",
      "Epoch 399/1500\n",
      "44/44 [==============================] - 0s 847us/step - loss: 0.1881 - accuracy: 0.9236\n",
      "Epoch 400/1500\n",
      "44/44 [==============================] - 0s 834us/step - loss: 0.1924 - accuracy: 0.9247\n",
      "Epoch 401/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.1798 - accuracy: 0.9305\n",
      "Epoch 402/1500\n",
      "44/44 [==============================] - 0s 810us/step - loss: 0.1899 - accuracy: 0.9203\n",
      "Epoch 403/1500\n",
      "44/44 [==============================] - 0s 799us/step - loss: 0.1838 - accuracy: 0.9312\n",
      "Epoch 404/1500\n",
      "44/44 [==============================] - 0s 818us/step - loss: 0.1973 - accuracy: 0.9221\n",
      "Epoch 405/1500\n",
      "44/44 [==============================] - 0s 815us/step - loss: 0.2033 - accuracy: 0.9113\n",
      "Epoch 406/1500\n",
      "44/44 [==============================] - 0s 785us/step - loss: 0.1941 - accuracy: 0.9218\n",
      "Epoch 407/1500\n",
      "44/44 [==============================] - 0s 808us/step - loss: 0.1868 - accuracy: 0.9218\n",
      "Epoch 408/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.1884 - accuracy: 0.9272\n",
      "Epoch 409/1500\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9308\n",
      "Epoch 410/1500\n",
      "44/44 [==============================] - 0s 822us/step - loss: 0.1907 - accuracy: 0.9283\n",
      "Epoch 411/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.1983 - accuracy: 0.9221\n",
      "Epoch 412/1500\n",
      "44/44 [==============================] - 0s 797us/step - loss: 0.1804 - accuracy: 0.9286\n",
      "Epoch 413/1500\n",
      "44/44 [==============================] - 0s 821us/step - loss: 0.1875 - accuracy: 0.9283\n",
      "Epoch 414/1500\n",
      "44/44 [==============================] - 0s 818us/step - loss: 0.1911 - accuracy: 0.9261\n",
      "Epoch 415/1500\n",
      "44/44 [==============================] - 0s 797us/step - loss: 0.1820 - accuracy: 0.9265\n",
      "Epoch 416/1500\n",
      "44/44 [==============================] - 0s 818us/step - loss: 0.1888 - accuracy: 0.9294\n",
      "Epoch 417/1500\n",
      "44/44 [==============================] - 0s 795us/step - loss: 0.1782 - accuracy: 0.9279\n",
      "Epoch 418/1500\n",
      "44/44 [==============================] - 0s 820us/step - loss: 0.1868 - accuracy: 0.9254\n",
      "Epoch 419/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.1915 - accuracy: 0.9250\n",
      "Epoch 420/1500\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.1850 - accuracy: 0.9239\n",
      "Epoch 421/1500\n",
      "44/44 [==============================] - 0s 808us/step - loss: 0.1831 - accuracy: 0.9276\n",
      "Epoch 422/1500\n",
      "44/44 [==============================] - 0s 797us/step - loss: 0.1888 - accuracy: 0.9276\n",
      "Epoch 423/1500\n",
      " 1/44 [..............................] - ETA: 0s - loss: 0.5437 - accuracy: 0.8438Restoring model weights from the end of the best epoch: 393.\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.1894 - accuracy: 0.9232\n",
      "Epoch 423: early stopping\n",
      "8/8 [==============================] - 0s 775us/step - loss: 0.7942 - accuracy: 0.6975\n",
      "8/8 [==============================] - 0s 643us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (24/29)\n",
      "Before appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "After appending - Cat IDs: 850, Predictions: 850, Actuals: 850, Gender: 850\n",
      "Final Test Results - Loss: 0.7941751480102539, Accuracy: 0.6974790096282959, Precision: 0.692588955594736, Recall: 0.6660512184109832, F1 Score: 0.6701710083289031\n",
      "Confusion Matrix:\n",
      " [[124   2  21]\n",
      " [  3  26   0]\n",
      " [ 46   0  16]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.714648612579068\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.723781555891037\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.748233750462532\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7237089739928014\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7231698192319154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 850, Predictions: 850, Actuals: 850, Gender: 850\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.80 (88/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, kitten, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, adult...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, adult, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[kitten, kitten, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, kitten, adult, adult, senior, ...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "68    062A                     [kitten, kitten, adult, adult]         adult            adult                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "56    051A  [adult, senior, adult, kitten, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, adult...        kitten           kitten                   True\n",
       "51    045A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [adult, senior, senior, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                    [senior, senior, adult, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, senior, adult, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "50    044A             [kitten, adult, kitten, adult, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [kitten, kitten, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, senior, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "4     003A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                     [senior, adult, kitten, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "36    029A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, adult,...        senior            adult                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, se...         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, adult, adult, adult, sen...         adult           senior                  False\n",
       "90    095A  [senior, senior, adult, senior, senior, senior...        senior            adult                  False\n",
       "18    016A  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "38    032A                                   [kitten, kitten]        kitten            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "65    059A  [senior, senior, senior, adult, adult, adult, ...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     63\n",
      "kitten    14\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             63  86.301370\n",
      "1           kitten           15             14  93.333333\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            448  80.720721\n",
      "1           kitten          117            103  88.034188\n",
      "2           senior          178             81  45.505618\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      152     71.36\n",
      "1          M    343      262     76.38\n",
      "2          X    294      218     74.15\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      152     71.36\n",
      "1          M    343      262     76.38\n",
      "2          X    294      218     74.15\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56cf220c-fe8f-42aa-92e6-d4973d768469",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1139, 1: 1115, 2: 1059})\n",
      "Epoch 1/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.1103 - accuracy: 0.5210\n",
      "Epoch 2/1500\n",
      "52/52 [==============================] - 0s 934us/step - loss: 0.8640 - accuracy: 0.6212\n",
      "Epoch 3/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.7668 - accuracy: 0.6610\n",
      "Epoch 4/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.7314 - accuracy: 0.6797\n",
      "Epoch 5/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.6930 - accuracy: 0.6979\n",
      "Epoch 6/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.6770 - accuracy: 0.7099\n",
      "Epoch 7/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.6494 - accuracy: 0.7142\n",
      "Epoch 8/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.6189 - accuracy: 0.7332\n",
      "Epoch 9/1500\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.6296 - accuracy: 0.7259\n",
      "Epoch 10/1500\n",
      "52/52 [==============================] - 0s 914us/step - loss: 0.5818 - accuracy: 0.7437\n",
      "Epoch 11/1500\n",
      "52/52 [==============================] - 0s 899us/step - loss: 0.5836 - accuracy: 0.7477\n",
      "Epoch 12/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.5936 - accuracy: 0.7326\n",
      "Epoch 13/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.5746 - accuracy: 0.7561\n",
      "Epoch 14/1500\n",
      "52/52 [==============================] - 0s 920us/step - loss: 0.5581 - accuracy: 0.7609\n",
      "Epoch 15/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.5592 - accuracy: 0.7643\n",
      "Epoch 16/1500\n",
      "52/52 [==============================] - 0s 943us/step - loss: 0.5484 - accuracy: 0.7540\n",
      "Epoch 17/1500\n",
      "52/52 [==============================] - 0s 906us/step - loss: 0.5366 - accuracy: 0.7679\n",
      "Epoch 18/1500\n",
      "52/52 [==============================] - 0s 916us/step - loss: 0.5358 - accuracy: 0.7721\n",
      "Epoch 19/1500\n",
      "52/52 [==============================] - 0s 897us/step - loss: 0.5245 - accuracy: 0.7715\n",
      "Epoch 20/1500\n",
      "52/52 [==============================] - 0s 896us/step - loss: 0.5302 - accuracy: 0.7718\n",
      "Epoch 21/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.5164 - accuracy: 0.7809\n",
      "Epoch 22/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.5049 - accuracy: 0.7827\n",
      "Epoch 23/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.5066 - accuracy: 0.7824\n",
      "Epoch 24/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.5002 - accuracy: 0.7809\n",
      "Epoch 25/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.4834 - accuracy: 0.7905\n",
      "Epoch 26/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.4832 - accuracy: 0.7966\n",
      "Epoch 27/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4947 - accuracy: 0.7896\n",
      "Epoch 28/1500\n",
      "52/52 [==============================] - 0s 934us/step - loss: 0.4926 - accuracy: 0.7851\n",
      "Epoch 29/1500\n",
      "52/52 [==============================] - 0s 931us/step - loss: 0.4766 - accuracy: 0.7911\n",
      "Epoch 30/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.4771 - accuracy: 0.7990\n",
      "Epoch 31/1500\n",
      "52/52 [==============================] - 0s 899us/step - loss: 0.4737 - accuracy: 0.7975\n",
      "Epoch 32/1500\n",
      "52/52 [==============================] - 0s 927us/step - loss: 0.4692 - accuracy: 0.7966\n",
      "Epoch 33/1500\n",
      "52/52 [==============================] - 0s 886us/step - loss: 0.4633 - accuracy: 0.7969\n",
      "Epoch 34/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.4676 - accuracy: 0.7993\n",
      "Epoch 35/1500\n",
      "52/52 [==============================] - 0s 933us/step - loss: 0.4665 - accuracy: 0.7996\n",
      "Epoch 36/1500\n",
      "52/52 [==============================] - 0s 947us/step - loss: 0.4542 - accuracy: 0.8056\n",
      "Epoch 37/1500\n",
      "52/52 [==============================] - 0s 907us/step - loss: 0.4523 - accuracy: 0.8092\n",
      "Epoch 38/1500\n",
      "52/52 [==============================] - 0s 946us/step - loss: 0.4486 - accuracy: 0.8035\n",
      "Epoch 39/1500\n",
      "52/52 [==============================] - 0s 929us/step - loss: 0.4580 - accuracy: 0.8044\n",
      "Epoch 40/1500\n",
      "52/52 [==============================] - 0s 926us/step - loss: 0.4417 - accuracy: 0.8204\n",
      "Epoch 41/1500\n",
      "52/52 [==============================] - 0s 905us/step - loss: 0.4505 - accuracy: 0.8101\n",
      "Epoch 42/1500\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.8135\n",
      "Epoch 43/1500\n",
      "52/52 [==============================] - 0s 868us/step - loss: 0.4482 - accuracy: 0.8050\n",
      "Epoch 44/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.4342 - accuracy: 0.8129\n",
      "Epoch 45/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.4291 - accuracy: 0.8117\n",
      "Epoch 46/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.4197 - accuracy: 0.8195\n",
      "Epoch 47/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.4399 - accuracy: 0.8135\n",
      "Epoch 48/1500\n",
      "52/52 [==============================] - 0s 942us/step - loss: 0.4114 - accuracy: 0.8237\n",
      "Epoch 49/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.4271 - accuracy: 0.8156\n",
      "Epoch 50/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.4461 - accuracy: 0.8113\n",
      "Epoch 51/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.4314 - accuracy: 0.8171\n",
      "Epoch 52/1500\n",
      "52/52 [==============================] - 0s 942us/step - loss: 0.4081 - accuracy: 0.8252\n",
      "Epoch 53/1500\n",
      "52/52 [==============================] - 0s 899us/step - loss: 0.4188 - accuracy: 0.8198\n",
      "Epoch 54/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.4199 - accuracy: 0.8204\n",
      "Epoch 55/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.4036 - accuracy: 0.8255\n",
      "Epoch 56/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.4113 - accuracy: 0.8234\n",
      "Epoch 57/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.4072 - accuracy: 0.8237\n",
      "Epoch 58/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.4034 - accuracy: 0.8301\n",
      "Epoch 59/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.3932 - accuracy: 0.8337\n",
      "Epoch 60/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.3938 - accuracy: 0.8283\n",
      "Epoch 61/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.3896 - accuracy: 0.8388\n",
      "Epoch 62/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.3971 - accuracy: 0.8301\n",
      "Epoch 63/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.4011 - accuracy: 0.8280\n",
      "Epoch 64/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.3887 - accuracy: 0.8391\n",
      "Epoch 65/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.3840 - accuracy: 0.8331\n",
      "Epoch 66/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.4016 - accuracy: 0.8313\n",
      "Epoch 67/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.3868 - accuracy: 0.8397\n",
      "Epoch 68/1500\n",
      "52/52 [==============================] - 0s 994us/step - loss: 0.3819 - accuracy: 0.8376\n",
      "Epoch 69/1500\n",
      "52/52 [==============================] - 0s 875us/step - loss: 0.3850 - accuracy: 0.8403\n",
      "Epoch 70/1500\n",
      "52/52 [==============================] - 0s 879us/step - loss: 0.3919 - accuracy: 0.8334\n",
      "Epoch 71/1500\n",
      "52/52 [==============================] - 0s 924us/step - loss: 0.3834 - accuracy: 0.8403\n",
      "Epoch 72/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3809 - accuracy: 0.8436\n",
      "Epoch 73/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8376\n",
      "Epoch 74/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8439\n",
      "Epoch 75/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3730 - accuracy: 0.8418\n",
      "Epoch 76/1500\n",
      "52/52 [==============================] - 0s 999us/step - loss: 0.3689 - accuracy: 0.8424\n",
      "Epoch 77/1500\n",
      "52/52 [==============================] - 0s 898us/step - loss: 0.3833 - accuracy: 0.8370\n",
      "Epoch 78/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.3816 - accuracy: 0.8391\n",
      "Epoch 79/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.3758 - accuracy: 0.8388\n",
      "Epoch 80/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.3685 - accuracy: 0.8458\n",
      "Epoch 81/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.3628 - accuracy: 0.8521\n",
      "Epoch 82/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.3683 - accuracy: 0.8494\n",
      "Epoch 83/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8509\n",
      "Epoch 84/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8473\n",
      "Epoch 85/1500\n",
      "52/52 [==============================] - 0s 965us/step - loss: 0.3608 - accuracy: 0.8467\n",
      "Epoch 86/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8461\n",
      "Epoch 87/1500\n",
      "52/52 [==============================] - 0s 991us/step - loss: 0.3584 - accuracy: 0.8512\n",
      "Epoch 88/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.8500\n",
      "Epoch 89/1500\n",
      "52/52 [==============================] - 0s 899us/step - loss: 0.3622 - accuracy: 0.8530\n",
      "Epoch 90/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.3644 - accuracy: 0.8473\n",
      "Epoch 91/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.3532 - accuracy: 0.8551\n",
      "Epoch 92/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.3513 - accuracy: 0.8560\n",
      "Epoch 93/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.3503 - accuracy: 0.8524\n",
      "Epoch 94/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.3511 - accuracy: 0.8557\n",
      "Epoch 95/1500\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.3625 - accuracy: 0.8524\n",
      "Epoch 96/1500\n",
      "52/52 [==============================] - 0s 964us/step - loss: 0.3513 - accuracy: 0.8497\n",
      "Epoch 97/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.3353 - accuracy: 0.8636\n",
      "Epoch 98/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.3374 - accuracy: 0.8602\n",
      "Epoch 99/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.3448 - accuracy: 0.8548\n",
      "Epoch 100/1500\n",
      "52/52 [==============================] - 0s 890us/step - loss: 0.3444 - accuracy: 0.8557\n",
      "Epoch 101/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.3399 - accuracy: 0.8599\n",
      "Epoch 102/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.3470 - accuracy: 0.8593\n",
      "Epoch 103/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.3442 - accuracy: 0.8530\n",
      "Epoch 104/1500\n",
      "52/52 [==============================] - 0s 874us/step - loss: 0.3392 - accuracy: 0.8542\n",
      "Epoch 105/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.3317 - accuracy: 0.8672\n",
      "Epoch 106/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.3505 - accuracy: 0.8572\n",
      "Epoch 107/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.3279 - accuracy: 0.8618\n",
      "Epoch 108/1500\n",
      "52/52 [==============================] - 0s 875us/step - loss: 0.3247 - accuracy: 0.8651\n",
      "Epoch 109/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.3317 - accuracy: 0.8599\n",
      "Epoch 110/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.3397 - accuracy: 0.8596\n",
      "Epoch 111/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.3274 - accuracy: 0.8669\n",
      "Epoch 112/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.3346 - accuracy: 0.8572\n",
      "Epoch 113/1500\n",
      "52/52 [==============================] - 0s 951us/step - loss: 0.3281 - accuracy: 0.8642\n",
      "Epoch 114/1500\n",
      "52/52 [==============================] - 0s 887us/step - loss: 0.3309 - accuracy: 0.8584\n",
      "Epoch 115/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.3224 - accuracy: 0.8669\n",
      "Epoch 116/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.3246 - accuracy: 0.8675\n",
      "Epoch 117/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.3304 - accuracy: 0.8648\n",
      "Epoch 118/1500\n",
      "52/52 [==============================] - 0s 884us/step - loss: 0.3234 - accuracy: 0.8696\n",
      "Epoch 119/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8663\n",
      "Epoch 120/1500\n",
      "52/52 [==============================] - 0s 985us/step - loss: 0.3184 - accuracy: 0.8705\n",
      "Epoch 121/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.3273 - accuracy: 0.8708\n",
      "Epoch 122/1500\n",
      "52/52 [==============================] - 0s 914us/step - loss: 0.3143 - accuracy: 0.8768\n",
      "Epoch 123/1500\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.3256 - accuracy: 0.8669\n",
      "Epoch 124/1500\n",
      "52/52 [==============================] - 0s 907us/step - loss: 0.3209 - accuracy: 0.8639\n",
      "Epoch 125/1500\n",
      "52/52 [==============================] - 0s 906us/step - loss: 0.3135 - accuracy: 0.8660\n",
      "Epoch 126/1500\n",
      "52/52 [==============================] - 0s 953us/step - loss: 0.3050 - accuracy: 0.8687\n",
      "Epoch 127/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.3288 - accuracy: 0.8627\n",
      "Epoch 128/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8672\n",
      "Epoch 129/1500\n",
      "52/52 [==============================] - 0s 901us/step - loss: 0.3222 - accuracy: 0.8672\n",
      "Epoch 130/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.3027 - accuracy: 0.8823\n",
      "Epoch 131/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.3172 - accuracy: 0.8741\n",
      "Epoch 132/1500\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8778\n",
      "Epoch 133/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.3150 - accuracy: 0.8702\n",
      "Epoch 134/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.3037 - accuracy: 0.8750\n",
      "Epoch 135/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.3051 - accuracy: 0.8772\n",
      "Epoch 136/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.3064 - accuracy: 0.8729\n",
      "Epoch 137/1500\n",
      "52/52 [==============================] - 0s 903us/step - loss: 0.3076 - accuracy: 0.8717\n",
      "Epoch 138/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.3135 - accuracy: 0.8639\n",
      "Epoch 139/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.2887 - accuracy: 0.8802\n",
      "Epoch 140/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.3273 - accuracy: 0.8627\n",
      "Epoch 141/1500\n",
      "52/52 [==============================] - 0s 906us/step - loss: 0.2962 - accuracy: 0.8802\n",
      "Epoch 142/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.3086 - accuracy: 0.8775\n",
      "Epoch 143/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.3025 - accuracy: 0.8759\n",
      "Epoch 144/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.2991 - accuracy: 0.8741\n",
      "Epoch 145/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.3000 - accuracy: 0.8802\n",
      "Epoch 146/1500\n",
      "52/52 [==============================] - 0s 891us/step - loss: 0.3051 - accuracy: 0.8787\n",
      "Epoch 147/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.2927 - accuracy: 0.8847\n",
      "Epoch 148/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.3043 - accuracy: 0.8772\n",
      "Epoch 149/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.2908 - accuracy: 0.8778\n",
      "Epoch 150/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.2962 - accuracy: 0.8759\n",
      "Epoch 151/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.2863 - accuracy: 0.8841\n",
      "Epoch 152/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.3022 - accuracy: 0.8747\n",
      "Epoch 153/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.2945 - accuracy: 0.8790\n",
      "Epoch 154/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.2921 - accuracy: 0.8790\n",
      "Epoch 155/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.3115 - accuracy: 0.8753\n",
      "Epoch 156/1500\n",
      "52/52 [==============================] - 0s 879us/step - loss: 0.2925 - accuracy: 0.8768\n",
      "Epoch 157/1500\n",
      "52/52 [==============================] - 0s 890us/step - loss: 0.2947 - accuracy: 0.8850\n",
      "Epoch 158/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.2982 - accuracy: 0.8784\n",
      "Epoch 159/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.2918 - accuracy: 0.8781\n",
      "Epoch 160/1500\n",
      "52/52 [==============================] - 0s 887us/step - loss: 0.2848 - accuracy: 0.8856\n",
      "Epoch 161/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.2971 - accuracy: 0.8793\n",
      "Epoch 162/1500\n",
      "52/52 [==============================] - 0s 915us/step - loss: 0.2806 - accuracy: 0.8853\n",
      "Epoch 163/1500\n",
      "52/52 [==============================] - 0s 960us/step - loss: 0.2819 - accuracy: 0.8814\n",
      "Epoch 164/1500\n",
      "52/52 [==============================] - 0s 868us/step - loss: 0.2855 - accuracy: 0.8814\n",
      "Epoch 165/1500\n",
      "52/52 [==============================] - 0s 910us/step - loss: 0.2852 - accuracy: 0.8853\n",
      "Epoch 166/1500\n",
      "52/52 [==============================] - 0s 947us/step - loss: 0.2904 - accuracy: 0.8781\n",
      "Epoch 167/1500\n",
      "52/52 [==============================] - 0s 884us/step - loss: 0.2798 - accuracy: 0.8799\n",
      "Epoch 168/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.2862 - accuracy: 0.8862\n",
      "Epoch 169/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.2829 - accuracy: 0.8853\n",
      "Epoch 170/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.2792 - accuracy: 0.8823\n",
      "Epoch 171/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.2890 - accuracy: 0.8796\n",
      "Epoch 172/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.2851 - accuracy: 0.8820\n",
      "Epoch 173/1500\n",
      "52/52 [==============================] - 0s 920us/step - loss: 0.2834 - accuracy: 0.8859\n",
      "Epoch 174/1500\n",
      "52/52 [==============================] - 0s 962us/step - loss: 0.2787 - accuracy: 0.8868\n",
      "Epoch 175/1500\n",
      "52/52 [==============================] - 0s 922us/step - loss: 0.2661 - accuracy: 0.8941\n",
      "Epoch 176/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.2778 - accuracy: 0.8925\n",
      "Epoch 177/1500\n",
      "52/52 [==============================] - 0s 899us/step - loss: 0.2864 - accuracy: 0.8844\n",
      "Epoch 178/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.2788 - accuracy: 0.8829\n",
      "Epoch 179/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8850\n",
      "Epoch 180/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8889\n",
      "Epoch 181/1500\n",
      "52/52 [==============================] - 0s 874us/step - loss: 0.2755 - accuracy: 0.8856\n",
      "Epoch 182/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.2854 - accuracy: 0.8814\n",
      "Epoch 183/1500\n",
      "52/52 [==============================] - 0s 889us/step - loss: 0.2729 - accuracy: 0.8950\n",
      "Epoch 184/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8826\n",
      "Epoch 185/1500\n",
      "52/52 [==============================] - 0s 946us/step - loss: 0.2727 - accuracy: 0.8865\n",
      "Epoch 186/1500\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8889\n",
      "Epoch 187/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2723 - accuracy: 0.8922\n",
      "Epoch 188/1500\n",
      "52/52 [==============================] - 0s 886us/step - loss: 0.2699 - accuracy: 0.8889\n",
      "Epoch 189/1500\n",
      "52/52 [==============================] - 0s 976us/step - loss: 0.2690 - accuracy: 0.8901\n",
      "Epoch 190/1500\n",
      "52/52 [==============================] - 0s 972us/step - loss: 0.2810 - accuracy: 0.8853\n",
      "Epoch 191/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2750 - accuracy: 0.8892\n",
      "Epoch 192/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.2758 - accuracy: 0.8904\n",
      "Epoch 193/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.2694 - accuracy: 0.8877\n",
      "Epoch 194/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.2598 - accuracy: 0.8995\n",
      "Epoch 195/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.2606 - accuracy: 0.8919\n",
      "Epoch 196/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.2660 - accuracy: 0.8868\n",
      "Epoch 197/1500\n",
      "52/52 [==============================] - 0s 953us/step - loss: 0.2616 - accuracy: 0.8910\n",
      "Epoch 198/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.2556 - accuracy: 0.8950\n",
      "Epoch 199/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.2644 - accuracy: 0.8931\n",
      "Epoch 200/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.2678 - accuracy: 0.8931\n",
      "Epoch 201/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.2631 - accuracy: 0.8907\n",
      "Epoch 202/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.2697 - accuracy: 0.8910\n",
      "Epoch 203/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.2549 - accuracy: 0.9007\n",
      "Epoch 204/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.2626 - accuracy: 0.8925\n",
      "Epoch 205/1500\n",
      "52/52 [==============================] - 0s 922us/step - loss: 0.2569 - accuracy: 0.8919\n",
      "Epoch 206/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.2643 - accuracy: 0.8931\n",
      "Epoch 207/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.2529 - accuracy: 0.8901\n",
      "Epoch 208/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.2568 - accuracy: 0.8956\n",
      "Epoch 209/1500\n",
      "52/52 [==============================] - 0s 771us/step - loss: 0.2652 - accuracy: 0.8950\n",
      "Epoch 210/1500\n",
      "52/52 [==============================] - 0s 736us/step - loss: 0.2601 - accuracy: 0.8925\n",
      "Epoch 211/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.2501 - accuracy: 0.8998\n",
      "Epoch 212/1500\n",
      "52/52 [==============================] - 0s 936us/step - loss: 0.2573 - accuracy: 0.8965\n",
      "Epoch 213/1500\n",
      "52/52 [==============================] - 0s 926us/step - loss: 0.2537 - accuracy: 0.8941\n",
      "Epoch 214/1500\n",
      "52/52 [==============================] - 0s 928us/step - loss: 0.2633 - accuracy: 0.8910\n",
      "Epoch 215/1500\n",
      "52/52 [==============================] - 0s 957us/step - loss: 0.2577 - accuracy: 0.8944\n",
      "Epoch 216/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.2552 - accuracy: 0.8947\n",
      "Epoch 217/1500\n",
      "52/52 [==============================] - 0s 882us/step - loss: 0.2603 - accuracy: 0.8904\n",
      "Epoch 218/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.2674 - accuracy: 0.8947\n",
      "Epoch 219/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.2495 - accuracy: 0.9016\n",
      "Epoch 220/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.2535 - accuracy: 0.8995\n",
      "Epoch 221/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.2448 - accuracy: 0.9055\n",
      "Epoch 222/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.2510 - accuracy: 0.9019\n",
      "Epoch 223/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.2469 - accuracy: 0.9031\n",
      "Epoch 224/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.2503 - accuracy: 0.8950\n",
      "Epoch 225/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.2468 - accuracy: 0.9022\n",
      "Epoch 226/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.2475 - accuracy: 0.9028\n",
      "Epoch 227/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.2436 - accuracy: 0.8998\n",
      "Epoch 228/1500\n",
      "52/52 [==============================] - 0s 920us/step - loss: 0.2465 - accuracy: 0.9016\n",
      "Epoch 229/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.2489 - accuracy: 0.9013\n",
      "Epoch 230/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.2337 - accuracy: 0.9070\n",
      "Epoch 231/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.2487 - accuracy: 0.8989\n",
      "Epoch 232/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.2445 - accuracy: 0.9040\n",
      "Epoch 233/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9058\n",
      "Epoch 234/1500\n",
      "52/52 [==============================] - 0s 907us/step - loss: 0.2397 - accuracy: 0.9049\n",
      "Epoch 235/1500\n",
      "52/52 [==============================] - 0s 949us/step - loss: 0.2425 - accuracy: 0.9043\n",
      "Epoch 236/1500\n",
      "52/52 [==============================] - 0s 932us/step - loss: 0.2362 - accuracy: 0.9082\n",
      "Epoch 237/1500\n",
      "52/52 [==============================] - 0s 985us/step - loss: 0.2549 - accuracy: 0.8986\n",
      "Epoch 238/1500\n",
      "52/52 [==============================] - 0s 988us/step - loss: 0.2274 - accuracy: 0.9101\n",
      "Epoch 239/1500\n",
      "52/52 [==============================] - 0s 939us/step - loss: 0.2392 - accuracy: 0.9022\n",
      "Epoch 240/1500\n",
      "52/52 [==============================] - 0s 925us/step - loss: 0.2324 - accuracy: 0.9040\n",
      "Epoch 241/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2324 - accuracy: 0.9046\n",
      "Epoch 242/1500\n",
      "52/52 [==============================] - 0s 946us/step - loss: 0.2386 - accuracy: 0.9058\n",
      "Epoch 243/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.2375 - accuracy: 0.9034\n",
      "Epoch 244/1500\n",
      "52/52 [==============================] - 0s 920us/step - loss: 0.2347 - accuracy: 0.9152\n",
      "Epoch 245/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.2390 - accuracy: 0.9040\n",
      "Epoch 246/1500\n",
      "52/52 [==============================] - 0s 889us/step - loss: 0.2468 - accuracy: 0.9031\n",
      "Epoch 247/1500\n",
      "52/52 [==============================] - 0s 932us/step - loss: 0.2332 - accuracy: 0.9034\n",
      "Epoch 248/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.2302 - accuracy: 0.9040\n",
      "Epoch 249/1500\n",
      "52/52 [==============================] - 0s 918us/step - loss: 0.2394 - accuracy: 0.9043\n",
      "Epoch 250/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.8965\n",
      "Epoch 251/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9097\n",
      "Epoch 252/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.9022\n",
      "Epoch 253/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.9028\n",
      "Epoch 254/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9034\n",
      "Epoch 255/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9082\n",
      "Epoch 256/1500\n",
      "52/52 [==============================] - 0s 988us/step - loss: 0.2453 - accuracy: 0.9052\n",
      "Epoch 257/1500\n",
      "52/52 [==============================] - 0s 948us/step - loss: 0.2238 - accuracy: 0.9085\n",
      "Epoch 258/1500\n",
      "52/52 [==============================] - 0s 932us/step - loss: 0.2372 - accuracy: 0.9037\n",
      "Epoch 259/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9037\n",
      "Epoch 260/1500\n",
      "52/52 [==============================] - 0s 995us/step - loss: 0.2316 - accuracy: 0.9097\n",
      "Epoch 261/1500\n",
      "52/52 [==============================] - 0s 886us/step - loss: 0.2306 - accuracy: 0.9076\n",
      "Epoch 262/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2300 - accuracy: 0.9067\n",
      "Epoch 263/1500\n",
      "52/52 [==============================] - 0s 925us/step - loss: 0.2373 - accuracy: 0.9043\n",
      "Epoch 264/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.2354 - accuracy: 0.9067\n",
      "Epoch 265/1500\n",
      "52/52 [==============================] - 0s 896us/step - loss: 0.2302 - accuracy: 0.9128\n",
      "Epoch 266/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.2344 - accuracy: 0.9040\n",
      "Epoch 267/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.2310 - accuracy: 0.9070\n",
      "Epoch 268/1500\n",
      "52/52 [==============================] - 0s 905us/step - loss: 0.2299 - accuracy: 0.9052\n",
      "Epoch 269/1500\n",
      "52/52 [==============================] - 0s 967us/step - loss: 0.2218 - accuracy: 0.9101\n",
      "Epoch 270/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.2231 - accuracy: 0.9107\n",
      "Epoch 271/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.2268 - accuracy: 0.9094\n",
      "Epoch 272/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.2298 - accuracy: 0.9082\n",
      "Epoch 273/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.2286 - accuracy: 0.9070\n",
      "Epoch 274/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.2259 - accuracy: 0.9067\n",
      "Epoch 275/1500\n",
      "52/52 [==============================] - 0s 916us/step - loss: 0.2315 - accuracy: 0.9052\n",
      "Epoch 276/1500\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.2494 - accuracy: 0.9001\n",
      "Epoch 277/1500\n",
      "52/52 [==============================] - 0s 891us/step - loss: 0.2340 - accuracy: 0.9007\n",
      "Epoch 278/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2252 - accuracy: 0.9113\n",
      "Epoch 279/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.2288 - accuracy: 0.9110\n",
      "Epoch 280/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.9082\n",
      "Epoch 281/1500\n",
      "52/52 [==============================] - 0s 909us/step - loss: 0.2343 - accuracy: 0.9007\n",
      "Epoch 282/1500\n",
      "52/52 [==============================] - 0s 925us/step - loss: 0.2226 - accuracy: 0.9122\n",
      "Epoch 283/1500\n",
      "52/52 [==============================] - 0s 908us/step - loss: 0.2298 - accuracy: 0.9101\n",
      "Epoch 284/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2198 - accuracy: 0.9119\n",
      "Epoch 285/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.9158\n",
      "Epoch 286/1500\n",
      "52/52 [==============================] - 0s 948us/step - loss: 0.2344 - accuracy: 0.9031\n",
      "Epoch 287/1500\n",
      "52/52 [==============================] - 0s 888us/step - loss: 0.2202 - accuracy: 0.9049\n",
      "Epoch 288/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.2281 - accuracy: 0.9113\n",
      "Epoch 289/1500\n",
      "52/52 [==============================] - 0s 895us/step - loss: 0.2213 - accuracy: 0.9122\n",
      "Epoch 290/1500\n",
      "52/52 [==============================] - 0s 954us/step - loss: 0.2175 - accuracy: 0.9140\n",
      "Epoch 291/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2296 - accuracy: 0.9064\n",
      "Epoch 292/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9137\n",
      "Epoch 293/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9137\n",
      "Epoch 294/1500\n",
      "52/52 [==============================] - 0s 926us/step - loss: 0.2356 - accuracy: 0.9058\n",
      "Epoch 295/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.2136 - accuracy: 0.9097\n",
      "Epoch 296/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.2052 - accuracy: 0.9167\n",
      "Epoch 297/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.2404 - accuracy: 0.8998\n",
      "Epoch 298/1500\n",
      "52/52 [==============================] - 0s 927us/step - loss: 0.2108 - accuracy: 0.9149\n",
      "Epoch 299/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.2241 - accuracy: 0.9113\n",
      "Epoch 300/1500\n",
      "52/52 [==============================] - 0s 881us/step - loss: 0.2182 - accuracy: 0.9125\n",
      "Epoch 301/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9073\n",
      "Epoch 302/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2201 - accuracy: 0.9116\n",
      "Epoch 303/1500\n",
      "52/52 [==============================] - 0s 957us/step - loss: 0.2233 - accuracy: 0.9119\n",
      "Epoch 304/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.2172 - accuracy: 0.9101\n",
      "Epoch 305/1500\n",
      "52/52 [==============================] - 0s 925us/step - loss: 0.2144 - accuracy: 0.9170\n",
      "Epoch 306/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.2121 - accuracy: 0.9152\n",
      "Epoch 307/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.2094 - accuracy: 0.9161\n",
      "Epoch 308/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.2152 - accuracy: 0.9125\n",
      "Epoch 309/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.2141 - accuracy: 0.9140\n",
      "Epoch 310/1500\n",
      "52/52 [==============================] - 0s 891us/step - loss: 0.2116 - accuracy: 0.9158\n",
      "Epoch 311/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9173\n",
      "Epoch 312/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9110\n",
      "Epoch 313/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9131\n",
      "Epoch 314/1500\n",
      "52/52 [==============================] - 0s 914us/step - loss: 0.2205 - accuracy: 0.9113\n",
      "Epoch 315/1500\n",
      "52/52 [==============================] - 0s 940us/step - loss: 0.2331 - accuracy: 0.9079\n",
      "Epoch 316/1500\n",
      "52/52 [==============================] - 0s 886us/step - loss: 0.2234 - accuracy: 0.9107\n",
      "Epoch 317/1500\n",
      "52/52 [==============================] - 0s 897us/step - loss: 0.2087 - accuracy: 0.9137\n",
      "Epoch 318/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.2227 - accuracy: 0.9128\n",
      "Epoch 319/1500\n",
      "52/52 [==============================] - 0s 879us/step - loss: 0.2216 - accuracy: 0.9101\n",
      "Epoch 320/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.2154 - accuracy: 0.9134\n",
      "Epoch 321/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.2116 - accuracy: 0.9170\n",
      "Epoch 322/1500\n",
      "52/52 [==============================] - 0s 936us/step - loss: 0.2183 - accuracy: 0.9137\n",
      "Epoch 323/1500\n",
      "52/52 [==============================] - 0s 988us/step - loss: 0.2213 - accuracy: 0.9119\n",
      "Epoch 324/1500\n",
      "52/52 [==============================] - 0s 951us/step - loss: 0.2249 - accuracy: 0.9149\n",
      "Epoch 325/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.2157 - accuracy: 0.9091\n",
      "Epoch 326/1500\n",
      "52/52 [==============================] - 0s 965us/step - loss: 0.2060 - accuracy: 0.9185\n",
      "Epoch 327/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9221\n",
      "Epoch 328/1500\n",
      "52/52 [==============================] - 0s 956us/step - loss: 0.1995 - accuracy: 0.9230\n",
      "Epoch 329/1500\n",
      "52/52 [==============================] - 0s 951us/step - loss: 0.1990 - accuracy: 0.9224\n",
      "Epoch 330/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9224\n",
      "Epoch 331/1500\n",
      "52/52 [==============================] - 0s 953us/step - loss: 0.2140 - accuracy: 0.9131\n",
      "Epoch 332/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9170\n",
      "Epoch 333/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9137\n",
      "Epoch 334/1500\n",
      "52/52 [==============================] - 0s 972us/step - loss: 0.2058 - accuracy: 0.9218\n",
      "Epoch 335/1500\n",
      "52/52 [==============================] - 0s 893us/step - loss: 0.2058 - accuracy: 0.9167\n",
      "Epoch 336/1500\n",
      "52/52 [==============================] - 0s 907us/step - loss: 0.2028 - accuracy: 0.9215\n",
      "Epoch 337/1500\n",
      "52/52 [==============================] - 0s 948us/step - loss: 0.2140 - accuracy: 0.9134\n",
      "Epoch 338/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9194\n",
      "Epoch 339/1500\n",
      "52/52 [==============================] - 0s 950us/step - loss: 0.2073 - accuracy: 0.9179\n",
      "Epoch 340/1500\n",
      "52/52 [==============================] - 0s 934us/step - loss: 0.2016 - accuracy: 0.9242\n",
      "Epoch 341/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.2189 - accuracy: 0.9134\n",
      "Epoch 342/1500\n",
      "52/52 [==============================] - 0s 925us/step - loss: 0.2151 - accuracy: 0.9122\n",
      "Epoch 343/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.1925 - accuracy: 0.9218\n",
      "Epoch 344/1500\n",
      "52/52 [==============================] - 0s 868us/step - loss: 0.1957 - accuracy: 0.9218\n",
      "Epoch 345/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.1960 - accuracy: 0.9179\n",
      "Epoch 346/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.2004 - accuracy: 0.9230\n",
      "Epoch 347/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.2171 - accuracy: 0.9149\n",
      "Epoch 348/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.1950 - accuracy: 0.9221\n",
      "Epoch 349/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.2190 - accuracy: 0.9149\n",
      "Epoch 350/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.1907 - accuracy: 0.9236\n",
      "Epoch 351/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.1933 - accuracy: 0.9245\n",
      "Epoch 352/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.1986 - accuracy: 0.9194\n",
      "Epoch 353/1500\n",
      "52/52 [==============================] - 0s 879us/step - loss: 0.2030 - accuracy: 0.9203\n",
      "Epoch 354/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.2249 - accuracy: 0.9119\n",
      "Epoch 355/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.1974 - accuracy: 0.9212\n",
      "Epoch 356/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.2108 - accuracy: 0.9137\n",
      "Epoch 357/1500\n",
      "52/52 [==============================] - 0s 890us/step - loss: 0.1944 - accuracy: 0.9215\n",
      "Epoch 358/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.1971 - accuracy: 0.9245\n",
      "Epoch 359/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.1933 - accuracy: 0.9221\n",
      "Epoch 360/1500\n",
      "52/52 [==============================] - 0s 897us/step - loss: 0.2001 - accuracy: 0.9224\n",
      "Epoch 361/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.1981 - accuracy: 0.9200\n",
      "Epoch 362/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.1972 - accuracy: 0.9218\n",
      "Epoch 363/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.1919 - accuracy: 0.9200\n",
      "Epoch 364/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.2006 - accuracy: 0.9191\n",
      "Epoch 365/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.2060 - accuracy: 0.9173\n",
      "Epoch 366/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9200\n",
      "Epoch 367/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.1966 - accuracy: 0.9230\n",
      "Epoch 368/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.1925 - accuracy: 0.9248\n",
      "Epoch 369/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.1917 - accuracy: 0.9224\n",
      "Epoch 370/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.2011 - accuracy: 0.9221\n",
      "Epoch 371/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.1939 - accuracy: 0.9227\n",
      "Epoch 372/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.2005 - accuracy: 0.9239\n",
      "Epoch 373/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9161\n",
      "Epoch 374/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9194\n",
      "Epoch 375/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.2019 - accuracy: 0.9230\n",
      "Epoch 376/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.1947 - accuracy: 0.9273\n",
      "Epoch 377/1500\n",
      "52/52 [==============================] - 0s 895us/step - loss: 0.1998 - accuracy: 0.9230\n",
      "Epoch 378/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.2108 - accuracy: 0.9116\n",
      "Epoch 379/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.1925 - accuracy: 0.9230\n",
      "Epoch 380/1500\n",
      " 1/52 [..............................] - ETA: 0s - loss: 0.2828 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 350.\n",
      "52/52 [==============================] - 0s 899us/step - loss: 0.2025 - accuracy: 0.9170\n",
      "Epoch 380: early stopping\n",
      "5/5 [==============================] - 0s 983us/step - loss: 0.6034 - accuracy: 0.7755\n",
      "5/5 [==============================] - 0s 795us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.6033971905708313, Accuracy: 0.7755101919174194, Precision: 0.6320517606071917, Recall: 0.8492690058479532, F1 Score: 0.6908117510354513\n",
      "Confusion Matrix:\n",
      " [[91  6 23]\n",
      " [ 0  8  0]\n",
      " [ 3  1 15]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group distribution: Counter({1: 1046, 2: 1022, 0: 1014})\n",
      "Epoch 1/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.1557 - accuracy: 0.5003\n",
      "Epoch 2/1500\n",
      "49/49 [==============================] - 0s 987us/step - loss: 0.9373 - accuracy: 0.5824\n",
      "Epoch 3/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8634 - accuracy: 0.6217\n",
      "Epoch 4/1500\n",
      "49/49 [==============================] - 0s 855us/step - loss: 0.7975 - accuracy: 0.6535\n",
      "Epoch 5/1500\n",
      "49/49 [==============================] - 0s 867us/step - loss: 0.7616 - accuracy: 0.6736\n",
      "Epoch 6/1500\n",
      "49/49 [==============================] - 0s 863us/step - loss: 0.7313 - accuracy: 0.6823\n",
      "Epoch 7/1500\n",
      "49/49 [==============================] - 0s 855us/step - loss: 0.6896 - accuracy: 0.6995\n",
      "Epoch 8/1500\n",
      "49/49 [==============================] - 0s 851us/step - loss: 0.6654 - accuracy: 0.7151\n",
      "Epoch 9/1500\n",
      "49/49 [==============================] - 0s 821us/step - loss: 0.6457 - accuracy: 0.7236\n",
      "Epoch 10/1500\n",
      "49/49 [==============================] - 0s 839us/step - loss: 0.6336 - accuracy: 0.7310\n",
      "Epoch 11/1500\n",
      "49/49 [==============================] - 0s 856us/step - loss: 0.6295 - accuracy: 0.7287\n",
      "Epoch 12/1500\n",
      "49/49 [==============================] - 0s 870us/step - loss: 0.6161 - accuracy: 0.7404\n",
      "Epoch 13/1500\n",
      "49/49 [==============================] - 0s 870us/step - loss: 0.6010 - accuracy: 0.7437\n",
      "Epoch 14/1500\n",
      "49/49 [==============================] - 0s 858us/step - loss: 0.5930 - accuracy: 0.7482\n",
      "Epoch 15/1500\n",
      "49/49 [==============================] - 0s 862us/step - loss: 0.5810 - accuracy: 0.7541\n",
      "Epoch 16/1500\n",
      "49/49 [==============================] - 0s 857us/step - loss: 0.5866 - accuracy: 0.7524\n",
      "Epoch 17/1500\n",
      "49/49 [==============================] - 0s 846us/step - loss: 0.5607 - accuracy: 0.7541\n",
      "Epoch 18/1500\n",
      "49/49 [==============================] - 0s 879us/step - loss: 0.5571 - accuracy: 0.7573\n",
      "Epoch 19/1500\n",
      "49/49 [==============================] - 0s 842us/step - loss: 0.5553 - accuracy: 0.7557\n",
      "Epoch 20/1500\n",
      "49/49 [==============================] - 0s 839us/step - loss: 0.5527 - accuracy: 0.7560\n",
      "Epoch 21/1500\n",
      "49/49 [==============================] - 0s 848us/step - loss: 0.5490 - accuracy: 0.7638\n",
      "Epoch 22/1500\n",
      "49/49 [==============================] - 0s 872us/step - loss: 0.5379 - accuracy: 0.7755\n",
      "Epoch 23/1500\n",
      "49/49 [==============================] - 0s 867us/step - loss: 0.5208 - accuracy: 0.7726\n",
      "Epoch 24/1500\n",
      "49/49 [==============================] - 0s 848us/step - loss: 0.5325 - accuracy: 0.7696\n",
      "Epoch 25/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.5135 - accuracy: 0.7761\n",
      "Epoch 26/1500\n",
      "49/49 [==============================] - 0s 822us/step - loss: 0.5277 - accuracy: 0.7664\n",
      "Epoch 27/1500\n",
      "49/49 [==============================] - 0s 853us/step - loss: 0.5217 - accuracy: 0.7755\n",
      "Epoch 28/1500\n",
      "49/49 [==============================] - 0s 846us/step - loss: 0.5166 - accuracy: 0.7794\n",
      "Epoch 29/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.4966 - accuracy: 0.7901\n",
      "Epoch 30/1500\n",
      "49/49 [==============================] - 0s 863us/step - loss: 0.5086 - accuracy: 0.7800\n",
      "Epoch 31/1500\n",
      "49/49 [==============================] - 0s 865us/step - loss: 0.5037 - accuracy: 0.7884\n",
      "Epoch 32/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.4849 - accuracy: 0.7878\n",
      "Epoch 33/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.5018 - accuracy: 0.7839\n",
      "Epoch 34/1500\n",
      "49/49 [==============================] - 0s 830us/step - loss: 0.4896 - accuracy: 0.7917\n",
      "Epoch 35/1500\n",
      "49/49 [==============================] - 0s 840us/step - loss: 0.4932 - accuracy: 0.7859\n",
      "Epoch 36/1500\n",
      "49/49 [==============================] - 0s 835us/step - loss: 0.4898 - accuracy: 0.7936\n",
      "Epoch 37/1500\n",
      "49/49 [==============================] - 0s 820us/step - loss: 0.4908 - accuracy: 0.7930\n",
      "Epoch 38/1500\n",
      "49/49 [==============================] - 0s 842us/step - loss: 0.4797 - accuracy: 0.7914\n",
      "Epoch 39/1500\n",
      "49/49 [==============================] - 0s 836us/step - loss: 0.4620 - accuracy: 0.7940\n",
      "Epoch 40/1500\n",
      "49/49 [==============================] - 0s 816us/step - loss: 0.4615 - accuracy: 0.7985\n",
      "Epoch 41/1500\n",
      "49/49 [==============================] - 0s 833us/step - loss: 0.4612 - accuracy: 0.7998\n",
      "Epoch 42/1500\n",
      "49/49 [==============================] - 0s 821us/step - loss: 0.4614 - accuracy: 0.8027\n",
      "Epoch 43/1500\n",
      "49/49 [==============================] - 0s 835us/step - loss: 0.4686 - accuracy: 0.8008\n",
      "Epoch 44/1500\n",
      "49/49 [==============================] - 0s 832us/step - loss: 0.4537 - accuracy: 0.8125\n",
      "Epoch 45/1500\n",
      "49/49 [==============================] - 0s 784us/step - loss: 0.4616 - accuracy: 0.8011\n",
      "Epoch 46/1500\n",
      "49/49 [==============================] - 0s 822us/step - loss: 0.4338 - accuracy: 0.8099\n",
      "Epoch 47/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.4473 - accuracy: 0.8092\n",
      "Epoch 48/1500\n",
      "49/49 [==============================] - 0s 776us/step - loss: 0.4484 - accuracy: 0.8121\n",
      "Epoch 49/1500\n",
      "49/49 [==============================] - 0s 785us/step - loss: 0.4441 - accuracy: 0.8128\n",
      "Epoch 50/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.4556 - accuracy: 0.8050\n",
      "Epoch 51/1500\n",
      "49/49 [==============================] - 0s 913us/step - loss: 0.4473 - accuracy: 0.8105\n",
      "Epoch 52/1500\n",
      "49/49 [==============================] - 0s 923us/step - loss: 0.4325 - accuracy: 0.8209\n",
      "Epoch 53/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.8018\n",
      "Epoch 54/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.8131\n",
      "Epoch 55/1500\n",
      "49/49 [==============================] - 0s 842us/step - loss: 0.4409 - accuracy: 0.8189\n",
      "Epoch 56/1500\n",
      "49/49 [==============================] - 0s 862us/step - loss: 0.4261 - accuracy: 0.8151\n",
      "Epoch 57/1500\n",
      "49/49 [==============================] - 0s 846us/step - loss: 0.4368 - accuracy: 0.8206\n",
      "Epoch 58/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.4350 - accuracy: 0.8157\n",
      "Epoch 59/1500\n",
      "49/49 [==============================] - 0s 875us/step - loss: 0.4229 - accuracy: 0.8209\n",
      "Epoch 60/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.4208 - accuracy: 0.8248\n",
      "Epoch 61/1500\n",
      "49/49 [==============================] - 0s 853us/step - loss: 0.4188 - accuracy: 0.8212\n",
      "Epoch 62/1500\n",
      "49/49 [==============================] - 0s 824us/step - loss: 0.4246 - accuracy: 0.8228\n",
      "Epoch 63/1500\n",
      "49/49 [==============================] - 0s 837us/step - loss: 0.4310 - accuracy: 0.8180\n",
      "Epoch 64/1500\n",
      "49/49 [==============================] - 0s 878us/step - loss: 0.4193 - accuracy: 0.8222\n",
      "Epoch 65/1500\n",
      "49/49 [==============================] - 0s 821us/step - loss: 0.4283 - accuracy: 0.8177\n",
      "Epoch 66/1500\n",
      "49/49 [==============================] - 0s 834us/step - loss: 0.4185 - accuracy: 0.8251\n",
      "Epoch 67/1500\n",
      "49/49 [==============================] - 0s 865us/step - loss: 0.4093 - accuracy: 0.8232\n",
      "Epoch 68/1500\n",
      "49/49 [==============================] - 0s 842us/step - loss: 0.4140 - accuracy: 0.8189\n",
      "Epoch 69/1500\n",
      "49/49 [==============================] - 0s 827us/step - loss: 0.4088 - accuracy: 0.8290\n",
      "Epoch 70/1500\n",
      "49/49 [==============================] - 0s 834us/step - loss: 0.4088 - accuracy: 0.8355\n",
      "Epoch 71/1500\n",
      "49/49 [==============================] - 0s 849us/step - loss: 0.4082 - accuracy: 0.8222\n",
      "Epoch 72/1500\n",
      "49/49 [==============================] - 0s 827us/step - loss: 0.4015 - accuracy: 0.8323\n",
      "Epoch 73/1500\n",
      "49/49 [==============================] - 0s 837us/step - loss: 0.4085 - accuracy: 0.8378\n",
      "Epoch 74/1500\n",
      "49/49 [==============================] - 0s 841us/step - loss: 0.3956 - accuracy: 0.8313\n",
      "Epoch 75/1500\n",
      "49/49 [==============================] - 0s 855us/step - loss: 0.4015 - accuracy: 0.8293\n",
      "Epoch 76/1500\n",
      "49/49 [==============================] - 0s 849us/step - loss: 0.4035 - accuracy: 0.8326\n",
      "Epoch 77/1500\n",
      "49/49 [==============================] - 0s 856us/step - loss: 0.3903 - accuracy: 0.8342\n",
      "Epoch 78/1500\n",
      "49/49 [==============================] - 0s 819us/step - loss: 0.3967 - accuracy: 0.8345\n",
      "Epoch 79/1500\n",
      "49/49 [==============================] - 0s 847us/step - loss: 0.3976 - accuracy: 0.8274\n",
      "Epoch 80/1500\n",
      "49/49 [==============================] - 0s 798us/step - loss: 0.3909 - accuracy: 0.8342\n",
      "Epoch 81/1500\n",
      "49/49 [==============================] - 0s 827us/step - loss: 0.3924 - accuracy: 0.8332\n",
      "Epoch 82/1500\n",
      "49/49 [==============================] - 0s 873us/step - loss: 0.3814 - accuracy: 0.8465\n",
      "Epoch 83/1500\n",
      "49/49 [==============================] - 0s 839us/step - loss: 0.3889 - accuracy: 0.8345\n",
      "Epoch 84/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3763 - accuracy: 0.8465\n",
      "Epoch 85/1500\n",
      "49/49 [==============================] - 0s 875us/step - loss: 0.3855 - accuracy: 0.8374\n",
      "Epoch 86/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3736 - accuracy: 0.8374\n",
      "Epoch 87/1500\n",
      "49/49 [==============================] - 0s 986us/step - loss: 0.3937 - accuracy: 0.8329\n",
      "Epoch 88/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3877 - accuracy: 0.8323\n",
      "Epoch 89/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3870 - accuracy: 0.8339\n",
      "Epoch 90/1500\n",
      "49/49 [==============================] - 0s 984us/step - loss: 0.3713 - accuracy: 0.8469\n",
      "Epoch 91/1500\n",
      "49/49 [==============================] - 0s 926us/step - loss: 0.3673 - accuracy: 0.8469\n",
      "Epoch 92/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3772 - accuracy: 0.8439\n",
      "Epoch 93/1500\n",
      "49/49 [==============================] - 0s 875us/step - loss: 0.3684 - accuracy: 0.8404\n",
      "Epoch 94/1500\n",
      "49/49 [==============================] - 0s 862us/step - loss: 0.3729 - accuracy: 0.8426\n",
      "Epoch 95/1500\n",
      "49/49 [==============================] - 0s 849us/step - loss: 0.3734 - accuracy: 0.8384\n",
      "Epoch 96/1500\n",
      "49/49 [==============================] - 0s 868us/step - loss: 0.3636 - accuracy: 0.8439\n",
      "Epoch 97/1500\n",
      "49/49 [==============================] - 0s 847us/step - loss: 0.3678 - accuracy: 0.8482\n",
      "Epoch 98/1500\n",
      "49/49 [==============================] - 0s 856us/step - loss: 0.3568 - accuracy: 0.8559\n",
      "Epoch 99/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3637 - accuracy: 0.8433\n",
      "Epoch 100/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3606 - accuracy: 0.8446\n",
      "Epoch 101/1500\n",
      "49/49 [==============================] - 0s 887us/step - loss: 0.3647 - accuracy: 0.8452\n",
      "Epoch 102/1500\n",
      "49/49 [==============================] - 0s 902us/step - loss: 0.3562 - accuracy: 0.8491\n",
      "Epoch 103/1500\n",
      "49/49 [==============================] - 0s 885us/step - loss: 0.3700 - accuracy: 0.8439\n",
      "Epoch 104/1500\n",
      "49/49 [==============================] - 0s 956us/step - loss: 0.3555 - accuracy: 0.8507\n",
      "Epoch 105/1500\n",
      "49/49 [==============================] - 0s 933us/step - loss: 0.3550 - accuracy: 0.8514\n",
      "Epoch 106/1500\n",
      "49/49 [==============================] - 0s 854us/step - loss: 0.3640 - accuracy: 0.8491\n",
      "Epoch 107/1500\n",
      "49/49 [==============================] - 0s 852us/step - loss: 0.3574 - accuracy: 0.8507\n",
      "Epoch 108/1500\n",
      "49/49 [==============================] - 0s 819us/step - loss: 0.3472 - accuracy: 0.8582\n",
      "Epoch 109/1500\n",
      "49/49 [==============================] - 0s 869us/step - loss: 0.3545 - accuracy: 0.8543\n",
      "Epoch 110/1500\n",
      "49/49 [==============================] - 0s 876us/step - loss: 0.3389 - accuracy: 0.8611\n",
      "Epoch 111/1500\n",
      "49/49 [==============================] - 0s 968us/step - loss: 0.3457 - accuracy: 0.8569\n",
      "Epoch 112/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8621\n",
      "Epoch 113/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.8530\n",
      "Epoch 114/1500\n",
      "49/49 [==============================] - 0s 987us/step - loss: 0.3442 - accuracy: 0.8618\n",
      "Epoch 115/1500\n",
      "49/49 [==============================] - 0s 911us/step - loss: 0.3455 - accuracy: 0.8569\n",
      "Epoch 116/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.3391 - accuracy: 0.8550\n",
      "Epoch 117/1500\n",
      "49/49 [==============================] - 0s 830us/step - loss: 0.3428 - accuracy: 0.8553\n",
      "Epoch 118/1500\n",
      "49/49 [==============================] - 0s 836us/step - loss: 0.3436 - accuracy: 0.8559\n",
      "Epoch 119/1500\n",
      "49/49 [==============================] - 0s 880us/step - loss: 0.3362 - accuracy: 0.8569\n",
      "Epoch 120/1500\n",
      "49/49 [==============================] - 0s 866us/step - loss: 0.3344 - accuracy: 0.8582\n",
      "Epoch 121/1500\n",
      "49/49 [==============================] - 0s 869us/step - loss: 0.3412 - accuracy: 0.8582\n",
      "Epoch 122/1500\n",
      "49/49 [==============================] - 0s 849us/step - loss: 0.3463 - accuracy: 0.8589\n",
      "Epoch 123/1500\n",
      "49/49 [==============================] - 0s 872us/step - loss: 0.3415 - accuracy: 0.8546\n",
      "Epoch 124/1500\n",
      "49/49 [==============================] - 0s 824us/step - loss: 0.3399 - accuracy: 0.8589\n",
      "Epoch 125/1500\n",
      "49/49 [==============================] - 0s 847us/step - loss: 0.3301 - accuracy: 0.8644\n",
      "Epoch 126/1500\n",
      "49/49 [==============================] - 0s 865us/step - loss: 0.3427 - accuracy: 0.8579\n",
      "Epoch 127/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8689\n",
      "Epoch 128/1500\n",
      "49/49 [==============================] - 0s 940us/step - loss: 0.3249 - accuracy: 0.8608\n",
      "Epoch 129/1500\n",
      "49/49 [==============================] - 0s 834us/step - loss: 0.3426 - accuracy: 0.8559\n",
      "Epoch 130/1500\n",
      "49/49 [==============================] - 0s 861us/step - loss: 0.3427 - accuracy: 0.8550\n",
      "Epoch 131/1500\n",
      "49/49 [==============================] - 0s 941us/step - loss: 0.3175 - accuracy: 0.8634\n",
      "Epoch 132/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.3239 - accuracy: 0.8657\n",
      "Epoch 133/1500\n",
      "49/49 [==============================] - 0s 891us/step - loss: 0.3343 - accuracy: 0.8637\n",
      "Epoch 134/1500\n",
      "49/49 [==============================] - 0s 862us/step - loss: 0.3133 - accuracy: 0.8686\n",
      "Epoch 135/1500\n",
      "49/49 [==============================] - 0s 888us/step - loss: 0.3249 - accuracy: 0.8653\n",
      "Epoch 136/1500\n",
      "49/49 [==============================] - 0s 862us/step - loss: 0.3267 - accuracy: 0.8644\n",
      "Epoch 137/1500\n",
      "49/49 [==============================] - 0s 973us/step - loss: 0.3151 - accuracy: 0.8728\n",
      "Epoch 138/1500\n",
      "49/49 [==============================] - 0s 864us/step - loss: 0.3217 - accuracy: 0.8660\n",
      "Epoch 139/1500\n",
      "49/49 [==============================] - 0s 878us/step - loss: 0.3126 - accuracy: 0.8699\n",
      "Epoch 140/1500\n",
      "49/49 [==============================] - 0s 896us/step - loss: 0.3209 - accuracy: 0.8666\n",
      "Epoch 141/1500\n",
      "49/49 [==============================] - 0s 844us/step - loss: 0.3155 - accuracy: 0.8741\n",
      "Epoch 142/1500\n",
      "49/49 [==============================] - 0s 848us/step - loss: 0.3108 - accuracy: 0.8770\n",
      "Epoch 143/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.8640\n",
      "Epoch 144/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3054 - accuracy: 0.8709\n",
      "Epoch 145/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.8761\n",
      "Epoch 146/1500\n",
      "49/49 [==============================] - 0s 965us/step - loss: 0.3054 - accuracy: 0.8699\n",
      "Epoch 147/1500\n",
      "49/49 [==============================] - 0s 974us/step - loss: 0.3243 - accuracy: 0.8631\n",
      "Epoch 148/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.8780\n",
      "Epoch 149/1500\n",
      "49/49 [==============================] - 0s 955us/step - loss: 0.3054 - accuracy: 0.8757\n",
      "Epoch 150/1500\n",
      "49/49 [==============================] - 0s 866us/step - loss: 0.3126 - accuracy: 0.8715\n",
      "Epoch 151/1500\n",
      "49/49 [==============================] - 0s 951us/step - loss: 0.3010 - accuracy: 0.8774\n",
      "Epoch 152/1500\n",
      "49/49 [==============================] - 0s 938us/step - loss: 0.3100 - accuracy: 0.8653\n",
      "Epoch 153/1500\n",
      "49/49 [==============================] - 0s 883us/step - loss: 0.3225 - accuracy: 0.8640\n",
      "Epoch 154/1500\n",
      "49/49 [==============================] - 0s 937us/step - loss: 0.3040 - accuracy: 0.8735\n",
      "Epoch 155/1500\n",
      "49/49 [==============================] - 0s 988us/step - loss: 0.2867 - accuracy: 0.8842\n",
      "Epoch 156/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.8712\n",
      "Epoch 157/1500\n",
      "49/49 [==============================] - 0s 884us/step - loss: 0.2956 - accuracy: 0.8812\n",
      "Epoch 158/1500\n",
      "49/49 [==============================] - 0s 863us/step - loss: 0.2934 - accuracy: 0.8764\n",
      "Epoch 159/1500\n",
      "49/49 [==============================] - 0s 864us/step - loss: 0.2977 - accuracy: 0.8699\n",
      "Epoch 160/1500\n",
      "49/49 [==============================] - 0s 882us/step - loss: 0.2904 - accuracy: 0.8757\n",
      "Epoch 161/1500\n",
      "49/49 [==============================] - 0s 865us/step - loss: 0.2919 - accuracy: 0.8777\n",
      "Epoch 162/1500\n",
      "49/49 [==============================] - 0s 845us/step - loss: 0.2916 - accuracy: 0.8757\n",
      "Epoch 163/1500\n",
      "49/49 [==============================] - 0s 853us/step - loss: 0.2893 - accuracy: 0.8848\n",
      "Epoch 164/1500\n",
      "49/49 [==============================] - 0s 861us/step - loss: 0.2975 - accuracy: 0.8829\n",
      "Epoch 165/1500\n",
      "49/49 [==============================] - 0s 867us/step - loss: 0.3073 - accuracy: 0.8679\n",
      "Epoch 166/1500\n",
      "49/49 [==============================] - 0s 894us/step - loss: 0.2917 - accuracy: 0.8770\n",
      "Epoch 167/1500\n",
      "49/49 [==============================] - 0s 842us/step - loss: 0.2874 - accuracy: 0.8822\n",
      "Epoch 168/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.2915 - accuracy: 0.8868\n",
      "Epoch 169/1500\n",
      "49/49 [==============================] - 0s 853us/step - loss: 0.2962 - accuracy: 0.8796\n",
      "Epoch 170/1500\n",
      "49/49 [==============================] - 0s 840us/step - loss: 0.3075 - accuracy: 0.8725\n",
      "Epoch 171/1500\n",
      "49/49 [==============================] - 0s 851us/step - loss: 0.2876 - accuracy: 0.8796\n",
      "Epoch 172/1500\n",
      "49/49 [==============================] - 0s 867us/step - loss: 0.2983 - accuracy: 0.8722\n",
      "Epoch 173/1500\n",
      "49/49 [==============================] - 0s 871us/step - loss: 0.2871 - accuracy: 0.8832\n",
      "Epoch 174/1500\n",
      "49/49 [==============================] - 0s 831us/step - loss: 0.2933 - accuracy: 0.8819\n",
      "Epoch 175/1500\n",
      "49/49 [==============================] - 0s 894us/step - loss: 0.2844 - accuracy: 0.8793\n",
      "Epoch 176/1500\n",
      "49/49 [==============================] - 0s 863us/step - loss: 0.2975 - accuracy: 0.8803\n",
      "Epoch 177/1500\n",
      "49/49 [==============================] - 0s 819us/step - loss: 0.2948 - accuracy: 0.8757\n",
      "Epoch 178/1500\n",
      "49/49 [==============================] - 0s 831us/step - loss: 0.2915 - accuracy: 0.8767\n",
      "Epoch 179/1500\n",
      "49/49 [==============================] - 0s 835us/step - loss: 0.2912 - accuracy: 0.8822\n",
      "Epoch 180/1500\n",
      "49/49 [==============================] - 0s 859us/step - loss: 0.2839 - accuracy: 0.8848\n",
      "Epoch 181/1500\n",
      "49/49 [==============================] - 0s 993us/step - loss: 0.2887 - accuracy: 0.8803\n",
      "Epoch 182/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8799\n",
      "Epoch 183/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2911 - accuracy: 0.8812\n",
      "Epoch 184/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.8829\n",
      "Epoch 185/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8803\n",
      "Epoch 186/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8757\n",
      "Epoch 187/1500\n",
      "49/49 [==============================] - 0s 997us/step - loss: 0.2912 - accuracy: 0.8838\n",
      "Epoch 188/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.8897\n",
      "Epoch 189/1500\n",
      "49/49 [==============================] - 0s 968us/step - loss: 0.2856 - accuracy: 0.8783\n",
      "Epoch 190/1500\n",
      "49/49 [==============================] - 0s 937us/step - loss: 0.2719 - accuracy: 0.8851\n",
      "Epoch 191/1500\n",
      "49/49 [==============================] - 0s 892us/step - loss: 0.2750 - accuracy: 0.8877\n",
      "Epoch 192/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.8884\n",
      "Epoch 193/1500\n",
      "49/49 [==============================] - 0s 945us/step - loss: 0.2724 - accuracy: 0.8920\n",
      "Epoch 194/1500\n",
      "49/49 [==============================] - 0s 956us/step - loss: 0.2726 - accuracy: 0.8903\n",
      "Epoch 195/1500\n",
      "49/49 [==============================] - 0s 942us/step - loss: 0.2754 - accuracy: 0.8894\n",
      "Epoch 196/1500\n",
      "49/49 [==============================] - 0s 978us/step - loss: 0.2723 - accuracy: 0.8910\n",
      "Epoch 197/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.8829\n",
      "Epoch 198/1500\n",
      "49/49 [==============================] - 0s 982us/step - loss: 0.2793 - accuracy: 0.8838\n",
      "Epoch 199/1500\n",
      "49/49 [==============================] - 0s 935us/step - loss: 0.2761 - accuracy: 0.8887\n",
      "Epoch 200/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2690 - accuracy: 0.8939\n",
      "Epoch 201/1500\n",
      "49/49 [==============================] - 0s 929us/step - loss: 0.2750 - accuracy: 0.8812\n",
      "Epoch 202/1500\n",
      "49/49 [==============================] - 0s 887us/step - loss: 0.2711 - accuracy: 0.8884\n",
      "Epoch 203/1500\n",
      "49/49 [==============================] - 0s 858us/step - loss: 0.2812 - accuracy: 0.8835\n",
      "Epoch 204/1500\n",
      "49/49 [==============================] - 0s 895us/step - loss: 0.2790 - accuracy: 0.8884\n",
      "Epoch 205/1500\n",
      "49/49 [==============================] - 0s 888us/step - loss: 0.2821 - accuracy: 0.8845\n",
      "Epoch 206/1500\n",
      "49/49 [==============================] - 0s 906us/step - loss: 0.2579 - accuracy: 0.8975\n",
      "Epoch 207/1500\n",
      "49/49 [==============================] - 0s 962us/step - loss: 0.2687 - accuracy: 0.8871\n",
      "Epoch 208/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.8994\n",
      "Epoch 209/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2730 - accuracy: 0.8855\n",
      "Epoch 210/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.8936\n",
      "Epoch 211/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8861\n",
      "Epoch 212/1500\n",
      "49/49 [==============================] - 0s 996us/step - loss: 0.2614 - accuracy: 0.8907\n",
      "Epoch 213/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8968\n",
      "Epoch 214/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2618 - accuracy: 0.9001\n",
      "Epoch 215/1500\n",
      "49/49 [==============================] - 0s 921us/step - loss: 0.2589 - accuracy: 0.9049\n",
      "Epoch 216/1500\n",
      "49/49 [==============================] - 0s 877us/step - loss: 0.2560 - accuracy: 0.8991\n",
      "Epoch 217/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.2640 - accuracy: 0.8955\n",
      "Epoch 218/1500\n",
      "49/49 [==============================] - 0s 848us/step - loss: 0.2724 - accuracy: 0.8933\n",
      "Epoch 219/1500\n",
      "49/49 [==============================] - 0s 852us/step - loss: 0.2658 - accuracy: 0.8988\n",
      "Epoch 220/1500\n",
      "49/49 [==============================] - 0s 888us/step - loss: 0.2684 - accuracy: 0.8887\n",
      "Epoch 221/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8881\n",
      "Epoch 222/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.9007\n",
      "Epoch 223/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.8936\n",
      "Epoch 224/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.8965\n",
      "Epoch 225/1500\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2643 - accuracy: 0.8920\n",
      "Epoch 226/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.9030\n",
      "Epoch 227/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9007\n",
      "Epoch 228/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2633 - accuracy: 0.8881\n",
      "Epoch 229/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.9004\n",
      "Epoch 230/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.8949\n",
      "Epoch 231/1500\n",
      "49/49 [==============================] - 0s 946us/step - loss: 0.2553 - accuracy: 0.8962\n",
      "Epoch 232/1500\n",
      "49/49 [==============================] - 0s 879us/step - loss: 0.2412 - accuracy: 0.8978\n",
      "Epoch 233/1500\n",
      "49/49 [==============================] - 0s 864us/step - loss: 0.2593 - accuracy: 0.9017\n",
      "Epoch 234/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.2517 - accuracy: 0.9007\n",
      "Epoch 235/1500\n",
      "49/49 [==============================] - 0s 870us/step - loss: 0.2432 - accuracy: 0.9043\n",
      "Epoch 236/1500\n",
      "49/49 [==============================] - 0s 853us/step - loss: 0.2424 - accuracy: 0.9007\n",
      "Epoch 237/1500\n",
      "49/49 [==============================] - 0s 822us/step - loss: 0.2547 - accuracy: 0.8971\n",
      "Epoch 238/1500\n",
      "49/49 [==============================] - 0s 826us/step - loss: 0.2528 - accuracy: 0.9010\n",
      "Epoch 239/1500\n",
      "49/49 [==============================] - 0s 846us/step - loss: 0.2486 - accuracy: 0.9036\n",
      "Epoch 240/1500\n",
      "49/49 [==============================] - 0s 835us/step - loss: 0.2534 - accuracy: 0.8955\n",
      "Epoch 241/1500\n",
      "49/49 [==============================] - 0s 858us/step - loss: 0.2665 - accuracy: 0.8929\n",
      "Epoch 242/1500\n",
      "49/49 [==============================] - 0s 911us/step - loss: 0.2378 - accuracy: 0.9014\n",
      "Epoch 243/1500\n",
      "49/49 [==============================] - 0s 863us/step - loss: 0.2475 - accuracy: 0.9053\n",
      "Epoch 244/1500\n",
      "49/49 [==============================] - 0s 930us/step - loss: 0.2432 - accuracy: 0.9049\n",
      "Epoch 245/1500\n",
      "49/49 [==============================] - 0s 893us/step - loss: 0.2503 - accuracy: 0.9030\n",
      "Epoch 246/1500\n",
      "49/49 [==============================] - 0s 872us/step - loss: 0.2480 - accuracy: 0.9023\n",
      "Epoch 247/1500\n",
      "49/49 [==============================] - 0s 817us/step - loss: 0.2448 - accuracy: 0.8997\n",
      "Epoch 248/1500\n",
      "49/49 [==============================] - 0s 959us/step - loss: 0.2457 - accuracy: 0.9095\n",
      "Epoch 249/1500\n",
      "49/49 [==============================] - 0s 814us/step - loss: 0.2497 - accuracy: 0.9036\n",
      "Epoch 250/1500\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.2471 - accuracy: 0.9004\n",
      "Epoch 251/1500\n",
      "49/49 [==============================] - 0s 815us/step - loss: 0.2371 - accuracy: 0.9053\n",
      "Epoch 252/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.2418 - accuracy: 0.9014\n",
      "Epoch 253/1500\n",
      "49/49 [==============================] - 0s 830us/step - loss: 0.2454 - accuracy: 0.9023\n",
      "Epoch 254/1500\n",
      "49/49 [==============================] - 0s 855us/step - loss: 0.2399 - accuracy: 0.9049\n",
      "Epoch 255/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2355 - accuracy: 0.9001\n",
      "Epoch 256/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2377 - accuracy: 0.9017\n",
      "Epoch 257/1500\n",
      "49/49 [==============================] - 0s 911us/step - loss: 0.2406 - accuracy: 0.9075\n",
      "Epoch 258/1500\n",
      "49/49 [==============================] - 0s 845us/step - loss: 0.2454 - accuracy: 0.8942\n",
      "Epoch 259/1500\n",
      "49/49 [==============================] - 0s 836us/step - loss: 0.2477 - accuracy: 0.8965\n",
      "Epoch 260/1500\n",
      "49/49 [==============================] - 0s 872us/step - loss: 0.2404 - accuracy: 0.9095\n",
      "Epoch 261/1500\n",
      "49/49 [==============================] - 0s 836us/step - loss: 0.2461 - accuracy: 0.9030\n",
      "Epoch 262/1500\n",
      "49/49 [==============================] - 0s 840us/step - loss: 0.2477 - accuracy: 0.9001\n",
      "Epoch 263/1500\n",
      "49/49 [==============================] - 0s 812us/step - loss: 0.2457 - accuracy: 0.9014\n",
      "Epoch 264/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.2266 - accuracy: 0.9163\n",
      "Epoch 265/1500\n",
      "49/49 [==============================] - 0s 808us/step - loss: 0.2394 - accuracy: 0.9040\n",
      "Epoch 266/1500\n",
      "49/49 [==============================] - 0s 796us/step - loss: 0.2351 - accuracy: 0.9033\n",
      "Epoch 267/1500\n",
      "49/49 [==============================] - 0s 857us/step - loss: 0.2346 - accuracy: 0.9075\n",
      "Epoch 268/1500\n",
      "49/49 [==============================] - 0s 842us/step - loss: 0.2190 - accuracy: 0.9111\n",
      "Epoch 269/1500\n",
      "49/49 [==============================] - 0s 816us/step - loss: 0.2491 - accuracy: 0.8968\n",
      "Epoch 270/1500\n",
      "49/49 [==============================] - 0s 857us/step - loss: 0.2239 - accuracy: 0.9114\n",
      "Epoch 271/1500\n",
      "49/49 [==============================] - 0s 814us/step - loss: 0.2247 - accuracy: 0.9127\n",
      "Epoch 272/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2333 - accuracy: 0.9040\n",
      "Epoch 273/1500\n",
      "49/49 [==============================] - 0s 821us/step - loss: 0.2401 - accuracy: 0.9010\n",
      "Epoch 274/1500\n",
      "49/49 [==============================] - 0s 799us/step - loss: 0.2349 - accuracy: 0.9062\n",
      "Epoch 275/1500\n",
      "49/49 [==============================] - 0s 813us/step - loss: 0.2189 - accuracy: 0.9095\n",
      "Epoch 276/1500\n",
      "49/49 [==============================] - 0s 839us/step - loss: 0.2417 - accuracy: 0.9020\n",
      "Epoch 277/1500\n",
      "49/49 [==============================] - 0s 807us/step - loss: 0.2452 - accuracy: 0.9046\n",
      "Epoch 278/1500\n",
      "49/49 [==============================] - 0s 809us/step - loss: 0.2365 - accuracy: 0.9027\n",
      "Epoch 279/1500\n",
      "49/49 [==============================] - 0s 851us/step - loss: 0.2440 - accuracy: 0.9001\n",
      "Epoch 280/1500\n",
      "49/49 [==============================] - 0s 889us/step - loss: 0.2330 - accuracy: 0.9075\n",
      "Epoch 281/1500\n",
      "49/49 [==============================] - 0s 860us/step - loss: 0.2403 - accuracy: 0.9030\n",
      "Epoch 282/1500\n",
      "49/49 [==============================] - 0s 845us/step - loss: 0.2274 - accuracy: 0.9075\n",
      "Epoch 283/1500\n",
      "49/49 [==============================] - 0s 817us/step - loss: 0.2269 - accuracy: 0.9117\n",
      "Epoch 284/1500\n",
      "49/49 [==============================] - 0s 815us/step - loss: 0.2251 - accuracy: 0.9169\n",
      "Epoch 285/1500\n",
      "49/49 [==============================] - 0s 809us/step - loss: 0.2322 - accuracy: 0.9053\n",
      "Epoch 286/1500\n",
      "49/49 [==============================] - 0s 846us/step - loss: 0.2362 - accuracy: 0.9023\n",
      "Epoch 287/1500\n",
      "49/49 [==============================] - 0s 863us/step - loss: 0.2263 - accuracy: 0.9114\n",
      "Epoch 288/1500\n",
      "49/49 [==============================] - 0s 833us/step - loss: 0.2307 - accuracy: 0.9040\n",
      "Epoch 289/1500\n",
      "49/49 [==============================] - 0s 903us/step - loss: 0.2280 - accuracy: 0.9101\n",
      "Epoch 290/1500\n",
      "49/49 [==============================] - 0s 902us/step - loss: 0.2327 - accuracy: 0.9030\n",
      "Epoch 291/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2161 - accuracy: 0.9130\n",
      "Epoch 292/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.9049\n",
      "Epoch 293/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2276 - accuracy: 0.9088\n",
      "Epoch 294/1500\n",
      "49/49 [==============================] - 0s 959us/step - loss: 0.2246 - accuracy: 0.9124\n",
      "Epoch 295/1500\n",
      "49/49 [==============================] - 0s 857us/step - loss: 0.2229 - accuracy: 0.9147\n",
      "Epoch 296/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9030\n",
      "Epoch 297/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9004\n",
      "Epoch 298/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2149 - accuracy: 0.9147\n",
      "Epoch 299/1500\n",
      "49/49 [==============================] - 0s 916us/step - loss: 0.2197 - accuracy: 0.9189\n",
      "Epoch 300/1500\n",
      "49/49 [==============================] - 0s 864us/step - loss: 0.2206 - accuracy: 0.9117\n",
      "Epoch 301/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9186\n",
      "Epoch 302/1500\n",
      "49/49 [==============================] - 0s 989us/step - loss: 0.2358 - accuracy: 0.9066\n",
      "Epoch 303/1500\n",
      "49/49 [==============================] - 0s 840us/step - loss: 0.2266 - accuracy: 0.9091\n",
      "Epoch 304/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.2283 - accuracy: 0.9040\n",
      "Epoch 305/1500\n",
      "49/49 [==============================] - 0s 917us/step - loss: 0.2176 - accuracy: 0.9134\n",
      "Epoch 306/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2216 - accuracy: 0.9111\n",
      "Epoch 307/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9137\n",
      "Epoch 308/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9127\n",
      "Epoch 309/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9127\n",
      "Epoch 310/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2225 - accuracy: 0.9075\n",
      "Epoch 311/1500\n",
      "49/49 [==============================] - 0s 995us/step - loss: 0.2249 - accuracy: 0.9088\n",
      "Epoch 312/1500\n",
      "49/49 [==============================] - 0s 916us/step - loss: 0.2245 - accuracy: 0.9036\n",
      "Epoch 313/1500\n",
      "49/49 [==============================] - 0s 883us/step - loss: 0.2140 - accuracy: 0.9182\n",
      "Epoch 314/1500\n",
      "49/49 [==============================] - 0s 913us/step - loss: 0.2198 - accuracy: 0.9121\n",
      "Epoch 315/1500\n",
      "49/49 [==============================] - 0s 880us/step - loss: 0.2179 - accuracy: 0.9150\n",
      "Epoch 316/1500\n",
      "49/49 [==============================] - 0s 844us/step - loss: 0.2186 - accuracy: 0.9117\n",
      "Epoch 317/1500\n",
      "49/49 [==============================] - 0s 890us/step - loss: 0.2146 - accuracy: 0.9169\n",
      "Epoch 318/1500\n",
      "49/49 [==============================] - 0s 884us/step - loss: 0.2147 - accuracy: 0.9140\n",
      "Epoch 319/1500\n",
      "49/49 [==============================] - 0s 829us/step - loss: 0.2260 - accuracy: 0.9043\n",
      "Epoch 320/1500\n",
      "49/49 [==============================] - 0s 890us/step - loss: 0.2058 - accuracy: 0.9215\n",
      "Epoch 321/1500\n",
      "49/49 [==============================] - 0s 831us/step - loss: 0.2070 - accuracy: 0.9173\n",
      "Epoch 322/1500\n",
      "49/49 [==============================] - 0s 782us/step - loss: 0.2114 - accuracy: 0.9160\n",
      "Epoch 323/1500\n",
      "49/49 [==============================] - 0s 795us/step - loss: 0.2144 - accuracy: 0.9091\n",
      "Epoch 324/1500\n",
      "49/49 [==============================] - 0s 827us/step - loss: 0.2069 - accuracy: 0.9166\n",
      "Epoch 325/1500\n",
      "49/49 [==============================] - 0s 818us/step - loss: 0.2191 - accuracy: 0.9143\n",
      "Epoch 326/1500\n",
      "49/49 [==============================] - 0s 868us/step - loss: 0.2082 - accuracy: 0.9189\n",
      "Epoch 327/1500\n",
      "49/49 [==============================] - 0s 814us/step - loss: 0.2176 - accuracy: 0.9117\n",
      "Epoch 328/1500\n",
      "49/49 [==============================] - 0s 827us/step - loss: 0.2206 - accuracy: 0.9130\n",
      "Epoch 329/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2220 - accuracy: 0.9098\n",
      "Epoch 330/1500\n",
      "49/49 [==============================] - 0s 808us/step - loss: 0.2163 - accuracy: 0.9137\n",
      "Epoch 331/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.2077 - accuracy: 0.9163\n",
      "Epoch 332/1500\n",
      "49/49 [==============================] - 0s 817us/step - loss: 0.2230 - accuracy: 0.9156\n",
      "Epoch 333/1500\n",
      "49/49 [==============================] - 0s 817us/step - loss: 0.2200 - accuracy: 0.9114\n",
      "Epoch 334/1500\n",
      "49/49 [==============================] - 0s 845us/step - loss: 0.2095 - accuracy: 0.9205\n",
      "Epoch 335/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.2165 - accuracy: 0.9079\n",
      "Epoch 336/1500\n",
      "49/49 [==============================] - 0s 820us/step - loss: 0.2124 - accuracy: 0.9176\n",
      "Epoch 337/1500\n",
      "49/49 [==============================] - 0s 827us/step - loss: 0.2052 - accuracy: 0.9160\n",
      "Epoch 338/1500\n",
      "49/49 [==============================] - 0s 856us/step - loss: 0.2024 - accuracy: 0.9202\n",
      "Epoch 339/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.2221 - accuracy: 0.9069\n",
      "Epoch 340/1500\n",
      "49/49 [==============================] - 0s 801us/step - loss: 0.2024 - accuracy: 0.9166\n",
      "Epoch 341/1500\n",
      "49/49 [==============================] - 0s 786us/step - loss: 0.2093 - accuracy: 0.9189\n",
      "Epoch 342/1500\n",
      "49/49 [==============================] - 0s 796us/step - loss: 0.2071 - accuracy: 0.9143\n",
      "Epoch 343/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.2094 - accuracy: 0.9150\n",
      "Epoch 344/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.2124 - accuracy: 0.9153\n",
      "Epoch 345/1500\n",
      "49/49 [==============================] - 0s 864us/step - loss: 0.2048 - accuracy: 0.9218\n",
      "Epoch 346/1500\n",
      "49/49 [==============================] - 0s 918us/step - loss: 0.2066 - accuracy: 0.9160\n",
      "Epoch 347/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9231\n",
      "Epoch 348/1500\n",
      "49/49 [==============================] - 0s 988us/step - loss: 0.2120 - accuracy: 0.9137\n",
      "Epoch 349/1500\n",
      "49/49 [==============================] - 0s 920us/step - loss: 0.2227 - accuracy: 0.9137\n",
      "Epoch 350/1500\n",
      "49/49 [==============================] - 0s 812us/step - loss: 0.2002 - accuracy: 0.9257\n",
      "Epoch 351/1500\n",
      "49/49 [==============================] - 0s 847us/step - loss: 0.2080 - accuracy: 0.9189\n",
      "Epoch 352/1500\n",
      "49/49 [==============================] - 0s 830us/step - loss: 0.2092 - accuracy: 0.9166\n",
      "Epoch 353/1500\n",
      "49/49 [==============================] - 0s 821us/step - loss: 0.2047 - accuracy: 0.9270\n",
      "Epoch 354/1500\n",
      "49/49 [==============================] - 0s 929us/step - loss: 0.2024 - accuracy: 0.9205\n",
      "Epoch 355/1500\n",
      "49/49 [==============================] - 0s 924us/step - loss: 0.2063 - accuracy: 0.9179\n",
      "Epoch 356/1500\n",
      "49/49 [==============================] - 0s 904us/step - loss: 0.2156 - accuracy: 0.9169\n",
      "Epoch 357/1500\n",
      "49/49 [==============================] - 0s 986us/step - loss: 0.2048 - accuracy: 0.9195\n",
      "Epoch 358/1500\n",
      "49/49 [==============================] - 0s 929us/step - loss: 0.2095 - accuracy: 0.9257\n",
      "Epoch 359/1500\n",
      "49/49 [==============================] - 0s 906us/step - loss: 0.2069 - accuracy: 0.9117\n",
      "Epoch 360/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9134\n",
      "Epoch 361/1500\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9127\n",
      "Epoch 362/1500\n",
      "49/49 [==============================] - 0s 820us/step - loss: 0.2001 - accuracy: 0.9260\n",
      "Epoch 363/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.2000 - accuracy: 0.9225\n",
      "Epoch 364/1500\n",
      "49/49 [==============================] - 0s 827us/step - loss: 0.2011 - accuracy: 0.9163\n",
      "Epoch 365/1500\n",
      "49/49 [==============================] - 0s 813us/step - loss: 0.2144 - accuracy: 0.9137\n",
      "Epoch 366/1500\n",
      "49/49 [==============================] - 0s 848us/step - loss: 0.2096 - accuracy: 0.9166\n",
      "Epoch 367/1500\n",
      "49/49 [==============================] - 0s 863us/step - loss: 0.2104 - accuracy: 0.9166\n",
      "Epoch 368/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.2123 - accuracy: 0.9212\n",
      "Epoch 369/1500\n",
      "49/49 [==============================] - 0s 835us/step - loss: 0.2096 - accuracy: 0.9205\n",
      "Epoch 370/1500\n",
      "49/49 [==============================] - 0s 805us/step - loss: 0.2067 - accuracy: 0.9156\n",
      "Epoch 371/1500\n",
      "49/49 [==============================] - 0s 825us/step - loss: 0.1918 - accuracy: 0.9247\n",
      "Epoch 372/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2055 - accuracy: 0.9169\n",
      "Epoch 373/1500\n",
      "49/49 [==============================] - 0s 828us/step - loss: 0.1980 - accuracy: 0.9199\n",
      "Epoch 374/1500\n",
      "49/49 [==============================] - 0s 816us/step - loss: 0.1996 - accuracy: 0.9234\n",
      "Epoch 375/1500\n",
      "49/49 [==============================] - 0s 854us/step - loss: 0.2146 - accuracy: 0.9091\n",
      "Epoch 376/1500\n",
      "49/49 [==============================] - 0s 860us/step - loss: 0.2123 - accuracy: 0.9124\n",
      "Epoch 377/1500\n",
      "49/49 [==============================] - 0s 815us/step - loss: 0.1981 - accuracy: 0.9270\n",
      "Epoch 378/1500\n",
      "49/49 [==============================] - 0s 790us/step - loss: 0.2100 - accuracy: 0.9163\n",
      "Epoch 379/1500\n",
      "49/49 [==============================] - 0s 861us/step - loss: 0.2079 - accuracy: 0.9153\n",
      "Epoch 380/1500\n",
      "49/49 [==============================] - 0s 833us/step - loss: 0.1885 - accuracy: 0.9263\n",
      "Epoch 381/1500\n",
      "49/49 [==============================] - 0s 838us/step - loss: 0.2042 - accuracy: 0.9153\n",
      "Epoch 382/1500\n",
      "49/49 [==============================] - 0s 800us/step - loss: 0.2224 - accuracy: 0.9114\n",
      "Epoch 383/1500\n",
      "49/49 [==============================] - 0s 858us/step - loss: 0.1991 - accuracy: 0.9176\n",
      "Epoch 384/1500\n",
      "49/49 [==============================] - 0s 850us/step - loss: 0.2020 - accuracy: 0.9166\n",
      "Epoch 385/1500\n",
      "49/49 [==============================] - 0s 821us/step - loss: 0.2009 - accuracy: 0.9176\n",
      "Epoch 386/1500\n",
      "49/49 [==============================] - 0s 844us/step - loss: 0.2056 - accuracy: 0.9189\n",
      "Epoch 387/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.9166\n",
      "Epoch 388/1500\n",
      "49/49 [==============================] - 0s 794us/step - loss: 0.2164 - accuracy: 0.9156\n",
      "Epoch 389/1500\n",
      "49/49 [==============================] - 0s 802us/step - loss: 0.2050 - accuracy: 0.9153\n",
      "Epoch 390/1500\n",
      "49/49 [==============================] - 0s 796us/step - loss: 0.2052 - accuracy: 0.9225\n",
      "Epoch 391/1500\n",
      "49/49 [==============================] - 0s 737us/step - loss: 0.2109 - accuracy: 0.9166\n",
      "Epoch 392/1500\n",
      "49/49 [==============================] - 0s 822us/step - loss: 0.1993 - accuracy: 0.9241\n",
      "Epoch 393/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.9250\n",
      "Epoch 394/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9250\n",
      "Epoch 395/1500\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9231\n",
      "Epoch 396/1500\n",
      "49/49 [==============================] - 0s 870us/step - loss: 0.2038 - accuracy: 0.9228\n",
      "Epoch 397/1500\n",
      "49/49 [==============================] - 0s 825us/step - loss: 0.1952 - accuracy: 0.9234\n",
      "Epoch 398/1500\n",
      "49/49 [==============================] - 0s 869us/step - loss: 0.2026 - accuracy: 0.9192\n",
      "Epoch 399/1500\n",
      "49/49 [==============================] - 0s 855us/step - loss: 0.1943 - accuracy: 0.9257\n",
      "Epoch 400/1500\n",
      "49/49 [==============================] - 0s 880us/step - loss: 0.1924 - accuracy: 0.9244\n",
      "Epoch 401/1500\n",
      "49/49 [==============================] - 0s 879us/step - loss: 0.2047 - accuracy: 0.9179\n",
      "Epoch 402/1500\n",
      "49/49 [==============================] - 0s 774us/step - loss: 0.1882 - accuracy: 0.9267\n",
      "Epoch 403/1500\n",
      "49/49 [==============================] - 0s 778us/step - loss: 0.1886 - accuracy: 0.9273\n",
      "Epoch 404/1500\n",
      "49/49 [==============================] - 0s 749us/step - loss: 0.1913 - accuracy: 0.9273\n",
      "Epoch 405/1500\n",
      "49/49 [==============================] - 0s 760us/step - loss: 0.2052 - accuracy: 0.9160\n",
      "Epoch 406/1500\n",
      "49/49 [==============================] - 0s 900us/step - loss: 0.1947 - accuracy: 0.9254\n",
      "Epoch 407/1500\n",
      "49/49 [==============================] - 0s 841us/step - loss: 0.1865 - accuracy: 0.9254\n",
      "Epoch 408/1500\n",
      "49/49 [==============================] - 0s 826us/step - loss: 0.2038 - accuracy: 0.9202\n",
      "Epoch 409/1500\n",
      "49/49 [==============================] - 0s 971us/step - loss: 0.1902 - accuracy: 0.9273\n",
      "Epoch 410/1500\n",
      "49/49 [==============================] - 0s 1000us/step - loss: 0.1947 - accuracy: 0.9202\n",
      "Epoch 411/1500\n",
      "49/49 [==============================] - 0s 967us/step - loss: 0.2046 - accuracy: 0.9163\n",
      "Epoch 412/1500\n",
      "49/49 [==============================] - 0s 834us/step - loss: 0.2049 - accuracy: 0.9189\n",
      "Epoch 413/1500\n",
      "49/49 [==============================] - 0s 833us/step - loss: 0.1966 - accuracy: 0.9238\n",
      "Epoch 414/1500\n",
      "49/49 [==============================] - 0s 844us/step - loss: 0.1932 - accuracy: 0.9244\n",
      "Epoch 415/1500\n",
      "49/49 [==============================] - 0s 849us/step - loss: 0.1975 - accuracy: 0.9263\n",
      "Epoch 416/1500\n",
      "49/49 [==============================] - 0s 888us/step - loss: 0.1899 - accuracy: 0.9286\n",
      "Epoch 417/1500\n",
      "49/49 [==============================] - 0s 903us/step - loss: 0.1845 - accuracy: 0.9283\n",
      "Epoch 418/1500\n",
      "49/49 [==============================] - 0s 931us/step - loss: 0.1916 - accuracy: 0.9228\n",
      "Epoch 419/1500\n",
      "49/49 [==============================] - 0s 859us/step - loss: 0.1895 - accuracy: 0.9276\n",
      "Epoch 420/1500\n",
      "49/49 [==============================] - 0s 968us/step - loss: 0.1967 - accuracy: 0.9247\n",
      "Epoch 421/1500\n",
      "49/49 [==============================] - 0s 909us/step - loss: 0.1860 - accuracy: 0.9244\n",
      "Epoch 422/1500\n",
      "49/49 [==============================] - 0s 886us/step - loss: 0.1991 - accuracy: 0.9250\n",
      "Epoch 423/1500\n",
      "49/49 [==============================] - 0s 808us/step - loss: 0.1854 - accuracy: 0.9296\n",
      "Epoch 424/1500\n",
      " 1/49 [..............................] - ETA: 0s - loss: 0.1906 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 394.\n",
      "49/49 [==============================] - 0s 841us/step - loss: 0.1903 - accuracy: 0.9238\n",
      "Epoch 424: early stopping\n",
      "7/7 [==============================] - 0s 873us/step - loss: 0.6656 - accuracy: 0.7740\n",
      "7/7 [==============================] - 0s 642us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.6656005382537842, Accuracy: 0.7740384340286255, Precision: 0.6779569892473117, Recall: 0.8766829086924192, F1 Score: 0.7221982750018957\n",
      "Confusion Matrix:\n",
      " [[122   8  37]\n",
      " [  1  16   0]\n",
      " [  1   0  23]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A', '000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'043A', '019A'}\n",
      "Moved to Test Set:\n",
      "{'043A', '019A'}\n",
      "Removed from Test Set\n",
      "{'046A', '000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '044A' '045A' '046A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '019A' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '043A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "674\n",
      "Length of y_train_val:\n",
      "674\n",
      "Length of groups_train_val:\n",
      "674\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     439\n",
      "kitten    122\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     149\n",
      "senior     65\n",
      "kitten     49\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1083, 1: 854, 2: 777})\n",
      "Epoch 1/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 1.1137 - accuracy: 0.4772\n",
      "Epoch 2/1500\n",
      "43/43 [==============================] - 0s 991us/step - loss: 0.9150 - accuracy: 0.5777\n",
      "Epoch 3/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.8066 - accuracy: 0.6459\n",
      "Epoch 4/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.7827 - accuracy: 0.6581\n",
      "Epoch 5/1500\n",
      "43/43 [==============================] - 0s 822us/step - loss: 0.7257 - accuracy: 0.6916\n",
      "Epoch 6/1500\n",
      "43/43 [==============================] - 0s 818us/step - loss: 0.6789 - accuracy: 0.7108\n",
      "Epoch 7/1500\n",
      "43/43 [==============================] - 0s 853us/step - loss: 0.6650 - accuracy: 0.7155\n",
      "Epoch 8/1500\n",
      "43/43 [==============================] - 0s 821us/step - loss: 0.6476 - accuracy: 0.7203\n",
      "Epoch 9/1500\n",
      "43/43 [==============================] - 0s 800us/step - loss: 0.6300 - accuracy: 0.7358\n",
      "Epoch 10/1500\n",
      "43/43 [==============================] - 0s 812us/step - loss: 0.6163 - accuracy: 0.7325\n",
      "Epoch 11/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.5937 - accuracy: 0.7542\n",
      "Epoch 12/1500\n",
      "43/43 [==============================] - 0s 910us/step - loss: 0.5840 - accuracy: 0.7546\n",
      "Epoch 13/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.5868 - accuracy: 0.7531\n",
      "Epoch 14/1500\n",
      "43/43 [==============================] - 0s 845us/step - loss: 0.5683 - accuracy: 0.7564\n",
      "Epoch 15/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.5602 - accuracy: 0.7520\n",
      "Epoch 16/1500\n",
      "43/43 [==============================] - 0s 826us/step - loss: 0.5681 - accuracy: 0.7535\n",
      "Epoch 17/1500\n",
      "43/43 [==============================] - 0s 829us/step - loss: 0.5366 - accuracy: 0.7668\n",
      "Epoch 18/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.5266 - accuracy: 0.7782\n",
      "Epoch 19/1500\n",
      "43/43 [==============================] - 0s 830us/step - loss: 0.5228 - accuracy: 0.7863\n",
      "Epoch 20/1500\n",
      "43/43 [==============================] - 0s 838us/step - loss: 0.5303 - accuracy: 0.7815\n",
      "Epoch 21/1500\n",
      "43/43 [==============================] - 0s 928us/step - loss: 0.5182 - accuracy: 0.7933\n",
      "Epoch 22/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.7892\n",
      "Epoch 23/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.4935 - accuracy: 0.7970\n",
      "Epoch 24/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.4786 - accuracy: 0.7999\n",
      "Epoch 25/1500\n",
      "43/43 [==============================] - 0s 895us/step - loss: 0.5011 - accuracy: 0.7822\n",
      "Epoch 26/1500\n",
      "43/43 [==============================] - 0s 907us/step - loss: 0.4806 - accuracy: 0.8040\n",
      "Epoch 27/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.4990 - accuracy: 0.7922\n",
      "Epoch 28/1500\n",
      "43/43 [==============================] - 0s 941us/step - loss: 0.4933 - accuracy: 0.7970\n",
      "Epoch 29/1500\n",
      "43/43 [==============================] - 0s 927us/step - loss: 0.4769 - accuracy: 0.7999\n",
      "Epoch 30/1500\n",
      "43/43 [==============================] - 0s 974us/step - loss: 0.4673 - accuracy: 0.8136\n",
      "Epoch 31/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.8018\n",
      "Epoch 32/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.8161\n",
      "Epoch 33/1500\n",
      "43/43 [==============================] - 0s 988us/step - loss: 0.4579 - accuracy: 0.8043\n",
      "Epoch 34/1500\n",
      "43/43 [==============================] - 0s 923us/step - loss: 0.4487 - accuracy: 0.8143\n",
      "Epoch 35/1500\n",
      "43/43 [==============================] - 0s 896us/step - loss: 0.4644 - accuracy: 0.8018\n",
      "Epoch 36/1500\n",
      "43/43 [==============================] - 0s 903us/step - loss: 0.4577 - accuracy: 0.8091\n",
      "Epoch 37/1500\n",
      "43/43 [==============================] - 0s 950us/step - loss: 0.4303 - accuracy: 0.8147\n",
      "Epoch 38/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.4456 - accuracy: 0.8125\n",
      "Epoch 39/1500\n",
      "43/43 [==============================] - 0s 888us/step - loss: 0.4420 - accuracy: 0.8165\n",
      "Epoch 40/1500\n",
      "43/43 [==============================] - 0s 883us/step - loss: 0.4274 - accuracy: 0.8198\n",
      "Epoch 41/1500\n",
      "43/43 [==============================] - 0s 873us/step - loss: 0.4402 - accuracy: 0.8187\n",
      "Epoch 42/1500\n",
      "43/43 [==============================] - 0s 885us/step - loss: 0.4298 - accuracy: 0.8235\n",
      "Epoch 43/1500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.4407 - accuracy: 0.8261\n",
      "Epoch 44/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.4235 - accuracy: 0.8272\n",
      "Epoch 45/1500\n",
      "43/43 [==============================] - 0s 911us/step - loss: 0.4256 - accuracy: 0.8298\n",
      "Epoch 46/1500\n",
      "43/43 [==============================] - 0s 914us/step - loss: 0.4290 - accuracy: 0.8231\n",
      "Epoch 47/1500\n",
      "43/43 [==============================] - 0s 899us/step - loss: 0.4223 - accuracy: 0.8254\n",
      "Epoch 48/1500\n",
      "43/43 [==============================] - 0s 916us/step - loss: 0.4150 - accuracy: 0.8309\n",
      "Epoch 49/1500\n",
      "43/43 [==============================] - 0s 970us/step - loss: 0.4168 - accuracy: 0.8331\n",
      "Epoch 50/1500\n",
      "43/43 [==============================] - 0s 943us/step - loss: 0.4170 - accuracy: 0.8272\n",
      "Epoch 51/1500\n",
      "43/43 [==============================] - 0s 921us/step - loss: 0.3939 - accuracy: 0.8401\n",
      "Epoch 52/1500\n",
      "43/43 [==============================] - 0s 853us/step - loss: 0.3989 - accuracy: 0.8434\n",
      "Epoch 53/1500\n",
      "43/43 [==============================] - 0s 860us/step - loss: 0.4123 - accuracy: 0.8386\n",
      "Epoch 54/1500\n",
      "43/43 [==============================] - 0s 851us/step - loss: 0.4096 - accuracy: 0.8379\n",
      "Epoch 55/1500\n",
      "43/43 [==============================] - 0s 875us/step - loss: 0.4125 - accuracy: 0.8298\n",
      "Epoch 56/1500\n",
      "43/43 [==============================] - 0s 883us/step - loss: 0.4077 - accuracy: 0.8301\n",
      "Epoch 57/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.3935 - accuracy: 0.8331\n",
      "Epoch 58/1500\n",
      "43/43 [==============================] - 0s 888us/step - loss: 0.3921 - accuracy: 0.8464\n",
      "Epoch 59/1500\n",
      "43/43 [==============================] - 0s 860us/step - loss: 0.3945 - accuracy: 0.8405\n",
      "Epoch 60/1500\n",
      "43/43 [==============================] - 0s 866us/step - loss: 0.4027 - accuracy: 0.8290\n",
      "Epoch 61/1500\n",
      "43/43 [==============================] - 0s 896us/step - loss: 0.3850 - accuracy: 0.8394\n",
      "Epoch 62/1500\n",
      "43/43 [==============================] - 0s 937us/step - loss: 0.3929 - accuracy: 0.8405\n",
      "Epoch 63/1500\n",
      "43/43 [==============================] - 0s 876us/step - loss: 0.3755 - accuracy: 0.8482\n",
      "Epoch 64/1500\n",
      "43/43 [==============================] - 0s 900us/step - loss: 0.3923 - accuracy: 0.8441\n",
      "Epoch 65/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.3978 - accuracy: 0.8434\n",
      "Epoch 66/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.3813 - accuracy: 0.8419\n",
      "Epoch 67/1500\n",
      "43/43 [==============================] - 0s 945us/step - loss: 0.3815 - accuracy: 0.8464\n",
      "Epoch 68/1500\n",
      "43/43 [==============================] - 0s 927us/step - loss: 0.3715 - accuracy: 0.8522\n",
      "Epoch 69/1500\n",
      "43/43 [==============================] - 0s 891us/step - loss: 0.3672 - accuracy: 0.8556\n",
      "Epoch 70/1500\n",
      "43/43 [==============================] - 0s 925us/step - loss: 0.3587 - accuracy: 0.8508\n",
      "Epoch 71/1500\n",
      "43/43 [==============================] - 0s 883us/step - loss: 0.3812 - accuracy: 0.8386\n",
      "Epoch 72/1500\n",
      "43/43 [==============================] - 0s 934us/step - loss: 0.3705 - accuracy: 0.8364\n",
      "Epoch 73/1500\n",
      "43/43 [==============================] - 0s 932us/step - loss: 0.3553 - accuracy: 0.8570\n",
      "Epoch 74/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8416\n",
      "Epoch 75/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8596\n",
      "Epoch 76/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8489\n",
      "Epoch 77/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3590 - accuracy: 0.8559\n",
      "Epoch 78/1500\n",
      "43/43 [==============================] - 0s 966us/step - loss: 0.3622 - accuracy: 0.8570\n",
      "Epoch 79/1500\n",
      "43/43 [==============================] - 0s 987us/step - loss: 0.3609 - accuracy: 0.8585\n",
      "Epoch 80/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8541\n",
      "Epoch 81/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.8526\n",
      "Epoch 82/1500\n",
      "43/43 [==============================] - 0s 969us/step - loss: 0.3556 - accuracy: 0.8629\n",
      "Epoch 83/1500\n",
      "43/43 [==============================] - 0s 887us/step - loss: 0.3485 - accuracy: 0.8622\n",
      "Epoch 84/1500\n",
      "43/43 [==============================] - 0s 970us/step - loss: 0.3371 - accuracy: 0.8600\n",
      "Epoch 85/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.3462 - accuracy: 0.8666\n",
      "Epoch 86/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.8651\n",
      "Epoch 87/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3486 - accuracy: 0.8592\n",
      "Epoch 88/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8662\n",
      "Epoch 89/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8526\n",
      "Epoch 90/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3379 - accuracy: 0.8648\n",
      "Epoch 91/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8633\n",
      "Epoch 92/1500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8718\n",
      "Epoch 93/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8674\n",
      "Epoch 94/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8666\n",
      "Epoch 95/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8725\n",
      "Epoch 96/1500\n",
      "43/43 [==============================] - 0s 923us/step - loss: 0.3319 - accuracy: 0.8670\n",
      "Epoch 97/1500\n",
      "43/43 [==============================] - 0s 906us/step - loss: 0.3364 - accuracy: 0.8692\n",
      "Epoch 98/1500\n",
      "43/43 [==============================] - 0s 963us/step - loss: 0.3327 - accuracy: 0.8637\n",
      "Epoch 99/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8604\n",
      "Epoch 100/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3257 - accuracy: 0.8744\n",
      "Epoch 101/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8744\n",
      "Epoch 102/1500\n",
      "43/43 [==============================] - 0s 979us/step - loss: 0.3230 - accuracy: 0.8721\n",
      "Epoch 103/1500\n",
      "43/43 [==============================] - 0s 978us/step - loss: 0.3310 - accuracy: 0.8721\n",
      "Epoch 104/1500\n",
      "43/43 [==============================] - 0s 899us/step - loss: 0.3117 - accuracy: 0.8710\n",
      "Epoch 105/1500\n",
      "43/43 [==============================] - 0s 859us/step - loss: 0.3205 - accuracy: 0.8644\n",
      "Epoch 106/1500\n",
      "43/43 [==============================] - 0s 822us/step - loss: 0.3249 - accuracy: 0.8721\n",
      "Epoch 107/1500\n",
      "43/43 [==============================] - 0s 867us/step - loss: 0.3181 - accuracy: 0.8692\n",
      "Epoch 108/1500\n",
      "43/43 [==============================] - 0s 843us/step - loss: 0.3240 - accuracy: 0.8655\n",
      "Epoch 109/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.3194 - accuracy: 0.8703\n",
      "Epoch 110/1500\n",
      "43/43 [==============================] - 0s 825us/step - loss: 0.3252 - accuracy: 0.8710\n",
      "Epoch 111/1500\n",
      "43/43 [==============================] - 0s 938us/step - loss: 0.3115 - accuracy: 0.8817\n",
      "Epoch 112/1500\n",
      "43/43 [==============================] - 0s 916us/step - loss: 0.3113 - accuracy: 0.8766\n",
      "Epoch 113/1500\n",
      "43/43 [==============================] - 0s 860us/step - loss: 0.3147 - accuracy: 0.8659\n",
      "Epoch 114/1500\n",
      "43/43 [==============================] - 0s 949us/step - loss: 0.3193 - accuracy: 0.8751\n",
      "Epoch 115/1500\n",
      "43/43 [==============================] - 0s 986us/step - loss: 0.3250 - accuracy: 0.8718\n",
      "Epoch 116/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3141 - accuracy: 0.8758\n",
      "Epoch 117/1500\n",
      "43/43 [==============================] - 0s 968us/step - loss: 0.3142 - accuracy: 0.8773\n",
      "Epoch 118/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.3135 - accuracy: 0.8810\n",
      "Epoch 119/1500\n",
      "43/43 [==============================] - 0s 858us/step - loss: 0.3009 - accuracy: 0.8854\n",
      "Epoch 120/1500\n",
      "43/43 [==============================] - 0s 818us/step - loss: 0.3128 - accuracy: 0.8780\n",
      "Epoch 121/1500\n",
      "43/43 [==============================] - 0s 845us/step - loss: 0.3188 - accuracy: 0.8799\n",
      "Epoch 122/1500\n",
      "43/43 [==============================] - 0s 853us/step - loss: 0.3101 - accuracy: 0.8803\n",
      "Epoch 123/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8688\n",
      "Epoch 124/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3034 - accuracy: 0.8791\n",
      "Epoch 125/1500\n",
      "43/43 [==============================] - 0s 933us/step - loss: 0.2951 - accuracy: 0.8825\n",
      "Epoch 126/1500\n",
      "43/43 [==============================] - 0s 943us/step - loss: 0.2987 - accuracy: 0.8762\n",
      "Epoch 127/1500\n",
      "43/43 [==============================] - 0s 962us/step - loss: 0.2946 - accuracy: 0.8773\n",
      "Epoch 128/1500\n",
      "43/43 [==============================] - 0s 917us/step - loss: 0.2962 - accuracy: 0.8821\n",
      "Epoch 129/1500\n",
      "43/43 [==============================] - 0s 928us/step - loss: 0.3052 - accuracy: 0.8887\n",
      "Epoch 130/1500\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.2957 - accuracy: 0.8803\n",
      "Epoch 131/1500\n",
      "43/43 [==============================] - 0s 966us/step - loss: 0.2989 - accuracy: 0.8777\n",
      "Epoch 132/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.8836\n",
      "Epoch 133/1500\n",
      "43/43 [==============================] - 0s 914us/step - loss: 0.2881 - accuracy: 0.8814\n",
      "Epoch 134/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2968 - accuracy: 0.8755\n",
      "Epoch 135/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2892 - accuracy: 0.8909\n",
      "Epoch 136/1500\n",
      "43/43 [==============================] - 0s 861us/step - loss: 0.2970 - accuracy: 0.8887\n",
      "Epoch 137/1500\n",
      "43/43 [==============================] - 0s 815us/step - loss: 0.2896 - accuracy: 0.8843\n",
      "Epoch 138/1500\n",
      "43/43 [==============================] - 0s 834us/step - loss: 0.2986 - accuracy: 0.8839\n",
      "Epoch 139/1500\n",
      "43/43 [==============================] - 0s 866us/step - loss: 0.2967 - accuracy: 0.8803\n",
      "Epoch 140/1500\n",
      "43/43 [==============================] - 0s 906us/step - loss: 0.2939 - accuracy: 0.8784\n",
      "Epoch 141/1500\n",
      "43/43 [==============================] - 0s 823us/step - loss: 0.2936 - accuracy: 0.8854\n",
      "Epoch 142/1500\n",
      "43/43 [==============================] - 0s 818us/step - loss: 0.2926 - accuracy: 0.8839\n",
      "Epoch 143/1500\n",
      "43/43 [==============================] - 0s 840us/step - loss: 0.2827 - accuracy: 0.8898\n",
      "Epoch 144/1500\n",
      "43/43 [==============================] - 0s 820us/step - loss: 0.2879 - accuracy: 0.8847\n",
      "Epoch 145/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2803 - accuracy: 0.8836\n",
      "Epoch 146/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.2943 - accuracy: 0.8784\n",
      "Epoch 147/1500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8880\n",
      "Epoch 148/1500\n",
      "43/43 [==============================] - 0s 932us/step - loss: 0.2862 - accuracy: 0.8873\n",
      "Epoch 149/1500\n",
      "43/43 [==============================] - 0s 854us/step - loss: 0.2687 - accuracy: 0.8950\n",
      "Epoch 150/1500\n",
      "43/43 [==============================] - 0s 890us/step - loss: 0.2802 - accuracy: 0.8928\n",
      "Epoch 151/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.2880 - accuracy: 0.8920\n",
      "Epoch 152/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2743 - accuracy: 0.8954\n",
      "Epoch 153/1500\n",
      "43/43 [==============================] - 0s 884us/step - loss: 0.2669 - accuracy: 0.8898\n",
      "Epoch 154/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2836 - accuracy: 0.8902\n",
      "Epoch 155/1500\n",
      "43/43 [==============================] - 0s 895us/step - loss: 0.2906 - accuracy: 0.8854\n",
      "Epoch 156/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.2833 - accuracy: 0.8843\n",
      "Epoch 157/1500\n",
      "43/43 [==============================] - 0s 953us/step - loss: 0.2639 - accuracy: 0.8983\n",
      "Epoch 158/1500\n",
      "43/43 [==============================] - 0s 879us/step - loss: 0.2686 - accuracy: 0.8990\n",
      "Epoch 159/1500\n",
      "43/43 [==============================] - 0s 840us/step - loss: 0.2807 - accuracy: 0.8887\n",
      "Epoch 160/1500\n",
      "43/43 [==============================] - 0s 851us/step - loss: 0.2620 - accuracy: 0.9005\n",
      "Epoch 161/1500\n",
      "43/43 [==============================] - 0s 872us/step - loss: 0.2728 - accuracy: 0.8895\n",
      "Epoch 162/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.2877 - accuracy: 0.8825\n",
      "Epoch 163/1500\n",
      "43/43 [==============================] - 0s 841us/step - loss: 0.2602 - accuracy: 0.8990\n",
      "Epoch 164/1500\n",
      "43/43 [==============================] - 0s 854us/step - loss: 0.2700 - accuracy: 0.8931\n",
      "Epoch 165/1500\n",
      "43/43 [==============================] - 0s 919us/step - loss: 0.2691 - accuracy: 0.8906\n",
      "Epoch 166/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.2713 - accuracy: 0.8939\n",
      "Epoch 167/1500\n",
      "43/43 [==============================] - 0s 830us/step - loss: 0.2623 - accuracy: 0.8961\n",
      "Epoch 168/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.2719 - accuracy: 0.8979\n",
      "Epoch 169/1500\n",
      "43/43 [==============================] - 0s 867us/step - loss: 0.2557 - accuracy: 0.8990\n",
      "Epoch 170/1500\n",
      "43/43 [==============================] - 0s 864us/step - loss: 0.2772 - accuracy: 0.8965\n",
      "Epoch 171/1500\n",
      "43/43 [==============================] - 0s 869us/step - loss: 0.2596 - accuracy: 0.8961\n",
      "Epoch 172/1500\n",
      "43/43 [==============================] - 0s 869us/step - loss: 0.2653 - accuracy: 0.9031\n",
      "Epoch 173/1500\n",
      "43/43 [==============================] - 0s 908us/step - loss: 0.2533 - accuracy: 0.9005\n",
      "Epoch 174/1500\n",
      "43/43 [==============================] - 0s 924us/step - loss: 0.2708 - accuracy: 0.8928\n",
      "Epoch 175/1500\n",
      "43/43 [==============================] - 0s 860us/step - loss: 0.2517 - accuracy: 0.8968\n",
      "Epoch 176/1500\n",
      "43/43 [==============================] - 0s 882us/step - loss: 0.2488 - accuracy: 0.9027\n",
      "Epoch 177/1500\n",
      "43/43 [==============================] - 0s 866us/step - loss: 0.2534 - accuracy: 0.9013\n",
      "Epoch 178/1500\n",
      "43/43 [==============================] - 0s 873us/step - loss: 0.2569 - accuracy: 0.9024\n",
      "Epoch 179/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2522 - accuracy: 0.9083\n",
      "Epoch 180/1500\n",
      "43/43 [==============================] - 0s 864us/step - loss: 0.2578 - accuracy: 0.8972\n",
      "Epoch 181/1500\n",
      "43/43 [==============================] - 0s 855us/step - loss: 0.2608 - accuracy: 0.8987\n",
      "Epoch 182/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.2562 - accuracy: 0.9005\n",
      "Epoch 183/1500\n",
      "43/43 [==============================] - 0s 933us/step - loss: 0.2485 - accuracy: 0.9031\n",
      "Epoch 184/1500\n",
      "43/43 [==============================] - 0s 952us/step - loss: 0.2620 - accuracy: 0.8968\n",
      "Epoch 185/1500\n",
      "43/43 [==============================] - 0s 922us/step - loss: 0.2544 - accuracy: 0.8946\n",
      "Epoch 186/1500\n",
      "43/43 [==============================] - 0s 944us/step - loss: 0.2382 - accuracy: 0.9079\n",
      "Epoch 187/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.2531 - accuracy: 0.9001\n",
      "Epoch 188/1500\n",
      "43/43 [==============================] - 0s 888us/step - loss: 0.2473 - accuracy: 0.9057\n",
      "Epoch 189/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2503 - accuracy: 0.8983\n",
      "Epoch 190/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.2541 - accuracy: 0.9075\n",
      "Epoch 191/1500\n",
      "43/43 [==============================] - 0s 812us/step - loss: 0.2593 - accuracy: 0.8983\n",
      "Epoch 192/1500\n",
      "43/43 [==============================] - 0s 847us/step - loss: 0.2442 - accuracy: 0.9009\n",
      "Epoch 193/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2389 - accuracy: 0.9071\n",
      "Epoch 194/1500\n",
      "43/43 [==============================] - 0s 830us/step - loss: 0.2388 - accuracy: 0.9075\n",
      "Epoch 195/1500\n",
      "43/43 [==============================] - 0s 885us/step - loss: 0.2457 - accuracy: 0.9060\n",
      "Epoch 196/1500\n",
      "43/43 [==============================] - 0s 894us/step - loss: 0.2326 - accuracy: 0.9141\n",
      "Epoch 197/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.2514 - accuracy: 0.9001\n",
      "Epoch 198/1500\n",
      "43/43 [==============================] - 0s 823us/step - loss: 0.2421 - accuracy: 0.9090\n",
      "Epoch 199/1500\n",
      "43/43 [==============================] - 0s 838us/step - loss: 0.2473 - accuracy: 0.9101\n",
      "Epoch 200/1500\n",
      "43/43 [==============================] - 0s 819us/step - loss: 0.2404 - accuracy: 0.9064\n",
      "Epoch 201/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.2416 - accuracy: 0.9071\n",
      "Epoch 202/1500\n",
      "43/43 [==============================] - 0s 817us/step - loss: 0.2362 - accuracy: 0.9071\n",
      "Epoch 203/1500\n",
      "43/43 [==============================] - 0s 857us/step - loss: 0.2417 - accuracy: 0.9105\n",
      "Epoch 204/1500\n",
      "43/43 [==============================] - 0s 910us/step - loss: 0.2340 - accuracy: 0.9001\n",
      "Epoch 205/1500\n",
      "43/43 [==============================] - 0s 844us/step - loss: 0.2402 - accuracy: 0.9057\n",
      "Epoch 206/1500\n",
      "43/43 [==============================] - 0s 879us/step - loss: 0.2301 - accuracy: 0.9108\n",
      "Epoch 207/1500\n",
      "43/43 [==============================] - 0s 892us/step - loss: 0.2297 - accuracy: 0.9108\n",
      "Epoch 208/1500\n",
      "43/43 [==============================] - 0s 880us/step - loss: 0.2456 - accuracy: 0.8994\n",
      "Epoch 209/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.2257 - accuracy: 0.9112\n",
      "Epoch 210/1500\n",
      "43/43 [==============================] - 0s 843us/step - loss: 0.2467 - accuracy: 0.9075\n",
      "Epoch 211/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.2445 - accuracy: 0.9049\n",
      "Epoch 212/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.2301 - accuracy: 0.9141\n",
      "Epoch 213/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.2236 - accuracy: 0.9116\n",
      "Epoch 214/1500\n",
      "43/43 [==============================] - 0s 826us/step - loss: 0.2337 - accuracy: 0.9083\n",
      "Epoch 215/1500\n",
      "43/43 [==============================] - 0s 824us/step - loss: 0.2338 - accuracy: 0.9071\n",
      "Epoch 216/1500\n",
      "43/43 [==============================] - 0s 828us/step - loss: 0.2418 - accuracy: 0.8994\n",
      "Epoch 217/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.2285 - accuracy: 0.9171\n",
      "Epoch 218/1500\n",
      "43/43 [==============================] - 0s 855us/step - loss: 0.2312 - accuracy: 0.9075\n",
      "Epoch 219/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.2403 - accuracy: 0.9105\n",
      "Epoch 220/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.2222 - accuracy: 0.9101\n",
      "Epoch 221/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.2220 - accuracy: 0.9153\n",
      "Epoch 222/1500\n",
      "43/43 [==============================] - 0s 845us/step - loss: 0.2250 - accuracy: 0.9141\n",
      "Epoch 223/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.2338 - accuracy: 0.9119\n",
      "Epoch 224/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2270 - accuracy: 0.9123\n",
      "Epoch 225/1500\n",
      "43/43 [==============================] - 0s 896us/step - loss: 0.2245 - accuracy: 0.9141\n",
      "Epoch 226/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2228 - accuracy: 0.9141\n",
      "Epoch 227/1500\n",
      "43/43 [==============================] - 0s 917us/step - loss: 0.2374 - accuracy: 0.9119\n",
      "Epoch 228/1500\n",
      "43/43 [==============================] - 0s 962us/step - loss: 0.2163 - accuracy: 0.9223\n",
      "Epoch 229/1500\n",
      "43/43 [==============================] - 0s 905us/step - loss: 0.2249 - accuracy: 0.9138\n",
      "Epoch 230/1500\n",
      "43/43 [==============================] - 0s 859us/step - loss: 0.2301 - accuracy: 0.9149\n",
      "Epoch 231/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.2116 - accuracy: 0.9204\n",
      "Epoch 232/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.2366 - accuracy: 0.9031\n",
      "Epoch 233/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.2428 - accuracy: 0.9038\n",
      "Epoch 234/1500\n",
      "43/43 [==============================] - 0s 908us/step - loss: 0.2245 - accuracy: 0.9112\n",
      "Epoch 235/1500\n",
      "43/43 [==============================] - 0s 941us/step - loss: 0.2202 - accuracy: 0.9134\n",
      "Epoch 236/1500\n",
      "43/43 [==============================] - 0s 929us/step - loss: 0.2312 - accuracy: 0.9108\n",
      "Epoch 237/1500\n",
      "43/43 [==============================] - 0s 904us/step - loss: 0.2154 - accuracy: 0.9123\n",
      "Epoch 238/1500\n",
      "43/43 [==============================] - 0s 825us/step - loss: 0.2084 - accuracy: 0.9164\n",
      "Epoch 239/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.2192 - accuracy: 0.9149\n",
      "Epoch 240/1500\n",
      "43/43 [==============================] - 0s 807us/step - loss: 0.2175 - accuracy: 0.9127\n",
      "Epoch 241/1500\n",
      "43/43 [==============================] - 0s 884us/step - loss: 0.2162 - accuracy: 0.9175\n",
      "Epoch 242/1500\n",
      "43/43 [==============================] - 0s 807us/step - loss: 0.2279 - accuracy: 0.9105\n",
      "Epoch 243/1500\n",
      "43/43 [==============================] - 0s 816us/step - loss: 0.2188 - accuracy: 0.9208\n",
      "Epoch 244/1500\n",
      "43/43 [==============================] - 0s 804us/step - loss: 0.2132 - accuracy: 0.9200\n",
      "Epoch 245/1500\n",
      "43/43 [==============================] - 0s 853us/step - loss: 0.2303 - accuracy: 0.9164\n",
      "Epoch 246/1500\n",
      "43/43 [==============================] - 0s 910us/step - loss: 0.2203 - accuracy: 0.9138\n",
      "Epoch 247/1500\n",
      "43/43 [==============================] - 0s 945us/step - loss: 0.2101 - accuracy: 0.9193\n",
      "Epoch 248/1500\n",
      "43/43 [==============================] - 0s 898us/step - loss: 0.2188 - accuracy: 0.9164\n",
      "Epoch 249/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.2181 - accuracy: 0.9134\n",
      "Epoch 250/1500\n",
      "43/43 [==============================] - 0s 812us/step - loss: 0.2263 - accuracy: 0.9097\n",
      "Epoch 251/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.2161 - accuracy: 0.9068\n",
      "Epoch 252/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2143 - accuracy: 0.9182\n",
      "Epoch 253/1500\n",
      "43/43 [==============================] - 0s 826us/step - loss: 0.2066 - accuracy: 0.9256\n",
      "Epoch 254/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.2243 - accuracy: 0.9160\n",
      "Epoch 255/1500\n",
      "43/43 [==============================] - 0s 886us/step - loss: 0.2011 - accuracy: 0.9160\n",
      "Epoch 256/1500\n",
      "43/43 [==============================] - 0s 898us/step - loss: 0.2123 - accuracy: 0.9211\n",
      "Epoch 257/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2149 - accuracy: 0.9153\n",
      "Epoch 258/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.2225 - accuracy: 0.9200\n",
      "Epoch 259/1500\n",
      "43/43 [==============================] - 0s 835us/step - loss: 0.2047 - accuracy: 0.9263\n",
      "Epoch 260/1500\n",
      "43/43 [==============================] - 0s 884us/step - loss: 0.2062 - accuracy: 0.9182\n",
      "Epoch 261/1500\n",
      "43/43 [==============================] - 0s 847us/step - loss: 0.2207 - accuracy: 0.9153\n",
      "Epoch 262/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9149\n",
      "Epoch 263/1500\n",
      "43/43 [==============================] - 0s 866us/step - loss: 0.2113 - accuracy: 0.9245\n",
      "Epoch 264/1500\n",
      "43/43 [==============================] - 0s 848us/step - loss: 0.1965 - accuracy: 0.9219\n",
      "Epoch 265/1500\n",
      "43/43 [==============================] - 0s 870us/step - loss: 0.2173 - accuracy: 0.9156\n",
      "Epoch 266/1500\n",
      "43/43 [==============================] - 0s 817us/step - loss: 0.2279 - accuracy: 0.9130\n",
      "Epoch 267/1500\n",
      "43/43 [==============================] - 0s 929us/step - loss: 0.1918 - accuracy: 0.9311\n",
      "Epoch 268/1500\n",
      "43/43 [==============================] - 0s 933us/step - loss: 0.2152 - accuracy: 0.9186\n",
      "Epoch 269/1500\n",
      "43/43 [==============================] - 0s 909us/step - loss: 0.2121 - accuracy: 0.9167\n",
      "Epoch 270/1500\n",
      "43/43 [==============================] - 0s 861us/step - loss: 0.2202 - accuracy: 0.9175\n",
      "Epoch 271/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.2150 - accuracy: 0.9164\n",
      "Epoch 272/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.1888 - accuracy: 0.9215\n",
      "Epoch 273/1500\n",
      "43/43 [==============================] - 0s 960us/step - loss: 0.1994 - accuracy: 0.9267\n",
      "Epoch 274/1500\n",
      "43/43 [==============================] - 0s 818us/step - loss: 0.2016 - accuracy: 0.9248\n",
      "Epoch 275/1500\n",
      "43/43 [==============================] - 0s 807us/step - loss: 0.2020 - accuracy: 0.9208\n",
      "Epoch 276/1500\n",
      "43/43 [==============================] - 0s 818us/step - loss: 0.2080 - accuracy: 0.9141\n",
      "Epoch 277/1500\n",
      "43/43 [==============================] - 0s 797us/step - loss: 0.1970 - accuracy: 0.9241\n",
      "Epoch 278/1500\n",
      "43/43 [==============================] - 0s 862us/step - loss: 0.2051 - accuracy: 0.9200\n",
      "Epoch 279/1500\n",
      "43/43 [==============================] - 0s 888us/step - loss: 0.1987 - accuracy: 0.9274\n",
      "Epoch 280/1500\n",
      "43/43 [==============================] - 0s 926us/step - loss: 0.1924 - accuracy: 0.9256\n",
      "Epoch 281/1500\n",
      "43/43 [==============================] - 0s 867us/step - loss: 0.2045 - accuracy: 0.9241\n",
      "Epoch 282/1500\n",
      "43/43 [==============================] - 0s 896us/step - loss: 0.2001 - accuracy: 0.9252\n",
      "Epoch 283/1500\n",
      "43/43 [==============================] - 0s 877us/step - loss: 0.2048 - accuracy: 0.9234\n",
      "Epoch 284/1500\n",
      "43/43 [==============================] - 0s 848us/step - loss: 0.1896 - accuracy: 0.9270\n",
      "Epoch 285/1500\n",
      "43/43 [==============================] - 0s 920us/step - loss: 0.1986 - accuracy: 0.9293\n",
      "Epoch 286/1500\n",
      "43/43 [==============================] - 0s 864us/step - loss: 0.1911 - accuracy: 0.9252\n",
      "Epoch 287/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.1984 - accuracy: 0.9252\n",
      "Epoch 288/1500\n",
      "43/43 [==============================] - 0s 930us/step - loss: 0.1894 - accuracy: 0.9278\n",
      "Epoch 289/1500\n",
      "43/43 [==============================] - 0s 919us/step - loss: 0.2036 - accuracy: 0.9223\n",
      "Epoch 290/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.2060 - accuracy: 0.9208\n",
      "Epoch 291/1500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.1904 - accuracy: 0.9270\n",
      "Epoch 292/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9248\n",
      "Epoch 293/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9197\n",
      "Epoch 294/1500\n",
      "43/43 [==============================] - 0s 978us/step - loss: 0.1928 - accuracy: 0.9208\n",
      "Epoch 295/1500\n",
      "43/43 [==============================] - 0s 946us/step - loss: 0.1972 - accuracy: 0.9226\n",
      "Epoch 296/1500\n",
      "43/43 [==============================] - 0s 936us/step - loss: 0.1904 - accuracy: 0.9248\n",
      "Epoch 297/1500\n",
      "43/43 [==============================] - 0s 882us/step - loss: 0.1905 - accuracy: 0.9237\n",
      "Epoch 298/1500\n",
      "43/43 [==============================] - 0s 844us/step - loss: 0.1981 - accuracy: 0.9278\n",
      "Epoch 299/1500\n",
      "43/43 [==============================] - 0s 901us/step - loss: 0.1768 - accuracy: 0.9377\n",
      "Epoch 300/1500\n",
      "43/43 [==============================] - 0s 886us/step - loss: 0.2140 - accuracy: 0.9197\n",
      "Epoch 301/1500\n",
      "43/43 [==============================] - 0s 824us/step - loss: 0.2021 - accuracy: 0.9234\n",
      "Epoch 302/1500\n",
      "43/43 [==============================] - 0s 848us/step - loss: 0.1950 - accuracy: 0.9223\n",
      "Epoch 303/1500\n",
      "43/43 [==============================] - 0s 820us/step - loss: 0.1950 - accuracy: 0.9215\n",
      "Epoch 304/1500\n",
      "43/43 [==============================] - 0s 791us/step - loss: 0.1862 - accuracy: 0.9293\n",
      "Epoch 305/1500\n",
      "43/43 [==============================] - 0s 822us/step - loss: 0.2017 - accuracy: 0.9175\n",
      "Epoch 306/1500\n",
      "43/43 [==============================] - 0s 843us/step - loss: 0.2012 - accuracy: 0.9138\n",
      "Epoch 307/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.1984 - accuracy: 0.9252\n",
      "Epoch 308/1500\n",
      "43/43 [==============================] - 0s 900us/step - loss: 0.1900 - accuracy: 0.9256\n",
      "Epoch 309/1500\n",
      "43/43 [==============================] - 0s 880us/step - loss: 0.1846 - accuracy: 0.9296\n",
      "Epoch 310/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.1810 - accuracy: 0.9296\n",
      "Epoch 311/1500\n",
      "43/43 [==============================] - 0s 867us/step - loss: 0.1804 - accuracy: 0.9344\n",
      "Epoch 312/1500\n",
      "43/43 [==============================] - 0s 818us/step - loss: 0.1878 - accuracy: 0.9282\n",
      "Epoch 313/1500\n",
      "43/43 [==============================] - 0s 838us/step - loss: 0.1945 - accuracy: 0.9293\n",
      "Epoch 314/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.2058 - accuracy: 0.9230\n",
      "Epoch 315/1500\n",
      "43/43 [==============================] - 0s 864us/step - loss: 0.1782 - accuracy: 0.9289\n",
      "Epoch 316/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.2043 - accuracy: 0.9219\n",
      "Epoch 317/1500\n",
      "43/43 [==============================] - 0s 975us/step - loss: 0.1837 - accuracy: 0.9274\n",
      "Epoch 318/1500\n",
      "43/43 [==============================] - 0s 918us/step - loss: 0.1826 - accuracy: 0.9282\n",
      "Epoch 319/1500\n",
      "43/43 [==============================] - 0s 918us/step - loss: 0.1788 - accuracy: 0.9315\n",
      "Epoch 320/1500\n",
      "43/43 [==============================] - 0s 950us/step - loss: 0.1961 - accuracy: 0.9256\n",
      "Epoch 321/1500\n",
      "43/43 [==============================] - 0s 997us/step - loss: 0.1991 - accuracy: 0.9270\n",
      "Epoch 322/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.2000 - accuracy: 0.9219\n",
      "Epoch 323/1500\n",
      "43/43 [==============================] - 0s 835us/step - loss: 0.1915 - accuracy: 0.9289\n",
      "Epoch 324/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.1850 - accuracy: 0.9263\n",
      "Epoch 325/1500\n",
      "43/43 [==============================] - 0s 873us/step - loss: 0.1822 - accuracy: 0.9315\n",
      "Epoch 326/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9304\n",
      "Epoch 327/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9300\n",
      "Epoch 328/1500\n",
      "43/43 [==============================] - 0s 932us/step - loss: 0.1718 - accuracy: 0.9340\n",
      "Epoch 329/1500\n",
      "43/43 [==============================] - 0s 907us/step - loss: 0.1914 - accuracy: 0.9259\n",
      "Epoch 330/1500\n",
      "43/43 [==============================] - 0s 869us/step - loss: 0.1807 - accuracy: 0.9322\n",
      "Epoch 331/1500\n",
      "43/43 [==============================] - 0s 795us/step - loss: 0.1969 - accuracy: 0.9259\n",
      "Epoch 332/1500\n",
      "43/43 [==============================] - 0s 816us/step - loss: 0.1888 - accuracy: 0.9289\n",
      "Epoch 333/1500\n",
      "43/43 [==============================] - 0s 819us/step - loss: 0.1820 - accuracy: 0.9311\n",
      "Epoch 334/1500\n",
      "43/43 [==============================] - 0s 799us/step - loss: 0.1870 - accuracy: 0.9293\n",
      "Epoch 335/1500\n",
      "43/43 [==============================] - 0s 797us/step - loss: 0.1807 - accuracy: 0.9326\n",
      "Epoch 336/1500\n",
      "43/43 [==============================] - 0s 813us/step - loss: 0.1815 - accuracy: 0.9315\n",
      "Epoch 337/1500\n",
      "43/43 [==============================] - 0s 831us/step - loss: 0.1820 - accuracy: 0.9274\n",
      "Epoch 338/1500\n",
      "43/43 [==============================] - 0s 807us/step - loss: 0.1808 - accuracy: 0.9274\n",
      "Epoch 339/1500\n",
      "43/43 [==============================] - 0s 810us/step - loss: 0.1752 - accuracy: 0.9370\n",
      "Epoch 340/1500\n",
      "43/43 [==============================] - 0s 812us/step - loss: 0.1882 - accuracy: 0.9267\n",
      "Epoch 341/1500\n",
      "43/43 [==============================] - 0s 860us/step - loss: 0.1780 - accuracy: 0.9315\n",
      "Epoch 342/1500\n",
      "43/43 [==============================] - 0s 884us/step - loss: 0.1707 - accuracy: 0.9322\n",
      "Epoch 343/1500\n",
      "43/43 [==============================] - 0s 851us/step - loss: 0.1733 - accuracy: 0.9381\n",
      "Epoch 344/1500\n",
      "43/43 [==============================] - 0s 865us/step - loss: 0.1766 - accuracy: 0.9300\n",
      "Epoch 345/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.1768 - accuracy: 0.9315\n",
      "Epoch 346/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.1765 - accuracy: 0.9296\n",
      "Epoch 347/1500\n",
      "43/43 [==============================] - 0s 830us/step - loss: 0.1846 - accuracy: 0.9315\n",
      "Epoch 348/1500\n",
      "43/43 [==============================] - 0s 826us/step - loss: 0.1762 - accuracy: 0.9315\n",
      "Epoch 349/1500\n",
      "43/43 [==============================] - 0s 813us/step - loss: 0.1707 - accuracy: 0.9326\n",
      "Epoch 350/1500\n",
      "43/43 [==============================] - 0s 814us/step - loss: 0.1688 - accuracy: 0.9352\n",
      "Epoch 351/1500\n",
      "43/43 [==============================] - 0s 819us/step - loss: 0.1817 - accuracy: 0.9315\n",
      "Epoch 352/1500\n",
      "43/43 [==============================] - 0s 828us/step - loss: 0.1960 - accuracy: 0.9245\n",
      "Epoch 353/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.1737 - accuracy: 0.9392\n",
      "Epoch 354/1500\n",
      "43/43 [==============================] - 0s 828us/step - loss: 0.1831 - accuracy: 0.9329\n",
      "Epoch 355/1500\n",
      "43/43 [==============================] - 0s 822us/step - loss: 0.1791 - accuracy: 0.9289\n",
      "Epoch 356/1500\n",
      "43/43 [==============================] - 0s 864us/step - loss: 0.1618 - accuracy: 0.9366\n",
      "Epoch 357/1500\n",
      "43/43 [==============================] - 0s 823us/step - loss: 0.1769 - accuracy: 0.9333\n",
      "Epoch 358/1500\n",
      "43/43 [==============================] - 0s 818us/step - loss: 0.1729 - accuracy: 0.9300\n",
      "Epoch 359/1500\n",
      "43/43 [==============================] - 0s 844us/step - loss: 0.1805 - accuracy: 0.9348\n",
      "Epoch 360/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.1874 - accuracy: 0.9259\n",
      "Epoch 361/1500\n",
      "43/43 [==============================] - 0s 820us/step - loss: 0.1784 - accuracy: 0.9329\n",
      "Epoch 362/1500\n",
      "43/43 [==============================] - 0s 900us/step - loss: 0.1647 - accuracy: 0.9340\n",
      "Epoch 363/1500\n",
      "43/43 [==============================] - 0s 899us/step - loss: 0.1522 - accuracy: 0.9436\n",
      "Epoch 364/1500\n",
      "43/43 [==============================] - 0s 886us/step - loss: 0.1700 - accuracy: 0.9370\n",
      "Epoch 365/1500\n",
      "43/43 [==============================] - 0s 879us/step - loss: 0.2065 - accuracy: 0.9245\n",
      "Epoch 366/1500\n",
      "43/43 [==============================] - 0s 841us/step - loss: 0.1680 - accuracy: 0.9322\n",
      "Epoch 367/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.1730 - accuracy: 0.9363\n",
      "Epoch 368/1500\n",
      "43/43 [==============================] - 0s 859us/step - loss: 0.1779 - accuracy: 0.9304\n",
      "Epoch 369/1500\n",
      "43/43 [==============================] - 0s 898us/step - loss: 0.1887 - accuracy: 0.9282\n",
      "Epoch 370/1500\n",
      "43/43 [==============================] - 0s 857us/step - loss: 0.1620 - accuracy: 0.9396\n",
      "Epoch 371/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.1662 - accuracy: 0.9348\n",
      "Epoch 372/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9355\n",
      "Epoch 373/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9407\n",
      "Epoch 374/1500\n",
      "43/43 [==============================] - 0s 997us/step - loss: 0.1715 - accuracy: 0.9377\n",
      "Epoch 375/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9270\n",
      "Epoch 376/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9344\n",
      "Epoch 377/1500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.9348\n",
      "Epoch 378/1500\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.1678 - accuracy: 0.9355\n",
      "Epoch 379/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9359\n",
      "Epoch 380/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9296\n",
      "Epoch 381/1500\n",
      "43/43 [==============================] - 0s 964us/step - loss: 0.1760 - accuracy: 0.9326\n",
      "Epoch 382/1500\n",
      "43/43 [==============================] - 0s 945us/step - loss: 0.1722 - accuracy: 0.9304\n",
      "Epoch 383/1500\n",
      "43/43 [==============================] - 0s 935us/step - loss: 0.1819 - accuracy: 0.9307\n",
      "Epoch 384/1500\n",
      "43/43 [==============================] - 0s 910us/step - loss: 0.1799 - accuracy: 0.9333\n",
      "Epoch 385/1500\n",
      "43/43 [==============================] - 0s 884us/step - loss: 0.1615 - accuracy: 0.9377\n",
      "Epoch 386/1500\n",
      "43/43 [==============================] - 0s 856us/step - loss: 0.1777 - accuracy: 0.9318\n",
      "Epoch 387/1500\n",
      "43/43 [==============================] - 0s 881us/step - loss: 0.1645 - accuracy: 0.9385\n",
      "Epoch 388/1500\n",
      "43/43 [==============================] - 0s 914us/step - loss: 0.1805 - accuracy: 0.9311\n",
      "Epoch 389/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.1620 - accuracy: 0.9359\n",
      "Epoch 390/1500\n",
      "43/43 [==============================] - 0s 830us/step - loss: 0.1607 - accuracy: 0.9333\n",
      "Epoch 391/1500\n",
      "43/43 [==============================] - 0s 830us/step - loss: 0.1680 - accuracy: 0.9340\n",
      "Epoch 392/1500\n",
      "43/43 [==============================] - 0s 831us/step - loss: 0.1727 - accuracy: 0.9322\n",
      "Epoch 393/1500\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.1466 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 363.\n",
      "43/43 [==============================] - 0s 908us/step - loss: 0.1552 - accuracy: 0.9418\n",
      "Epoch 393: early stopping\n",
      "9/9 [==============================] - 0s 681us/step - loss: 0.7563 - accuracy: 0.7414\n",
      "9/9 [==============================] - 0s 677us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.87 (26/30)\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 618, Predictions: 618, Actuals: 618, Gender: 618\n",
      "Final Test Results - Loss: 0.756289005279541, Accuracy: 0.7414448857307434, Precision: 0.7586678890819019, Recall: 0.7154868142404096, F1 Score: 0.7338684375409351\n",
      "Confusion Matrix:\n",
      " [[121   1  27]\n",
      " [  6  39   4]\n",
      " [ 30   0  35]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1103, 1: 888, 2: 736})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 1.0916 - accuracy: 0.5042\n",
      "Epoch 2/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.9172 - accuracy: 0.6047\n",
      "Epoch 3/1500\n",
      "43/43 [==============================] - 0s 876us/step - loss: 0.8631 - accuracy: 0.6260\n",
      "Epoch 4/1500\n",
      "43/43 [==============================] - 0s 853us/step - loss: 0.8253 - accuracy: 0.6344\n",
      "Epoch 5/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.8042 - accuracy: 0.6571\n",
      "Epoch 6/1500\n",
      "43/43 [==============================] - 0s 869us/step - loss: 0.7575 - accuracy: 0.6667\n",
      "Epoch 7/1500\n",
      "43/43 [==============================] - 0s 873us/step - loss: 0.7220 - accuracy: 0.6949\n",
      "Epoch 8/1500\n",
      "43/43 [==============================] - 0s 861us/step - loss: 0.7200 - accuracy: 0.6942\n",
      "Epoch 9/1500\n",
      "43/43 [==============================] - 0s 843us/step - loss: 0.6885 - accuracy: 0.7070\n",
      "Epoch 10/1500\n",
      "43/43 [==============================] - 0s 858us/step - loss: 0.6937 - accuracy: 0.6971\n",
      "Epoch 11/1500\n",
      "43/43 [==============================] - 0s 816us/step - loss: 0.6680 - accuracy: 0.7070\n",
      "Epoch 12/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.6505 - accuracy: 0.7246\n",
      "Epoch 13/1500\n",
      "43/43 [==============================] - 0s 975us/step - loss: 0.6535 - accuracy: 0.7165\n",
      "Epoch 14/1500\n",
      "43/43 [==============================] - 0s 933us/step - loss: 0.6549 - accuracy: 0.7224\n",
      "Epoch 15/1500\n",
      "43/43 [==============================] - 0s 885us/step - loss: 0.6183 - accuracy: 0.7400\n",
      "Epoch 16/1500\n",
      "43/43 [==============================] - 0s 913us/step - loss: 0.6331 - accuracy: 0.7297\n",
      "Epoch 17/1500\n",
      "43/43 [==============================] - 0s 946us/step - loss: 0.6082 - accuracy: 0.7360\n",
      "Epoch 18/1500\n",
      "43/43 [==============================] - 0s 957us/step - loss: 0.5971 - accuracy: 0.7488\n",
      "Epoch 19/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.5810 - accuracy: 0.7543\n",
      "Epoch 20/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.5991 - accuracy: 0.7363\n",
      "Epoch 21/1500\n",
      "43/43 [==============================] - 0s 862us/step - loss: 0.5799 - accuracy: 0.7422\n",
      "Epoch 22/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.5703 - accuracy: 0.7594\n",
      "Epoch 23/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.5749 - accuracy: 0.7583\n",
      "Epoch 24/1500\n",
      "43/43 [==============================] - 0s 884us/step - loss: 0.5739 - accuracy: 0.7569\n",
      "Epoch 25/1500\n",
      "43/43 [==============================] - 0s 915us/step - loss: 0.5488 - accuracy: 0.7609\n",
      "Epoch 26/1500\n",
      "43/43 [==============================] - 0s 924us/step - loss: 0.5484 - accuracy: 0.7627\n",
      "Epoch 27/1500\n",
      "43/43 [==============================] - 0s 910us/step - loss: 0.5334 - accuracy: 0.7682\n",
      "Epoch 28/1500\n",
      "43/43 [==============================] - 0s 885us/step - loss: 0.5410 - accuracy: 0.7646\n",
      "Epoch 29/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.5364 - accuracy: 0.7635\n",
      "Epoch 30/1500\n",
      "43/43 [==============================] - 0s 861us/step - loss: 0.5312 - accuracy: 0.7745\n",
      "Epoch 31/1500\n",
      "43/43 [==============================] - 0s 845us/step - loss: 0.5189 - accuracy: 0.7825\n",
      "Epoch 32/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.5217 - accuracy: 0.7800\n",
      "Epoch 33/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.5267 - accuracy: 0.7756\n",
      "Epoch 34/1500\n",
      "43/43 [==============================] - 0s 895us/step - loss: 0.5008 - accuracy: 0.7822\n",
      "Epoch 35/1500\n",
      "43/43 [==============================] - 0s 847us/step - loss: 0.5084 - accuracy: 0.7785\n",
      "Epoch 36/1500\n",
      "43/43 [==============================] - 0s 868us/step - loss: 0.5237 - accuracy: 0.7781\n",
      "Epoch 37/1500\n",
      "43/43 [==============================] - 0s 840us/step - loss: 0.4938 - accuracy: 0.7873\n",
      "Epoch 38/1500\n",
      "43/43 [==============================] - 0s 857us/step - loss: 0.5028 - accuracy: 0.7829\n",
      "Epoch 39/1500\n",
      "43/43 [==============================] - 0s 883us/step - loss: 0.4817 - accuracy: 0.7932\n",
      "Epoch 40/1500\n",
      "43/43 [==============================] - 0s 863us/step - loss: 0.4747 - accuracy: 0.7968\n",
      "Epoch 41/1500\n",
      "43/43 [==============================] - 0s 848us/step - loss: 0.4756 - accuracy: 0.7946\n",
      "Epoch 42/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.4793 - accuracy: 0.8012\n",
      "Epoch 43/1500\n",
      "43/43 [==============================] - 0s 824us/step - loss: 0.4869 - accuracy: 0.7932\n",
      "Epoch 44/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.4593 - accuracy: 0.8049\n",
      "Epoch 45/1500\n",
      "43/43 [==============================] - 0s 805us/step - loss: 0.4665 - accuracy: 0.8130\n",
      "Epoch 46/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.8038\n",
      "Epoch 47/1500\n",
      "43/43 [==============================] - 0s 881us/step - loss: 0.4555 - accuracy: 0.8001\n",
      "Epoch 48/1500\n",
      "43/43 [==============================] - 0s 873us/step - loss: 0.4549 - accuracy: 0.8056\n",
      "Epoch 49/1500\n",
      "43/43 [==============================] - 0s 884us/step - loss: 0.4570 - accuracy: 0.8122\n",
      "Epoch 50/1500\n",
      "43/43 [==============================] - 0s 872us/step - loss: 0.4573 - accuracy: 0.8016\n",
      "Epoch 51/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.4542 - accuracy: 0.8031\n",
      "Epoch 52/1500\n",
      "43/43 [==============================] - 0s 843us/step - loss: 0.4564 - accuracy: 0.8089\n",
      "Epoch 53/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.4448 - accuracy: 0.8137\n",
      "Epoch 54/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.4454 - accuracy: 0.8100\n",
      "Epoch 55/1500\n",
      "43/43 [==============================] - 0s 880us/step - loss: 0.4405 - accuracy: 0.8082\n",
      "Epoch 56/1500\n",
      "43/43 [==============================] - 0s 865us/step - loss: 0.4371 - accuracy: 0.8115\n",
      "Epoch 57/1500\n",
      "43/43 [==============================] - 0s 825us/step - loss: 0.4161 - accuracy: 0.8269\n",
      "Epoch 58/1500\n",
      "43/43 [==============================] - 0s 814us/step - loss: 0.4361 - accuracy: 0.8097\n",
      "Epoch 59/1500\n",
      "43/43 [==============================] - 0s 820us/step - loss: 0.4266 - accuracy: 0.8240\n",
      "Epoch 60/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.4227 - accuracy: 0.8229\n",
      "Epoch 61/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.4229 - accuracy: 0.8240\n",
      "Epoch 62/1500\n",
      "43/43 [==============================] - 0s 927us/step - loss: 0.4303 - accuracy: 0.8181\n",
      "Epoch 63/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.4216 - accuracy: 0.8247\n",
      "Epoch 64/1500\n",
      "43/43 [==============================] - 0s 904us/step - loss: 0.4117 - accuracy: 0.8343\n",
      "Epoch 65/1500\n",
      "43/43 [==============================] - 0s 873us/step - loss: 0.4047 - accuracy: 0.8306\n",
      "Epoch 66/1500\n",
      "43/43 [==============================] - 0s 894us/step - loss: 0.4010 - accuracy: 0.8379\n",
      "Epoch 67/1500\n",
      "43/43 [==============================] - 0s 906us/step - loss: 0.4202 - accuracy: 0.8254\n",
      "Epoch 68/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.4071 - accuracy: 0.8265\n",
      "Epoch 69/1500\n",
      "43/43 [==============================] - 0s 855us/step - loss: 0.4075 - accuracy: 0.8328\n",
      "Epoch 70/1500\n",
      "43/43 [==============================] - 0s 868us/step - loss: 0.4100 - accuracy: 0.8346\n",
      "Epoch 71/1500\n",
      "43/43 [==============================] - 0s 889us/step - loss: 0.4073 - accuracy: 0.8291\n",
      "Epoch 72/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.4061 - accuracy: 0.8276\n",
      "Epoch 73/1500\n",
      "43/43 [==============================] - 0s 974us/step - loss: 0.3961 - accuracy: 0.8335\n",
      "Epoch 74/1500\n",
      "43/43 [==============================] - 0s 823us/step - loss: 0.4088 - accuracy: 0.8243\n",
      "Epoch 75/1500\n",
      "43/43 [==============================] - 0s 879us/step - loss: 0.3953 - accuracy: 0.8401\n",
      "Epoch 76/1500\n",
      "43/43 [==============================] - 0s 896us/step - loss: 0.3877 - accuracy: 0.8298\n",
      "Epoch 77/1500\n",
      "43/43 [==============================] - 0s 887us/step - loss: 0.3893 - accuracy: 0.8361\n",
      "Epoch 78/1500\n",
      "43/43 [==============================] - 0s 860us/step - loss: 0.4037 - accuracy: 0.8291\n",
      "Epoch 79/1500\n",
      "43/43 [==============================] - 0s 866us/step - loss: 0.3952 - accuracy: 0.8365\n",
      "Epoch 80/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.3942 - accuracy: 0.8383\n",
      "Epoch 81/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.3844 - accuracy: 0.8431\n",
      "Epoch 82/1500\n",
      "43/43 [==============================] - 0s 863us/step - loss: 0.3748 - accuracy: 0.8453\n",
      "Epoch 83/1500\n",
      "43/43 [==============================] - 0s 871us/step - loss: 0.3911 - accuracy: 0.8383\n",
      "Epoch 84/1500\n",
      "43/43 [==============================] - 0s 855us/step - loss: 0.3802 - accuracy: 0.8376\n",
      "Epoch 85/1500\n",
      "43/43 [==============================] - 0s 865us/step - loss: 0.3753 - accuracy: 0.8482\n",
      "Epoch 86/1500\n",
      "43/43 [==============================] - 0s 828us/step - loss: 0.3867 - accuracy: 0.8471\n",
      "Epoch 87/1500\n",
      "43/43 [==============================] - 0s 899us/step - loss: 0.3873 - accuracy: 0.8379\n",
      "Epoch 88/1500\n",
      "43/43 [==============================] - 0s 872us/step - loss: 0.3803 - accuracy: 0.8515\n",
      "Epoch 89/1500\n",
      "43/43 [==============================] - 0s 883us/step - loss: 0.3609 - accuracy: 0.8497\n",
      "Epoch 90/1500\n",
      "43/43 [==============================] - 0s 843us/step - loss: 0.3728 - accuracy: 0.8449\n",
      "Epoch 91/1500\n",
      "43/43 [==============================] - 0s 954us/step - loss: 0.3806 - accuracy: 0.8401\n",
      "Epoch 92/1500\n",
      "43/43 [==============================] - 0s 948us/step - loss: 0.3606 - accuracy: 0.8530\n",
      "Epoch 93/1500\n",
      "43/43 [==============================] - 0s 983us/step - loss: 0.3802 - accuracy: 0.8405\n",
      "Epoch 94/1500\n",
      "43/43 [==============================] - 0s 976us/step - loss: 0.3757 - accuracy: 0.8398\n",
      "Epoch 95/1500\n",
      "43/43 [==============================] - 0s 977us/step - loss: 0.3666 - accuracy: 0.8486\n",
      "Epoch 96/1500\n",
      "43/43 [==============================] - 0s 926us/step - loss: 0.3674 - accuracy: 0.8533\n",
      "Epoch 97/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.3584 - accuracy: 0.8497\n",
      "Epoch 98/1500\n",
      "43/43 [==============================] - 0s 856us/step - loss: 0.3596 - accuracy: 0.8519\n",
      "Epoch 99/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.3579 - accuracy: 0.8504\n",
      "Epoch 100/1500\n",
      "43/43 [==============================] - 0s 863us/step - loss: 0.3651 - accuracy: 0.8511\n",
      "Epoch 101/1500\n",
      "43/43 [==============================] - 0s 890us/step - loss: 0.3474 - accuracy: 0.8629\n",
      "Epoch 102/1500\n",
      "43/43 [==============================] - 0s 862us/step - loss: 0.3556 - accuracy: 0.8482\n",
      "Epoch 103/1500\n",
      "43/43 [==============================] - 0s 964us/step - loss: 0.3397 - accuracy: 0.8654\n",
      "Epoch 104/1500\n",
      "43/43 [==============================] - 0s 865us/step - loss: 0.3418 - accuracy: 0.8610\n",
      "Epoch 105/1500\n",
      "43/43 [==============================] - 0s 862us/step - loss: 0.3625 - accuracy: 0.8526\n",
      "Epoch 106/1500\n",
      "43/43 [==============================] - 0s 858us/step - loss: 0.3626 - accuracy: 0.8464\n",
      "Epoch 107/1500\n",
      "43/43 [==============================] - 0s 824us/step - loss: 0.3596 - accuracy: 0.8511\n",
      "Epoch 108/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.3472 - accuracy: 0.8651\n",
      "Epoch 109/1500\n",
      "43/43 [==============================] - 0s 838us/step - loss: 0.3458 - accuracy: 0.8581\n",
      "Epoch 110/1500\n",
      "43/43 [==============================] - 0s 838us/step - loss: 0.3426 - accuracy: 0.8621\n",
      "Epoch 111/1500\n",
      "43/43 [==============================] - 0s 824us/step - loss: 0.3468 - accuracy: 0.8618\n",
      "Epoch 112/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.3429 - accuracy: 0.8614\n",
      "Epoch 113/1500\n",
      "43/43 [==============================] - 0s 844us/step - loss: 0.3414 - accuracy: 0.8654\n",
      "Epoch 114/1500\n",
      "43/43 [==============================] - 0s 817us/step - loss: 0.3410 - accuracy: 0.8610\n",
      "Epoch 115/1500\n",
      "43/43 [==============================] - 0s 831us/step - loss: 0.3459 - accuracy: 0.8599\n",
      "Epoch 116/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.3374 - accuracy: 0.8596\n",
      "Epoch 117/1500\n",
      "43/43 [==============================] - 0s 828us/step - loss: 0.3328 - accuracy: 0.8614\n",
      "Epoch 118/1500\n",
      "43/43 [==============================] - 0s 854us/step - loss: 0.3282 - accuracy: 0.8702\n",
      "Epoch 119/1500\n",
      "43/43 [==============================] - 0s 888us/step - loss: 0.3387 - accuracy: 0.8643\n",
      "Epoch 120/1500\n",
      "43/43 [==============================] - 0s 903us/step - loss: 0.3240 - accuracy: 0.8669\n",
      "Epoch 121/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8537\n",
      "Epoch 122/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8698\n",
      "Epoch 123/1500\n",
      "43/43 [==============================] - 0s 995us/step - loss: 0.3249 - accuracy: 0.8603\n",
      "Epoch 124/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8559\n",
      "Epoch 125/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8618\n",
      "Epoch 126/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8731\n",
      "Epoch 127/1500\n",
      "43/43 [==============================] - 0s 925us/step - loss: 0.3276 - accuracy: 0.8735\n",
      "Epoch 128/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8651\n",
      "Epoch 129/1500\n",
      "43/43 [==============================] - 0s 960us/step - loss: 0.3217 - accuracy: 0.8702\n",
      "Epoch 130/1500\n",
      "43/43 [==============================] - 0s 961us/step - loss: 0.3281 - accuracy: 0.8673\n",
      "Epoch 131/1500\n",
      "43/43 [==============================] - 0s 917us/step - loss: 0.3099 - accuracy: 0.8735\n",
      "Epoch 132/1500\n",
      "43/43 [==============================] - 0s 943us/step - loss: 0.3310 - accuracy: 0.8658\n",
      "Epoch 133/1500\n",
      "43/43 [==============================] - 0s 979us/step - loss: 0.3247 - accuracy: 0.8695\n",
      "Epoch 134/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3302 - accuracy: 0.8625\n",
      "Epoch 135/1500\n",
      "43/43 [==============================] - 0s 981us/step - loss: 0.3205 - accuracy: 0.8735\n",
      "Epoch 136/1500\n",
      "43/43 [==============================] - 0s 930us/step - loss: 0.3235 - accuracy: 0.8691\n",
      "Epoch 137/1500\n",
      "43/43 [==============================] - 0s 893us/step - loss: 0.3348 - accuracy: 0.8621\n",
      "Epoch 138/1500\n",
      "43/43 [==============================] - 0s 862us/step - loss: 0.3046 - accuracy: 0.8838\n",
      "Epoch 139/1500\n",
      "43/43 [==============================] - 0s 930us/step - loss: 0.3188 - accuracy: 0.8753\n",
      "Epoch 140/1500\n",
      "43/43 [==============================] - 0s 946us/step - loss: 0.3263 - accuracy: 0.8662\n",
      "Epoch 141/1500\n",
      "43/43 [==============================] - 0s 972us/step - loss: 0.3211 - accuracy: 0.8687\n",
      "Epoch 142/1500\n",
      "43/43 [==============================] - 0s 995us/step - loss: 0.3244 - accuracy: 0.8691\n",
      "Epoch 143/1500\n",
      "43/43 [==============================] - 0s 909us/step - loss: 0.3103 - accuracy: 0.8735\n",
      "Epoch 144/1500\n",
      "43/43 [==============================] - 0s 925us/step - loss: 0.3145 - accuracy: 0.8746\n",
      "Epoch 145/1500\n",
      "43/43 [==============================] - 0s 914us/step - loss: 0.3137 - accuracy: 0.8739\n",
      "Epoch 146/1500\n",
      "43/43 [==============================] - 0s 932us/step - loss: 0.2850 - accuracy: 0.8838\n",
      "Epoch 147/1500\n",
      "43/43 [==============================] - 0s 939us/step - loss: 0.3269 - accuracy: 0.8599\n",
      "Epoch 148/1500\n",
      "43/43 [==============================] - 0s 913us/step - loss: 0.3008 - accuracy: 0.8834\n",
      "Epoch 149/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.3201 - accuracy: 0.8698\n",
      "Epoch 150/1500\n",
      "43/43 [==============================] - 0s 882us/step - loss: 0.3317 - accuracy: 0.8654\n",
      "Epoch 151/1500\n",
      "43/43 [==============================] - 0s 858us/step - loss: 0.3002 - accuracy: 0.8863\n",
      "Epoch 152/1500\n",
      "43/43 [==============================] - 0s 861us/step - loss: 0.3172 - accuracy: 0.8724\n",
      "Epoch 153/1500\n",
      "43/43 [==============================] - 0s 805us/step - loss: 0.3180 - accuracy: 0.8772\n",
      "Epoch 154/1500\n",
      "43/43 [==============================] - 0s 824us/step - loss: 0.3137 - accuracy: 0.8742\n",
      "Epoch 155/1500\n",
      "43/43 [==============================] - 0s 821us/step - loss: 0.3052 - accuracy: 0.8816\n",
      "Epoch 156/1500\n",
      "43/43 [==============================] - 0s 831us/step - loss: 0.3066 - accuracy: 0.8746\n",
      "Epoch 157/1500\n",
      "43/43 [==============================] - 0s 841us/step - loss: 0.3013 - accuracy: 0.8856\n",
      "Epoch 158/1500\n",
      "43/43 [==============================] - 0s 881us/step - loss: 0.3036 - accuracy: 0.8753\n",
      "Epoch 159/1500\n",
      "43/43 [==============================] - 0s 854us/step - loss: 0.3053 - accuracy: 0.8808\n",
      "Epoch 160/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.2883 - accuracy: 0.8823\n",
      "Epoch 161/1500\n",
      "43/43 [==============================] - 0s 801us/step - loss: 0.2957 - accuracy: 0.8783\n",
      "Epoch 162/1500\n",
      "43/43 [==============================] - 0s 830us/step - loss: 0.3007 - accuracy: 0.8830\n",
      "Epoch 163/1500\n",
      "43/43 [==============================] - 0s 843us/step - loss: 0.3009 - accuracy: 0.8717\n",
      "Epoch 164/1500\n",
      "43/43 [==============================] - 0s 915us/step - loss: 0.3078 - accuracy: 0.8728\n",
      "Epoch 165/1500\n",
      "43/43 [==============================] - 0s 855us/step - loss: 0.2923 - accuracy: 0.8779\n",
      "Epoch 166/1500\n",
      "43/43 [==============================] - 0s 875us/step - loss: 0.2982 - accuracy: 0.8790\n",
      "Epoch 167/1500\n",
      "43/43 [==============================] - 0s 874us/step - loss: 0.2970 - accuracy: 0.8768\n",
      "Epoch 168/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.2857 - accuracy: 0.8889\n",
      "Epoch 169/1500\n",
      "43/43 [==============================] - 0s 848us/step - loss: 0.2925 - accuracy: 0.8779\n",
      "Epoch 170/1500\n",
      "43/43 [==============================] - 0s 971us/step - loss: 0.3018 - accuracy: 0.8746\n",
      "Epoch 171/1500\n",
      "43/43 [==============================] - 0s 894us/step - loss: 0.2923 - accuracy: 0.8808\n",
      "Epoch 172/1500\n",
      "43/43 [==============================] - 0s 950us/step - loss: 0.2960 - accuracy: 0.8801\n",
      "Epoch 173/1500\n",
      "43/43 [==============================] - 0s 882us/step - loss: 0.3074 - accuracy: 0.8768\n",
      "Epoch 174/1500\n",
      "43/43 [==============================] - 0s 878us/step - loss: 0.2801 - accuracy: 0.8911\n",
      "Epoch 175/1500\n",
      "43/43 [==============================] - 0s 961us/step - loss: 0.2874 - accuracy: 0.8830\n",
      "Epoch 176/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.2940 - accuracy: 0.8805\n",
      "Epoch 177/1500\n",
      "43/43 [==============================] - 0s 910us/step - loss: 0.2985 - accuracy: 0.8676\n",
      "Epoch 178/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2916 - accuracy: 0.8878\n",
      "Epoch 179/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.3024 - accuracy: 0.8772\n",
      "Epoch 180/1500\n",
      "43/43 [==============================] - 0s 864us/step - loss: 0.2803 - accuracy: 0.8907\n",
      "Epoch 181/1500\n",
      "43/43 [==============================] - 0s 968us/step - loss: 0.2912 - accuracy: 0.8808\n",
      "Epoch 182/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8959\n",
      "Epoch 183/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.8827\n",
      "Epoch 184/1500\n",
      "43/43 [==============================] - 0s 801us/step - loss: 0.2772 - accuracy: 0.8922\n",
      "Epoch 185/1500\n",
      "43/43 [==============================] - 0s 842us/step - loss: 0.2837 - accuracy: 0.8878\n",
      "Epoch 186/1500\n",
      "43/43 [==============================] - 0s 883us/step - loss: 0.2673 - accuracy: 0.8970\n",
      "Epoch 187/1500\n",
      "43/43 [==============================] - 0s 931us/step - loss: 0.3031 - accuracy: 0.8717\n",
      "Epoch 188/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8757\n",
      "Epoch 189/1500\n",
      "43/43 [==============================] - 0s 962us/step - loss: 0.2827 - accuracy: 0.8874\n",
      "Epoch 190/1500\n",
      "43/43 [==============================] - 0s 964us/step - loss: 0.2757 - accuracy: 0.8951\n",
      "Epoch 191/1500\n",
      "43/43 [==============================] - 0s 958us/step - loss: 0.2834 - accuracy: 0.8900\n",
      "Epoch 192/1500\n",
      "43/43 [==============================] - 0s 852us/step - loss: 0.2826 - accuracy: 0.8893\n",
      "Epoch 193/1500\n",
      "43/43 [==============================] - 0s 922us/step - loss: 0.2740 - accuracy: 0.8878\n",
      "Epoch 194/1500\n",
      "43/43 [==============================] - 0s 937us/step - loss: 0.2844 - accuracy: 0.8819\n",
      "Epoch 195/1500\n",
      "43/43 [==============================] - 0s 806us/step - loss: 0.2856 - accuracy: 0.8885\n",
      "Epoch 196/1500\n",
      "43/43 [==============================] - 0s 865us/step - loss: 0.2719 - accuracy: 0.8948\n",
      "Epoch 197/1500\n",
      "43/43 [==============================] - 0s 879us/step - loss: 0.2729 - accuracy: 0.8907\n",
      "Epoch 198/1500\n",
      "43/43 [==============================] - 0s 910us/step - loss: 0.2723 - accuracy: 0.8896\n",
      "Epoch 199/1500\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.8885\n",
      "Epoch 200/1500\n",
      "43/43 [==============================] - 0s 989us/step - loss: 0.2747 - accuracy: 0.8867\n",
      "Epoch 201/1500\n",
      "43/43 [==============================] - 0s 927us/step - loss: 0.2778 - accuracy: 0.8882\n",
      "Epoch 202/1500\n",
      "43/43 [==============================] - 0s 962us/step - loss: 0.2680 - accuracy: 0.9014\n",
      "Epoch 203/1500\n",
      "43/43 [==============================] - 0s 922us/step - loss: 0.2815 - accuracy: 0.8830\n",
      "Epoch 204/1500\n",
      "43/43 [==============================] - 0s 946us/step - loss: 0.2616 - accuracy: 0.8962\n",
      "Epoch 205/1500\n",
      "43/43 [==============================] - 0s 896us/step - loss: 0.2723 - accuracy: 0.8962\n",
      "Epoch 206/1500\n",
      "43/43 [==============================] - 0s 894us/step - loss: 0.2640 - accuracy: 0.8981\n",
      "Epoch 207/1500\n",
      "43/43 [==============================] - 0s 859us/step - loss: 0.2789 - accuracy: 0.8819\n",
      "Epoch 208/1500\n",
      "43/43 [==============================] - 0s 854us/step - loss: 0.2694 - accuracy: 0.8904\n",
      "Epoch 209/1500\n",
      "43/43 [==============================] - 0s 876us/step - loss: 0.2653 - accuracy: 0.8882\n",
      "Epoch 210/1500\n",
      "43/43 [==============================] - 0s 902us/step - loss: 0.2787 - accuracy: 0.8885\n",
      "Epoch 211/1500\n",
      "43/43 [==============================] - 0s 993us/step - loss: 0.2539 - accuracy: 0.8955\n",
      "Epoch 212/1500\n",
      "43/43 [==============================] - 0s 970us/step - loss: 0.2750 - accuracy: 0.8981\n",
      "Epoch 213/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2770 - accuracy: 0.8966\n",
      "Epoch 214/1500\n",
      "43/43 [==============================] - 0s 867us/step - loss: 0.2796 - accuracy: 0.8871\n",
      "Epoch 215/1500\n",
      "43/43 [==============================] - 0s 985us/step - loss: 0.2559 - accuracy: 0.8933\n",
      "Epoch 216/1500\n",
      "43/43 [==============================] - 0s 829us/step - loss: 0.2614 - accuracy: 0.8973\n",
      "Epoch 217/1500\n",
      "43/43 [==============================] - 0s 932us/step - loss: 0.2764 - accuracy: 0.8918\n",
      "Epoch 218/1500\n",
      "43/43 [==============================] - 0s 835us/step - loss: 0.2682 - accuracy: 0.8944\n",
      "Epoch 219/1500\n",
      "43/43 [==============================] - 0s 948us/step - loss: 0.2430 - accuracy: 0.8984\n",
      "Epoch 220/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.2778 - accuracy: 0.8885\n",
      "Epoch 221/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.2672 - accuracy: 0.8944\n",
      "Epoch 222/1500\n",
      "43/43 [==============================] - 0s 829us/step - loss: 0.2632 - accuracy: 0.8944\n",
      "Epoch 223/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.2701 - accuracy: 0.8915\n",
      "Epoch 224/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2707 - accuracy: 0.8918\n",
      "Epoch 225/1500\n",
      "43/43 [==============================] - 0s 935us/step - loss: 0.2490 - accuracy: 0.9006\n",
      "Epoch 226/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2651 - accuracy: 0.8937\n",
      "Epoch 227/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.2705 - accuracy: 0.8889\n",
      "Epoch 228/1500\n",
      "43/43 [==============================] - 0s 863us/step - loss: 0.2528 - accuracy: 0.8981\n",
      "Epoch 229/1500\n",
      "43/43 [==============================] - 0s 801us/step - loss: 0.2666 - accuracy: 0.8922\n",
      "Epoch 230/1500\n",
      "43/43 [==============================] - 0s 824us/step - loss: 0.2478 - accuracy: 0.9010\n",
      "Epoch 231/1500\n",
      "43/43 [==============================] - 0s 790us/step - loss: 0.2605 - accuracy: 0.8878\n",
      "Epoch 232/1500\n",
      "43/43 [==============================] - 0s 846us/step - loss: 0.2447 - accuracy: 0.9032\n",
      "Epoch 233/1500\n",
      "43/43 [==============================] - 0s 819us/step - loss: 0.2591 - accuracy: 0.8970\n",
      "Epoch 234/1500\n",
      "43/43 [==============================] - 0s 806us/step - loss: 0.2571 - accuracy: 0.9050\n",
      "Epoch 235/1500\n",
      "43/43 [==============================] - 0s 772us/step - loss: 0.2651 - accuracy: 0.8926\n",
      "Epoch 236/1500\n",
      "43/43 [==============================] - 0s 760us/step - loss: 0.2625 - accuracy: 0.8951\n",
      "Epoch 237/1500\n",
      "43/43 [==============================] - 0s 778us/step - loss: 0.2665 - accuracy: 0.8871\n",
      "Epoch 238/1500\n",
      "43/43 [==============================] - 0s 815us/step - loss: 0.2653 - accuracy: 0.8955\n",
      "Epoch 239/1500\n",
      "43/43 [==============================] - 0s 819us/step - loss: 0.2494 - accuracy: 0.8999\n",
      "Epoch 240/1500\n",
      "43/43 [==============================] - 0s 786us/step - loss: 0.2501 - accuracy: 0.9003\n",
      "Epoch 241/1500\n",
      "43/43 [==============================] - 0s 833us/step - loss: 0.2293 - accuracy: 0.9116\n",
      "Epoch 242/1500\n",
      "43/43 [==============================] - 0s 848us/step - loss: 0.2526 - accuracy: 0.9028\n",
      "Epoch 243/1500\n",
      "43/43 [==============================] - 0s 892us/step - loss: 0.2516 - accuracy: 0.9006\n",
      "Epoch 244/1500\n",
      "43/43 [==============================] - 0s 868us/step - loss: 0.2580 - accuracy: 0.8999\n",
      "Epoch 245/1500\n",
      "43/43 [==============================] - 0s 820us/step - loss: 0.2581 - accuracy: 0.8973\n",
      "Epoch 246/1500\n",
      "43/43 [==============================] - 0s 903us/step - loss: 0.2505 - accuracy: 0.8984\n",
      "Epoch 247/1500\n",
      "43/43 [==============================] - 0s 839us/step - loss: 0.2537 - accuracy: 0.9003\n",
      "Epoch 248/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2420 - accuracy: 0.9069\n",
      "Epoch 249/1500\n",
      "43/43 [==============================] - 0s 828us/step - loss: 0.2550 - accuracy: 0.8937\n",
      "Epoch 250/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.2376 - accuracy: 0.9076\n",
      "Epoch 251/1500\n",
      "43/43 [==============================] - 0s 817us/step - loss: 0.2239 - accuracy: 0.9120\n",
      "Epoch 252/1500\n",
      "43/43 [==============================] - 0s 829us/step - loss: 0.2359 - accuracy: 0.9138\n",
      "Epoch 253/1500\n",
      "43/43 [==============================] - 0s 831us/step - loss: 0.2494 - accuracy: 0.8988\n",
      "Epoch 254/1500\n",
      "43/43 [==============================] - 0s 832us/step - loss: 0.2595 - accuracy: 0.8988\n",
      "Epoch 255/1500\n",
      "43/43 [==============================] - 0s 858us/step - loss: 0.2488 - accuracy: 0.9065\n",
      "Epoch 256/1500\n",
      "43/43 [==============================] - 0s 816us/step - loss: 0.2442 - accuracy: 0.9050\n",
      "Epoch 257/1500\n",
      "43/43 [==============================] - 0s 854us/step - loss: 0.2378 - accuracy: 0.9120\n",
      "Epoch 258/1500\n",
      "43/43 [==============================] - 0s 840us/step - loss: 0.2409 - accuracy: 0.9061\n",
      "Epoch 259/1500\n",
      "43/43 [==============================] - 0s 840us/step - loss: 0.2328 - accuracy: 0.9087\n",
      "Epoch 260/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2348 - accuracy: 0.9058\n",
      "Epoch 261/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2321 - accuracy: 0.9083\n",
      "Epoch 262/1500\n",
      "43/43 [==============================] - 0s 869us/step - loss: 0.2319 - accuracy: 0.9025\n",
      "Epoch 263/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2433 - accuracy: 0.9025\n",
      "Epoch 264/1500\n",
      "43/43 [==============================] - 0s 820us/step - loss: 0.2431 - accuracy: 0.9006\n",
      "Epoch 265/1500\n",
      "43/43 [==============================] - 0s 836us/step - loss: 0.2420 - accuracy: 0.9014\n",
      "Epoch 266/1500\n",
      "43/43 [==============================] - 0s 851us/step - loss: 0.2443 - accuracy: 0.9039\n",
      "Epoch 267/1500\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.2458 - accuracy: 0.9087\n",
      "Epoch 268/1500\n",
      "43/43 [==============================] - 0s 969us/step - loss: 0.2212 - accuracy: 0.9124\n",
      "Epoch 269/1500\n",
      "43/43 [==============================] - 0s 955us/step - loss: 0.2369 - accuracy: 0.9105\n",
      "Epoch 270/1500\n",
      "43/43 [==============================] - 0s 932us/step - loss: 0.2266 - accuracy: 0.9109\n",
      "Epoch 271/1500\n",
      "43/43 [==============================] - 0s 825us/step - loss: 0.2369 - accuracy: 0.9069\n",
      "Epoch 272/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2433 - accuracy: 0.9025\n",
      "Epoch 273/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2214 - accuracy: 0.9098\n",
      "Epoch 274/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.2322 - accuracy: 0.9054\n",
      "Epoch 275/1500\n",
      "43/43 [==============================] - 0s 810us/step - loss: 0.2259 - accuracy: 0.9149\n",
      "Epoch 276/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2427 - accuracy: 0.9010\n",
      "Epoch 277/1500\n",
      "43/43 [==============================] - 0s 829us/step - loss: 0.2352 - accuracy: 0.9080\n",
      "Epoch 278/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2414 - accuracy: 0.9010\n",
      "Epoch 279/1500\n",
      "43/43 [==============================] - 0s 864us/step - loss: 0.2379 - accuracy: 0.9065\n",
      "Epoch 280/1500\n",
      "43/43 [==============================] - 0s 847us/step - loss: 0.2434 - accuracy: 0.9047\n",
      "Epoch 281/1500\n",
      "43/43 [==============================] - 0s 877us/step - loss: 0.2433 - accuracy: 0.8970\n",
      "Epoch 282/1500\n",
      "43/43 [==============================] - 0s 835us/step - loss: 0.2379 - accuracy: 0.9003\n",
      "Epoch 283/1500\n",
      "43/43 [==============================] - 0s 828us/step - loss: 0.2305 - accuracy: 0.9094\n",
      "Epoch 284/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2292 - accuracy: 0.9091\n",
      "Epoch 285/1500\n",
      "43/43 [==============================] - 0s 837us/step - loss: 0.2249 - accuracy: 0.9146\n",
      "Epoch 286/1500\n",
      "43/43 [==============================] - 0s 860us/step - loss: 0.2412 - accuracy: 0.9028\n",
      "Epoch 287/1500\n",
      "43/43 [==============================] - 0s 804us/step - loss: 0.2304 - accuracy: 0.9069\n",
      "Epoch 288/1500\n",
      "43/43 [==============================] - 0s 886us/step - loss: 0.2313 - accuracy: 0.9069\n",
      "Epoch 289/1500\n",
      "43/43 [==============================] - 0s 829us/step - loss: 0.2282 - accuracy: 0.9083\n",
      "Epoch 290/1500\n",
      "43/43 [==============================] - 0s 876us/step - loss: 0.2350 - accuracy: 0.9036\n",
      "Epoch 291/1500\n",
      "43/43 [==============================] - 0s 827us/step - loss: 0.2225 - accuracy: 0.9105\n",
      "Epoch 292/1500\n",
      "43/43 [==============================] - 0s 820us/step - loss: 0.2292 - accuracy: 0.9058\n",
      "Epoch 293/1500\n",
      "43/43 [==============================] - 0s 882us/step - loss: 0.2453 - accuracy: 0.9032\n",
      "Epoch 294/1500\n",
      "43/43 [==============================] - 0s 845us/step - loss: 0.2222 - accuracy: 0.9098\n",
      "Epoch 295/1500\n",
      "43/43 [==============================] - 0s 850us/step - loss: 0.2208 - accuracy: 0.9135\n",
      "Epoch 296/1500\n",
      "43/43 [==============================] - 0s 847us/step - loss: 0.2231 - accuracy: 0.9087\n",
      "Epoch 297/1500\n",
      "43/43 [==============================] - 0s 849us/step - loss: 0.2269 - accuracy: 0.9138\n",
      "Epoch 298/1500\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.2512 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 268.\n",
      "43/43 [==============================] - 0s 898us/step - loss: 0.2238 - accuracy: 0.9142\n",
      "Epoch 298: early stopping\n",
      "8/8 [==============================] - 0s 788us/step - loss: 0.6343 - accuracy: 0.7660\n",
      "8/8 [==============================] - 0s 675us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 618, Predictions: 618, Actuals: 618, Gender: 618\n",
      "After appending - Cat IDs: 853, Predictions: 853, Actuals: 853, Gender: 853\n",
      "Final Test Results - Loss: 0.634340763092041, Accuracy: 0.7659574747085571, Precision: 0.8344223216888752, Recall: 0.7249084249084249, F1 Score: 0.7465843155615112\n",
      "Confusion Matrix:\n",
      " [[123   2   5]\n",
      " [  6  29   0]\n",
      " [ 42   0  28]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7233656947849483\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.6649068742990494\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7642377465963364\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.72577474015632\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7915867884223018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 853, Predictions: 853, Actuals: 853, Gender: 853\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.79 (87/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, adult...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, senior, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, senior, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, kitten, senior, kitten, senior, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "81    075A              [adult, adult, senior, senior, adult]         adult            adult                   True\n",
       "63    057A  [adult, adult, adult, senior, adult, senior, s...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, adult...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "48    042A  [adult, adult, kitten, kitten, kitten, kitten,...        kitten           kitten                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "103   109A    [adult, kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, senior, senior, adult, senior, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "49    043A                                   [kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "46    040A  [kitten, kitten, senior, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "18    016A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, senior, adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "41    035A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, kitten, adult, adult, ki...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "42    036A  [adult, senior, adult, senior, adult, adult, a...         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...        senior            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "102   108A        [adult, adult, adult, adult, adult, senior]         adult           senior                  False\n",
       "74    068A  [adult, adult, senior, senior, senior, adult, ...        senior            adult                  False\n",
       "56    051A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "65    059A  [senior, adult, adult, adult, senior, senior, ...         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [kitten, adult]         adult           senior                  False\n",
       "69    063A  [senior, kitten, senior, kitten, senior, kitte...        kitten            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     63\n",
      "kitten    13\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             63  86.301370\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          566            457  80.742049\n",
      "1           kitten          109             92  84.403670\n",
      "2           senior          178            101  56.741573\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      164     77.00\n",
      "1          M    337      249     73.89\n",
      "2          X    303      237     78.22\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1156, 2: 1109, 1: 1003})\n",
      "Epoch 1/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.0979 - accuracy: 0.5070\n",
      "Epoch 2/1500\n",
      "52/52 [==============================] - 0s 930us/step - loss: 0.9278 - accuracy: 0.5985\n",
      "Epoch 3/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.8644 - accuracy: 0.6258\n",
      "Epoch 4/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.8060 - accuracy: 0.6521\n",
      "Epoch 5/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.7905 - accuracy: 0.6521\n",
      "Epoch 6/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.7625 - accuracy: 0.6659\n",
      "Epoch 7/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.7389 - accuracy: 0.6759\n",
      "Epoch 8/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.7244 - accuracy: 0.6845\n",
      "Epoch 9/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.6908 - accuracy: 0.7084\n",
      "Epoch 10/1500\n",
      "52/52 [==============================] - 0s 900us/step - loss: 0.6654 - accuracy: 0.7166\n",
      "Epoch 11/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.6754 - accuracy: 0.7114\n",
      "Epoch 12/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.6465 - accuracy: 0.7163\n",
      "Epoch 13/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.6419 - accuracy: 0.7258\n",
      "Epoch 14/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.6388 - accuracy: 0.7283\n",
      "Epoch 15/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.6194 - accuracy: 0.7319\n",
      "Epoch 16/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.6227 - accuracy: 0.7298\n",
      "Epoch 17/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.6186 - accuracy: 0.7344\n",
      "Epoch 18/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.6008 - accuracy: 0.7472\n",
      "Epoch 19/1500\n",
      "52/52 [==============================] - 0s 923us/step - loss: 0.5731 - accuracy: 0.7650\n",
      "Epoch 20/1500\n",
      "52/52 [==============================] - 0s 918us/step - loss: 0.5813 - accuracy: 0.7552\n",
      "Epoch 21/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.5775 - accuracy: 0.7635\n",
      "Epoch 22/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.5436 - accuracy: 0.7625\n",
      "Epoch 23/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.5645 - accuracy: 0.7653\n",
      "Epoch 24/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.5548 - accuracy: 0.7607\n",
      "Epoch 25/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.5403 - accuracy: 0.7665\n",
      "Epoch 26/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.5367 - accuracy: 0.7754\n",
      "Epoch 27/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.5138 - accuracy: 0.7763\n",
      "Epoch 28/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.5237 - accuracy: 0.7827\n",
      "Epoch 29/1500\n",
      "52/52 [==============================] - 0s 922us/step - loss: 0.5017 - accuracy: 0.7867\n",
      "Epoch 30/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.5119 - accuracy: 0.7800\n",
      "Epoch 31/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.5212 - accuracy: 0.7769\n",
      "Epoch 32/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.5146 - accuracy: 0.7855\n",
      "Epoch 33/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.5167 - accuracy: 0.7815\n",
      "Epoch 34/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.4977 - accuracy: 0.7895\n",
      "Epoch 35/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.5022 - accuracy: 0.7907\n",
      "Epoch 36/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.4919 - accuracy: 0.7879\n",
      "Epoch 37/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.4915 - accuracy: 0.7886\n",
      "Epoch 38/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.4736 - accuracy: 0.7971\n",
      "Epoch 39/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.4842 - accuracy: 0.7892\n",
      "Epoch 40/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.4688 - accuracy: 0.7993\n",
      "Epoch 41/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.4752 - accuracy: 0.7987\n",
      "Epoch 42/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4771 - accuracy: 0.7962\n",
      "Epoch 43/1500\n",
      "52/52 [==============================] - 0s 985us/step - loss: 0.4772 - accuracy: 0.7987\n",
      "Epoch 44/1500\n",
      "52/52 [==============================] - 0s 932us/step - loss: 0.4711 - accuracy: 0.8023\n",
      "Epoch 45/1500\n",
      "52/52 [==============================] - 0s 922us/step - loss: 0.4580 - accuracy: 0.8081\n",
      "Epoch 46/1500\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8069\n",
      "Epoch 47/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.8039\n",
      "Epoch 48/1500\n",
      "52/52 [==============================] - 0s 921us/step - loss: 0.4566 - accuracy: 0.8088\n",
      "Epoch 49/1500\n",
      "52/52 [==============================] - 0s 885us/step - loss: 0.4690 - accuracy: 0.8011\n",
      "Epoch 50/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.4402 - accuracy: 0.8133\n",
      "Epoch 51/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.4504 - accuracy: 0.8002\n",
      "Epoch 52/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.4389 - accuracy: 0.8118\n",
      "Epoch 53/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.4374 - accuracy: 0.8143\n",
      "Epoch 54/1500\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.4324 - accuracy: 0.8170\n",
      "Epoch 55/1500\n",
      "52/52 [==============================] - 0s 888us/step - loss: 0.4353 - accuracy: 0.8136\n",
      "Epoch 56/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.4390 - accuracy: 0.8112\n",
      "Epoch 57/1500\n",
      "52/52 [==============================] - 0s 896us/step - loss: 0.4311 - accuracy: 0.8213\n",
      "Epoch 58/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.4257 - accuracy: 0.8167\n",
      "Epoch 59/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.4334 - accuracy: 0.8161\n",
      "Epoch 60/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.4378 - accuracy: 0.8161\n",
      "Epoch 61/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.4126 - accuracy: 0.8216\n",
      "Epoch 62/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.4184 - accuracy: 0.8265\n",
      "Epoch 63/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.4171 - accuracy: 0.8207\n",
      "Epoch 64/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.4086 - accuracy: 0.8283\n",
      "Epoch 65/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.4219 - accuracy: 0.8228\n",
      "Epoch 66/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.4172 - accuracy: 0.8231\n",
      "Epoch 67/1500\n",
      "52/52 [==============================] - 0s 875us/step - loss: 0.4041 - accuracy: 0.8308\n",
      "Epoch 68/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.4127 - accuracy: 0.8283\n",
      "Epoch 69/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.3979 - accuracy: 0.8326\n",
      "Epoch 70/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.4064 - accuracy: 0.8271\n",
      "Epoch 71/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.3980 - accuracy: 0.8354\n",
      "Epoch 72/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.3892 - accuracy: 0.8400\n",
      "Epoch 73/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.3964 - accuracy: 0.8332\n",
      "Epoch 74/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.3804 - accuracy: 0.8387\n",
      "Epoch 75/1500\n",
      "52/52 [==============================] - 0s 936us/step - loss: 0.4026 - accuracy: 0.8369\n",
      "Epoch 76/1500\n",
      "52/52 [==============================] - 0s 887us/step - loss: 0.3921 - accuracy: 0.8265\n",
      "Epoch 77/1500\n",
      "52/52 [==============================] - 0s 924us/step - loss: 0.3921 - accuracy: 0.8314\n",
      "Epoch 78/1500\n",
      "52/52 [==============================] - 0s 954us/step - loss: 0.3857 - accuracy: 0.8403\n",
      "Epoch 79/1500\n",
      "52/52 [==============================] - 0s 983us/step - loss: 0.3801 - accuracy: 0.8467\n",
      "Epoch 80/1500\n",
      "52/52 [==============================] - 0s 975us/step - loss: 0.3824 - accuracy: 0.8439\n",
      "Epoch 81/1500\n",
      "52/52 [==============================] - 0s 882us/step - loss: 0.3674 - accuracy: 0.8501\n",
      "Epoch 82/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.3920 - accuracy: 0.8338\n",
      "Epoch 83/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.3858 - accuracy: 0.8378\n",
      "Epoch 84/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.3799 - accuracy: 0.8418\n",
      "Epoch 85/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.3821 - accuracy: 0.8397\n",
      "Epoch 86/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.3598 - accuracy: 0.8540\n",
      "Epoch 87/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.3666 - accuracy: 0.8455\n",
      "Epoch 88/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.3744 - accuracy: 0.8427\n",
      "Epoch 89/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.3687 - accuracy: 0.8433\n",
      "Epoch 90/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.3806 - accuracy: 0.8354\n",
      "Epoch 91/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.3698 - accuracy: 0.8467\n",
      "Epoch 92/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.3635 - accuracy: 0.8485\n",
      "Epoch 93/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.3652 - accuracy: 0.8522\n",
      "Epoch 94/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.3670 - accuracy: 0.8439\n",
      "Epoch 95/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.3573 - accuracy: 0.8513\n",
      "Epoch 96/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.3634 - accuracy: 0.8507\n",
      "Epoch 97/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8409\n",
      "Epoch 98/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.3552 - accuracy: 0.8485\n",
      "Epoch 99/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.3590 - accuracy: 0.8510\n",
      "Epoch 100/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.3522 - accuracy: 0.8556\n",
      "Epoch 101/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.3605 - accuracy: 0.8476\n",
      "Epoch 102/1500\n",
      "52/52 [==============================] - 0s 893us/step - loss: 0.3548 - accuracy: 0.8543\n",
      "Epoch 103/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.3603 - accuracy: 0.8525\n",
      "Epoch 104/1500\n",
      "52/52 [==============================] - 0s 876us/step - loss: 0.3563 - accuracy: 0.8507\n",
      "Epoch 105/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.3658 - accuracy: 0.8430\n",
      "Epoch 106/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.3648 - accuracy: 0.8513\n",
      "Epoch 107/1500\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.3572 - accuracy: 0.8516\n",
      "Epoch 108/1500\n",
      "52/52 [==============================] - 0s 881us/step - loss: 0.3481 - accuracy: 0.8568\n",
      "Epoch 109/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.3481 - accuracy: 0.8537\n",
      "Epoch 110/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.3488 - accuracy: 0.8620\n",
      "Epoch 111/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.3384 - accuracy: 0.8562\n",
      "Epoch 112/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.3362 - accuracy: 0.8595\n",
      "Epoch 113/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.3595 - accuracy: 0.8562\n",
      "Epoch 114/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.3482 - accuracy: 0.8611\n",
      "Epoch 115/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.3547 - accuracy: 0.8513\n",
      "Epoch 116/1500\n",
      "52/52 [==============================] - 0s 924us/step - loss: 0.3412 - accuracy: 0.8571\n",
      "Epoch 117/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.3398 - accuracy: 0.8651\n",
      "Epoch 118/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.3421 - accuracy: 0.8583\n",
      "Epoch 119/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.3362 - accuracy: 0.8678\n",
      "Epoch 120/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.3389 - accuracy: 0.8666\n",
      "Epoch 121/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.3456 - accuracy: 0.8599\n",
      "Epoch 122/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.3260 - accuracy: 0.8605\n",
      "Epoch 123/1500\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.3273 - accuracy: 0.8675\n",
      "Epoch 124/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.3400 - accuracy: 0.8556\n",
      "Epoch 125/1500\n",
      "52/52 [==============================] - 0s 911us/step - loss: 0.3506 - accuracy: 0.8507\n",
      "Epoch 126/1500\n",
      "52/52 [==============================] - 0s 929us/step - loss: 0.3248 - accuracy: 0.8657\n",
      "Epoch 127/1500\n",
      "52/52 [==============================] - 0s 917us/step - loss: 0.3226 - accuracy: 0.8647\n",
      "Epoch 128/1500\n",
      "52/52 [==============================] - 0s 915us/step - loss: 0.3235 - accuracy: 0.8675\n",
      "Epoch 129/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.3373 - accuracy: 0.8599\n",
      "Epoch 130/1500\n",
      "52/52 [==============================] - 0s 893us/step - loss: 0.3281 - accuracy: 0.8654\n",
      "Epoch 131/1500\n",
      "52/52 [==============================] - 0s 881us/step - loss: 0.3292 - accuracy: 0.8678\n",
      "Epoch 132/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.3062 - accuracy: 0.8758\n",
      "Epoch 133/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.3272 - accuracy: 0.8660\n",
      "Epoch 134/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.3231 - accuracy: 0.8666\n",
      "Epoch 135/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.3167 - accuracy: 0.8690\n",
      "Epoch 136/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.3109 - accuracy: 0.8724\n",
      "Epoch 137/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.3155 - accuracy: 0.8785\n",
      "Epoch 138/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.3188 - accuracy: 0.8678\n",
      "Epoch 139/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.3252 - accuracy: 0.8666\n",
      "Epoch 140/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.3260 - accuracy: 0.8718\n",
      "Epoch 141/1500\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3139 - accuracy: 0.8721\n",
      "Epoch 142/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8718\n",
      "Epoch 143/1500\n",
      "52/52 [==============================] - 0s 925us/step - loss: 0.3123 - accuracy: 0.8748\n",
      "Epoch 144/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.3130 - accuracy: 0.8666\n",
      "Epoch 145/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.3076 - accuracy: 0.8727\n",
      "Epoch 146/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.2932 - accuracy: 0.8819\n",
      "Epoch 147/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.3093 - accuracy: 0.8755\n",
      "Epoch 148/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.3171 - accuracy: 0.8712\n",
      "Epoch 149/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.3095 - accuracy: 0.8730\n",
      "Epoch 150/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.3141 - accuracy: 0.8684\n",
      "Epoch 151/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.3072 - accuracy: 0.8696\n",
      "Epoch 152/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.3052 - accuracy: 0.8730\n",
      "Epoch 153/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.3159 - accuracy: 0.8733\n",
      "Epoch 154/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.3239 - accuracy: 0.8703\n",
      "Epoch 155/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.2989 - accuracy: 0.8859\n",
      "Epoch 156/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.2983 - accuracy: 0.8856\n",
      "Epoch 157/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.2949 - accuracy: 0.8813\n",
      "Epoch 158/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.2890 - accuracy: 0.8813\n",
      "Epoch 159/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.2907 - accuracy: 0.8804\n",
      "Epoch 160/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.2994 - accuracy: 0.8807\n",
      "Epoch 161/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.3027 - accuracy: 0.8773\n",
      "Epoch 162/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.3009 - accuracy: 0.8794\n",
      "Epoch 163/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.3086 - accuracy: 0.8745\n",
      "Epoch 164/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.3016 - accuracy: 0.8761\n",
      "Epoch 165/1500\n",
      "52/52 [==============================] - 0s 874us/step - loss: 0.2891 - accuracy: 0.8853\n",
      "Epoch 166/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.2997 - accuracy: 0.8745\n",
      "Epoch 167/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.2964 - accuracy: 0.8819\n",
      "Epoch 168/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.3023 - accuracy: 0.8748\n",
      "Epoch 169/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.3003 - accuracy: 0.8764\n",
      "Epoch 170/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.2994 - accuracy: 0.8800\n",
      "Epoch 171/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.2885 - accuracy: 0.8874\n",
      "Epoch 172/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.2734 - accuracy: 0.8846\n",
      "Epoch 173/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.2961 - accuracy: 0.8800\n",
      "Epoch 174/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.3028 - accuracy: 0.8813\n",
      "Epoch 175/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.2956 - accuracy: 0.8831\n",
      "Epoch 176/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.2939 - accuracy: 0.8856\n",
      "Epoch 177/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.2946 - accuracy: 0.8791\n",
      "Epoch 178/1500\n",
      "52/52 [==============================] - 0s 954us/step - loss: 0.2789 - accuracy: 0.8831\n",
      "Epoch 179/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.2870 - accuracy: 0.8883\n",
      "Epoch 180/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.2910 - accuracy: 0.8804\n",
      "Epoch 181/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.2813 - accuracy: 0.8859\n",
      "Epoch 182/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.2858 - accuracy: 0.8837\n",
      "Epoch 183/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.2822 - accuracy: 0.8853\n",
      "Epoch 184/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.2889 - accuracy: 0.8886\n",
      "Epoch 185/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.3021 - accuracy: 0.8828\n",
      "Epoch 186/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.2808 - accuracy: 0.8859\n",
      "Epoch 187/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.2837 - accuracy: 0.8853\n",
      "Epoch 188/1500\n",
      "52/52 [==============================] - 0s 794us/step - loss: 0.2849 - accuracy: 0.8862\n",
      "Epoch 189/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.2968 - accuracy: 0.8785\n",
      "Epoch 190/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.2844 - accuracy: 0.8840\n",
      "Epoch 191/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.2790 - accuracy: 0.8843\n",
      "Epoch 192/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.2967 - accuracy: 0.8791\n",
      "Epoch 193/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.2766 - accuracy: 0.8886\n",
      "Epoch 194/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.2794 - accuracy: 0.8846\n",
      "Epoch 195/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.2665 - accuracy: 0.8963\n",
      "Epoch 196/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.2871 - accuracy: 0.8874\n",
      "Epoch 197/1500\n",
      "52/52 [==============================] - 0s 884us/step - loss: 0.2813 - accuracy: 0.8868\n",
      "Epoch 198/1500\n",
      "52/52 [==============================] - 0s 906us/step - loss: 0.2737 - accuracy: 0.8908\n",
      "Epoch 199/1500\n",
      "52/52 [==============================] - 0s 904us/step - loss: 0.2807 - accuracy: 0.8868\n",
      "Epoch 200/1500\n",
      "52/52 [==============================] - 0s 905us/step - loss: 0.2819 - accuracy: 0.8791\n",
      "Epoch 201/1500\n",
      "52/52 [==============================] - 0s 939us/step - loss: 0.2763 - accuracy: 0.8883\n",
      "Epoch 202/1500\n",
      "52/52 [==============================] - 0s 927us/step - loss: 0.2680 - accuracy: 0.8905\n",
      "Epoch 203/1500\n",
      "52/52 [==============================] - 0s 885us/step - loss: 0.2764 - accuracy: 0.8877\n",
      "Epoch 204/1500\n",
      "52/52 [==============================] - 0s 935us/step - loss: 0.2732 - accuracy: 0.8911\n",
      "Epoch 205/1500\n",
      "52/52 [==============================] - 0s 936us/step - loss: 0.2814 - accuracy: 0.8877\n",
      "Epoch 206/1500\n",
      "52/52 [==============================] - 0s 914us/step - loss: 0.2696 - accuracy: 0.8914\n",
      "Epoch 207/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.2715 - accuracy: 0.8898\n",
      "Epoch 208/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.2723 - accuracy: 0.8880\n",
      "Epoch 209/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.2713 - accuracy: 0.8892\n",
      "Epoch 210/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.2656 - accuracy: 0.8908\n",
      "Epoch 211/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.2856 - accuracy: 0.8892\n",
      "Epoch 212/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.2732 - accuracy: 0.8889\n",
      "Epoch 213/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.2738 - accuracy: 0.8889\n",
      "Epoch 214/1500\n",
      "52/52 [==============================] - 0s 907us/step - loss: 0.2732 - accuracy: 0.8856\n",
      "Epoch 215/1500\n",
      "52/52 [==============================] - 0s 902us/step - loss: 0.2711 - accuracy: 0.8920\n",
      "Epoch 216/1500\n",
      "52/52 [==============================] - 0s 916us/step - loss: 0.2738 - accuracy: 0.8846\n",
      "Epoch 217/1500\n",
      "52/52 [==============================] - 0s 906us/step - loss: 0.2833 - accuracy: 0.8868\n",
      "Epoch 218/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.2658 - accuracy: 0.8905\n",
      "Epoch 219/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.2748 - accuracy: 0.8868\n",
      "Epoch 220/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.2730 - accuracy: 0.8905\n",
      "Epoch 221/1500\n",
      "52/52 [==============================] - 0s 873us/step - loss: 0.2691 - accuracy: 0.8920\n",
      "Epoch 222/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.2581 - accuracy: 0.8944\n",
      "Epoch 223/1500\n",
      "52/52 [==============================] - 0s 913us/step - loss: 0.2544 - accuracy: 0.9021\n",
      "Epoch 224/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.2712 - accuracy: 0.8944\n",
      "Epoch 225/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.2675 - accuracy: 0.8886\n",
      "Epoch 226/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.2642 - accuracy: 0.8993\n",
      "Epoch 227/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.2613 - accuracy: 0.8917\n",
      "Epoch 228/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.2705 - accuracy: 0.8901\n",
      "Epoch 229/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.2706 - accuracy: 0.8886\n",
      "Epoch 230/1500\n",
      "52/52 [==============================] - 0s 942us/step - loss: 0.2638 - accuracy: 0.8953\n",
      "Epoch 231/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8935\n",
      "Epoch 232/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.8920\n",
      "Epoch 233/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.8990\n",
      "Epoch 234/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8950\n",
      "Epoch 235/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2486 - accuracy: 0.8966\n",
      "Epoch 236/1500\n",
      "52/52 [==============================] - 0s 913us/step - loss: 0.2614 - accuracy: 0.8963\n",
      "Epoch 237/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.2654 - accuracy: 0.8966\n",
      "Epoch 238/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.2586 - accuracy: 0.8941\n",
      "Epoch 239/1500\n",
      "52/52 [==============================] - 0s 931us/step - loss: 0.2519 - accuracy: 0.8999\n",
      "Epoch 240/1500\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.2711 - accuracy: 0.8950\n",
      "Epoch 241/1500\n",
      "52/52 [==============================] - 0s 901us/step - loss: 0.2538 - accuracy: 0.9009\n",
      "Epoch 242/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.2587 - accuracy: 0.8960\n",
      "Epoch 243/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.2607 - accuracy: 0.8984\n",
      "Epoch 244/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.2603 - accuracy: 0.8935\n",
      "Epoch 245/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.2621 - accuracy: 0.8999\n",
      "Epoch 246/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.2595 - accuracy: 0.8950\n",
      "Epoch 247/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.2602 - accuracy: 0.8938\n",
      "Epoch 248/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.2632 - accuracy: 0.8901\n",
      "Epoch 249/1500\n",
      "52/52 [==============================] - 0s 925us/step - loss: 0.2507 - accuracy: 0.8990\n",
      "Epoch 250/1500\n",
      "52/52 [==============================] - 0s 899us/step - loss: 0.2497 - accuracy: 0.9006\n",
      "Epoch 251/1500\n",
      "52/52 [==============================] - 0s 971us/step - loss: 0.2473 - accuracy: 0.9018\n",
      "Epoch 252/1500\n",
      "52/52 [==============================] - 0s 926us/step - loss: 0.2390 - accuracy: 0.9058\n",
      "Epoch 253/1500\n",
      "52/52 [==============================] - 0s 905us/step - loss: 0.2466 - accuracy: 0.9054\n",
      "Epoch 254/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.2524 - accuracy: 0.8969\n",
      "Epoch 255/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.2620 - accuracy: 0.8990\n",
      "Epoch 256/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.2393 - accuracy: 0.9067\n",
      "Epoch 257/1500\n",
      "52/52 [==============================] - 0s 873us/step - loss: 0.2520 - accuracy: 0.8935\n",
      "Epoch 258/1500\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2511 - accuracy: 0.9018\n",
      "Epoch 259/1500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.2498 - accuracy: 0.8996\n",
      "Epoch 260/1500\n",
      "52/52 [==============================] - 0s 956us/step - loss: 0.2560 - accuracy: 0.8944\n",
      "Epoch 261/1500\n",
      "52/52 [==============================] - 0s 895us/step - loss: 0.2491 - accuracy: 0.8963\n",
      "Epoch 262/1500\n",
      "52/52 [==============================] - 0s 902us/step - loss: 0.2394 - accuracy: 0.9067\n",
      "Epoch 263/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.2545 - accuracy: 0.9009\n",
      "Epoch 264/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.2409 - accuracy: 0.9076\n",
      "Epoch 265/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.2457 - accuracy: 0.8963\n",
      "Epoch 266/1500\n",
      "52/52 [==============================] - 0s 914us/step - loss: 0.2462 - accuracy: 0.9048\n",
      "Epoch 267/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.2489 - accuracy: 0.9002\n",
      "Epoch 268/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.2457 - accuracy: 0.8996\n",
      "Epoch 269/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.2551 - accuracy: 0.9006\n",
      "Epoch 270/1500\n",
      "52/52 [==============================] - 0s 874us/step - loss: 0.2339 - accuracy: 0.9106\n",
      "Epoch 271/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.2523 - accuracy: 0.8966\n",
      "Epoch 272/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.2394 - accuracy: 0.9051\n",
      "Epoch 273/1500\n",
      "52/52 [==============================] - 0s 951us/step - loss: 0.2386 - accuracy: 0.9036\n",
      "Epoch 274/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.2419 - accuracy: 0.8987\n",
      "Epoch 275/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.2503 - accuracy: 0.8963\n",
      "Epoch 276/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.2413 - accuracy: 0.9033\n",
      "Epoch 277/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.2489 - accuracy: 0.8960\n",
      "Epoch 278/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.2369 - accuracy: 0.9009\n",
      "Epoch 279/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.2358 - accuracy: 0.9018\n",
      "Epoch 280/1500\n",
      "52/52 [==============================] - 0s 943us/step - loss: 0.2599 - accuracy: 0.9009\n",
      "Epoch 281/1500\n",
      "52/52 [==============================] - 0s 955us/step - loss: 0.2501 - accuracy: 0.8990\n",
      "Epoch 282/1500\n",
      "52/52 [==============================] - 0s 900us/step - loss: 0.2354 - accuracy: 0.9122\n",
      "Epoch 283/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.2235 - accuracy: 0.9146\n",
      "Epoch 284/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.2443 - accuracy: 0.9015\n",
      "Epoch 285/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.2428 - accuracy: 0.9030\n",
      "Epoch 286/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.2437 - accuracy: 0.9024\n",
      "Epoch 287/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.2320 - accuracy: 0.9073\n",
      "Epoch 288/1500\n",
      "52/52 [==============================] - 0s 887us/step - loss: 0.2445 - accuracy: 0.9064\n",
      "Epoch 289/1500\n",
      "52/52 [==============================] - 0s 894us/step - loss: 0.2360 - accuracy: 0.9027\n",
      "Epoch 290/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.2316 - accuracy: 0.9067\n",
      "Epoch 291/1500\n",
      "52/52 [==============================] - 0s 888us/step - loss: 0.2289 - accuracy: 0.9048\n",
      "Epoch 292/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.2295 - accuracy: 0.9082\n",
      "Epoch 293/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.2441 - accuracy: 0.9012\n",
      "Epoch 294/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.2304 - accuracy: 0.9100\n",
      "Epoch 295/1500\n",
      "52/52 [==============================] - 0s 934us/step - loss: 0.2330 - accuracy: 0.9076\n",
      "Epoch 296/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.2330 - accuracy: 0.9054\n",
      "Epoch 297/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.2409 - accuracy: 0.9018\n",
      "Epoch 298/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.2367 - accuracy: 0.9024\n",
      "Epoch 299/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.2410 - accuracy: 0.9048\n",
      "Epoch 300/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.2364 - accuracy: 0.9024\n",
      "Epoch 301/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.2333 - accuracy: 0.9061\n",
      "Epoch 302/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.2392 - accuracy: 0.9085\n",
      "Epoch 303/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.2414 - accuracy: 0.9024\n",
      "Epoch 304/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.2425 - accuracy: 0.8996\n",
      "Epoch 305/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.2362 - accuracy: 0.9024\n",
      "Epoch 306/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.2337 - accuracy: 0.9082\n",
      "Epoch 307/1500\n",
      "52/52 [==============================] - 0s 907us/step - loss: 0.2417 - accuracy: 0.9082\n",
      "Epoch 308/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.2279 - accuracy: 0.9082\n",
      "Epoch 309/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.2459 - accuracy: 0.8981\n",
      "Epoch 310/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.2318 - accuracy: 0.9070\n",
      "Epoch 311/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.2284 - accuracy: 0.9094\n",
      "Epoch 312/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.2292 - accuracy: 0.9027\n",
      "Epoch 313/1500\n",
      " 1/52 [..............................] - ETA: 0s - loss: 0.2719 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 283.\n",
      "52/52 [==============================] - 0s 909us/step - loss: 0.2331 - accuracy: 0.9128\n",
      "Epoch 313: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6657 - accuracy: 0.7124\n",
      "5/5 [==============================] - 0s 742us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.64 (16/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.6656640768051147, Accuracy: 0.7124183177947998, Precision: 0.5777777777777777, Recall: 0.5812702623047451, F1 Score: 0.567697653223969\n",
      "Confusion Matrix:\n",
      " [[90  6 20]\n",
      " [10 14  0]\n",
      " [ 8  0  5]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'026A'}\n",
      "Moved to Test Set:\n",
      "{'026A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A'\n",
      " '011A' '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '026A' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "701\n",
      "Length of y_train_val:\n",
      "701\n",
      "Length of groups_train_val:\n",
      "701\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     444\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     144\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1071, 2: 941, 1: 788})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.9909 - accuracy: 0.5521\n",
      "Epoch 2/1500\n",
      "44/44 [==============================] - 0s 982us/step - loss: 0.8749 - accuracy: 0.6064\n",
      "Epoch 3/1500\n",
      "44/44 [==============================] - 0s 905us/step - loss: 0.8019 - accuracy: 0.6475\n",
      "Epoch 4/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.7696 - accuracy: 0.6636\n",
      "Epoch 5/1500\n",
      "44/44 [==============================] - 0s 927us/step - loss: 0.7685 - accuracy: 0.6679\n",
      "Epoch 6/1500\n",
      "44/44 [==============================] - 0s 979us/step - loss: 0.7403 - accuracy: 0.6771\n",
      "Epoch 7/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.7027 - accuracy: 0.6957\n",
      "Epoch 8/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.6782 - accuracy: 0.7111\n",
      "Epoch 9/1500\n",
      "44/44 [==============================] - 0s 972us/step - loss: 0.6775 - accuracy: 0.7068\n",
      "Epoch 10/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.6766 - accuracy: 0.6986\n",
      "Epoch 11/1500\n",
      "44/44 [==============================] - 0s 945us/step - loss: 0.6703 - accuracy: 0.7096\n",
      "Epoch 12/1500\n",
      "44/44 [==============================] - 0s 989us/step - loss: 0.6424 - accuracy: 0.7143\n",
      "Epoch 13/1500\n",
      "44/44 [==============================] - 0s 946us/step - loss: 0.6502 - accuracy: 0.7204\n",
      "Epoch 14/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.6339 - accuracy: 0.7111\n",
      "Epoch 15/1500\n",
      "44/44 [==============================] - 0s 884us/step - loss: 0.6003 - accuracy: 0.7307\n",
      "Epoch 16/1500\n",
      "44/44 [==============================] - 0s 841us/step - loss: 0.6193 - accuracy: 0.7257\n",
      "Epoch 17/1500\n",
      "44/44 [==============================] - 0s 830us/step - loss: 0.6076 - accuracy: 0.7307\n",
      "Epoch 18/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.5973 - accuracy: 0.7375\n",
      "Epoch 19/1500\n",
      "44/44 [==============================] - 0s 857us/step - loss: 0.5874 - accuracy: 0.7393\n",
      "Epoch 20/1500\n",
      "44/44 [==============================] - 0s 932us/step - loss: 0.5776 - accuracy: 0.7518\n",
      "Epoch 21/1500\n",
      "44/44 [==============================] - 0s 872us/step - loss: 0.5845 - accuracy: 0.7407\n",
      "Epoch 22/1500\n",
      "44/44 [==============================] - 0s 859us/step - loss: 0.5681 - accuracy: 0.7486\n",
      "Epoch 23/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.5650 - accuracy: 0.7457\n",
      "Epoch 24/1500\n",
      "44/44 [==============================] - 0s 854us/step - loss: 0.5635 - accuracy: 0.7557\n",
      "Epoch 25/1500\n",
      "44/44 [==============================] - 0s 860us/step - loss: 0.5540 - accuracy: 0.7557\n",
      "Epoch 26/1500\n",
      "44/44 [==============================] - 0s 942us/step - loss: 0.5479 - accuracy: 0.7586\n",
      "Epoch 27/1500\n",
      "44/44 [==============================] - 0s 908us/step - loss: 0.5421 - accuracy: 0.7600\n",
      "Epoch 28/1500\n",
      "44/44 [==============================] - 0s 888us/step - loss: 0.5216 - accuracy: 0.7764\n",
      "Epoch 29/1500\n",
      "44/44 [==============================] - 0s 805us/step - loss: 0.5302 - accuracy: 0.7661\n",
      "Epoch 30/1500\n",
      "44/44 [==============================] - 0s 871us/step - loss: 0.5252 - accuracy: 0.7686\n",
      "Epoch 31/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.5323 - accuracy: 0.7682\n",
      "Epoch 32/1500\n",
      "44/44 [==============================] - 0s 946us/step - loss: 0.5127 - accuracy: 0.7657\n",
      "Epoch 33/1500\n",
      "44/44 [==============================] - 0s 896us/step - loss: 0.5179 - accuracy: 0.7729\n",
      "Epoch 34/1500\n",
      "44/44 [==============================] - 0s 912us/step - loss: 0.5092 - accuracy: 0.7707\n",
      "Epoch 35/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.5134 - accuracy: 0.7757\n",
      "Epoch 36/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.5038 - accuracy: 0.7861\n",
      "Epoch 37/1500\n",
      "44/44 [==============================] - 0s 828us/step - loss: 0.4932 - accuracy: 0.7818\n",
      "Epoch 38/1500\n",
      "44/44 [==============================] - 0s 799us/step - loss: 0.5058 - accuracy: 0.7793\n",
      "Epoch 39/1500\n",
      "44/44 [==============================] - 0s 812us/step - loss: 0.4987 - accuracy: 0.7782\n",
      "Epoch 40/1500\n",
      "44/44 [==============================] - 0s 821us/step - loss: 0.4847 - accuracy: 0.7864\n",
      "Epoch 41/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.4992 - accuracy: 0.7729\n",
      "Epoch 42/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.4777 - accuracy: 0.7964\n",
      "Epoch 43/1500\n",
      "44/44 [==============================] - 0s 913us/step - loss: 0.4838 - accuracy: 0.7925\n",
      "Epoch 44/1500\n",
      "44/44 [==============================] - 0s 890us/step - loss: 0.4722 - accuracy: 0.7936\n",
      "Epoch 45/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.4773 - accuracy: 0.7957\n",
      "Epoch 46/1500\n",
      "44/44 [==============================] - 0s 941us/step - loss: 0.4641 - accuracy: 0.8046\n",
      "Epoch 47/1500\n",
      "44/44 [==============================] - 0s 958us/step - loss: 0.4583 - accuracy: 0.8093\n",
      "Epoch 48/1500\n",
      "44/44 [==============================] - 0s 899us/step - loss: 0.4580 - accuracy: 0.8014\n",
      "Epoch 49/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.4738 - accuracy: 0.7954\n",
      "Epoch 50/1500\n",
      "44/44 [==============================] - 0s 905us/step - loss: 0.4593 - accuracy: 0.7871\n",
      "Epoch 51/1500\n",
      "44/44 [==============================] - 0s 942us/step - loss: 0.4468 - accuracy: 0.8118\n",
      "Epoch 52/1500\n",
      "44/44 [==============================] - 0s 929us/step - loss: 0.4573 - accuracy: 0.8000\n",
      "Epoch 53/1500\n",
      "44/44 [==============================] - 0s 865us/step - loss: 0.4463 - accuracy: 0.8086\n",
      "Epoch 54/1500\n",
      "44/44 [==============================] - 0s 893us/step - loss: 0.4513 - accuracy: 0.8046\n",
      "Epoch 55/1500\n",
      "44/44 [==============================] - 0s 3ms/step - loss: 0.4576 - accuracy: 0.7993\n",
      "Epoch 56/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.8079\n",
      "Epoch 57/1500\n",
      "44/44 [==============================] - 0s 951us/step - loss: 0.4407 - accuracy: 0.8032\n",
      "Epoch 58/1500\n",
      "44/44 [==============================] - 0s 898us/step - loss: 0.4397 - accuracy: 0.8118\n",
      "Epoch 59/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.4217 - accuracy: 0.8207\n",
      "Epoch 60/1500\n",
      "44/44 [==============================] - 0s 858us/step - loss: 0.4253 - accuracy: 0.8182\n",
      "Epoch 61/1500\n",
      "44/44 [==============================] - 0s 909us/step - loss: 0.4274 - accuracy: 0.8136\n",
      "Epoch 62/1500\n",
      "44/44 [==============================] - 0s 906us/step - loss: 0.4291 - accuracy: 0.8229\n",
      "Epoch 63/1500\n",
      "44/44 [==============================] - 0s 898us/step - loss: 0.4184 - accuracy: 0.8214\n",
      "Epoch 64/1500\n",
      "44/44 [==============================] - 0s 880us/step - loss: 0.4321 - accuracy: 0.8161\n",
      "Epoch 65/1500\n",
      "44/44 [==============================] - 0s 890us/step - loss: 0.4306 - accuracy: 0.8161\n",
      "Epoch 66/1500\n",
      "44/44 [==============================] - 0s 866us/step - loss: 0.4271 - accuracy: 0.8229\n",
      "Epoch 67/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.4367 - accuracy: 0.8089\n",
      "Epoch 68/1500\n",
      "44/44 [==============================] - 0s 853us/step - loss: 0.4282 - accuracy: 0.8154\n",
      "Epoch 69/1500\n",
      "44/44 [==============================] - 0s 817us/step - loss: 0.4203 - accuracy: 0.8193\n",
      "Epoch 70/1500\n",
      "44/44 [==============================] - 0s 879us/step - loss: 0.4238 - accuracy: 0.8229\n",
      "Epoch 71/1500\n",
      "44/44 [==============================] - 0s 865us/step - loss: 0.4077 - accuracy: 0.8296\n",
      "Epoch 72/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.4079 - accuracy: 0.8243\n",
      "Epoch 73/1500\n",
      "44/44 [==============================] - 0s 916us/step - loss: 0.4120 - accuracy: 0.8243\n",
      "Epoch 74/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.4087 - accuracy: 0.8314\n",
      "Epoch 75/1500\n",
      "44/44 [==============================] - 0s 960us/step - loss: 0.3967 - accuracy: 0.8264\n",
      "Epoch 76/1500\n",
      "44/44 [==============================] - 0s 895us/step - loss: 0.4100 - accuracy: 0.8221\n",
      "Epoch 77/1500\n",
      "44/44 [==============================] - 0s 873us/step - loss: 0.4026 - accuracy: 0.8193\n",
      "Epoch 78/1500\n",
      "44/44 [==============================] - 0s 904us/step - loss: 0.3868 - accuracy: 0.8418\n",
      "Epoch 79/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.4063 - accuracy: 0.8350\n",
      "Epoch 80/1500\n",
      "44/44 [==============================] - 0s 879us/step - loss: 0.4080 - accuracy: 0.8243\n",
      "Epoch 81/1500\n",
      "44/44 [==============================] - 0s 956us/step - loss: 0.3986 - accuracy: 0.8314\n",
      "Epoch 82/1500\n",
      "44/44 [==============================] - 0s 944us/step - loss: 0.3934 - accuracy: 0.8318\n",
      "Epoch 83/1500\n",
      "44/44 [==============================] - 0s 877us/step - loss: 0.3918 - accuracy: 0.8343\n",
      "Epoch 84/1500\n",
      "44/44 [==============================] - 0s 956us/step - loss: 0.4017 - accuracy: 0.8336\n",
      "Epoch 85/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8339\n",
      "Epoch 86/1500\n",
      "44/44 [==============================] - 0s 939us/step - loss: 0.3899 - accuracy: 0.8364\n",
      "Epoch 87/1500\n",
      "44/44 [==============================] - 0s 880us/step - loss: 0.3840 - accuracy: 0.8314\n",
      "Epoch 88/1500\n",
      "44/44 [==============================] - 0s 885us/step - loss: 0.3907 - accuracy: 0.8371\n",
      "Epoch 89/1500\n",
      "44/44 [==============================] - 0s 883us/step - loss: 0.3794 - accuracy: 0.8357\n",
      "Epoch 90/1500\n",
      "44/44 [==============================] - 0s 854us/step - loss: 0.3826 - accuracy: 0.8425\n",
      "Epoch 91/1500\n",
      "44/44 [==============================] - 0s 928us/step - loss: 0.3785 - accuracy: 0.8364\n",
      "Epoch 92/1500\n",
      "44/44 [==============================] - 0s 916us/step - loss: 0.3830 - accuracy: 0.8411\n",
      "Epoch 93/1500\n",
      "44/44 [==============================] - 0s 899us/step - loss: 0.3872 - accuracy: 0.8379\n",
      "Epoch 94/1500\n",
      "44/44 [==============================] - 0s 882us/step - loss: 0.3642 - accuracy: 0.8489\n",
      "Epoch 95/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.3636 - accuracy: 0.8529\n",
      "Epoch 96/1500\n",
      "44/44 [==============================] - 0s 870us/step - loss: 0.3943 - accuracy: 0.8279\n",
      "Epoch 97/1500\n",
      "44/44 [==============================] - 0s 886us/step - loss: 0.3676 - accuracy: 0.8471\n",
      "Epoch 98/1500\n",
      "44/44 [==============================] - 0s 931us/step - loss: 0.3710 - accuracy: 0.8432\n",
      "Epoch 99/1500\n",
      "44/44 [==============================] - 0s 883us/step - loss: 0.3812 - accuracy: 0.8332\n",
      "Epoch 100/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.3669 - accuracy: 0.8439\n",
      "Epoch 101/1500\n",
      "44/44 [==============================] - 0s 866us/step - loss: 0.3732 - accuracy: 0.8364\n",
      "Epoch 102/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.3754 - accuracy: 0.8461\n",
      "Epoch 103/1500\n",
      "44/44 [==============================] - 0s 908us/step - loss: 0.3821 - accuracy: 0.8425\n",
      "Epoch 104/1500\n",
      "44/44 [==============================] - 0s 891us/step - loss: 0.3538 - accuracy: 0.8525\n",
      "Epoch 105/1500\n",
      "44/44 [==============================] - 0s 926us/step - loss: 0.3636 - accuracy: 0.8407\n",
      "Epoch 106/1500\n",
      "44/44 [==============================] - 0s 876us/step - loss: 0.3595 - accuracy: 0.8536\n",
      "Epoch 107/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.3542 - accuracy: 0.8479\n",
      "Epoch 108/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.3569 - accuracy: 0.8500\n",
      "Epoch 109/1500\n",
      "44/44 [==============================] - 0s 863us/step - loss: 0.3560 - accuracy: 0.8536\n",
      "Epoch 110/1500\n",
      "44/44 [==============================] - 0s 876us/step - loss: 0.3608 - accuracy: 0.8464\n",
      "Epoch 111/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.3565 - accuracy: 0.8500\n",
      "Epoch 112/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.3596 - accuracy: 0.8443\n",
      "Epoch 113/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.3556 - accuracy: 0.8543\n",
      "Epoch 114/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.3531 - accuracy: 0.8496\n",
      "Epoch 115/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.3429 - accuracy: 0.8629\n",
      "Epoch 116/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.3440 - accuracy: 0.8550\n",
      "Epoch 117/1500\n",
      "44/44 [==============================] - 0s 863us/step - loss: 0.3472 - accuracy: 0.8586\n",
      "Epoch 118/1500\n",
      "44/44 [==============================] - 0s 857us/step - loss: 0.3600 - accuracy: 0.8489\n",
      "Epoch 119/1500\n",
      "44/44 [==============================] - 0s 953us/step - loss: 0.3347 - accuracy: 0.8632\n",
      "Epoch 120/1500\n",
      "44/44 [==============================] - 0s 984us/step - loss: 0.3420 - accuracy: 0.8546\n",
      "Epoch 121/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8536\n",
      "Epoch 122/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3644 - accuracy: 0.8579\n",
      "Epoch 123/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8514\n",
      "Epoch 124/1500\n",
      "44/44 [==============================] - 0s 959us/step - loss: 0.3347 - accuracy: 0.8582\n",
      "Epoch 125/1500\n",
      "44/44 [==============================] - 0s 887us/step - loss: 0.3280 - accuracy: 0.8664\n",
      "Epoch 126/1500\n",
      "44/44 [==============================] - 0s 915us/step - loss: 0.3397 - accuracy: 0.8646\n",
      "Epoch 127/1500\n",
      "44/44 [==============================] - 0s 877us/step - loss: 0.3392 - accuracy: 0.8582\n",
      "Epoch 128/1500\n",
      "44/44 [==============================] - 0s 890us/step - loss: 0.3407 - accuracy: 0.8550\n",
      "Epoch 129/1500\n",
      "44/44 [==============================] - 0s 994us/step - loss: 0.3425 - accuracy: 0.8554\n",
      "Epoch 130/1500\n",
      "44/44 [==============================] - 0s 977us/step - loss: 0.3287 - accuracy: 0.8643\n",
      "Epoch 131/1500\n",
      "44/44 [==============================] - 0s 852us/step - loss: 0.3199 - accuracy: 0.8661\n",
      "Epoch 132/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.3274 - accuracy: 0.8607\n",
      "Epoch 133/1500\n",
      "44/44 [==============================] - 0s 951us/step - loss: 0.3313 - accuracy: 0.8639\n",
      "Epoch 134/1500\n",
      "44/44 [==============================] - 0s 979us/step - loss: 0.3238 - accuracy: 0.8718\n",
      "Epoch 135/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.8725\n",
      "Epoch 136/1500\n",
      "44/44 [==============================] - 0s 944us/step - loss: 0.3239 - accuracy: 0.8775\n",
      "Epoch 137/1500\n",
      "44/44 [==============================] - 0s 882us/step - loss: 0.3287 - accuracy: 0.8664\n",
      "Epoch 138/1500\n",
      "44/44 [==============================] - 0s 846us/step - loss: 0.3347 - accuracy: 0.8639\n",
      "Epoch 139/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.3238 - accuracy: 0.8646\n",
      "Epoch 140/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.3224 - accuracy: 0.8689\n",
      "Epoch 141/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.3280 - accuracy: 0.8618\n",
      "Epoch 142/1500\n",
      "44/44 [==============================] - 0s 847us/step - loss: 0.3201 - accuracy: 0.8650\n",
      "Epoch 143/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.3134 - accuracy: 0.8675\n",
      "Epoch 144/1500\n",
      "44/44 [==============================] - 0s 934us/step - loss: 0.3232 - accuracy: 0.8611\n",
      "Epoch 145/1500\n",
      "44/44 [==============================] - 0s 943us/step - loss: 0.3094 - accuracy: 0.8771\n",
      "Epoch 146/1500\n",
      "44/44 [==============================] - 0s 974us/step - loss: 0.3155 - accuracy: 0.8736\n",
      "Epoch 147/1500\n",
      "44/44 [==============================] - 0s 927us/step - loss: 0.3164 - accuracy: 0.8711\n",
      "Epoch 148/1500\n",
      "44/44 [==============================] - 0s 839us/step - loss: 0.3102 - accuracy: 0.8718\n",
      "Epoch 149/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.3092 - accuracy: 0.8700\n",
      "Epoch 150/1500\n",
      "44/44 [==============================] - 0s 835us/step - loss: 0.3091 - accuracy: 0.8679\n",
      "Epoch 151/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.3163 - accuracy: 0.8668\n",
      "Epoch 152/1500\n",
      "44/44 [==============================] - 0s 938us/step - loss: 0.3159 - accuracy: 0.8671\n",
      "Epoch 153/1500\n",
      "44/44 [==============================] - 0s 946us/step - loss: 0.3155 - accuracy: 0.8754\n",
      "Epoch 154/1500\n",
      "44/44 [==============================] - 0s 947us/step - loss: 0.3045 - accuracy: 0.8704\n",
      "Epoch 155/1500\n",
      "44/44 [==============================] - 0s 905us/step - loss: 0.2996 - accuracy: 0.8782\n",
      "Epoch 156/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.3066 - accuracy: 0.8711\n",
      "Epoch 157/1500\n",
      "44/44 [==============================] - 0s 858us/step - loss: 0.2847 - accuracy: 0.8882\n",
      "Epoch 158/1500\n",
      "44/44 [==============================] - 0s 913us/step - loss: 0.2902 - accuracy: 0.8829\n",
      "Epoch 159/1500\n",
      "44/44 [==============================] - 0s 955us/step - loss: 0.3076 - accuracy: 0.8764\n",
      "Epoch 160/1500\n",
      "44/44 [==============================] - 0s 929us/step - loss: 0.2978 - accuracy: 0.8757\n",
      "Epoch 161/1500\n",
      "44/44 [==============================] - 0s 928us/step - loss: 0.2997 - accuracy: 0.8754\n",
      "Epoch 162/1500\n",
      "44/44 [==============================] - 0s 889us/step - loss: 0.2897 - accuracy: 0.8771\n",
      "Epoch 163/1500\n",
      "44/44 [==============================] - 0s 873us/step - loss: 0.2920 - accuracy: 0.8800\n",
      "Epoch 164/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.3017 - accuracy: 0.8782\n",
      "Epoch 165/1500\n",
      "44/44 [==============================] - 0s 908us/step - loss: 0.2873 - accuracy: 0.8871\n",
      "Epoch 166/1500\n",
      "44/44 [==============================] - 0s 945us/step - loss: 0.2989 - accuracy: 0.8771\n",
      "Epoch 167/1500\n",
      "44/44 [==============================] - 0s 953us/step - loss: 0.2816 - accuracy: 0.8829\n",
      "Epoch 168/1500\n",
      "44/44 [==============================] - 0s 870us/step - loss: 0.2866 - accuracy: 0.8889\n",
      "Epoch 169/1500\n",
      "44/44 [==============================] - 0s 847us/step - loss: 0.2949 - accuracy: 0.8829\n",
      "Epoch 170/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2944 - accuracy: 0.8836\n",
      "Epoch 171/1500\n",
      "44/44 [==============================] - 0s 910us/step - loss: 0.2913 - accuracy: 0.8811\n",
      "Epoch 172/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.2921 - accuracy: 0.8821\n",
      "Epoch 173/1500\n",
      "44/44 [==============================] - 0s 888us/step - loss: 0.3103 - accuracy: 0.8682\n",
      "Epoch 174/1500\n",
      "44/44 [==============================] - 0s 941us/step - loss: 0.2928 - accuracy: 0.8839\n",
      "Epoch 175/1500\n",
      "44/44 [==============================] - 0s 941us/step - loss: 0.2896 - accuracy: 0.8814\n",
      "Epoch 176/1500\n",
      "44/44 [==============================] - 0s 867us/step - loss: 0.2926 - accuracy: 0.8811\n",
      "Epoch 177/1500\n",
      "44/44 [==============================] - 0s 873us/step - loss: 0.2821 - accuracy: 0.8861\n",
      "Epoch 178/1500\n",
      "44/44 [==============================] - 0s 3ms/step - loss: 0.2726 - accuracy: 0.8886\n",
      "Epoch 179/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8854\n",
      "Epoch 180/1500\n",
      "44/44 [==============================] - 0s 930us/step - loss: 0.2862 - accuracy: 0.8868\n",
      "Epoch 181/1500\n",
      "44/44 [==============================] - 0s 888us/step - loss: 0.2776 - accuracy: 0.8864\n",
      "Epoch 182/1500\n",
      "44/44 [==============================] - 0s 926us/step - loss: 0.2758 - accuracy: 0.8882\n",
      "Epoch 183/1500\n",
      "44/44 [==============================] - 0s 886us/step - loss: 0.2814 - accuracy: 0.8882\n",
      "Epoch 184/1500\n",
      "44/44 [==============================] - 0s 869us/step - loss: 0.2815 - accuracy: 0.8875\n",
      "Epoch 185/1500\n",
      "44/44 [==============================] - 0s 969us/step - loss: 0.2753 - accuracy: 0.8929\n",
      "Epoch 186/1500\n",
      "44/44 [==============================] - 0s 950us/step - loss: 0.2841 - accuracy: 0.8893\n",
      "Epoch 187/1500\n",
      "44/44 [==============================] - 0s 959us/step - loss: 0.2866 - accuracy: 0.8864\n",
      "Epoch 188/1500\n",
      "44/44 [==============================] - 0s 871us/step - loss: 0.2824 - accuracy: 0.8839\n",
      "Epoch 189/1500\n",
      "44/44 [==============================] - 0s 953us/step - loss: 0.2682 - accuracy: 0.8893\n",
      "Epoch 190/1500\n",
      "44/44 [==============================] - 0s 908us/step - loss: 0.2776 - accuracy: 0.8857\n",
      "Epoch 191/1500\n",
      "44/44 [==============================] - 0s 908us/step - loss: 0.2700 - accuracy: 0.8896\n",
      "Epoch 192/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2756 - accuracy: 0.8904\n",
      "Epoch 193/1500\n",
      "44/44 [==============================] - 0s 927us/step - loss: 0.2793 - accuracy: 0.8807\n",
      "Epoch 194/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.2744 - accuracy: 0.8893\n",
      "Epoch 195/1500\n",
      "44/44 [==============================] - 0s 916us/step - loss: 0.2580 - accuracy: 0.8982\n",
      "Epoch 196/1500\n",
      "44/44 [==============================] - 0s 907us/step - loss: 0.2766 - accuracy: 0.8871\n",
      "Epoch 197/1500\n",
      "44/44 [==============================] - 0s 879us/step - loss: 0.2738 - accuracy: 0.8929\n",
      "Epoch 198/1500\n",
      "44/44 [==============================] - 0s 900us/step - loss: 0.2702 - accuracy: 0.8943\n",
      "Epoch 199/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2697 - accuracy: 0.8939\n",
      "Epoch 200/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2678 - accuracy: 0.8929\n",
      "Epoch 201/1500\n",
      "44/44 [==============================] - 0s 818us/step - loss: 0.2634 - accuracy: 0.8900\n",
      "Epoch 202/1500\n",
      "44/44 [==============================] - 0s 813us/step - loss: 0.2706 - accuracy: 0.8900\n",
      "Epoch 203/1500\n",
      "44/44 [==============================] - 0s 863us/step - loss: 0.2671 - accuracy: 0.8900\n",
      "Epoch 204/1500\n",
      "44/44 [==============================] - 0s 880us/step - loss: 0.2593 - accuracy: 0.8904\n",
      "Epoch 205/1500\n",
      "44/44 [==============================] - 0s 852us/step - loss: 0.2696 - accuracy: 0.8911\n",
      "Epoch 206/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.2658 - accuracy: 0.8904\n",
      "Epoch 207/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.2557 - accuracy: 0.8996\n",
      "Epoch 208/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.2637 - accuracy: 0.8946\n",
      "Epoch 209/1500\n",
      "44/44 [==============================] - 0s 937us/step - loss: 0.2497 - accuracy: 0.9011\n",
      "Epoch 210/1500\n",
      "44/44 [==============================] - 0s 945us/step - loss: 0.2540 - accuracy: 0.9007\n",
      "Epoch 211/1500\n",
      "44/44 [==============================] - 0s 914us/step - loss: 0.2357 - accuracy: 0.9021\n",
      "Epoch 212/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.8893\n",
      "Epoch 213/1500\n",
      "44/44 [==============================] - 0s 888us/step - loss: 0.2576 - accuracy: 0.8989\n",
      "Epoch 214/1500\n",
      "44/44 [==============================] - 0s 884us/step - loss: 0.2466 - accuracy: 0.8996\n",
      "Epoch 215/1500\n",
      "44/44 [==============================] - 0s 891us/step - loss: 0.2533 - accuracy: 0.9036\n",
      "Epoch 216/1500\n",
      "44/44 [==============================] - 0s 852us/step - loss: 0.2488 - accuracy: 0.8961\n",
      "Epoch 217/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2732 - accuracy: 0.8929\n",
      "Epoch 218/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.2549 - accuracy: 0.9046\n",
      "Epoch 219/1500\n",
      "44/44 [==============================] - 0s 858us/step - loss: 0.2661 - accuracy: 0.8950\n",
      "Epoch 220/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.2568 - accuracy: 0.9018\n",
      "Epoch 221/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.2473 - accuracy: 0.9079\n",
      "Epoch 222/1500\n",
      "44/44 [==============================] - 0s 871us/step - loss: 0.2552 - accuracy: 0.8957\n",
      "Epoch 223/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.2554 - accuracy: 0.8982\n",
      "Epoch 224/1500\n",
      "44/44 [==============================] - 0s 891us/step - loss: 0.2301 - accuracy: 0.9132\n",
      "Epoch 225/1500\n",
      "44/44 [==============================] - 0s 930us/step - loss: 0.2387 - accuracy: 0.9011\n",
      "Epoch 226/1500\n",
      "44/44 [==============================] - 0s 859us/step - loss: 0.2478 - accuracy: 0.8989\n",
      "Epoch 227/1500\n",
      "44/44 [==============================] - 0s 885us/step - loss: 0.2430 - accuracy: 0.9018\n",
      "Epoch 228/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.2397 - accuracy: 0.9029\n",
      "Epoch 229/1500\n",
      "44/44 [==============================] - 0s 946us/step - loss: 0.2416 - accuracy: 0.9036\n",
      "Epoch 230/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2411 - accuracy: 0.9089\n",
      "Epoch 231/1500\n",
      "44/44 [==============================] - 0s 794us/step - loss: 0.2391 - accuracy: 0.9043\n",
      "Epoch 232/1500\n",
      "44/44 [==============================] - 0s 807us/step - loss: 0.2367 - accuracy: 0.9064\n",
      "Epoch 233/1500\n",
      "44/44 [==============================] - 0s 850us/step - loss: 0.2507 - accuracy: 0.9004\n",
      "Epoch 234/1500\n",
      "44/44 [==============================] - 0s 807us/step - loss: 0.2547 - accuracy: 0.8982\n",
      "Epoch 235/1500\n",
      "44/44 [==============================] - 0s 789us/step - loss: 0.2407 - accuracy: 0.9050\n",
      "Epoch 236/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.2412 - accuracy: 0.9089\n",
      "Epoch 237/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.9032\n",
      "Epoch 238/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9014\n",
      "Epoch 239/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9121\n",
      "Epoch 240/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9096\n",
      "Epoch 241/1500\n",
      "44/44 [==============================] - 0s 966us/step - loss: 0.2431 - accuracy: 0.9021\n",
      "Epoch 242/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.8986\n",
      "Epoch 243/1500\n",
      "44/44 [==============================] - 0s 884us/step - loss: 0.2340 - accuracy: 0.9100\n",
      "Epoch 244/1500\n",
      "44/44 [==============================] - 0s 968us/step - loss: 0.2344 - accuracy: 0.9096\n",
      "Epoch 245/1500\n",
      "44/44 [==============================] - 0s 935us/step - loss: 0.2293 - accuracy: 0.9093\n",
      "Epoch 246/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2422 - accuracy: 0.9014\n",
      "Epoch 247/1500\n",
      "44/44 [==============================] - 0s 932us/step - loss: 0.2431 - accuracy: 0.8975\n",
      "Epoch 248/1500\n",
      "44/44 [==============================] - 0s 880us/step - loss: 0.2428 - accuracy: 0.9007\n",
      "Epoch 249/1500\n",
      "44/44 [==============================] - 0s 858us/step - loss: 0.2280 - accuracy: 0.9104\n",
      "Epoch 250/1500\n",
      "44/44 [==============================] - 0s 860us/step - loss: 0.2436 - accuracy: 0.9068\n",
      "Epoch 251/1500\n",
      "44/44 [==============================] - 0s 818us/step - loss: 0.2381 - accuracy: 0.9079\n",
      "Epoch 252/1500\n",
      "44/44 [==============================] - 0s 853us/step - loss: 0.2369 - accuracy: 0.9050\n",
      "Epoch 253/1500\n",
      "44/44 [==============================] - 0s 898us/step - loss: 0.2430 - accuracy: 0.8993\n",
      "Epoch 254/1500\n",
      "44/44 [==============================] - 0s 891us/step - loss: 0.2285 - accuracy: 0.9121\n",
      "Epoch 255/1500\n",
      "44/44 [==============================] - 0s 923us/step - loss: 0.2227 - accuracy: 0.9121\n",
      "Epoch 256/1500\n",
      "44/44 [==============================] - 0s 970us/step - loss: 0.2260 - accuracy: 0.9093\n",
      "Epoch 257/1500\n",
      "44/44 [==============================] - 0s 951us/step - loss: 0.2345 - accuracy: 0.9029\n",
      "Epoch 258/1500\n",
      "44/44 [==============================] - 0s 954us/step - loss: 0.2403 - accuracy: 0.9036\n",
      "Epoch 259/1500\n",
      "44/44 [==============================] - 0s 876us/step - loss: 0.2347 - accuracy: 0.9082\n",
      "Epoch 260/1500\n",
      "44/44 [==============================] - 0s 924us/step - loss: 0.2243 - accuracy: 0.9114\n",
      "Epoch 261/1500\n",
      "44/44 [==============================] - 0s 847us/step - loss: 0.2313 - accuracy: 0.9064\n",
      "Epoch 262/1500\n",
      "44/44 [==============================] - 0s 873us/step - loss: 0.2345 - accuracy: 0.9036\n",
      "Epoch 263/1500\n",
      "44/44 [==============================] - 0s 879us/step - loss: 0.2181 - accuracy: 0.9089\n",
      "Epoch 264/1500\n",
      "44/44 [==============================] - 0s 861us/step - loss: 0.2275 - accuracy: 0.9061\n",
      "Epoch 265/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.2317 - accuracy: 0.9075\n",
      "Epoch 266/1500\n",
      "44/44 [==============================] - 0s 874us/step - loss: 0.2365 - accuracy: 0.9057\n",
      "Epoch 267/1500\n",
      "44/44 [==============================] - 0s 790us/step - loss: 0.2232 - accuracy: 0.9054\n",
      "Epoch 268/1500\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.2317 - accuracy: 0.9043\n",
      "Epoch 269/1500\n",
      "44/44 [==============================] - 0s 809us/step - loss: 0.2205 - accuracy: 0.9132\n",
      "Epoch 270/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.2312 - accuracy: 0.9050\n",
      "Epoch 271/1500\n",
      "44/44 [==============================] - 0s 848us/step - loss: 0.2181 - accuracy: 0.9093\n",
      "Epoch 272/1500\n",
      "44/44 [==============================] - 0s 806us/step - loss: 0.2344 - accuracy: 0.9050\n",
      "Epoch 273/1500\n",
      "44/44 [==============================] - 0s 828us/step - loss: 0.2335 - accuracy: 0.9039\n",
      "Epoch 274/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.2085 - accuracy: 0.9182\n",
      "Epoch 275/1500\n",
      "44/44 [==============================] - 0s 817us/step - loss: 0.2161 - accuracy: 0.9118\n",
      "Epoch 276/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.2214 - accuracy: 0.9189\n",
      "Epoch 277/1500\n",
      "44/44 [==============================] - 0s 863us/step - loss: 0.2261 - accuracy: 0.9071\n",
      "Epoch 278/1500\n",
      "44/44 [==============================] - 0s 958us/step - loss: 0.2271 - accuracy: 0.9046\n",
      "Epoch 279/1500\n",
      "44/44 [==============================] - 0s 907us/step - loss: 0.2149 - accuracy: 0.9146\n",
      "Epoch 280/1500\n",
      "44/44 [==============================] - 0s 880us/step - loss: 0.2218 - accuracy: 0.9161\n",
      "Epoch 281/1500\n",
      "44/44 [==============================] - 0s 866us/step - loss: 0.2107 - accuracy: 0.9204\n",
      "Epoch 282/1500\n",
      "44/44 [==============================] - 0s 876us/step - loss: 0.2062 - accuracy: 0.9225\n",
      "Epoch 283/1500\n",
      "44/44 [==============================] - 0s 860us/step - loss: 0.2270 - accuracy: 0.9104\n",
      "Epoch 284/1500\n",
      "44/44 [==============================] - 0s 881us/step - loss: 0.2302 - accuracy: 0.9104\n",
      "Epoch 285/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.2314 - accuracy: 0.9089\n",
      "Epoch 286/1500\n",
      "44/44 [==============================] - 0s 882us/step - loss: 0.2193 - accuracy: 0.9100\n",
      "Epoch 287/1500\n",
      "44/44 [==============================] - 0s 924us/step - loss: 0.2311 - accuracy: 0.9061\n",
      "Epoch 288/1500\n",
      "44/44 [==============================] - 0s 933us/step - loss: 0.2248 - accuracy: 0.9111\n",
      "Epoch 289/1500\n",
      "44/44 [==============================] - 0s 900us/step - loss: 0.2252 - accuracy: 0.9107\n",
      "Epoch 290/1500\n",
      "44/44 [==============================] - 0s 890us/step - loss: 0.2209 - accuracy: 0.9132\n",
      "Epoch 291/1500\n",
      "44/44 [==============================] - 0s 874us/step - loss: 0.2199 - accuracy: 0.9154\n",
      "Epoch 292/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.2100 - accuracy: 0.9200\n",
      "Epoch 293/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.2057 - accuracy: 0.9143\n",
      "Epoch 294/1500\n",
      "44/44 [==============================] - 0s 869us/step - loss: 0.2028 - accuracy: 0.9225\n",
      "Epoch 295/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2130 - accuracy: 0.9104\n",
      "Epoch 296/1500\n",
      "44/44 [==============================] - 0s 844us/step - loss: 0.1976 - accuracy: 0.9196\n",
      "Epoch 297/1500\n",
      "44/44 [==============================] - 0s 887us/step - loss: 0.2096 - accuracy: 0.9186\n",
      "Epoch 298/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2099 - accuracy: 0.9139\n",
      "Epoch 299/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9182\n",
      "Epoch 300/1500\n",
      "44/44 [==============================] - 0s 955us/step - loss: 0.2125 - accuracy: 0.9082\n",
      "Epoch 301/1500\n",
      "44/44 [==============================] - 0s 874us/step - loss: 0.2068 - accuracy: 0.9196\n",
      "Epoch 302/1500\n",
      "44/44 [==============================] - 0s 926us/step - loss: 0.2161 - accuracy: 0.9125\n",
      "Epoch 303/1500\n",
      "44/44 [==============================] - 0s 941us/step - loss: 0.2146 - accuracy: 0.9132\n",
      "Epoch 304/1500\n",
      "44/44 [==============================] - 0s 942us/step - loss: 0.2141 - accuracy: 0.9186\n",
      "Epoch 305/1500\n",
      "44/44 [==============================] - 0s 912us/step - loss: 0.2025 - accuracy: 0.9157\n",
      "Epoch 306/1500\n",
      "44/44 [==============================] - 0s 925us/step - loss: 0.2073 - accuracy: 0.9157\n",
      "Epoch 307/1500\n",
      "44/44 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9164\n",
      "Epoch 308/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.9161\n",
      "Epoch 309/1500\n",
      "44/44 [==============================] - 0s 859us/step - loss: 0.2102 - accuracy: 0.9221\n",
      "Epoch 310/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2011 - accuracy: 0.9171\n",
      "Epoch 311/1500\n",
      "44/44 [==============================] - 0s 875us/step - loss: 0.2091 - accuracy: 0.9200\n",
      "Epoch 312/1500\n",
      "44/44 [==============================] - 0s 866us/step - loss: 0.1987 - accuracy: 0.9168\n",
      "Epoch 313/1500\n",
      "44/44 [==============================] - 0s 827us/step - loss: 0.2220 - accuracy: 0.9132\n",
      "Epoch 314/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.2012 - accuracy: 0.9189\n",
      "Epoch 315/1500\n",
      "44/44 [==============================] - 0s 854us/step - loss: 0.2070 - accuracy: 0.9164\n",
      "Epoch 316/1500\n",
      "44/44 [==============================] - 0s 829us/step - loss: 0.2081 - accuracy: 0.9161\n",
      "Epoch 317/1500\n",
      "44/44 [==============================] - 0s 925us/step - loss: 0.2068 - accuracy: 0.9146\n",
      "Epoch 318/1500\n",
      "44/44 [==============================] - 0s 885us/step - loss: 0.1983 - accuracy: 0.9214\n",
      "Epoch 319/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.2006 - accuracy: 0.9189\n",
      "Epoch 320/1500\n",
      "44/44 [==============================] - 0s 918us/step - loss: 0.2051 - accuracy: 0.9218\n",
      "Epoch 321/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.1974 - accuracy: 0.9168\n",
      "Epoch 322/1500\n",
      "44/44 [==============================] - 0s 853us/step - loss: 0.2023 - accuracy: 0.9182\n",
      "Epoch 323/1500\n",
      "44/44 [==============================] - 0s 841us/step - loss: 0.2025 - accuracy: 0.9121\n",
      "Epoch 324/1500\n",
      "44/44 [==============================] - 0s 834us/step - loss: 0.1961 - accuracy: 0.9254\n",
      "Epoch 325/1500\n",
      "44/44 [==============================] - 0s 851us/step - loss: 0.2164 - accuracy: 0.9125\n",
      "Epoch 326/1500\n",
      "44/44 [==============================] - 0s 879us/step - loss: 0.1855 - accuracy: 0.9246\n",
      "Epoch 327/1500\n",
      "44/44 [==============================] - 0s 825us/step - loss: 0.2056 - accuracy: 0.9207\n",
      "Epoch 328/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.2047 - accuracy: 0.9154\n",
      "Epoch 329/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.2004 - accuracy: 0.9207\n",
      "Epoch 330/1500\n",
      "44/44 [==============================] - 0s 824us/step - loss: 0.2024 - accuracy: 0.9157\n",
      "Epoch 331/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.1926 - accuracy: 0.9196\n",
      "Epoch 332/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.1991 - accuracy: 0.9186\n",
      "Epoch 333/1500\n",
      "44/44 [==============================] - 0s 866us/step - loss: 0.2007 - accuracy: 0.9236\n",
      "Epoch 334/1500\n",
      "44/44 [==============================] - 0s 789us/step - loss: 0.1900 - accuracy: 0.9211\n",
      "Epoch 335/1500\n",
      "44/44 [==============================] - 0s 824us/step - loss: 0.1960 - accuracy: 0.9250\n",
      "Epoch 336/1500\n",
      "44/44 [==============================] - 0s 843us/step - loss: 0.2002 - accuracy: 0.9268\n",
      "Epoch 337/1500\n",
      "44/44 [==============================] - 0s 862us/step - loss: 0.2027 - accuracy: 0.9204\n",
      "Epoch 338/1500\n",
      "44/44 [==============================] - 0s 800us/step - loss: 0.2024 - accuracy: 0.9154\n",
      "Epoch 339/1500\n",
      "44/44 [==============================] - 0s 819us/step - loss: 0.2022 - accuracy: 0.9232\n",
      "Epoch 340/1500\n",
      "44/44 [==============================] - 0s 873us/step - loss: 0.1984 - accuracy: 0.9168\n",
      "Epoch 341/1500\n",
      "44/44 [==============================] - 0s 869us/step - loss: 0.2028 - accuracy: 0.9246\n",
      "Epoch 342/1500\n",
      "44/44 [==============================] - 0s 826us/step - loss: 0.1969 - accuracy: 0.9207\n",
      "Epoch 343/1500\n",
      "44/44 [==============================] - 0s 853us/step - loss: 0.2011 - accuracy: 0.9229\n",
      "Epoch 344/1500\n",
      "44/44 [==============================] - 0s 856us/step - loss: 0.1980 - accuracy: 0.9236\n",
      "Epoch 345/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.1859 - accuracy: 0.9282\n",
      "Epoch 346/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.2027 - accuracy: 0.9179\n",
      "Epoch 347/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.1977 - accuracy: 0.9214\n",
      "Epoch 348/1500\n",
      "44/44 [==============================] - 0s 836us/step - loss: 0.2024 - accuracy: 0.9189\n",
      "Epoch 349/1500\n",
      "44/44 [==============================] - 0s 913us/step - loss: 0.1799 - accuracy: 0.9336\n",
      "Epoch 350/1500\n",
      "44/44 [==============================] - 0s 864us/step - loss: 0.1831 - accuracy: 0.9321\n",
      "Epoch 351/1500\n",
      "44/44 [==============================] - 0s 904us/step - loss: 0.2034 - accuracy: 0.9182\n",
      "Epoch 352/1500\n",
      "44/44 [==============================] - 0s 840us/step - loss: 0.1881 - accuracy: 0.9239\n",
      "Epoch 353/1500\n",
      "44/44 [==============================] - 0s 832us/step - loss: 0.1967 - accuracy: 0.9211\n",
      "Epoch 354/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.1957 - accuracy: 0.9218\n",
      "Epoch 355/1500\n",
      "44/44 [==============================] - 0s 881us/step - loss: 0.2046 - accuracy: 0.9157\n",
      "Epoch 356/1500\n",
      "44/44 [==============================] - 0s 854us/step - loss: 0.1837 - accuracy: 0.9261\n",
      "Epoch 357/1500\n",
      "44/44 [==============================] - 0s 837us/step - loss: 0.2010 - accuracy: 0.9175\n",
      "Epoch 358/1500\n",
      "44/44 [==============================] - 0s 868us/step - loss: 0.1865 - accuracy: 0.9286\n",
      "Epoch 359/1500\n",
      "44/44 [==============================] - 0s 853us/step - loss: 0.2039 - accuracy: 0.9236\n",
      "Epoch 360/1500\n",
      "44/44 [==============================] - 0s 842us/step - loss: 0.1941 - accuracy: 0.9254\n",
      "Epoch 361/1500\n",
      "44/44 [==============================] - 0s 821us/step - loss: 0.1880 - accuracy: 0.9314\n",
      "Epoch 362/1500\n",
      "44/44 [==============================] - 0s 833us/step - loss: 0.1938 - accuracy: 0.9211\n",
      "Epoch 363/1500\n",
      "44/44 [==============================] - 0s 820us/step - loss: 0.1879 - accuracy: 0.9229\n",
      "Epoch 364/1500\n",
      "44/44 [==============================] - 0s 849us/step - loss: 0.1936 - accuracy: 0.9279\n",
      "Epoch 365/1500\n",
      "44/44 [==============================] - 0s 830us/step - loss: 0.1887 - accuracy: 0.9264\n",
      "Epoch 366/1500\n",
      "44/44 [==============================] - 0s 817us/step - loss: 0.1997 - accuracy: 0.9182\n",
      "Epoch 367/1500\n",
      "44/44 [==============================] - 0s 847us/step - loss: 0.1875 - accuracy: 0.9246\n",
      "Epoch 368/1500\n",
      "44/44 [==============================] - 0s 809us/step - loss: 0.1855 - accuracy: 0.9214\n",
      "Epoch 369/1500\n",
      "44/44 [==============================] - 0s 840us/step - loss: 0.1924 - accuracy: 0.9261\n",
      "Epoch 370/1500\n",
      "44/44 [==============================] - 0s 838us/step - loss: 0.1861 - accuracy: 0.9282\n",
      "Epoch 371/1500\n",
      "44/44 [==============================] - 0s 855us/step - loss: 0.1804 - accuracy: 0.9250\n",
      "Epoch 372/1500\n",
      "44/44 [==============================] - 0s 823us/step - loss: 0.1932 - accuracy: 0.9236\n",
      "Epoch 373/1500\n",
      "44/44 [==============================] - 0s 809us/step - loss: 0.1886 - accuracy: 0.9268\n",
      "Epoch 374/1500\n",
      "44/44 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9293\n",
      "Epoch 375/1500\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9311\n",
      "Epoch 376/1500\n",
      "44/44 [==============================] - 0s 878us/step - loss: 0.1892 - accuracy: 0.9264\n",
      "Epoch 377/1500\n",
      "44/44 [==============================] - 0s 845us/step - loss: 0.1863 - accuracy: 0.9246\n",
      "Epoch 378/1500\n",
      "44/44 [==============================] - 0s 852us/step - loss: 0.1799 - accuracy: 0.9307\n",
      "Epoch 379/1500\n",
      " 1/44 [..............................] - ETA: 0s - loss: 0.1832 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 349.\n",
      "44/44 [==============================] - 0s 898us/step - loss: 0.1894 - accuracy: 0.9261\n",
      "Epoch 379: early stopping\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.7320 - accuracy: 0.7161\n",
      "8/8 [==============================] - 0s 649us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (23/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 389, Predictions: 389, Actuals: 389, Gender: 389\n",
      "Final Test Results - Loss: 0.7320085763931274, Accuracy: 0.7161017060279846, Precision: 0.6865131578947369, Recall: 0.6468960890589298, F1 Score: 0.662045839602562\n",
      "Confusion Matrix:\n",
      " [[119  15  10]\n",
      " [ 29  22   0]\n",
      " [ 12   1  28]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({1: 1098, 0: 1085, 2: 673})\n",
      "Epoch 1/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 1.0551 - accuracy: 0.5252\n",
      "Epoch 2/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.8284 - accuracy: 0.6404\n",
      "Epoch 3/1500\n",
      "45/45 [==============================] - 0s 864us/step - loss: 0.7628 - accuracy: 0.6793\n",
      "Epoch 4/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.6899 - accuracy: 0.7003\n",
      "Epoch 5/1500\n",
      "45/45 [==============================] - 0s 870us/step - loss: 0.6512 - accuracy: 0.7234\n",
      "Epoch 6/1500\n",
      "45/45 [==============================] - 0s 859us/step - loss: 0.6452 - accuracy: 0.7202\n",
      "Epoch 7/1500\n",
      "45/45 [==============================] - 0s 855us/step - loss: 0.6180 - accuracy: 0.7437\n",
      "Epoch 8/1500\n",
      "45/45 [==============================] - 0s 850us/step - loss: 0.6018 - accuracy: 0.7518\n",
      "Epoch 9/1500\n",
      "45/45 [==============================] - 0s 858us/step - loss: 0.5794 - accuracy: 0.7496\n",
      "Epoch 10/1500\n",
      "45/45 [==============================] - 0s 858us/step - loss: 0.5745 - accuracy: 0.7556\n",
      "Epoch 11/1500\n",
      "45/45 [==============================] - 0s 884us/step - loss: 0.5642 - accuracy: 0.7602\n",
      "Epoch 12/1500\n",
      "45/45 [==============================] - 0s 829us/step - loss: 0.5472 - accuracy: 0.7770\n",
      "Epoch 13/1500\n",
      "45/45 [==============================] - 0s 818us/step - loss: 0.5313 - accuracy: 0.7857\n",
      "Epoch 14/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.5322 - accuracy: 0.7822\n",
      "Epoch 15/1500\n",
      "45/45 [==============================] - 0s 842us/step - loss: 0.5437 - accuracy: 0.7735\n",
      "Epoch 16/1500\n",
      "45/45 [==============================] - 0s 848us/step - loss: 0.5182 - accuracy: 0.7815\n",
      "Epoch 17/1500\n",
      "45/45 [==============================] - 0s 843us/step - loss: 0.5014 - accuracy: 0.7871\n",
      "Epoch 18/1500\n",
      "45/45 [==============================] - 0s 848us/step - loss: 0.5041 - accuracy: 0.7945\n",
      "Epoch 19/1500\n",
      "45/45 [==============================] - 0s 874us/step - loss: 0.4854 - accuracy: 0.7969\n",
      "Epoch 20/1500\n",
      "45/45 [==============================] - 0s 983us/step - loss: 0.4912 - accuracy: 0.7955\n",
      "Epoch 21/1500\n",
      "45/45 [==============================] - 0s 974us/step - loss: 0.4786 - accuracy: 0.7941\n",
      "Epoch 22/1500\n",
      "45/45 [==============================] - 0s 868us/step - loss: 0.4696 - accuracy: 0.8025\n",
      "Epoch 23/1500\n",
      "45/45 [==============================] - 0s 864us/step - loss: 0.4682 - accuracy: 0.8064\n",
      "Epoch 24/1500\n",
      "45/45 [==============================] - 0s 854us/step - loss: 0.4660 - accuracy: 0.8088\n",
      "Epoch 25/1500\n",
      "45/45 [==============================] - 0s 845us/step - loss: 0.4574 - accuracy: 0.8078\n",
      "Epoch 26/1500\n",
      "45/45 [==============================] - 0s 852us/step - loss: 0.4455 - accuracy: 0.8190\n",
      "Epoch 27/1500\n",
      "45/45 [==============================] - 0s 844us/step - loss: 0.4498 - accuracy: 0.8144\n",
      "Epoch 28/1500\n",
      "45/45 [==============================] - 0s 867us/step - loss: 0.4253 - accuracy: 0.8246\n",
      "Epoch 29/1500\n",
      "45/45 [==============================] - 0s 863us/step - loss: 0.4336 - accuracy: 0.8176\n",
      "Epoch 30/1500\n",
      "45/45 [==============================] - 0s 903us/step - loss: 0.4419 - accuracy: 0.8169\n",
      "Epoch 31/1500\n",
      "45/45 [==============================] - 0s 827us/step - loss: 0.4398 - accuracy: 0.8211\n",
      "Epoch 32/1500\n",
      "45/45 [==============================] - 0s 840us/step - loss: 0.4324 - accuracy: 0.8130\n",
      "Epoch 33/1500\n",
      "45/45 [==============================] - 0s 844us/step - loss: 0.4163 - accuracy: 0.8281\n",
      "Epoch 34/1500\n",
      "45/45 [==============================] - 0s 862us/step - loss: 0.4135 - accuracy: 0.8312\n",
      "Epoch 35/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.4211 - accuracy: 0.8260\n",
      "Epoch 36/1500\n",
      "45/45 [==============================] - 0s 852us/step - loss: 0.4113 - accuracy: 0.8225\n",
      "Epoch 37/1500\n",
      "45/45 [==============================] - 0s 851us/step - loss: 0.4207 - accuracy: 0.8151\n",
      "Epoch 38/1500\n",
      "45/45 [==============================] - 0s 855us/step - loss: 0.4074 - accuracy: 0.8417\n",
      "Epoch 39/1500\n",
      "45/45 [==============================] - 0s 839us/step - loss: 0.4190 - accuracy: 0.8284\n",
      "Epoch 40/1500\n",
      "45/45 [==============================] - 0s 872us/step - loss: 0.4157 - accuracy: 0.8288\n",
      "Epoch 41/1500\n",
      "45/45 [==============================] - 0s 850us/step - loss: 0.4172 - accuracy: 0.8260\n",
      "Epoch 42/1500\n",
      "45/45 [==============================] - 0s 850us/step - loss: 0.3957 - accuracy: 0.8428\n",
      "Epoch 43/1500\n",
      "45/45 [==============================] - 0s 840us/step - loss: 0.3862 - accuracy: 0.8424\n",
      "Epoch 44/1500\n",
      "45/45 [==============================] - 0s 842us/step - loss: 0.3946 - accuracy: 0.8358\n",
      "Epoch 45/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.3874 - accuracy: 0.8410\n",
      "Epoch 46/1500\n",
      "45/45 [==============================] - 0s 863us/step - loss: 0.3854 - accuracy: 0.8407\n",
      "Epoch 47/1500\n",
      "45/45 [==============================] - 0s 870us/step - loss: 0.3937 - accuracy: 0.8424\n",
      "Epoch 48/1500\n",
      "45/45 [==============================] - 0s 861us/step - loss: 0.3898 - accuracy: 0.8480\n",
      "Epoch 49/1500\n",
      "45/45 [==============================] - 0s 842us/step - loss: 0.3879 - accuracy: 0.8382\n",
      "Epoch 50/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.3722 - accuracy: 0.8505\n",
      "Epoch 51/1500\n",
      "45/45 [==============================] - 0s 843us/step - loss: 0.3913 - accuracy: 0.8375\n",
      "Epoch 52/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.3749 - accuracy: 0.8435\n",
      "Epoch 53/1500\n",
      "45/45 [==============================] - 0s 865us/step - loss: 0.3713 - accuracy: 0.8445\n",
      "Epoch 54/1500\n",
      "45/45 [==============================] - 0s 839us/step - loss: 0.3651 - accuracy: 0.8519\n",
      "Epoch 55/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.3898 - accuracy: 0.8473\n",
      "Epoch 56/1500\n",
      "45/45 [==============================] - 0s 822us/step - loss: 0.3776 - accuracy: 0.8494\n",
      "Epoch 57/1500\n",
      "45/45 [==============================] - 0s 822us/step - loss: 0.3626 - accuracy: 0.8505\n",
      "Epoch 58/1500\n",
      "45/45 [==============================] - 0s 872us/step - loss: 0.3650 - accuracy: 0.8449\n",
      "Epoch 59/1500\n",
      "45/45 [==============================] - 0s 840us/step - loss: 0.3691 - accuracy: 0.8414\n",
      "Epoch 60/1500\n",
      "45/45 [==============================] - 0s 843us/step - loss: 0.3660 - accuracy: 0.8505\n",
      "Epoch 61/1500\n",
      "45/45 [==============================] - 0s 845us/step - loss: 0.3619 - accuracy: 0.8519\n",
      "Epoch 62/1500\n",
      "45/45 [==============================] - 0s 837us/step - loss: 0.3730 - accuracy: 0.8414\n",
      "Epoch 63/1500\n",
      "45/45 [==============================] - 0s 861us/step - loss: 0.3466 - accuracy: 0.8582\n",
      "Epoch 64/1500\n",
      "45/45 [==============================] - 0s 857us/step - loss: 0.3394 - accuracy: 0.8617\n",
      "Epoch 65/1500\n",
      "45/45 [==============================] - 0s 837us/step - loss: 0.3539 - accuracy: 0.8624\n",
      "Epoch 66/1500\n",
      "45/45 [==============================] - 0s 859us/step - loss: 0.3533 - accuracy: 0.8575\n",
      "Epoch 67/1500\n",
      "45/45 [==============================] - 0s 879us/step - loss: 0.3451 - accuracy: 0.8484\n",
      "Epoch 68/1500\n",
      "45/45 [==============================] - 0s 831us/step - loss: 0.3488 - accuracy: 0.8624\n",
      "Epoch 69/1500\n",
      "45/45 [==============================] - 0s 857us/step - loss: 0.3409 - accuracy: 0.8606\n",
      "Epoch 70/1500\n",
      "45/45 [==============================] - 0s 857us/step - loss: 0.3439 - accuracy: 0.8543\n",
      "Epoch 71/1500\n",
      "45/45 [==============================] - 0s 827us/step - loss: 0.3365 - accuracy: 0.8631\n",
      "Epoch 72/1500\n",
      "45/45 [==============================] - 0s 845us/step - loss: 0.3357 - accuracy: 0.8669\n",
      "Epoch 73/1500\n",
      "45/45 [==============================] - 0s 820us/step - loss: 0.3345 - accuracy: 0.8613\n",
      "Epoch 74/1500\n",
      "45/45 [==============================] - 0s 869us/step - loss: 0.3224 - accuracy: 0.8690\n",
      "Epoch 75/1500\n",
      "45/45 [==============================] - 0s 833us/step - loss: 0.3294 - accuracy: 0.8662\n",
      "Epoch 76/1500\n",
      "45/45 [==============================] - 0s 833us/step - loss: 0.3329 - accuracy: 0.8620\n",
      "Epoch 77/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.3373 - accuracy: 0.8673\n",
      "Epoch 78/1500\n",
      "45/45 [==============================] - 0s 860us/step - loss: 0.3307 - accuracy: 0.8676\n",
      "Epoch 79/1500\n",
      "45/45 [==============================] - 0s 859us/step - loss: 0.3521 - accuracy: 0.8568\n",
      "Epoch 80/1500\n",
      "45/45 [==============================] - 0s 830us/step - loss: 0.3334 - accuracy: 0.8617\n",
      "Epoch 81/1500\n",
      "45/45 [==============================] - 0s 871us/step - loss: 0.3378 - accuracy: 0.8648\n",
      "Epoch 82/1500\n",
      "45/45 [==============================] - 0s 853us/step - loss: 0.3297 - accuracy: 0.8666\n",
      "Epoch 83/1500\n",
      "45/45 [==============================] - 0s 844us/step - loss: 0.3212 - accuracy: 0.8662\n",
      "Epoch 84/1500\n",
      "45/45 [==============================] - 0s 823us/step - loss: 0.3269 - accuracy: 0.8666\n",
      "Epoch 85/1500\n",
      "45/45 [==============================] - 0s 860us/step - loss: 0.3066 - accuracy: 0.8729\n",
      "Epoch 86/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.3292 - accuracy: 0.8606\n",
      "Epoch 87/1500\n",
      "45/45 [==============================] - 0s 836us/step - loss: 0.3131 - accuracy: 0.8771\n",
      "Epoch 88/1500\n",
      "45/45 [==============================] - 0s 833us/step - loss: 0.3231 - accuracy: 0.8739\n",
      "Epoch 89/1500\n",
      "45/45 [==============================] - 0s 815us/step - loss: 0.3305 - accuracy: 0.8652\n",
      "Epoch 90/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.3229 - accuracy: 0.8655\n",
      "Epoch 91/1500\n",
      "45/45 [==============================] - 0s 831us/step - loss: 0.3065 - accuracy: 0.8739\n",
      "Epoch 92/1500\n",
      "45/45 [==============================] - 0s 840us/step - loss: 0.2890 - accuracy: 0.8834\n",
      "Epoch 93/1500\n",
      "45/45 [==============================] - 0s 839us/step - loss: 0.3253 - accuracy: 0.8645\n",
      "Epoch 94/1500\n",
      "45/45 [==============================] - 0s 862us/step - loss: 0.2941 - accuracy: 0.8750\n",
      "Epoch 95/1500\n",
      "45/45 [==============================] - 0s 860us/step - loss: 0.3198 - accuracy: 0.8743\n",
      "Epoch 96/1500\n",
      "45/45 [==============================] - 0s 814us/step - loss: 0.3026 - accuracy: 0.8782\n",
      "Epoch 97/1500\n",
      "45/45 [==============================] - 0s 856us/step - loss: 0.3015 - accuracy: 0.8743\n",
      "Epoch 98/1500\n",
      "45/45 [==============================] - 0s 864us/step - loss: 0.3150 - accuracy: 0.8743\n",
      "Epoch 99/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.3065 - accuracy: 0.8757\n",
      "Epoch 100/1500\n",
      "45/45 [==============================] - 0s 863us/step - loss: 0.2987 - accuracy: 0.8845\n",
      "Epoch 101/1500\n",
      "45/45 [==============================] - 0s 854us/step - loss: 0.3002 - accuracy: 0.8810\n",
      "Epoch 102/1500\n",
      "45/45 [==============================] - 0s 829us/step - loss: 0.3012 - accuracy: 0.8803\n",
      "Epoch 103/1500\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3049 - accuracy: 0.8775\n",
      "Epoch 104/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8768\n",
      "Epoch 105/1500\n",
      "45/45 [==============================] - 0s 837us/step - loss: 0.3037 - accuracy: 0.8764\n",
      "Epoch 106/1500\n",
      "45/45 [==============================] - 0s 807us/step - loss: 0.3009 - accuracy: 0.8761\n",
      "Epoch 107/1500\n",
      "45/45 [==============================] - 0s 880us/step - loss: 0.3034 - accuracy: 0.8792\n",
      "Epoch 108/1500\n",
      "45/45 [==============================] - 0s 811us/step - loss: 0.2949 - accuracy: 0.8813\n",
      "Epoch 109/1500\n",
      "45/45 [==============================] - 0s 848us/step - loss: 0.2992 - accuracy: 0.8803\n",
      "Epoch 110/1500\n",
      "45/45 [==============================] - 0s 838us/step - loss: 0.2945 - accuracy: 0.8869\n",
      "Epoch 111/1500\n",
      "45/45 [==============================] - 0s 809us/step - loss: 0.2820 - accuracy: 0.8876\n",
      "Epoch 112/1500\n",
      "45/45 [==============================] - 0s 799us/step - loss: 0.2971 - accuracy: 0.8852\n",
      "Epoch 113/1500\n",
      "45/45 [==============================] - 0s 773us/step - loss: 0.2893 - accuracy: 0.8838\n",
      "Epoch 114/1500\n",
      "45/45 [==============================] - 0s 800us/step - loss: 0.2939 - accuracy: 0.8796\n",
      "Epoch 115/1500\n",
      "45/45 [==============================] - 0s 770us/step - loss: 0.2809 - accuracy: 0.8901\n",
      "Epoch 116/1500\n",
      "45/45 [==============================] - 0s 764us/step - loss: 0.2965 - accuracy: 0.8757\n",
      "Epoch 117/1500\n",
      "45/45 [==============================] - 0s 776us/step - loss: 0.2792 - accuracy: 0.8883\n",
      "Epoch 118/1500\n",
      "45/45 [==============================] - 0s 781us/step - loss: 0.2918 - accuracy: 0.8876\n",
      "Epoch 119/1500\n",
      "45/45 [==============================] - 0s 898us/step - loss: 0.2812 - accuracy: 0.8869\n",
      "Epoch 120/1500\n",
      "45/45 [==============================] - 0s 877us/step - loss: 0.2743 - accuracy: 0.8908\n",
      "Epoch 121/1500\n",
      "45/45 [==============================] - 0s 848us/step - loss: 0.2744 - accuracy: 0.8904\n",
      "Epoch 122/1500\n",
      "45/45 [==============================] - 0s 834us/step - loss: 0.2786 - accuracy: 0.8880\n",
      "Epoch 123/1500\n",
      "45/45 [==============================] - 0s 839us/step - loss: 0.2830 - accuracy: 0.8859\n",
      "Epoch 124/1500\n",
      "45/45 [==============================] - 0s 857us/step - loss: 0.2655 - accuracy: 0.8925\n",
      "Epoch 125/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.2694 - accuracy: 0.8901\n",
      "Epoch 126/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.2777 - accuracy: 0.8810\n",
      "Epoch 127/1500\n",
      "45/45 [==============================] - 0s 856us/step - loss: 0.2909 - accuracy: 0.8792\n",
      "Epoch 128/1500\n",
      "45/45 [==============================] - 0s 826us/step - loss: 0.2699 - accuracy: 0.8922\n",
      "Epoch 129/1500\n",
      "45/45 [==============================] - 0s 857us/step - loss: 0.2767 - accuracy: 0.8936\n",
      "Epoch 130/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.2794 - accuracy: 0.8950\n",
      "Epoch 131/1500\n",
      "45/45 [==============================] - 0s 840us/step - loss: 0.2782 - accuracy: 0.8901\n",
      "Epoch 132/1500\n",
      "45/45 [==============================] - 0s 854us/step - loss: 0.2556 - accuracy: 0.8999\n",
      "Epoch 133/1500\n",
      "45/45 [==============================] - 0s 827us/step - loss: 0.2749 - accuracy: 0.8831\n",
      "Epoch 134/1500\n",
      "45/45 [==============================] - 0s 820us/step - loss: 0.2688 - accuracy: 0.8876\n",
      "Epoch 135/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.2732 - accuracy: 0.8817\n",
      "Epoch 136/1500\n",
      "45/45 [==============================] - 0s 860us/step - loss: 0.2525 - accuracy: 0.8992\n",
      "Epoch 137/1500\n",
      "45/45 [==============================] - 0s 860us/step - loss: 0.2588 - accuracy: 0.8974\n",
      "Epoch 138/1500\n",
      "45/45 [==============================] - 0s 849us/step - loss: 0.2680 - accuracy: 0.8953\n",
      "Epoch 139/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.2624 - accuracy: 0.8971\n",
      "Epoch 140/1500\n",
      "45/45 [==============================] - 0s 842us/step - loss: 0.2605 - accuracy: 0.8999\n",
      "Epoch 141/1500\n",
      "45/45 [==============================] - 0s 854us/step - loss: 0.2658 - accuracy: 0.8894\n",
      "Epoch 142/1500\n",
      "45/45 [==============================] - 0s 820us/step - loss: 0.2560 - accuracy: 0.8988\n",
      "Epoch 143/1500\n",
      "45/45 [==============================] - 0s 853us/step - loss: 0.2661 - accuracy: 0.8918\n",
      "Epoch 144/1500\n",
      "45/45 [==============================] - 0s 845us/step - loss: 0.2710 - accuracy: 0.8908\n",
      "Epoch 145/1500\n",
      "45/45 [==============================] - 0s 868us/step - loss: 0.2603 - accuracy: 0.8932\n",
      "Epoch 146/1500\n",
      "45/45 [==============================] - 0s 838us/step - loss: 0.2615 - accuracy: 0.8943\n",
      "Epoch 147/1500\n",
      "45/45 [==============================] - 0s 832us/step - loss: 0.2527 - accuracy: 0.9027\n",
      "Epoch 148/1500\n",
      "45/45 [==============================] - 0s 859us/step - loss: 0.2560 - accuracy: 0.8985\n",
      "Epoch 149/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.2538 - accuracy: 0.8974\n",
      "Epoch 150/1500\n",
      "45/45 [==============================] - 0s 828us/step - loss: 0.2601 - accuracy: 0.8974\n",
      "Epoch 151/1500\n",
      "45/45 [==============================] - 0s 850us/step - loss: 0.2628 - accuracy: 0.8988\n",
      "Epoch 152/1500\n",
      "45/45 [==============================] - 0s 891us/step - loss: 0.2439 - accuracy: 0.9051\n",
      "Epoch 153/1500\n",
      "45/45 [==============================] - 0s 843us/step - loss: 0.2486 - accuracy: 0.9020\n",
      "Epoch 154/1500\n",
      "45/45 [==============================] - 0s 828us/step - loss: 0.2552 - accuracy: 0.8960\n",
      "Epoch 155/1500\n",
      "45/45 [==============================] - 0s 839us/step - loss: 0.2396 - accuracy: 0.9118\n",
      "Epoch 156/1500\n",
      "45/45 [==============================] - 0s 879us/step - loss: 0.2503 - accuracy: 0.8992\n",
      "Epoch 157/1500\n",
      "45/45 [==============================] - 0s 872us/step - loss: 0.2541 - accuracy: 0.8974\n",
      "Epoch 158/1500\n",
      "45/45 [==============================] - 0s 813us/step - loss: 0.2391 - accuracy: 0.9048\n",
      "Epoch 159/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.2485 - accuracy: 0.9065\n",
      "Epoch 160/1500\n",
      "45/45 [==============================] - 0s 855us/step - loss: 0.2505 - accuracy: 0.9027\n",
      "Epoch 161/1500\n",
      "45/45 [==============================] - 0s 877us/step - loss: 0.2555 - accuracy: 0.9023\n",
      "Epoch 162/1500\n",
      "45/45 [==============================] - 0s 847us/step - loss: 0.2452 - accuracy: 0.9041\n",
      "Epoch 163/1500\n",
      "45/45 [==============================] - 0s 851us/step - loss: 0.2389 - accuracy: 0.9037\n",
      "Epoch 164/1500\n",
      "45/45 [==============================] - 0s 817us/step - loss: 0.2446 - accuracy: 0.9044\n",
      "Epoch 165/1500\n",
      "45/45 [==============================] - 0s 833us/step - loss: 0.2465 - accuracy: 0.9041\n",
      "Epoch 166/1500\n",
      "45/45 [==============================] - 0s 985us/step - loss: 0.2346 - accuracy: 0.9051\n",
      "Epoch 167/1500\n",
      "45/45 [==============================] - 0s 986us/step - loss: 0.2400 - accuracy: 0.9076\n",
      "Epoch 168/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.8999\n",
      "Epoch 169/1500\n",
      "45/45 [==============================] - 0s 882us/step - loss: 0.2305 - accuracy: 0.9065\n",
      "Epoch 170/1500\n",
      "45/45 [==============================] - 0s 911us/step - loss: 0.2333 - accuracy: 0.9107\n",
      "Epoch 171/1500\n",
      "45/45 [==============================] - 0s 842us/step - loss: 0.2386 - accuracy: 0.8981\n",
      "Epoch 172/1500\n",
      "45/45 [==============================] - 0s 865us/step - loss: 0.2343 - accuracy: 0.9051\n",
      "Epoch 173/1500\n",
      "45/45 [==============================] - 0s 869us/step - loss: 0.2433 - accuracy: 0.9006\n",
      "Epoch 174/1500\n",
      "45/45 [==============================] - 0s 865us/step - loss: 0.2346 - accuracy: 0.9006\n",
      "Epoch 175/1500\n",
      "45/45 [==============================] - 0s 944us/step - loss: 0.2293 - accuracy: 0.9125\n",
      "Epoch 176/1500\n",
      "45/45 [==============================] - 0s 885us/step - loss: 0.2374 - accuracy: 0.9048\n",
      "Epoch 177/1500\n",
      "45/45 [==============================] - 0s 873us/step - loss: 0.2405 - accuracy: 0.9055\n",
      "Epoch 178/1500\n",
      "45/45 [==============================] - 0s 897us/step - loss: 0.2276 - accuracy: 0.9100\n",
      "Epoch 179/1500\n",
      "45/45 [==============================] - 0s 945us/step - loss: 0.2388 - accuracy: 0.9076\n",
      "Epoch 180/1500\n",
      "45/45 [==============================] - 0s 853us/step - loss: 0.2217 - accuracy: 0.9139\n",
      "Epoch 181/1500\n",
      "45/45 [==============================] - 0s 864us/step - loss: 0.2324 - accuracy: 0.9097\n",
      "Epoch 182/1500\n",
      "45/45 [==============================] - 0s 890us/step - loss: 0.2308 - accuracy: 0.9044\n",
      "Epoch 183/1500\n",
      "45/45 [==============================] - 0s 836us/step - loss: 0.2236 - accuracy: 0.9118\n",
      "Epoch 184/1500\n",
      "45/45 [==============================] - 0s 830us/step - loss: 0.2275 - accuracy: 0.9114\n",
      "Epoch 185/1500\n",
      "45/45 [==============================] - 0s 833us/step - loss: 0.2308 - accuracy: 0.9090\n",
      "Epoch 186/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.2330 - accuracy: 0.9065\n",
      "Epoch 187/1500\n",
      "45/45 [==============================] - 0s 848us/step - loss: 0.2316 - accuracy: 0.9062\n",
      "Epoch 188/1500\n",
      "45/45 [==============================] - 0s 874us/step - loss: 0.2267 - accuracy: 0.9118\n",
      "Epoch 189/1500\n",
      "45/45 [==============================] - 0s 886us/step - loss: 0.2302 - accuracy: 0.9135\n",
      "Epoch 190/1500\n",
      "45/45 [==============================] - 0s 899us/step - loss: 0.2227 - accuracy: 0.9076\n",
      "Epoch 191/1500\n",
      "45/45 [==============================] - 0s 859us/step - loss: 0.2394 - accuracy: 0.9051\n",
      "Epoch 192/1500\n",
      "45/45 [==============================] - 0s 827us/step - loss: 0.2280 - accuracy: 0.9128\n",
      "Epoch 193/1500\n",
      "45/45 [==============================] - 0s 844us/step - loss: 0.2153 - accuracy: 0.9156\n",
      "Epoch 194/1500\n",
      "45/45 [==============================] - 0s 866us/step - loss: 0.2437 - accuracy: 0.9041\n",
      "Epoch 195/1500\n",
      "45/45 [==============================] - 0s 857us/step - loss: 0.2251 - accuracy: 0.9083\n",
      "Epoch 196/1500\n",
      "45/45 [==============================] - 0s 836us/step - loss: 0.2302 - accuracy: 0.9086\n",
      "Epoch 197/1500\n",
      "45/45 [==============================] - 0s 868us/step - loss: 0.2170 - accuracy: 0.9184\n",
      "Epoch 198/1500\n",
      "45/45 [==============================] - 0s 848us/step - loss: 0.2262 - accuracy: 0.9121\n",
      "Epoch 199/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.2394 - accuracy: 0.9020\n",
      "Epoch 200/1500\n",
      "45/45 [==============================] - 0s 882us/step - loss: 0.2173 - accuracy: 0.9058\n",
      "Epoch 201/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.2178 - accuracy: 0.9079\n",
      "Epoch 202/1500\n",
      "45/45 [==============================] - 0s 840us/step - loss: 0.2172 - accuracy: 0.9135\n",
      "Epoch 203/1500\n",
      "45/45 [==============================] - 0s 837us/step - loss: 0.2214 - accuracy: 0.9184\n",
      "Epoch 204/1500\n",
      "45/45 [==============================] - 0s 856us/step - loss: 0.2300 - accuracy: 0.9100\n",
      "Epoch 205/1500\n",
      "45/45 [==============================] - 0s 838us/step - loss: 0.2186 - accuracy: 0.9100\n",
      "Epoch 206/1500\n",
      "45/45 [==============================] - 0s 839us/step - loss: 0.2137 - accuracy: 0.9153\n",
      "Epoch 207/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.2305 - accuracy: 0.9058\n",
      "Epoch 208/1500\n",
      "45/45 [==============================] - 0s 810us/step - loss: 0.2189 - accuracy: 0.9139\n",
      "Epoch 209/1500\n",
      "45/45 [==============================] - 0s 851us/step - loss: 0.2242 - accuracy: 0.9146\n",
      "Epoch 210/1500\n",
      "45/45 [==============================] - 0s 844us/step - loss: 0.2115 - accuracy: 0.9142\n",
      "Epoch 211/1500\n",
      "45/45 [==============================] - 0s 862us/step - loss: 0.2104 - accuracy: 0.9174\n",
      "Epoch 212/1500\n",
      "45/45 [==============================] - 0s 823us/step - loss: 0.2150 - accuracy: 0.9160\n",
      "Epoch 213/1500\n",
      "45/45 [==============================] - 0s 845us/step - loss: 0.2152 - accuracy: 0.9170\n",
      "Epoch 214/1500\n",
      "45/45 [==============================] - 0s 843us/step - loss: 0.2204 - accuracy: 0.9118\n",
      "Epoch 215/1500\n",
      "45/45 [==============================] - 0s 838us/step - loss: 0.2168 - accuracy: 0.9167\n",
      "Epoch 216/1500\n",
      "45/45 [==============================] - 0s 842us/step - loss: 0.2243 - accuracy: 0.9146\n",
      "Epoch 217/1500\n",
      "45/45 [==============================] - 0s 855us/step - loss: 0.2108 - accuracy: 0.9142\n",
      "Epoch 218/1500\n",
      "45/45 [==============================] - 0s 845us/step - loss: 0.2159 - accuracy: 0.9139\n",
      "Epoch 219/1500\n",
      "45/45 [==============================] - 0s 861us/step - loss: 0.2161 - accuracy: 0.9146\n",
      "Epoch 220/1500\n",
      "45/45 [==============================] - 0s 852us/step - loss: 0.2051 - accuracy: 0.9202\n",
      "Epoch 221/1500\n",
      "45/45 [==============================] - 0s 858us/step - loss: 0.2078 - accuracy: 0.9195\n",
      "Epoch 222/1500\n",
      "45/45 [==============================] - 0s 843us/step - loss: 0.2045 - accuracy: 0.9202\n",
      "Epoch 223/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.2037 - accuracy: 0.9205\n",
      "Epoch 224/1500\n",
      "45/45 [==============================] - 0s 847us/step - loss: 0.2104 - accuracy: 0.9188\n",
      "Epoch 225/1500\n",
      "45/45 [==============================] - 0s 848us/step - loss: 0.2113 - accuracy: 0.9191\n",
      "Epoch 226/1500\n",
      "45/45 [==============================] - 0s 909us/step - loss: 0.1985 - accuracy: 0.9216\n",
      "Epoch 227/1500\n",
      "45/45 [==============================] - 0s 863us/step - loss: 0.2051 - accuracy: 0.9156\n",
      "Epoch 228/1500\n",
      "45/45 [==============================] - 0s 856us/step - loss: 0.2099 - accuracy: 0.9181\n",
      "Epoch 229/1500\n",
      "45/45 [==============================] - 0s 823us/step - loss: 0.2109 - accuracy: 0.9156\n",
      "Epoch 230/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.2148 - accuracy: 0.9181\n",
      "Epoch 231/1500\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2021 - accuracy: 0.9153\n",
      "Epoch 232/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1906 - accuracy: 0.9258\n",
      "Epoch 233/1500\n",
      "45/45 [==============================] - 0s 911us/step - loss: 0.1959 - accuracy: 0.9223\n",
      "Epoch 234/1500\n",
      "45/45 [==============================] - 0s 824us/step - loss: 0.2056 - accuracy: 0.9202\n",
      "Epoch 235/1500\n",
      "45/45 [==============================] - 0s 866us/step - loss: 0.2126 - accuracy: 0.9174\n",
      "Epoch 236/1500\n",
      "45/45 [==============================] - 0s 878us/step - loss: 0.2081 - accuracy: 0.9216\n",
      "Epoch 237/1500\n",
      "45/45 [==============================] - 0s 852us/step - loss: 0.1984 - accuracy: 0.9268\n",
      "Epoch 238/1500\n",
      "45/45 [==============================] - 0s 874us/step - loss: 0.1971 - accuracy: 0.9289\n",
      "Epoch 239/1500\n",
      "45/45 [==============================] - 0s 836us/step - loss: 0.1999 - accuracy: 0.9202\n",
      "Epoch 240/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.2077 - accuracy: 0.9135\n",
      "Epoch 241/1500\n",
      "45/45 [==============================] - 0s 827us/step - loss: 0.1935 - accuracy: 0.9275\n",
      "Epoch 242/1500\n",
      "45/45 [==============================] - 0s 795us/step - loss: 0.2022 - accuracy: 0.9226\n",
      "Epoch 243/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.1944 - accuracy: 0.9244\n",
      "Epoch 244/1500\n",
      "45/45 [==============================] - 0s 801us/step - loss: 0.2058 - accuracy: 0.9184\n",
      "Epoch 245/1500\n",
      "45/45 [==============================] - 0s 843us/step - loss: 0.1958 - accuracy: 0.9237\n",
      "Epoch 246/1500\n",
      "45/45 [==============================] - 0s 855us/step - loss: 0.1919 - accuracy: 0.9254\n",
      "Epoch 247/1500\n",
      "45/45 [==============================] - 0s 831us/step - loss: 0.2011 - accuracy: 0.9265\n",
      "Epoch 248/1500\n",
      "45/45 [==============================] - 0s 877us/step - loss: 0.1995 - accuracy: 0.9177\n",
      "Epoch 249/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.1958 - accuracy: 0.9240\n",
      "Epoch 250/1500\n",
      "45/45 [==============================] - 0s 858us/step - loss: 0.1881 - accuracy: 0.9268\n",
      "Epoch 251/1500\n",
      "45/45 [==============================] - 0s 895us/step - loss: 0.1903 - accuracy: 0.9261\n",
      "Epoch 252/1500\n",
      "45/45 [==============================] - 0s 862us/step - loss: 0.1991 - accuracy: 0.9212\n",
      "Epoch 253/1500\n",
      "45/45 [==============================] - 0s 846us/step - loss: 0.2075 - accuracy: 0.9160\n",
      "Epoch 254/1500\n",
      "45/45 [==============================] - 0s 819us/step - loss: 0.1959 - accuracy: 0.9198\n",
      "Epoch 255/1500\n",
      "45/45 [==============================] - 0s 891us/step - loss: 0.1955 - accuracy: 0.9272\n",
      "Epoch 256/1500\n",
      "45/45 [==============================] - 0s 887us/step - loss: 0.2016 - accuracy: 0.9233\n",
      "Epoch 257/1500\n",
      "45/45 [==============================] - 0s 824us/step - loss: 0.1974 - accuracy: 0.9191\n",
      "Epoch 258/1500\n",
      "45/45 [==============================] - 0s 822us/step - loss: 0.1940 - accuracy: 0.9310\n",
      "Epoch 259/1500\n",
      "45/45 [==============================] - 0s 831us/step - loss: 0.2127 - accuracy: 0.9149\n",
      "Epoch 260/1500\n",
      "45/45 [==============================] - 0s 828us/step - loss: 0.1874 - accuracy: 0.9202\n",
      "Epoch 261/1500\n",
      "45/45 [==============================] - 0s 818us/step - loss: 0.1909 - accuracy: 0.9247\n",
      "Epoch 262/1500\n",
      "45/45 [==============================] - 0s 841us/step - loss: 0.1863 - accuracy: 0.9289\n",
      "Epoch 263/1500\n",
      "45/45 [==============================] - 0s 851us/step - loss: 0.1761 - accuracy: 0.9324\n",
      "Epoch 264/1500\n",
      "45/45 [==============================] - 0s 840us/step - loss: 0.1984 - accuracy: 0.9216\n",
      "Epoch 265/1500\n",
      "45/45 [==============================] - 0s 830us/step - loss: 0.1874 - accuracy: 0.9293\n",
      "Epoch 266/1500\n",
      "45/45 [==============================] - 0s 835us/step - loss: 0.1787 - accuracy: 0.9293\n",
      "Epoch 267/1500\n",
      "45/45 [==============================] - 0s 838us/step - loss: 0.1890 - accuracy: 0.9251\n",
      "Epoch 268/1500\n",
      "45/45 [==============================] - 0s 828us/step - loss: 0.1793 - accuracy: 0.9293\n",
      "Epoch 269/1500\n",
      "45/45 [==============================] - 0s 805us/step - loss: 0.1825 - accuracy: 0.9258\n",
      "Epoch 270/1500\n",
      "45/45 [==============================] - 0s 847us/step - loss: 0.1727 - accuracy: 0.9331\n",
      "Epoch 271/1500\n",
      "45/45 [==============================] - 0s 810us/step - loss: 0.1787 - accuracy: 0.9352\n",
      "Epoch 272/1500\n",
      "45/45 [==============================] - 0s 831us/step - loss: 0.1853 - accuracy: 0.9258\n",
      "Epoch 273/1500\n",
      "45/45 [==============================] - 0s 847us/step - loss: 0.1661 - accuracy: 0.9373\n",
      "Epoch 274/1500\n",
      "45/45 [==============================] - 0s 932us/step - loss: 0.1878 - accuracy: 0.9223\n",
      "Epoch 275/1500\n",
      "45/45 [==============================] - 0s 937us/step - loss: 0.1854 - accuracy: 0.9303\n",
      "Epoch 276/1500\n",
      "45/45 [==============================] - 0s 851us/step - loss: 0.1836 - accuracy: 0.9293\n",
      "Epoch 277/1500\n",
      "45/45 [==============================] - 0s 869us/step - loss: 0.1793 - accuracy: 0.9279\n",
      "Epoch 278/1500\n",
      "45/45 [==============================] - 0s 920us/step - loss: 0.1912 - accuracy: 0.9247\n",
      "Epoch 279/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9272\n",
      "Epoch 280/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9310\n",
      "Epoch 281/1500\n",
      "45/45 [==============================] - 0s 930us/step - loss: 0.2008 - accuracy: 0.9195\n",
      "Epoch 282/1500\n",
      "45/45 [==============================] - 0s 945us/step - loss: 0.1928 - accuracy: 0.9268\n",
      "Epoch 283/1500\n",
      "45/45 [==============================] - 0s 892us/step - loss: 0.1796 - accuracy: 0.9314\n",
      "Epoch 284/1500\n",
      "45/45 [==============================] - 0s 910us/step - loss: 0.1837 - accuracy: 0.9314\n",
      "Epoch 285/1500\n",
      "45/45 [==============================] - 0s 930us/step - loss: 0.1968 - accuracy: 0.9240\n",
      "Epoch 286/1500\n",
      "45/45 [==============================] - 0s 977us/step - loss: 0.1844 - accuracy: 0.9216\n",
      "Epoch 287/1500\n",
      "45/45 [==============================] - 0s 909us/step - loss: 0.1874 - accuracy: 0.9296\n",
      "Epoch 288/1500\n",
      "45/45 [==============================] - 0s 916us/step - loss: 0.1878 - accuracy: 0.9303\n",
      "Epoch 289/1500\n",
      "45/45 [==============================] - 0s 885us/step - loss: 0.1907 - accuracy: 0.9237\n",
      "Epoch 290/1500\n",
      "45/45 [==============================] - 0s 892us/step - loss: 0.1816 - accuracy: 0.9303\n",
      "Epoch 291/1500\n",
      "45/45 [==============================] - 0s 810us/step - loss: 0.1858 - accuracy: 0.9275\n",
      "Epoch 292/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9268\n",
      "Epoch 293/1500\n",
      "45/45 [==============================] - 0s 869us/step - loss: 0.1900 - accuracy: 0.9282\n",
      "Epoch 294/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9342\n",
      "Epoch 295/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9310\n",
      "Epoch 296/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9289\n",
      "Epoch 297/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9303\n",
      "Epoch 298/1500\n",
      "45/45 [==============================] - 0s 958us/step - loss: 0.1707 - accuracy: 0.9352\n",
      "Epoch 299/1500\n",
      "45/45 [==============================] - 0s 945us/step - loss: 0.1890 - accuracy: 0.9289\n",
      "Epoch 300/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9331\n",
      "Epoch 301/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9289\n",
      "Epoch 302/1500\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9321\n",
      "Epoch 303/1500\n",
      " 1/45 [..............................] - ETA: 0s - loss: 0.1287 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 273.\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9310\n",
      "Epoch 303: early stopping\n",
      "7/7 [==============================] - 0s 932us/step - loss: 1.1051 - accuracy: 0.6861\n",
      "7/7 [==============================] - 0s 771us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.70 (21/30)\n",
      "Before appending - Cat IDs: 389, Predictions: 389, Actuals: 389, Gender: 389\n",
      "After appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "Final Test Results - Loss: 1.1051465272903442, Accuracy: 0.6860986351966858, Precision: 0.6578496555855047, Recall: 0.7424716402818593, F1 Score: 0.6781037285145467\n",
      "Confusion Matrix:\n",
      " [[113   5  19]\n",
      " [  0   9   0]\n",
      " [ 46   0  31]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1053, 1: 1014, 2: 871})\n",
      "Epoch 1/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 1.3061 - accuracy: 0.4479\n",
      "Epoch 2/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.9705 - accuracy: 0.5769\n",
      "Epoch 3/1500\n",
      "46/46 [==============================] - 0s 979us/step - loss: 0.8349 - accuracy: 0.6375\n",
      "Epoch 4/1500\n",
      "46/46 [==============================] - 0s 887us/step - loss: 0.8089 - accuracy: 0.6542\n",
      "Epoch 5/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7654 - accuracy: 0.6675\n",
      "Epoch 6/1500\n",
      "46/46 [==============================] - 0s 998us/step - loss: 0.7414 - accuracy: 0.6814\n",
      "Epoch 7/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.7474 - accuracy: 0.6807\n",
      "Epoch 8/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.6946 - accuracy: 0.7039\n",
      "Epoch 9/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.6958 - accuracy: 0.6995\n",
      "Epoch 10/1500\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.6650 - accuracy: 0.7192\n",
      "Epoch 11/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.6790 - accuracy: 0.7076\n",
      "Epoch 12/1500\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.6534 - accuracy: 0.7151\n",
      "Epoch 13/1500\n",
      "46/46 [==============================] - 0s 891us/step - loss: 0.6436 - accuracy: 0.7202\n",
      "Epoch 14/1500\n",
      "46/46 [==============================] - 0s 870us/step - loss: 0.6236 - accuracy: 0.7314\n",
      "Epoch 15/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.6263 - accuracy: 0.7403\n",
      "Epoch 16/1500\n",
      "46/46 [==============================] - 0s 853us/step - loss: 0.6285 - accuracy: 0.7263\n",
      "Epoch 17/1500\n",
      "46/46 [==============================] - 0s 880us/step - loss: 0.5958 - accuracy: 0.7471\n",
      "Epoch 18/1500\n",
      "46/46 [==============================] - 0s 933us/step - loss: 0.6145 - accuracy: 0.7437\n",
      "Epoch 19/1500\n",
      "46/46 [==============================] - 0s 918us/step - loss: 0.5975 - accuracy: 0.7478\n",
      "Epoch 20/1500\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.5795 - accuracy: 0.7447\n",
      "Epoch 21/1500\n",
      "46/46 [==============================] - 0s 821us/step - loss: 0.5828 - accuracy: 0.7502\n",
      "Epoch 22/1500\n",
      "46/46 [==============================] - 0s 842us/step - loss: 0.5781 - accuracy: 0.7543\n",
      "Epoch 23/1500\n",
      "46/46 [==============================] - 0s 928us/step - loss: 0.5728 - accuracy: 0.7536\n",
      "Epoch 24/1500\n",
      "46/46 [==============================] - 0s 997us/step - loss: 0.5666 - accuracy: 0.7600\n",
      "Epoch 25/1500\n",
      "46/46 [==============================] - 0s 972us/step - loss: 0.5665 - accuracy: 0.7655\n",
      "Epoch 26/1500\n",
      "46/46 [==============================] - 0s 914us/step - loss: 0.5569 - accuracy: 0.7655\n",
      "Epoch 27/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.5484 - accuracy: 0.7573\n",
      "Epoch 28/1500\n",
      "46/46 [==============================] - 0s 842us/step - loss: 0.5505 - accuracy: 0.7750\n",
      "Epoch 29/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.5315 - accuracy: 0.7726\n",
      "Epoch 30/1500\n",
      "46/46 [==============================] - 0s 824us/step - loss: 0.5456 - accuracy: 0.7658\n",
      "Epoch 31/1500\n",
      "46/46 [==============================] - 0s 869us/step - loss: 0.5192 - accuracy: 0.7781\n",
      "Epoch 32/1500\n",
      "46/46 [==============================] - 0s 850us/step - loss: 0.5111 - accuracy: 0.7862\n",
      "Epoch 33/1500\n",
      "46/46 [==============================] - 0s 848us/step - loss: 0.5092 - accuracy: 0.7811\n",
      "Epoch 34/1500\n",
      "46/46 [==============================] - 0s 898us/step - loss: 0.5127 - accuracy: 0.7771\n",
      "Epoch 35/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.5138 - accuracy: 0.7760\n",
      "Epoch 36/1500\n",
      "46/46 [==============================] - 0s 854us/step - loss: 0.4998 - accuracy: 0.7880\n",
      "Epoch 37/1500\n",
      "46/46 [==============================] - 0s 855us/step - loss: 0.5114 - accuracy: 0.7856\n",
      "Epoch 38/1500\n",
      "46/46 [==============================] - 0s 873us/step - loss: 0.4944 - accuracy: 0.7914\n",
      "Epoch 39/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.4889 - accuracy: 0.7982\n",
      "Epoch 40/1500\n",
      "46/46 [==============================] - 0s 864us/step - loss: 0.4814 - accuracy: 0.7988\n",
      "Epoch 41/1500\n",
      "46/46 [==============================] - 0s 823us/step - loss: 0.4846 - accuracy: 0.7968\n",
      "Epoch 42/1500\n",
      "46/46 [==============================] - 0s 810us/step - loss: 0.4820 - accuracy: 0.8009\n",
      "Epoch 43/1500\n",
      "46/46 [==============================] - 0s 821us/step - loss: 0.4854 - accuracy: 0.7971\n",
      "Epoch 44/1500\n",
      "46/46 [==============================] - 0s 826us/step - loss: 0.4859 - accuracy: 0.7992\n",
      "Epoch 45/1500\n",
      "46/46 [==============================] - 0s 848us/step - loss: 0.4739 - accuracy: 0.7975\n",
      "Epoch 46/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.4823 - accuracy: 0.7958\n",
      "Epoch 47/1500\n",
      "46/46 [==============================] - 0s 857us/step - loss: 0.4707 - accuracy: 0.7995\n",
      "Epoch 48/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.4714 - accuracy: 0.7961\n",
      "Epoch 49/1500\n",
      "46/46 [==============================] - 0s 846us/step - loss: 0.4699 - accuracy: 0.7995\n",
      "Epoch 50/1500\n",
      "46/46 [==============================] - 0s 857us/step - loss: 0.4605 - accuracy: 0.8036\n",
      "Epoch 51/1500\n",
      "46/46 [==============================] - 0s 852us/step - loss: 0.4567 - accuracy: 0.8046\n",
      "Epoch 52/1500\n",
      "46/46 [==============================] - 0s 850us/step - loss: 0.4607 - accuracy: 0.8012\n",
      "Epoch 53/1500\n",
      "46/46 [==============================] - 0s 805us/step - loss: 0.4621 - accuracy: 0.8046\n",
      "Epoch 54/1500\n",
      "46/46 [==============================] - 0s 858us/step - loss: 0.4545 - accuracy: 0.8114\n",
      "Epoch 55/1500\n",
      "46/46 [==============================] - 0s 839us/step - loss: 0.4567 - accuracy: 0.8005\n",
      "Epoch 56/1500\n",
      "46/46 [==============================] - 0s 833us/step - loss: 0.4429 - accuracy: 0.8108\n",
      "Epoch 57/1500\n",
      "46/46 [==============================] - 0s 833us/step - loss: 0.4377 - accuracy: 0.8138\n",
      "Epoch 58/1500\n",
      "46/46 [==============================] - 0s 811us/step - loss: 0.4452 - accuracy: 0.8142\n",
      "Epoch 59/1500\n",
      "46/46 [==============================] - 0s 847us/step - loss: 0.4354 - accuracy: 0.8189\n",
      "Epoch 60/1500\n",
      "46/46 [==============================] - 0s 882us/step - loss: 0.4342 - accuracy: 0.8155\n",
      "Epoch 61/1500\n",
      "46/46 [==============================] - 0s 854us/step - loss: 0.4362 - accuracy: 0.8233\n",
      "Epoch 62/1500\n",
      "46/46 [==============================] - 0s 806us/step - loss: 0.4384 - accuracy: 0.8152\n",
      "Epoch 63/1500\n",
      "46/46 [==============================] - 0s 836us/step - loss: 0.4242 - accuracy: 0.8254\n",
      "Epoch 64/1500\n",
      "46/46 [==============================] - 0s 846us/step - loss: 0.4358 - accuracy: 0.8193\n",
      "Epoch 65/1500\n",
      "46/46 [==============================] - 0s 842us/step - loss: 0.4294 - accuracy: 0.8237\n",
      "Epoch 66/1500\n",
      "46/46 [==============================] - 0s 841us/step - loss: 0.4281 - accuracy: 0.8206\n",
      "Epoch 67/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.4339 - accuracy: 0.8230\n",
      "Epoch 68/1500\n",
      "46/46 [==============================] - 0s 831us/step - loss: 0.4086 - accuracy: 0.8261\n",
      "Epoch 69/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.4159 - accuracy: 0.8210\n",
      "Epoch 70/1500\n",
      "46/46 [==============================] - 0s 874us/step - loss: 0.4070 - accuracy: 0.8325\n",
      "Epoch 71/1500\n",
      "46/46 [==============================] - 0s 826us/step - loss: 0.4200 - accuracy: 0.8240\n",
      "Epoch 72/1500\n",
      "46/46 [==============================] - 0s 829us/step - loss: 0.4098 - accuracy: 0.8305\n",
      "Epoch 73/1500\n",
      "46/46 [==============================] - 0s 828us/step - loss: 0.4147 - accuracy: 0.8237\n",
      "Epoch 74/1500\n",
      "46/46 [==============================] - 0s 823us/step - loss: 0.4266 - accuracy: 0.8186\n",
      "Epoch 75/1500\n",
      "46/46 [==============================] - 0s 842us/step - loss: 0.4022 - accuracy: 0.8339\n",
      "Epoch 76/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.4143 - accuracy: 0.8268\n",
      "Epoch 77/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.4024 - accuracy: 0.8393\n",
      "Epoch 78/1500\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.4067 - accuracy: 0.8295\n",
      "Epoch 79/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.4034 - accuracy: 0.8359\n",
      "Epoch 80/1500\n",
      "46/46 [==============================] - 0s 832us/step - loss: 0.3991 - accuracy: 0.8359\n",
      "Epoch 81/1500\n",
      "46/46 [==============================] - 0s 826us/step - loss: 0.3945 - accuracy: 0.8400\n",
      "Epoch 82/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.3925 - accuracy: 0.8472\n",
      "Epoch 83/1500\n",
      "46/46 [==============================] - 0s 845us/step - loss: 0.4039 - accuracy: 0.8332\n",
      "Epoch 84/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.3810 - accuracy: 0.8434\n",
      "Epoch 85/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.3960 - accuracy: 0.8356\n",
      "Epoch 86/1500\n",
      "46/46 [==============================] - 0s 822us/step - loss: 0.4011 - accuracy: 0.8339\n",
      "Epoch 87/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.4076 - accuracy: 0.8353\n",
      "Epoch 88/1500\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.3928 - accuracy: 0.8448\n",
      "Epoch 89/1500\n",
      "46/46 [==============================] - 0s 820us/step - loss: 0.3951 - accuracy: 0.8414\n",
      "Epoch 90/1500\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.3937 - accuracy: 0.8380\n",
      "Epoch 91/1500\n",
      "46/46 [==============================] - 0s 807us/step - loss: 0.3815 - accuracy: 0.8482\n",
      "Epoch 92/1500\n",
      "46/46 [==============================] - 0s 822us/step - loss: 0.3822 - accuracy: 0.8468\n",
      "Epoch 93/1500\n",
      "46/46 [==============================] - 0s 804us/step - loss: 0.3821 - accuracy: 0.8458\n",
      "Epoch 94/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.3832 - accuracy: 0.8414\n",
      "Epoch 95/1500\n",
      "46/46 [==============================] - 0s 837us/step - loss: 0.3707 - accuracy: 0.8567\n",
      "Epoch 96/1500\n",
      "46/46 [==============================] - 0s 846us/step - loss: 0.3799 - accuracy: 0.8410\n",
      "Epoch 97/1500\n",
      "46/46 [==============================] - 0s 857us/step - loss: 0.3789 - accuracy: 0.8445\n",
      "Epoch 98/1500\n",
      "46/46 [==============================] - 0s 795us/step - loss: 0.3739 - accuracy: 0.8445\n",
      "Epoch 99/1500\n",
      "46/46 [==============================] - 0s 804us/step - loss: 0.3717 - accuracy: 0.8424\n",
      "Epoch 100/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.3686 - accuracy: 0.8496\n",
      "Epoch 101/1500\n",
      "46/46 [==============================] - 0s 774us/step - loss: 0.3707 - accuracy: 0.8472\n",
      "Epoch 102/1500\n",
      "46/46 [==============================] - 0s 785us/step - loss: 0.3797 - accuracy: 0.8455\n",
      "Epoch 103/1500\n",
      "46/46 [==============================] - 0s 768us/step - loss: 0.3662 - accuracy: 0.8519\n",
      "Epoch 104/1500\n",
      "46/46 [==============================] - 0s 759us/step - loss: 0.3734 - accuracy: 0.8536\n",
      "Epoch 105/1500\n",
      "46/46 [==============================] - 0s 784us/step - loss: 0.3727 - accuracy: 0.8506\n",
      "Epoch 106/1500\n",
      "46/46 [==============================] - 0s 777us/step - loss: 0.3604 - accuracy: 0.8462\n",
      "Epoch 107/1500\n",
      "46/46 [==============================] - 0s 770us/step - loss: 0.3612 - accuracy: 0.8509\n",
      "Epoch 108/1500\n",
      "46/46 [==============================] - 0s 895us/step - loss: 0.3599 - accuracy: 0.8492\n",
      "Epoch 109/1500\n",
      "46/46 [==============================] - 0s 873us/step - loss: 0.3617 - accuracy: 0.8519\n",
      "Epoch 110/1500\n",
      "46/46 [==============================] - 0s 836us/step - loss: 0.3606 - accuracy: 0.8509\n",
      "Epoch 111/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.3583 - accuracy: 0.8530\n",
      "Epoch 112/1500\n",
      "46/46 [==============================] - 0s 816us/step - loss: 0.3556 - accuracy: 0.8604\n",
      "Epoch 113/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.3567 - accuracy: 0.8489\n",
      "Epoch 114/1500\n",
      "46/46 [==============================] - 0s 886us/step - loss: 0.3538 - accuracy: 0.8598\n",
      "Epoch 115/1500\n",
      "46/46 [==============================] - 0s 902us/step - loss: 0.3505 - accuracy: 0.8553\n",
      "Epoch 116/1500\n",
      "46/46 [==============================] - 0s 865us/step - loss: 0.3665 - accuracy: 0.8502\n",
      "Epoch 117/1500\n",
      "46/46 [==============================] - 0s 853us/step - loss: 0.3572 - accuracy: 0.8577\n",
      "Epoch 118/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.3415 - accuracy: 0.8557\n",
      "Epoch 119/1500\n",
      "46/46 [==============================] - 0s 856us/step - loss: 0.3438 - accuracy: 0.8604\n",
      "Epoch 120/1500\n",
      "46/46 [==============================] - 0s 878us/step - loss: 0.3489 - accuracy: 0.8564\n",
      "Epoch 121/1500\n",
      "46/46 [==============================] - 0s 880us/step - loss: 0.3457 - accuracy: 0.8611\n",
      "Epoch 122/1500\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.3599 - accuracy: 0.8591\n",
      "Epoch 123/1500\n",
      "46/46 [==============================] - 0s 978us/step - loss: 0.3487 - accuracy: 0.8649\n",
      "Epoch 124/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.3412 - accuracy: 0.8690\n",
      "Epoch 125/1500\n",
      "46/46 [==============================] - 0s 857us/step - loss: 0.3453 - accuracy: 0.8560\n",
      "Epoch 126/1500\n",
      "46/46 [==============================] - 0s 832us/step - loss: 0.3441 - accuracy: 0.8645\n",
      "Epoch 127/1500\n",
      "46/46 [==============================] - 0s 882us/step - loss: 0.3305 - accuracy: 0.8642\n",
      "Epoch 128/1500\n",
      "46/46 [==============================] - 0s 836us/step - loss: 0.3435 - accuracy: 0.8662\n",
      "Epoch 129/1500\n",
      "46/46 [==============================] - 0s 854us/step - loss: 0.3328 - accuracy: 0.8676\n",
      "Epoch 130/1500\n",
      "46/46 [==============================] - 0s 866us/step - loss: 0.3305 - accuracy: 0.8693\n",
      "Epoch 131/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.3371 - accuracy: 0.8659\n",
      "Epoch 132/1500\n",
      "46/46 [==============================] - 0s 837us/step - loss: 0.3363 - accuracy: 0.8683\n",
      "Epoch 133/1500\n",
      "46/46 [==============================] - 0s 879us/step - loss: 0.3333 - accuracy: 0.8707\n",
      "Epoch 134/1500\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.3396 - accuracy: 0.8625\n",
      "Epoch 135/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.3304 - accuracy: 0.8656\n",
      "Epoch 136/1500\n",
      "46/46 [==============================] - 0s 827us/step - loss: 0.3332 - accuracy: 0.8693\n",
      "Epoch 137/1500\n",
      "46/46 [==============================] - 0s 856us/step - loss: 0.3250 - accuracy: 0.8676\n",
      "Epoch 138/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.3353 - accuracy: 0.8615\n",
      "Epoch 139/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.3237 - accuracy: 0.8734\n",
      "Epoch 140/1500\n",
      "46/46 [==============================] - 0s 878us/step - loss: 0.3235 - accuracy: 0.8628\n",
      "Epoch 141/1500\n",
      "46/46 [==============================] - 0s 825us/step - loss: 0.3431 - accuracy: 0.8608\n",
      "Epoch 142/1500\n",
      "46/46 [==============================] - 0s 879us/step - loss: 0.3187 - accuracy: 0.8744\n",
      "Epoch 143/1500\n",
      "46/46 [==============================] - 0s 858us/step - loss: 0.3286 - accuracy: 0.8659\n",
      "Epoch 144/1500\n",
      "46/46 [==============================] - 0s 825us/step - loss: 0.3244 - accuracy: 0.8744\n",
      "Epoch 145/1500\n",
      "46/46 [==============================] - 0s 855us/step - loss: 0.3405 - accuracy: 0.8577\n",
      "Epoch 146/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.3411 - accuracy: 0.8622\n",
      "Epoch 147/1500\n",
      "46/46 [==============================] - 0s 857us/step - loss: 0.3250 - accuracy: 0.8724\n",
      "Epoch 148/1500\n",
      "46/46 [==============================] - 0s 863us/step - loss: 0.3156 - accuracy: 0.8764\n",
      "Epoch 149/1500\n",
      "46/46 [==============================] - 0s 842us/step - loss: 0.3185 - accuracy: 0.8707\n",
      "Epoch 150/1500\n",
      "46/46 [==============================] - 0s 829us/step - loss: 0.3401 - accuracy: 0.8656\n",
      "Epoch 151/1500\n",
      "46/46 [==============================] - 0s 824us/step - loss: 0.3146 - accuracy: 0.8816\n",
      "Epoch 152/1500\n",
      "46/46 [==============================] - 0s 810us/step - loss: 0.3091 - accuracy: 0.8805\n",
      "Epoch 153/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.3064 - accuracy: 0.8781\n",
      "Epoch 154/1500\n",
      "46/46 [==============================] - 0s 823us/step - loss: 0.3219 - accuracy: 0.8635\n",
      "Epoch 155/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.3169 - accuracy: 0.8693\n",
      "Epoch 156/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.3043 - accuracy: 0.8822\n",
      "Epoch 157/1500\n",
      "46/46 [==============================] - 0s 823us/step - loss: 0.3064 - accuracy: 0.8788\n",
      "Epoch 158/1500\n",
      "46/46 [==============================] - 0s 807us/step - loss: 0.3248 - accuracy: 0.8720\n",
      "Epoch 159/1500\n",
      "46/46 [==============================] - 0s 811us/step - loss: 0.3146 - accuracy: 0.8747\n",
      "Epoch 160/1500\n",
      "46/46 [==============================] - 0s 852us/step - loss: 0.3092 - accuracy: 0.8747\n",
      "Epoch 161/1500\n",
      "46/46 [==============================] - 0s 818us/step - loss: 0.3081 - accuracy: 0.8734\n",
      "Epoch 162/1500\n",
      "46/46 [==============================] - 0s 831us/step - loss: 0.3026 - accuracy: 0.8805\n",
      "Epoch 163/1500\n",
      "46/46 [==============================] - 0s 859us/step - loss: 0.3136 - accuracy: 0.8764\n",
      "Epoch 164/1500\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.3037 - accuracy: 0.8833\n",
      "Epoch 165/1500\n",
      "46/46 [==============================] - 0s 805us/step - loss: 0.3049 - accuracy: 0.8799\n",
      "Epoch 166/1500\n",
      "46/46 [==============================] - 0s 908us/step - loss: 0.2925 - accuracy: 0.8856\n",
      "Epoch 167/1500\n",
      "46/46 [==============================] - 0s 947us/step - loss: 0.3037 - accuracy: 0.8802\n",
      "Epoch 168/1500\n",
      "46/46 [==============================] - 0s 974us/step - loss: 0.3090 - accuracy: 0.8781\n",
      "Epoch 169/1500\n",
      "46/46 [==============================] - 0s 922us/step - loss: 0.3171 - accuracy: 0.8700\n",
      "Epoch 170/1500\n",
      "46/46 [==============================] - 0s 923us/step - loss: 0.3199 - accuracy: 0.8751\n",
      "Epoch 171/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.2947 - accuracy: 0.8829\n",
      "Epoch 172/1500\n",
      "46/46 [==============================] - 0s 937us/step - loss: 0.3010 - accuracy: 0.8795\n",
      "Epoch 173/1500\n",
      "46/46 [==============================] - 0s 951us/step - loss: 0.2894 - accuracy: 0.8860\n",
      "Epoch 174/1500\n",
      "46/46 [==============================] - 0s 921us/step - loss: 0.3055 - accuracy: 0.8802\n",
      "Epoch 175/1500\n",
      "46/46 [==============================] - 0s 889us/step - loss: 0.2997 - accuracy: 0.8792\n",
      "Epoch 176/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.3028 - accuracy: 0.8781\n",
      "Epoch 177/1500\n",
      "46/46 [==============================] - 0s 816us/step - loss: 0.3038 - accuracy: 0.8867\n",
      "Epoch 178/1500\n",
      "46/46 [==============================] - 0s 831us/step - loss: 0.3007 - accuracy: 0.8764\n",
      "Epoch 179/1500\n",
      "46/46 [==============================] - 0s 777us/step - loss: 0.2936 - accuracy: 0.8812\n",
      "Epoch 180/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.2975 - accuracy: 0.8778\n",
      "Epoch 181/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2900 - accuracy: 0.8860\n",
      "Epoch 182/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.3046 - accuracy: 0.8775\n",
      "Epoch 183/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8887\n",
      "Epoch 184/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.8761\n",
      "Epoch 185/1500\n",
      "46/46 [==============================] - 0s 984us/step - loss: 0.3076 - accuracy: 0.8768\n",
      "Epoch 186/1500\n",
      "46/46 [==============================] - 0s 954us/step - loss: 0.3013 - accuracy: 0.8754\n",
      "Epoch 187/1500\n",
      "46/46 [==============================] - 0s 912us/step - loss: 0.2901 - accuracy: 0.8867\n",
      "Epoch 188/1500\n",
      "46/46 [==============================] - 0s 884us/step - loss: 0.2859 - accuracy: 0.8863\n",
      "Epoch 189/1500\n",
      "46/46 [==============================] - 0s 873us/step - loss: 0.2894 - accuracy: 0.8853\n",
      "Epoch 190/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.2964 - accuracy: 0.8846\n",
      "Epoch 191/1500\n",
      "46/46 [==============================] - 0s 973us/step - loss: 0.2718 - accuracy: 0.8907\n",
      "Epoch 192/1500\n",
      "46/46 [==============================] - 0s 991us/step - loss: 0.2962 - accuracy: 0.8846\n",
      "Epoch 193/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8931\n",
      "Epoch 194/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.8884\n",
      "Epoch 195/1500\n",
      "46/46 [==============================] - 0s 953us/step - loss: 0.2885 - accuracy: 0.8836\n",
      "Epoch 196/1500\n",
      "46/46 [==============================] - 0s 968us/step - loss: 0.2825 - accuracy: 0.8829\n",
      "Epoch 197/1500\n",
      "46/46 [==============================] - 0s 987us/step - loss: 0.2912 - accuracy: 0.8863\n",
      "Epoch 198/1500\n",
      "46/46 [==============================] - 0s 915us/step - loss: 0.2748 - accuracy: 0.8901\n",
      "Epoch 199/1500\n",
      "46/46 [==============================] - 0s 842us/step - loss: 0.2817 - accuracy: 0.8829\n",
      "Epoch 200/1500\n",
      "46/46 [==============================] - 0s 833us/step - loss: 0.2722 - accuracy: 0.8938\n",
      "Epoch 201/1500\n",
      "46/46 [==============================] - 0s 869us/step - loss: 0.2748 - accuracy: 0.8863\n",
      "Epoch 202/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.2899 - accuracy: 0.8839\n",
      "Epoch 203/1500\n",
      "46/46 [==============================] - 0s 824us/step - loss: 0.2932 - accuracy: 0.8856\n",
      "Epoch 204/1500\n",
      "46/46 [==============================] - 0s 823us/step - loss: 0.2930 - accuracy: 0.8833\n",
      "Epoch 205/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.2714 - accuracy: 0.8901\n",
      "Epoch 206/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.2783 - accuracy: 0.8833\n",
      "Epoch 207/1500\n",
      "46/46 [==============================] - 0s 829us/step - loss: 0.2761 - accuracy: 0.8907\n",
      "Epoch 208/1500\n",
      "46/46 [==============================] - 0s 876us/step - loss: 0.2661 - accuracy: 0.8965\n",
      "Epoch 209/1500\n",
      "46/46 [==============================] - 0s 956us/step - loss: 0.2745 - accuracy: 0.8914\n",
      "Epoch 210/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.8822\n",
      "Epoch 211/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8901\n",
      "Epoch 212/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.8836\n",
      "Epoch 213/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2678 - accuracy: 0.8918\n",
      "Epoch 214/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2694 - accuracy: 0.8945\n",
      "Epoch 215/1500\n",
      "46/46 [==============================] - 0s 829us/step - loss: 0.2836 - accuracy: 0.8863\n",
      "Epoch 216/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.2705 - accuracy: 0.8911\n",
      "Epoch 217/1500\n",
      "46/46 [==============================] - 0s 816us/step - loss: 0.2723 - accuracy: 0.8924\n",
      "Epoch 218/1500\n",
      "46/46 [==============================] - 0s 829us/step - loss: 0.2772 - accuracy: 0.8955\n",
      "Epoch 219/1500\n",
      "46/46 [==============================] - 0s 880us/step - loss: 0.2679 - accuracy: 0.8890\n",
      "Epoch 220/1500\n",
      "46/46 [==============================] - 0s 896us/step - loss: 0.2695 - accuracy: 0.8907\n",
      "Epoch 221/1500\n",
      "46/46 [==============================] - 0s 856us/step - loss: 0.2756 - accuracy: 0.8928\n",
      "Epoch 222/1500\n",
      "46/46 [==============================] - 0s 883us/step - loss: 0.2604 - accuracy: 0.8948\n",
      "Epoch 223/1500\n",
      "46/46 [==============================] - 0s 818us/step - loss: 0.2707 - accuracy: 0.8986\n",
      "Epoch 224/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.2739 - accuracy: 0.8931\n",
      "Epoch 225/1500\n",
      "46/46 [==============================] - 0s 876us/step - loss: 0.2670 - accuracy: 0.8952\n",
      "Epoch 226/1500\n",
      "46/46 [==============================] - 0s 874us/step - loss: 0.2670 - accuracy: 0.8918\n",
      "Epoch 227/1500\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.2647 - accuracy: 0.8986\n",
      "Epoch 228/1500\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.2699 - accuracy: 0.8945\n",
      "Epoch 229/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2648 - accuracy: 0.8935\n",
      "Epoch 230/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.8962\n",
      "Epoch 231/1500\n",
      "46/46 [==============================] - 0s 997us/step - loss: 0.2692 - accuracy: 0.8948\n",
      "Epoch 232/1500\n",
      "46/46 [==============================] - 0s 861us/step - loss: 0.2700 - accuracy: 0.8904\n",
      "Epoch 233/1500\n",
      "46/46 [==============================] - 0s 933us/step - loss: 0.2634 - accuracy: 0.8955\n",
      "Epoch 234/1500\n",
      "46/46 [==============================] - 0s 996us/step - loss: 0.2615 - accuracy: 0.8928\n",
      "Epoch 235/1500\n",
      "46/46 [==============================] - 0s 892us/step - loss: 0.2602 - accuracy: 0.8952\n",
      "Epoch 236/1500\n",
      "46/46 [==============================] - 0s 858us/step - loss: 0.2761 - accuracy: 0.8833\n",
      "Epoch 237/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8969\n",
      "Epoch 238/1500\n",
      "46/46 [==============================] - 0s 933us/step - loss: 0.2722 - accuracy: 0.8931\n",
      "Epoch 239/1500\n",
      "46/46 [==============================] - 0s 980us/step - loss: 0.2541 - accuracy: 0.8993\n",
      "Epoch 240/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2593 - accuracy: 0.8931\n",
      "Epoch 241/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2602 - accuracy: 0.8969\n",
      "Epoch 242/1500\n",
      "46/46 [==============================] - 0s 899us/step - loss: 0.2689 - accuracy: 0.8935\n",
      "Epoch 243/1500\n",
      "46/46 [==============================] - 0s 946us/step - loss: 0.2599 - accuracy: 0.8894\n",
      "Epoch 244/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.9023\n",
      "Epoch 245/1500\n",
      "46/46 [==============================] - 0s 846us/step - loss: 0.2692 - accuracy: 0.8955\n",
      "Epoch 246/1500\n",
      "46/46 [==============================] - 0s 841us/step - loss: 0.2615 - accuracy: 0.8972\n",
      "Epoch 247/1500\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.2548 - accuracy: 0.9020\n",
      "Epoch 248/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2588 - accuracy: 0.8982\n",
      "Epoch 249/1500\n",
      "46/46 [==============================] - 0s 993us/step - loss: 0.2590 - accuracy: 0.8945\n",
      "Epoch 250/1500\n",
      "46/46 [==============================] - 0s 992us/step - loss: 0.2478 - accuracy: 0.9010\n",
      "Epoch 251/1500\n",
      "46/46 [==============================] - 0s 985us/step - loss: 0.2579 - accuracy: 0.8948\n",
      "Epoch 252/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.8999\n",
      "Epoch 253/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.8996\n",
      "Epoch 254/1500\n",
      "46/46 [==============================] - 0s 890us/step - loss: 0.2459 - accuracy: 0.9050\n",
      "Epoch 255/1500\n",
      "46/46 [==============================] - 0s 848us/step - loss: 0.2497 - accuracy: 0.8969\n",
      "Epoch 256/1500\n",
      "46/46 [==============================] - 0s 856us/step - loss: 0.2505 - accuracy: 0.9050\n",
      "Epoch 257/1500\n",
      "46/46 [==============================] - 0s 821us/step - loss: 0.2489 - accuracy: 0.9003\n",
      "Epoch 258/1500\n",
      "46/46 [==============================] - 0s 863us/step - loss: 0.2585 - accuracy: 0.8969\n",
      "Epoch 259/1500\n",
      "46/46 [==============================] - 0s 845us/step - loss: 0.2499 - accuracy: 0.9010\n",
      "Epoch 260/1500\n",
      "46/46 [==============================] - 0s 836us/step - loss: 0.2504 - accuracy: 0.8945\n",
      "Epoch 261/1500\n",
      "46/46 [==============================] - 0s 853us/step - loss: 0.2480 - accuracy: 0.8993\n",
      "Epoch 262/1500\n",
      "46/46 [==============================] - 0s 855us/step - loss: 0.2532 - accuracy: 0.8962\n",
      "Epoch 263/1500\n",
      "46/46 [==============================] - 0s 875us/step - loss: 0.2425 - accuracy: 0.9027\n",
      "Epoch 264/1500\n",
      "46/46 [==============================] - 0s 957us/step - loss: 0.2414 - accuracy: 0.9054\n",
      "Epoch 265/1500\n",
      "46/46 [==============================] - 0s 939us/step - loss: 0.2552 - accuracy: 0.9016\n",
      "Epoch 266/1500\n",
      "46/46 [==============================] - 0s 917us/step - loss: 0.2719 - accuracy: 0.8938\n",
      "Epoch 267/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.2423 - accuracy: 0.9040\n",
      "Epoch 268/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.2432 - accuracy: 0.9027\n",
      "Epoch 269/1500\n",
      "46/46 [==============================] - 0s 857us/step - loss: 0.2491 - accuracy: 0.8999\n",
      "Epoch 270/1500\n",
      "46/46 [==============================] - 0s 822us/step - loss: 0.2463 - accuracy: 0.9020\n",
      "Epoch 271/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.2325 - accuracy: 0.9115\n",
      "Epoch 272/1500\n",
      "46/46 [==============================] - 0s 856us/step - loss: 0.2499 - accuracy: 0.9023\n",
      "Epoch 273/1500\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.2526 - accuracy: 0.9006\n",
      "Epoch 274/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.2600 - accuracy: 0.8907\n",
      "Epoch 275/1500\n",
      "46/46 [==============================] - 0s 939us/step - loss: 0.2450 - accuracy: 0.9023\n",
      "Epoch 276/1500\n",
      "46/46 [==============================] - 0s 900us/step - loss: 0.2313 - accuracy: 0.9064\n",
      "Epoch 277/1500\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.2475 - accuracy: 0.9054\n",
      "Epoch 278/1500\n",
      "46/46 [==============================] - 0s 891us/step - loss: 0.2358 - accuracy: 0.9057\n",
      "Epoch 279/1500\n",
      "46/46 [==============================] - 0s 861us/step - loss: 0.2460 - accuracy: 0.9064\n",
      "Epoch 280/1500\n",
      "46/46 [==============================] - 0s 862us/step - loss: 0.2468 - accuracy: 0.9095\n",
      "Epoch 281/1500\n",
      "46/46 [==============================] - 0s 878us/step - loss: 0.2403 - accuracy: 0.9101\n",
      "Epoch 282/1500\n",
      "46/46 [==============================] - 0s 850us/step - loss: 0.2486 - accuracy: 0.9020\n",
      "Epoch 283/1500\n",
      "46/46 [==============================] - 0s 855us/step - loss: 0.2435 - accuracy: 0.9037\n",
      "Epoch 284/1500\n",
      "46/46 [==============================] - 0s 868us/step - loss: 0.2507 - accuracy: 0.8955\n",
      "Epoch 285/1500\n",
      "46/46 [==============================] - 0s 915us/step - loss: 0.2484 - accuracy: 0.9044\n",
      "Epoch 286/1500\n",
      "46/46 [==============================] - 0s 878us/step - loss: 0.2522 - accuracy: 0.8975\n",
      "Epoch 287/1500\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.2359 - accuracy: 0.9061\n",
      "Epoch 288/1500\n",
      "46/46 [==============================] - 0s 869us/step - loss: 0.2235 - accuracy: 0.9193\n",
      "Epoch 289/1500\n",
      "46/46 [==============================] - 0s 811us/step - loss: 0.2575 - accuracy: 0.9040\n",
      "Epoch 290/1500\n",
      "46/46 [==============================] - 0s 851us/step - loss: 0.2266 - accuracy: 0.9112\n",
      "Epoch 291/1500\n",
      "46/46 [==============================] - 0s 869us/step - loss: 0.2395 - accuracy: 0.9095\n",
      "Epoch 292/1500\n",
      "46/46 [==============================] - 0s 885us/step - loss: 0.2453 - accuracy: 0.9057\n",
      "Epoch 293/1500\n",
      "46/46 [==============================] - 0s 918us/step - loss: 0.2188 - accuracy: 0.9159\n",
      "Epoch 294/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.2251 - accuracy: 0.9187\n",
      "Epoch 295/1500\n",
      "46/46 [==============================] - 0s 824us/step - loss: 0.2319 - accuracy: 0.9146\n",
      "Epoch 296/1500\n",
      "46/46 [==============================] - 0s 846us/step - loss: 0.2298 - accuracy: 0.9115\n",
      "Epoch 297/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.2382 - accuracy: 0.9078\n",
      "Epoch 298/1500\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.2386 - accuracy: 0.9078\n",
      "Epoch 299/1500\n",
      "46/46 [==============================] - 0s 880us/step - loss: 0.2302 - accuracy: 0.9078\n",
      "Epoch 300/1500\n",
      "46/46 [==============================] - 0s 824us/step - loss: 0.2347 - accuracy: 0.9078\n",
      "Epoch 301/1500\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.2407 - accuracy: 0.9084\n",
      "Epoch 302/1500\n",
      "46/46 [==============================] - 0s 853us/step - loss: 0.2515 - accuracy: 0.9050\n",
      "Epoch 303/1500\n",
      "46/46 [==============================] - 0s 859us/step - loss: 0.2345 - accuracy: 0.9101\n",
      "Epoch 304/1500\n",
      "46/46 [==============================] - 0s 940us/step - loss: 0.2248 - accuracy: 0.9132\n",
      "Epoch 305/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.2296 - accuracy: 0.9129\n",
      "Epoch 306/1500\n",
      "46/46 [==============================] - 0s 864us/step - loss: 0.2292 - accuracy: 0.9098\n",
      "Epoch 307/1500\n",
      "46/46 [==============================] - 0s 852us/step - loss: 0.2380 - accuracy: 0.9047\n",
      "Epoch 308/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.2281 - accuracy: 0.9071\n",
      "Epoch 309/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.2294 - accuracy: 0.9037\n",
      "Epoch 310/1500\n",
      "46/46 [==============================] - 0s 844us/step - loss: 0.2398 - accuracy: 0.9044\n",
      "Epoch 311/1500\n",
      "46/46 [==============================] - 0s 895us/step - loss: 0.2111 - accuracy: 0.9190\n",
      "Epoch 312/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9088\n",
      "Epoch 313/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9129\n",
      "Epoch 314/1500\n",
      "46/46 [==============================] - 0s 993us/step - loss: 0.2283 - accuracy: 0.9139\n",
      "Epoch 315/1500\n",
      "46/46 [==============================] - 0s 861us/step - loss: 0.2191 - accuracy: 0.9125\n",
      "Epoch 316/1500\n",
      "46/46 [==============================] - 0s 851us/step - loss: 0.2328 - accuracy: 0.9108\n",
      "Epoch 317/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.2389 - accuracy: 0.8986\n",
      "Epoch 318/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.2195 - accuracy: 0.9156\n",
      "Epoch 319/1500\n",
      "46/46 [==============================] - 0s 850us/step - loss: 0.2293 - accuracy: 0.9088\n",
      "Epoch 320/1500\n",
      "46/46 [==============================] - 0s 832us/step - loss: 0.2133 - accuracy: 0.9084\n",
      "Epoch 321/1500\n",
      "46/46 [==============================] - 0s 866us/step - loss: 0.2294 - accuracy: 0.9112\n",
      "Epoch 322/1500\n",
      "46/46 [==============================] - 0s 848us/step - loss: 0.2243 - accuracy: 0.9122\n",
      "Epoch 323/1500\n",
      "46/46 [==============================] - 0s 870us/step - loss: 0.2274 - accuracy: 0.9166\n",
      "Epoch 324/1500\n",
      "46/46 [==============================] - 0s 936us/step - loss: 0.2226 - accuracy: 0.9146\n",
      "Epoch 325/1500\n",
      "46/46 [==============================] - 0s 866us/step - loss: 0.2180 - accuracy: 0.9122\n",
      "Epoch 326/1500\n",
      "46/46 [==============================] - 0s 847us/step - loss: 0.2095 - accuracy: 0.9183\n",
      "Epoch 327/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.2301 - accuracy: 0.9118\n",
      "Epoch 328/1500\n",
      "46/46 [==============================] - 0s 822us/step - loss: 0.2239 - accuracy: 0.9156\n",
      "Epoch 329/1500\n",
      "46/46 [==============================] - 0s 873us/step - loss: 0.2074 - accuracy: 0.9255\n",
      "Epoch 330/1500\n",
      "46/46 [==============================] - 0s 870us/step - loss: 0.2169 - accuracy: 0.9156\n",
      "Epoch 331/1500\n",
      "46/46 [==============================] - 0s 855us/step - loss: 0.2327 - accuracy: 0.9071\n",
      "Epoch 332/1500\n",
      "46/46 [==============================] - 0s 919us/step - loss: 0.2258 - accuracy: 0.9149\n",
      "Epoch 333/1500\n",
      "46/46 [==============================] - 0s 880us/step - loss: 0.2219 - accuracy: 0.9193\n",
      "Epoch 334/1500\n",
      "46/46 [==============================] - 0s 863us/step - loss: 0.2359 - accuracy: 0.9064\n",
      "Epoch 335/1500\n",
      "46/46 [==============================] - 0s 892us/step - loss: 0.2312 - accuracy: 0.9081\n",
      "Epoch 336/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.2183 - accuracy: 0.9057\n",
      "Epoch 337/1500\n",
      "46/46 [==============================] - 0s 879us/step - loss: 0.2170 - accuracy: 0.9122\n",
      "Epoch 338/1500\n",
      "46/46 [==============================] - 0s 823us/step - loss: 0.2329 - accuracy: 0.9098\n",
      "Epoch 339/1500\n",
      "46/46 [==============================] - 0s 846us/step - loss: 0.2099 - accuracy: 0.9187\n",
      "Epoch 340/1500\n",
      "46/46 [==============================] - 0s 811us/step - loss: 0.2120 - accuracy: 0.9166\n",
      "Epoch 341/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.2104 - accuracy: 0.9180\n",
      "Epoch 342/1500\n",
      "46/46 [==============================] - 0s 819us/step - loss: 0.2252 - accuracy: 0.9112\n",
      "Epoch 343/1500\n",
      "46/46 [==============================] - 0s 809us/step - loss: 0.2279 - accuracy: 0.9088\n",
      "Epoch 344/1500\n",
      "46/46 [==============================] - 0s 817us/step - loss: 0.2236 - accuracy: 0.9142\n",
      "Epoch 345/1500\n",
      "46/46 [==============================] - 0s 828us/step - loss: 0.2238 - accuracy: 0.9166\n",
      "Epoch 346/1500\n",
      "46/46 [==============================] - 0s 824us/step - loss: 0.2163 - accuracy: 0.9152\n",
      "Epoch 347/1500\n",
      "46/46 [==============================] - 0s 848us/step - loss: 0.2103 - accuracy: 0.9135\n",
      "Epoch 348/1500\n",
      "46/46 [==============================] - 0s 817us/step - loss: 0.2091 - accuracy: 0.9221\n",
      "Epoch 349/1500\n",
      "46/46 [==============================] - 0s 819us/step - loss: 0.2251 - accuracy: 0.9105\n",
      "Epoch 350/1500\n",
      "46/46 [==============================] - 0s 825us/step - loss: 0.2186 - accuracy: 0.9101\n",
      "Epoch 351/1500\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.2223 - accuracy: 0.9098\n",
      "Epoch 352/1500\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9183\n",
      "Epoch 353/1500\n",
      "46/46 [==============================] - 0s 911us/step - loss: 0.2037 - accuracy: 0.9241\n",
      "Epoch 354/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.2028 - accuracy: 0.9217\n",
      "Epoch 355/1500\n",
      "46/46 [==============================] - 0s 831us/step - loss: 0.2180 - accuracy: 0.9142\n",
      "Epoch 356/1500\n",
      "46/46 [==============================] - 0s 830us/step - loss: 0.2166 - accuracy: 0.9129\n",
      "Epoch 357/1500\n",
      "46/46 [==============================] - 0s 852us/step - loss: 0.2079 - accuracy: 0.9163\n",
      "Epoch 358/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.2084 - accuracy: 0.9173\n",
      "Epoch 359/1500\n",
      "46/46 [==============================] - 0s 864us/step - loss: 0.2154 - accuracy: 0.9146\n",
      "Epoch 360/1500\n",
      "46/46 [==============================] - 0s 864us/step - loss: 0.1959 - accuracy: 0.9275\n",
      "Epoch 361/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.2189 - accuracy: 0.9166\n",
      "Epoch 362/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.2161 - accuracy: 0.9152\n",
      "Epoch 363/1500\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.9176\n",
      "Epoch 364/1500\n",
      "46/46 [==============================] - 0s 834us/step - loss: 0.1997 - accuracy: 0.9227\n",
      "Epoch 365/1500\n",
      "46/46 [==============================] - 0s 819us/step - loss: 0.2235 - accuracy: 0.9067\n",
      "Epoch 366/1500\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.2334 - accuracy: 0.9061\n",
      "Epoch 367/1500\n",
      "46/46 [==============================] - 0s 855us/step - loss: 0.2047 - accuracy: 0.9159\n",
      "Epoch 368/1500\n",
      "46/46 [==============================] - 0s 871us/step - loss: 0.2078 - accuracy: 0.9207\n",
      "Epoch 369/1500\n",
      "46/46 [==============================] - 0s 848us/step - loss: 0.2105 - accuracy: 0.9173\n",
      "Epoch 370/1500\n",
      "46/46 [==============================] - 0s 857us/step - loss: 0.1986 - accuracy: 0.9258\n",
      "Epoch 371/1500\n",
      "46/46 [==============================] - 0s 797us/step - loss: 0.2292 - accuracy: 0.9108\n",
      "Epoch 372/1500\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.1969 - accuracy: 0.9261\n",
      "Epoch 373/1500\n",
      "46/46 [==============================] - 0s 892us/step - loss: 0.1998 - accuracy: 0.9227\n",
      "Epoch 374/1500\n",
      "46/46 [==============================] - 0s 876us/step - loss: 0.2025 - accuracy: 0.9221\n",
      "Epoch 375/1500\n",
      "46/46 [==============================] - 0s 829us/step - loss: 0.2028 - accuracy: 0.9217\n",
      "Epoch 376/1500\n",
      "46/46 [==============================] - 0s 866us/step - loss: 0.2038 - accuracy: 0.9227\n",
      "Epoch 377/1500\n",
      "46/46 [==============================] - 0s 856us/step - loss: 0.2072 - accuracy: 0.9197\n",
      "Epoch 378/1500\n",
      "46/46 [==============================] - 0s 831us/step - loss: 0.1980 - accuracy: 0.9241\n",
      "Epoch 379/1500\n",
      "46/46 [==============================] - 0s 854us/step - loss: 0.1989 - accuracy: 0.9238\n",
      "Epoch 380/1500\n",
      "46/46 [==============================] - 0s 837us/step - loss: 0.1856 - accuracy: 0.9302\n",
      "Epoch 381/1500\n",
      "46/46 [==============================] - 0s 866us/step - loss: 0.2108 - accuracy: 0.9197\n",
      "Epoch 382/1500\n",
      "46/46 [==============================] - 0s 864us/step - loss: 0.1971 - accuracy: 0.9204\n",
      "Epoch 383/1500\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.2102 - accuracy: 0.9200\n",
      "Epoch 384/1500\n",
      "46/46 [==============================] - 0s 897us/step - loss: 0.2011 - accuracy: 0.9231\n",
      "Epoch 385/1500\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.2081 - accuracy: 0.9238\n",
      "Epoch 386/1500\n",
      "46/46 [==============================] - 0s 871us/step - loss: 0.2127 - accuracy: 0.9180\n",
      "Epoch 387/1500\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.1991 - accuracy: 0.9200\n",
      "Epoch 388/1500\n",
      "46/46 [==============================] - 0s 840us/step - loss: 0.2091 - accuracy: 0.9152\n",
      "Epoch 389/1500\n",
      "46/46 [==============================] - 0s 944us/step - loss: 0.2081 - accuracy: 0.9173\n",
      "Epoch 390/1500\n",
      "46/46 [==============================] - 0s 872us/step - loss: 0.1976 - accuracy: 0.9234\n",
      "Epoch 391/1500\n",
      "46/46 [==============================] - 0s 820us/step - loss: 0.1923 - accuracy: 0.9265\n",
      "Epoch 392/1500\n",
      "46/46 [==============================] - 0s 817us/step - loss: 0.2076 - accuracy: 0.9156\n",
      "Epoch 393/1500\n",
      "46/46 [==============================] - 0s 878us/step - loss: 0.2112 - accuracy: 0.9193\n",
      "Epoch 394/1500\n",
      "46/46 [==============================] - 0s 824us/step - loss: 0.1964 - accuracy: 0.9190\n",
      "Epoch 395/1500\n",
      "46/46 [==============================] - 0s 826us/step - loss: 0.2245 - accuracy: 0.9115\n",
      "Epoch 396/1500\n",
      "46/46 [==============================] - 0s 845us/step - loss: 0.2055 - accuracy: 0.9187\n",
      "Epoch 397/1500\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.2008 - accuracy: 0.9224\n",
      "Epoch 398/1500\n",
      "46/46 [==============================] - 0s 841us/step - loss: 0.1987 - accuracy: 0.9190\n",
      "Epoch 399/1500\n",
      "46/46 [==============================] - 0s 938us/step - loss: 0.1992 - accuracy: 0.9231\n",
      "Epoch 400/1500\n",
      "46/46 [==============================] - 0s 856us/step - loss: 0.2117 - accuracy: 0.9122\n",
      "Epoch 401/1500\n",
      "46/46 [==============================] - 0s 846us/step - loss: 0.2068 - accuracy: 0.9193\n",
      "Epoch 402/1500\n",
      "46/46 [==============================] - 0s 831us/step - loss: 0.2100 - accuracy: 0.9166\n",
      "Epoch 403/1500\n",
      "46/46 [==============================] - 0s 848us/step - loss: 0.1990 - accuracy: 0.9244\n",
      "Epoch 404/1500\n",
      "46/46 [==============================] - 0s 928us/step - loss: 0.2053 - accuracy: 0.9238\n",
      "Epoch 405/1500\n",
      "46/46 [==============================] - 0s 970us/step - loss: 0.2082 - accuracy: 0.9217\n",
      "Epoch 406/1500\n",
      "46/46 [==============================] - 0s 987us/step - loss: 0.1985 - accuracy: 0.9278\n",
      "Epoch 407/1500\n",
      "46/46 [==============================] - 0s 932us/step - loss: 0.2105 - accuracy: 0.9221\n",
      "Epoch 408/1500\n",
      "46/46 [==============================] - 0s 842us/step - loss: 0.1960 - accuracy: 0.9244\n",
      "Epoch 409/1500\n",
      "46/46 [==============================] - 0s 901us/step - loss: 0.2036 - accuracy: 0.9221\n",
      "Epoch 410/1500\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.1368 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 380.\n",
      "46/46 [==============================] - 0s 931us/step - loss: 0.2064 - accuracy: 0.9261\n",
      "Epoch 410: early stopping\n",
      "8/8 [==============================] - 0s 927us/step - loss: 0.6495 - accuracy: 0.7412\n",
      "8/8 [==============================] - 0s 723us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "After appending - Cat IDs: 840, Predictions: 840, Actuals: 840, Gender: 840\n",
      "Final Test Results - Loss: 0.6494501829147339, Accuracy: 0.7412280440330505, Precision: 0.710672514619883, Recall: 0.808225131842153, F1 Score: 0.7391087297600665\n",
      "Confusion Matrix:\n",
      " [[109   7  40]\n",
      " [  1  24   0]\n",
      " [ 10   1  36]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6617389877752861\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7880673408508301\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7139616757631302\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6582032764694755\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6947157808719218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 840, Predictions: 840, Actuals: 840, Gender: 840\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.73 (80/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, kitten, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[kitten, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, kitten, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, adult, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, kitten, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                             [kitten, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "61    055A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "56    051A  [adult, kitten, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [adult, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "81    075A              [adult, adult, senior, senior, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                    [kitten, adult]         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A        [kitten, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "18    016A  [senior, adult, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "17    015A  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "41    035A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, kitten, adult, adult, ki...         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...         adult           senior                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "12    011A                                    [adult, senior]         adult           senior                  False\n",
       "102   108A       [adult, senior, adult, senior, adult, adult]         adult           senior                  False\n",
       "101   106A  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "93    097B  [senior, adult, senior, senior, adult, senior,...        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, senior,...        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "48    042A  [adult, adult, adult, adult, adult, adult, adu...         adult           kitten                  False\n",
       "50    044A              [adult, adult, adult, kitten, kitten]         adult           kitten                  False\n",
       "42    036A  [senior, senior, adult, senior, adult, adult, ...        senior            adult                  False\n",
       "52    047A  [kitten, adult, adult, kitten, adult, kitten, ...         adult           kitten                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "40    034A             [adult, senior, senior, senior, adult]        senior            adult                  False\n",
       "38    032A                                   [kitten, kitten]        kitten            adult                  False\n",
       "62    056A                             [adult, senior, adult]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, senior, adult, s...         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [kitten, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "69    063A  [kitten, kitten, kitten, kitten, adult, kitten...        kitten            adult                  False\n",
       "30    025C             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "47    041A                                    [adult, kitten]         adult           kitten                  False\n",
       "109   117A  [senior, senior, adult, adult, senior, adult, ...         adult           senior                  False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten     9\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15              9  60.000000\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          553            431  77.938517\n",
      "1           kitten          109             69  63.302752\n",
      "2           senior          178            100  56.179775\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    217      170     78.34\n",
      "1          M    337      258     76.56\n",
      "2          X    286      172     60.14\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
