{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 87, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 988, 1: 805, 2: 600})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1998 - accuracy: 0.4877\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.9312 - accuracy: 0.5942\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.8536 - accuracy: 0.6389\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.7871 - accuracy: 0.6569\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.7656 - accuracy: 0.6649\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.7436 - accuracy: 0.6862\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.7214 - accuracy: 0.6995\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.7031 - accuracy: 0.7041\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.6787 - accuracy: 0.7104\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.6767 - accuracy: 0.7075\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.6369 - accuracy: 0.7158\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.6528 - accuracy: 0.7183\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.6352 - accuracy: 0.7280\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.6043 - accuracy: 0.7417\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.6035 - accuracy: 0.7317\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.5890 - accuracy: 0.7547\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.5860 - accuracy: 0.7568\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.5846 - accuracy: 0.7493\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.5672 - accuracy: 0.7576\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.5606 - accuracy: 0.7564\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.5416 - accuracy: 0.7748\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.5415 - accuracy: 0.7739\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.5345 - accuracy: 0.7756\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.5117 - accuracy: 0.7810\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.5282 - accuracy: 0.7768\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.5020 - accuracy: 0.7877\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.5125 - accuracy: 0.7802\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.5198 - accuracy: 0.7789\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.5080 - accuracy: 0.7848\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.5053 - accuracy: 0.7873\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.4855 - accuracy: 0.7860\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.4920 - accuracy: 0.7810\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4933 - accuracy: 0.7890\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.4964 - accuracy: 0.7894\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4778 - accuracy: 0.8003\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 995us/step - loss: 0.4800 - accuracy: 0.7965\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.4694 - accuracy: 0.7973\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.4746 - accuracy: 0.7865\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.4669 - accuracy: 0.8078\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.4720 - accuracy: 0.8011\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.4593 - accuracy: 0.8048\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.4559 - accuracy: 0.7994\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.4521 - accuracy: 0.8061\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.4549 - accuracy: 0.8044\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.4561 - accuracy: 0.8053\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.4331 - accuracy: 0.8161\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.4472 - accuracy: 0.7977\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.4213 - accuracy: 0.8211\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.4268 - accuracy: 0.8203\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4287 - accuracy: 0.8203\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.4168 - accuracy: 0.8333\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.4275 - accuracy: 0.8157\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.4157 - accuracy: 0.8257\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4265 - accuracy: 0.8207\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.4299 - accuracy: 0.8111\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4338 - accuracy: 0.8178\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4183 - accuracy: 0.8245\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.4252 - accuracy: 0.8165\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4054 - accuracy: 0.8249\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.4048 - accuracy: 0.8324\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.4016 - accuracy: 0.8312\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3872 - accuracy: 0.8349\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3997 - accuracy: 0.8249\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.4026 - accuracy: 0.8299\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3975 - accuracy: 0.8266\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.4018 - accuracy: 0.8316\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3933 - accuracy: 0.8253\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3991 - accuracy: 0.8312\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3811 - accuracy: 0.8341\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3737 - accuracy: 0.8445\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.3916 - accuracy: 0.8416\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3699 - accuracy: 0.8454\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3816 - accuracy: 0.8370\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3754 - accuracy: 0.8437\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.3671 - accuracy: 0.8437\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3720 - accuracy: 0.8404\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3621 - accuracy: 0.8491\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.3780 - accuracy: 0.8391\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3701 - accuracy: 0.8433\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.3576 - accuracy: 0.8517\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3677 - accuracy: 0.8454\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.3698 - accuracy: 0.8454\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.3540 - accuracy: 0.8475\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3861 - accuracy: 0.8333\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.3522 - accuracy: 0.8450\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3523 - accuracy: 0.8433\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.3622 - accuracy: 0.8487\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3446 - accuracy: 0.8479\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3504 - accuracy: 0.8471\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3498 - accuracy: 0.8487\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3574 - accuracy: 0.8471\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3630 - accuracy: 0.8458\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.3602 - accuracy: 0.8454\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3423 - accuracy: 0.8525\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.3403 - accuracy: 0.8588\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3417 - accuracy: 0.8521\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.3489 - accuracy: 0.8546\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.3300 - accuracy: 0.8517\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.3383 - accuracy: 0.8600\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.3285 - accuracy: 0.8600\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.3354 - accuracy: 0.8496\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8617\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3375 - accuracy: 0.8533\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8696\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.3189 - accuracy: 0.8675\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.3312 - accuracy: 0.8654\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.3182 - accuracy: 0.8671\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.3154 - accuracy: 0.8663\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.3160 - accuracy: 0.8717\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.3312 - accuracy: 0.8621\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3172 - accuracy: 0.8654\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3246 - accuracy: 0.8600\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.3149 - accuracy: 0.8709\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3007 - accuracy: 0.8730\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.3150 - accuracy: 0.8684\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.3191 - accuracy: 0.8667\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3140 - accuracy: 0.8579\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3025 - accuracy: 0.8709\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3011 - accuracy: 0.8776\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2992 - accuracy: 0.8763\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3152 - accuracy: 0.8659\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3146 - accuracy: 0.8721\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3102 - accuracy: 0.8617\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3142 - accuracy: 0.8742\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3014 - accuracy: 0.8721\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2969 - accuracy: 0.8788\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3001 - accuracy: 0.8746\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2875 - accuracy: 0.8868\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3030 - accuracy: 0.8763\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3087 - accuracy: 0.8646\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2985 - accuracy: 0.8780\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3009 - accuracy: 0.8730\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2998 - accuracy: 0.8755\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2840 - accuracy: 0.8872\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2898 - accuracy: 0.8868\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2778 - accuracy: 0.8855\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.2823 - accuracy: 0.8868\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2893 - accuracy: 0.8788\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2882 - accuracy: 0.8796\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2778 - accuracy: 0.8901\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2810 - accuracy: 0.8805\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2767 - accuracy: 0.8847\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2801 - accuracy: 0.8868\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2721 - accuracy: 0.8876\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2772 - accuracy: 0.8880\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2956 - accuracy: 0.8746\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2832 - accuracy: 0.8863\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.2714 - accuracy: 0.8847\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.2668 - accuracy: 0.8876\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2669 - accuracy: 0.8939\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.2892 - accuracy: 0.8713\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2786 - accuracy: 0.8855\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2740 - accuracy: 0.8876\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.2806 - accuracy: 0.8792\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.2617 - accuracy: 0.8926\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2738 - accuracy: 0.8922\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2876 - accuracy: 0.8805\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2655 - accuracy: 0.8930\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2620 - accuracy: 0.8947\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2604 - accuracy: 0.8959\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2655 - accuracy: 0.8947\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2743 - accuracy: 0.8784\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2656 - accuracy: 0.8947\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2705 - accuracy: 0.8876\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2776 - accuracy: 0.8863\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2689 - accuracy: 0.8934\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2564 - accuracy: 0.8959\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2725 - accuracy: 0.8905\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2671 - accuracy: 0.8926\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.2502 - accuracy: 0.8943\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2436 - accuracy: 0.9031\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2567 - accuracy: 0.8893\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2538 - accuracy: 0.8893\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2614 - accuracy: 0.8972\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2486 - accuracy: 0.8985\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2540 - accuracy: 0.9039\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2545 - accuracy: 0.8993\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2439 - accuracy: 0.8997\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2436 - accuracy: 0.9056\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2390 - accuracy: 0.9014\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2387 - accuracy: 0.9039\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2465 - accuracy: 0.9022\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2394 - accuracy: 0.9076\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2396 - accuracy: 0.9005\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.2378 - accuracy: 0.9031\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2466 - accuracy: 0.9047\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2480 - accuracy: 0.9031\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2405 - accuracy: 0.9089\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2431 - accuracy: 0.8997\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2323 - accuracy: 0.9043\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2283 - accuracy: 0.9135\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2351 - accuracy: 0.9064\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2249 - accuracy: 0.9085\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9131\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2366 - accuracy: 0.9081\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2218 - accuracy: 0.9122\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2343 - accuracy: 0.9106\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2378 - accuracy: 0.9039\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2293 - accuracy: 0.9102\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2418 - accuracy: 0.9026\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2351 - accuracy: 0.9072\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2390 - accuracy: 0.9039\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2255 - accuracy: 0.9056\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2369 - accuracy: 0.9026\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2318 - accuracy: 0.9014\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2175 - accuracy: 0.9106\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2163 - accuracy: 0.9122\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.2361 - accuracy: 0.9022\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2342 - accuracy: 0.9047\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2302 - accuracy: 0.9072\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2373 - accuracy: 0.9022\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.2303 - accuracy: 0.9043\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2214 - accuracy: 0.9089\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2101 - accuracy: 0.9164\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2210 - accuracy: 0.9102\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2314 - accuracy: 0.9039\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2206 - accuracy: 0.9106\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2312 - accuracy: 0.9035\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2119 - accuracy: 0.9118\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2171 - accuracy: 0.9056\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2227 - accuracy: 0.9102\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2320 - accuracy: 0.9110\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2270 - accuracy: 0.9035\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2206 - accuracy: 0.9097\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2225 - accuracy: 0.9106\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2089 - accuracy: 0.9168\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2215 - accuracy: 0.9102\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2161 - accuracy: 0.9122\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2223 - accuracy: 0.9110\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2278 - accuracy: 0.9068\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2240 - accuracy: 0.9143\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2009 - accuracy: 0.9185\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2022 - accuracy: 0.9198\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2002 - accuracy: 0.9269\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2157 - accuracy: 0.9131\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2121 - accuracy: 0.9164\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2227 - accuracy: 0.9131\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2139 - accuracy: 0.9148\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2116 - accuracy: 0.9143\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2059 - accuracy: 0.9206\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2026 - accuracy: 0.9193\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.2129 - accuracy: 0.9089\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.2054 - accuracy: 0.9185\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2095 - accuracy: 0.9202\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2125 - accuracy: 0.9118\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2120 - accuracy: 0.9156\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2104 - accuracy: 0.9156\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.2100 - accuracy: 0.9164\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2019 - accuracy: 0.9177\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1978 - accuracy: 0.9206\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2108 - accuracy: 0.9143\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.1939 - accuracy: 0.9227\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1961 - accuracy: 0.9198\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2056 - accuracy: 0.9219\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2011 - accuracy: 0.9185\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2120 - accuracy: 0.9181\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2002 - accuracy: 0.9227\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1947 - accuracy: 0.9265\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2166 - accuracy: 0.9185\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.2039 - accuracy: 0.9231\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.1831 - accuracy: 0.9269\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2053 - accuracy: 0.9164\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2056 - accuracy: 0.9164\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.2028 - accuracy: 0.9193\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2094 - accuracy: 0.9152\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.1896 - accuracy: 0.9310\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2012 - accuracy: 0.9206\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1858 - accuracy: 0.9260\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.1930 - accuracy: 0.9181\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1919 - accuracy: 0.9248\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1888 - accuracy: 0.9239\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1991 - accuracy: 0.9281\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.2004 - accuracy: 0.9156\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1948 - accuracy: 0.9260\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1952 - accuracy: 0.9214\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1952 - accuracy: 0.9252\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1924 - accuracy: 0.9244\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2000 - accuracy: 0.9173\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1874 - accuracy: 0.9252\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2046 - accuracy: 0.9122\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.1871 - accuracy: 0.9260\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1777 - accuracy: 0.9310\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.1867 - accuracy: 0.9223\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1888 - accuracy: 0.9248\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1987 - accuracy: 0.9223\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1958 - accuracy: 0.9231\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1992 - accuracy: 0.9202\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9306\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.1835 - accuracy: 0.9265\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1772 - accuracy: 0.9285\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1952 - accuracy: 0.9193\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1876 - accuracy: 0.9260\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.1944 - accuracy: 0.9298\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1870 - accuracy: 0.9302\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1963 - accuracy: 0.9214\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.1948 - accuracy: 0.9223\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1882 - accuracy: 0.9315\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1858 - accuracy: 0.9256\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1727 - accuracy: 0.9277\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1825 - accuracy: 0.9323\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1761 - accuracy: 0.9340\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1804 - accuracy: 0.9269\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1939 - accuracy: 0.9235\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1688 - accuracy: 0.9398\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1908 - accuracy: 0.9248\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1894 - accuracy: 0.9290\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1782 - accuracy: 0.9298\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1793 - accuracy: 0.9298\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1735 - accuracy: 0.9373\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1844 - accuracy: 0.9290\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1655 - accuracy: 0.9340\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1689 - accuracy: 0.9352\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1873 - accuracy: 0.9269\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1776 - accuracy: 0.9248\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1829 - accuracy: 0.9290\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1826 - accuracy: 0.9285\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1813 - accuracy: 0.9294\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1872 - accuracy: 0.9310\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1674 - accuracy: 0.9340\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1708 - accuracy: 0.9398\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1648 - accuracy: 0.9331\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1826 - accuracy: 0.9323\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1887 - accuracy: 0.9252\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1806 - accuracy: 0.9260\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1738 - accuracy: 0.9331\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1682 - accuracy: 0.9365\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1634 - accuracy: 0.9365\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1773 - accuracy: 0.9336\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1698 - accuracy: 0.9386\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1736 - accuracy: 0.9310\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1753 - accuracy: 0.9306\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1525 - accuracy: 0.9390\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1719 - accuracy: 0.9319\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1788 - accuracy: 0.9310\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1801 - accuracy: 0.9306\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1732 - accuracy: 0.9306\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1872 - accuracy: 0.9306\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1757 - accuracy: 0.9348\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1609 - accuracy: 0.9336\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1783 - accuracy: 0.9265\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1629 - accuracy: 0.9402\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1707 - accuracy: 0.9294\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1782 - accuracy: 0.9256\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1761 - accuracy: 0.9340\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1711 - accuracy: 0.9356\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1645 - accuracy: 0.9419\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1742 - accuracy: 0.9348\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1768 - accuracy: 0.9319\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1574 - accuracy: 0.9394\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1690 - accuracy: 0.9352\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1659 - accuracy: 0.9302\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1546 - accuracy: 0.9394\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1589 - accuracy: 0.9427\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1654 - accuracy: 0.9386\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1600 - accuracy: 0.9361\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1588 - accuracy: 0.9373\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1622 - accuracy: 0.9331\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1551 - accuracy: 0.9444\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1652 - accuracy: 0.9327\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1571 - accuracy: 0.9382\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1870 - accuracy: 0.9285\n",
      "Epoch 362/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1630 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 332.\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.1660 - accuracy: 0.9302\n",
      "Epoch 362: early stopping\n",
      "6/6 [==============================] - 0s 881us/step - loss: 0.8781 - accuracy: 0.6852\n",
      "6/6 [==============================] - 0s 651us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 0.8780808448791504, Accuracy: 0.6851851940155029, Precision: 0.6524374176548089, Recall: 0.7365370506236243, F1 Score: 0.6843175690677832\n",
      "Confusion Matrix:\n",
      " [[68  5 21]\n",
      " [ 1  9  0]\n",
      " [23  1 34]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "        ..\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, Length: 83, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'047A'}\n",
      "Moved to Test Set:\n",
      "{'047A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '047A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "Length of X_train_val:\n",
      "690\n",
      "Length of y_train_val:\n",
      "690\n",
      "Length of groups_train_val:\n",
      "690\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    136\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 1: 680, 2: 630})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.1728 - accuracy: 0.4621\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9759 - accuracy: 0.5706\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 970us/step - loss: 0.8714 - accuracy: 0.6191\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.8424 - accuracy: 0.6348\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.7455 - accuracy: 0.6847\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.7243 - accuracy: 0.6934\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.6903 - accuracy: 0.7091\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.6911 - accuracy: 0.7073\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.6626 - accuracy: 0.7276\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.6510 - accuracy: 0.7341\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.6297 - accuracy: 0.7470\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.6015 - accuracy: 0.7530\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.5931 - accuracy: 0.7535\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.6026 - accuracy: 0.7535\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5821 - accuracy: 0.7678\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5609 - accuracy: 0.7724\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.5639 - accuracy: 0.7599\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5702 - accuracy: 0.7636\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.5369 - accuracy: 0.7696\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.5159 - accuracy: 0.7890\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5403 - accuracy: 0.7775\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.5298 - accuracy: 0.7872\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.5317 - accuracy: 0.7798\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.5007 - accuracy: 0.7909\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.5202 - accuracy: 0.7821\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.4999 - accuracy: 0.7964\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5047 - accuracy: 0.7853\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.4855 - accuracy: 0.7987\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.4742 - accuracy: 0.8061\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.4891 - accuracy: 0.7992\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.4713 - accuracy: 0.7955\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.4704 - accuracy: 0.8038\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.4649 - accuracy: 0.8121\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.4577 - accuracy: 0.8139\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.4708 - accuracy: 0.8024\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.4550 - accuracy: 0.8149\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.4559 - accuracy: 0.8098\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.4509 - accuracy: 0.8158\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.4276 - accuracy: 0.8269\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.4473 - accuracy: 0.8241\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4515 - accuracy: 0.8126\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.4212 - accuracy: 0.8310\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.4303 - accuracy: 0.8338\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8264\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4103 - accuracy: 0.8218\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.4139 - accuracy: 0.8241\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.4123 - accuracy: 0.8269\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3906 - accuracy: 0.8380\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.4000 - accuracy: 0.8324\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.4047 - accuracy: 0.8315\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3994 - accuracy: 0.8356\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.4031 - accuracy: 0.8343\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3935 - accuracy: 0.8426\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.3823 - accuracy: 0.8384\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.3860 - accuracy: 0.8361\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.3900 - accuracy: 0.8407\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.3645 - accuracy: 0.8476\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.3694 - accuracy: 0.8500\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.3609 - accuracy: 0.8601\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.3771 - accuracy: 0.8472\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3702 - accuracy: 0.8463\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.3720 - accuracy: 0.8393\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.3542 - accuracy: 0.8546\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.3644 - accuracy: 0.8532\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.3717 - accuracy: 0.8444\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3668 - accuracy: 0.8472\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.3371 - accuracy: 0.8652\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3802 - accuracy: 0.8416\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.3410 - accuracy: 0.8638\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3475 - accuracy: 0.8629\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.3551 - accuracy: 0.8560\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.3422 - accuracy: 0.8596\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3462 - accuracy: 0.8569\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3486 - accuracy: 0.8592\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 987us/step - loss: 0.3484 - accuracy: 0.8518\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.3432 - accuracy: 0.8606\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.3262 - accuracy: 0.8680\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8712\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8758\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.3269 - accuracy: 0.8763\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.8781\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.3484 - accuracy: 0.8555\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.3353 - accuracy: 0.8532\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.3159 - accuracy: 0.8712\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.3247 - accuracy: 0.8753\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3322 - accuracy: 0.8675\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.3179 - accuracy: 0.8735\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.3312 - accuracy: 0.8629\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.3220 - accuracy: 0.8647\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3043 - accuracy: 0.8813\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.3161 - accuracy: 0.8730\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2965 - accuracy: 0.8777\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3130 - accuracy: 0.8735\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2921 - accuracy: 0.8901\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.3043 - accuracy: 0.8813\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.2952 - accuracy: 0.8781\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.3095 - accuracy: 0.8781\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2908 - accuracy: 0.8809\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.3096 - accuracy: 0.8721\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.3006 - accuracy: 0.8800\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.3020 - accuracy: 0.8777\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3052 - accuracy: 0.8841\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2937 - accuracy: 0.8832\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.3019 - accuracy: 0.8790\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2823 - accuracy: 0.8887\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2934 - accuracy: 0.8786\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.2880 - accuracy: 0.8818\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.2812 - accuracy: 0.8869\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.2781 - accuracy: 0.8892\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2811 - accuracy: 0.8901\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 947us/step - loss: 0.2740 - accuracy: 0.8938\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.2746 - accuracy: 0.8952\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.2865 - accuracy: 0.8827\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2867 - accuracy: 0.8841\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.2863 - accuracy: 0.8883\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.2806 - accuracy: 0.8860\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.2710 - accuracy: 0.8901\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 960us/step - loss: 0.2664 - accuracy: 0.8860\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2687 - accuracy: 0.8901\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.2655 - accuracy: 0.8980\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.2851 - accuracy: 0.8827\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.2874 - accuracy: 0.8818\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2628 - accuracy: 0.8952\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.2662 - accuracy: 0.8915\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2705 - accuracy: 0.8901\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2557 - accuracy: 0.9007\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2559 - accuracy: 0.8952\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2681 - accuracy: 0.8887\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.2461 - accuracy: 0.9077\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2503 - accuracy: 0.9049\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2717 - accuracy: 0.8920\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2504 - accuracy: 0.9030\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2645 - accuracy: 0.8957\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2640 - accuracy: 0.8947\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2675 - accuracy: 0.8883\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.2503 - accuracy: 0.9012\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.2569 - accuracy: 0.9021\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2543 - accuracy: 0.8998\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2467 - accuracy: 0.8975\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.2503 - accuracy: 0.9017\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.2514 - accuracy: 0.9026\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.2397 - accuracy: 0.8994\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2484 - accuracy: 0.9017\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.2537 - accuracy: 0.8970\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.2408 - accuracy: 0.9040\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.2471 - accuracy: 0.9007\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.2492 - accuracy: 0.9035\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.2381 - accuracy: 0.9095\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.2403 - accuracy: 0.9026\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 926us/step - loss: 0.2465 - accuracy: 0.9003\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.2266 - accuracy: 0.9123\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2273 - accuracy: 0.9049\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.2384 - accuracy: 0.9090\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.2506 - accuracy: 0.9007\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2261 - accuracy: 0.9077\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.2450 - accuracy: 0.8994\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.2359 - accuracy: 0.9072\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9040\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2299 - accuracy: 0.9132\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.2355 - accuracy: 0.9044\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2503 - accuracy: 0.8961\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2373 - accuracy: 0.9021\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2224 - accuracy: 0.9063\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2334 - accuracy: 0.9077\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2290 - accuracy: 0.9109\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2439 - accuracy: 0.9077\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.2143 - accuracy: 0.9178\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2142 - accuracy: 0.9178\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2254 - accuracy: 0.9164\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2212 - accuracy: 0.9141\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 963us/step - loss: 0.2265 - accuracy: 0.9183\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2125 - accuracy: 0.9174\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.2145 - accuracy: 0.9197\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2303 - accuracy: 0.9077\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2226 - accuracy: 0.9164\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2125 - accuracy: 0.9183\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2168 - accuracy: 0.9197\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2192 - accuracy: 0.9164\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.2139 - accuracy: 0.9201\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2368 - accuracy: 0.9077\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.2138 - accuracy: 0.9164\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2151 - accuracy: 0.9141\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2143 - accuracy: 0.9132\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2232 - accuracy: 0.9067\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2126 - accuracy: 0.9174\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2333 - accuracy: 0.8989\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2180 - accuracy: 0.9160\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2117 - accuracy: 0.9178\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2231 - accuracy: 0.9127\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.2115 - accuracy: 0.9160\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.2135 - accuracy: 0.9224\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.2174 - accuracy: 0.9132\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.2197 - accuracy: 0.9086\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2026 - accuracy: 0.9178\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9090\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.2149 - accuracy: 0.9211\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.1978 - accuracy: 0.9307\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1982 - accuracy: 0.9192\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.1962 - accuracy: 0.9252\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.2035 - accuracy: 0.9169\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.2147 - accuracy: 0.9206\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9252\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9137\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9229\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.1980 - accuracy: 0.9243\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9155\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9252\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9215\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1962 - accuracy: 0.9252\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1982 - accuracy: 0.9247\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2110 - accuracy: 0.9160\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1969 - accuracy: 0.9243\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.1886 - accuracy: 0.9238\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.1938 - accuracy: 0.9266\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1950 - accuracy: 0.9243\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2095 - accuracy: 0.9247\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1921 - accuracy: 0.9238\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9280\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9238\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9294\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.2253 - accuracy: 0.9104\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9247\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9266\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9294\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 980us/step - loss: 0.1931 - accuracy: 0.9284\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9280\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2002 - accuracy: 0.9201\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1976 - accuracy: 0.9183\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1988 - accuracy: 0.9224\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1979 - accuracy: 0.9275\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1926 - accuracy: 0.9187\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.1739 - accuracy: 0.9326\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2064 - accuracy: 0.9238\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1881 - accuracy: 0.9312\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.1793 - accuracy: 0.9284\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.1821 - accuracy: 0.9340\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.2056 - accuracy: 0.9187\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.1940 - accuracy: 0.9257\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1837 - accuracy: 0.9229\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1732 - accuracy: 0.9317\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1993 - accuracy: 0.9183\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1830 - accuracy: 0.9289\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1658 - accuracy: 0.9372\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.1817 - accuracy: 0.9317\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.1919 - accuracy: 0.9280\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1799 - accuracy: 0.9331\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1667 - accuracy: 0.9344\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1758 - accuracy: 0.9294\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.1936 - accuracy: 0.9261\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1928 - accuracy: 0.9229\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1876 - accuracy: 0.9266\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1975 - accuracy: 0.9271\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1868 - accuracy: 0.9271\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1725 - accuracy: 0.9344\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1737 - accuracy: 0.9307\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1720 - accuracy: 0.9331\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1814 - accuracy: 0.9266\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1675 - accuracy: 0.9335\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1714 - accuracy: 0.9321\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1827 - accuracy: 0.9298\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1817 - accuracy: 0.9280\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1778 - accuracy: 0.9335\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1785 - accuracy: 0.9326\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9312\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.1875 - accuracy: 0.9317\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1905 - accuracy: 0.9284\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1707 - accuracy: 0.9358\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1819 - accuracy: 0.9266\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1744 - accuracy: 0.9326\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.1735 - accuracy: 0.9321\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.1638 - accuracy: 0.9349\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9317\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.9391\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 965us/step - loss: 0.1544 - accuracy: 0.9400\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1896 - accuracy: 0.9284\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.1769 - accuracy: 0.9349\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.1542 - accuracy: 0.9409\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.1740 - accuracy: 0.9280\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.1670 - accuracy: 0.9377\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 917us/step - loss: 0.1753 - accuracy: 0.9261\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.1673 - accuracy: 0.9381\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.1669 - accuracy: 0.9317\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.1732 - accuracy: 0.9354\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.1668 - accuracy: 0.9377\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9321\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9391\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 969us/step - loss: 0.1586 - accuracy: 0.9386\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1497 - accuracy: 0.9441\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1535 - accuracy: 0.9441\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.1671 - accuracy: 0.9349\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9428\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.1669 - accuracy: 0.9372\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.1665 - accuracy: 0.9414\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1603 - accuracy: 0.9428\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1622 - accuracy: 0.9354\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1564 - accuracy: 0.9428\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1661 - accuracy: 0.9395\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1634 - accuracy: 0.9335\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.1445 - accuracy: 0.9432\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1508 - accuracy: 0.9437\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1589 - accuracy: 0.9469\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1594 - accuracy: 0.9367\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1613 - accuracy: 0.9377\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.1513 - accuracy: 0.9404\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1467 - accuracy: 0.9501\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.1669 - accuracy: 0.9344\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1438 - accuracy: 0.9492\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.1552 - accuracy: 0.9414\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1602 - accuracy: 0.9437\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1530 - accuracy: 0.9423\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1627 - accuracy: 0.9381\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.1524 - accuracy: 0.9460\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1606 - accuracy: 0.9344\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1563 - accuracy: 0.9344\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1613 - accuracy: 0.9344\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1511 - accuracy: 0.9395\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1511 - accuracy: 0.9428\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1650 - accuracy: 0.9340\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1502 - accuracy: 0.9414\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.1636 - accuracy: 0.9344\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.1435 - accuracy: 0.9469\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.1462 - accuracy: 0.9446\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1498 - accuracy: 0.9377\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1514 - accuracy: 0.9478\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1676 - accuracy: 0.9298\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1490 - accuracy: 0.9441\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1526 - accuracy: 0.9437\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.1470 - accuracy: 0.9441\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.1383 - accuracy: 0.9488\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1482 - accuracy: 0.9437\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1514 - accuracy: 0.9437\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1493 - accuracy: 0.9464\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.1517 - accuracy: 0.9409\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.1546 - accuracy: 0.9414\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1469 - accuracy: 0.9372\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1491 - accuracy: 0.9418\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.1387 - accuracy: 0.9524\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.1436 - accuracy: 0.9400\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.1428 - accuracy: 0.9497\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1609 - accuracy: 0.9404\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1502 - accuracy: 0.9395\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 992us/step - loss: 0.1440 - accuracy: 0.9428\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.1489 - accuracy: 0.9437\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1468 - accuracy: 0.9391\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1305 - accuracy: 0.9529\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1404 - accuracy: 0.9446\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.1386 - accuracy: 0.9501\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1439 - accuracy: 0.9460\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1496 - accuracy: 0.9414\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1624 - accuracy: 0.9381\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.1447 - accuracy: 0.9372\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1318 - accuracy: 0.9543\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.1453 - accuracy: 0.9441\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.1435 - accuracy: 0.9460\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.1424 - accuracy: 0.9414\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.1575 - accuracy: 0.9418\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1474 - accuracy: 0.9423\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1471 - accuracy: 0.9400\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 962us/step - loss: 0.1320 - accuracy: 0.9506\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1445 - accuracy: 0.9455\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1400 - accuracy: 0.9474\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9446\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9423\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.1345 - accuracy: 0.9441\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1311 - accuracy: 0.9543\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1416 - accuracy: 0.9446\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.1400 - accuracy: 0.9506\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.1524 - accuracy: 0.9446\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9478\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9594\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.1403 - accuracy: 0.9446\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9395\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9497\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9446\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 912us/step - loss: 0.1335 - accuracy: 0.9552\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.1537 - accuracy: 0.9441\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9409\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9460\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9377\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.1370 - accuracy: 0.9497\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.1337 - accuracy: 0.9460\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.1401 - accuracy: 0.9455\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1412 - accuracy: 0.9455\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1251 - accuracy: 0.9501\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1297 - accuracy: 0.9515\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.1399 - accuracy: 0.9501\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.1369 - accuracy: 0.9474\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.1261 - accuracy: 0.9520\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.1334 - accuracy: 0.9488\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.1248 - accuracy: 0.9538\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.1344 - accuracy: 0.9511\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1364 - accuracy: 0.9497\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1256 - accuracy: 0.9520\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1334 - accuracy: 0.9488\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1407 - accuracy: 0.9446\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1464 - accuracy: 0.9400\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.1448 - accuracy: 0.9441\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1296 - accuracy: 0.9506\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.1289 - accuracy: 0.9492\n",
      "Epoch 400/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1095 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 370.\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1350 - accuracy: 0.9460\n",
      "Epoch 400: early stopping\n",
      "8/8 [==============================] - 0s 775us/step - loss: 1.0192 - accuracy: 0.6599\n",
      "8/8 [==============================] - 0s 643us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "Final Test Results - Loss: 1.0191649198532104, Accuracy: 0.659919023513794, Precision: 0.6494135317542663, Recall: 0.6090201465201465, F1 Score: 0.6252415142046132\n",
      "Confusion Matrix:\n",
      " [[118   3  39]\n",
      " [ 10  24   1]\n",
      " [ 30   1  21]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "        ..\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 82, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 748, 1: 675, 2: 615})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1313 - accuracy: 0.4794\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9388 - accuracy: 0.5765\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8624 - accuracy: 0.6197\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.8289 - accuracy: 0.6251\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.7570 - accuracy: 0.6629\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.7547 - accuracy: 0.6811\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.7283 - accuracy: 0.6811\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.7360 - accuracy: 0.6649\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.6971 - accuracy: 0.6860\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.6895 - accuracy: 0.6791\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.6705 - accuracy: 0.6997\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.6538 - accuracy: 0.7179\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.6483 - accuracy: 0.7115\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.6490 - accuracy: 0.7120\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.6271 - accuracy: 0.7164\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.6045 - accuracy: 0.7321\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.6298 - accuracy: 0.7125\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 986us/step - loss: 0.6162 - accuracy: 0.7252\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.5944 - accuracy: 0.7326\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.5843 - accuracy: 0.7399\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.5922 - accuracy: 0.7360\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.5918 - accuracy: 0.7409\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.5755 - accuracy: 0.7429\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.5734 - accuracy: 0.7434\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 912us/step - loss: 0.5630 - accuracy: 0.7566\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5783 - accuracy: 0.7463\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 990us/step - loss: 0.5323 - accuracy: 0.7679\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 972us/step - loss: 0.5348 - accuracy: 0.7718\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 917us/step - loss: 0.5456 - accuracy: 0.7635\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 990us/step - loss: 0.5434 - accuracy: 0.7552\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.5319 - accuracy: 0.7576\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 979us/step - loss: 0.5128 - accuracy: 0.7723\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5261 - accuracy: 0.7738\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5200 - accuracy: 0.7679\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.5145 - accuracy: 0.7772\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 990us/step - loss: 0.5171 - accuracy: 0.7723\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 943us/step - loss: 0.4958 - accuracy: 0.7826\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5168 - accuracy: 0.7753\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.7816\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 953us/step - loss: 0.5111 - accuracy: 0.7738\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.5011 - accuracy: 0.7758\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.4862 - accuracy: 0.7890\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.4756 - accuracy: 0.7939\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.4755 - accuracy: 0.7969\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.4823 - accuracy: 0.7826\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 0.4663 - accuracy: 0.7978\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.4676 - accuracy: 0.8057\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 952us/step - loss: 0.4624 - accuracy: 0.8052\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.4581 - accuracy: 0.8013\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.4691 - accuracy: 0.7964\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 959us/step - loss: 0.4784 - accuracy: 0.7915\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7978\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.8081\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.8067\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8111\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7954\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.7978\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8140\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4185 - accuracy: 0.8145\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.8057\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 988us/step - loss: 0.4460 - accuracy: 0.7974\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 977us/step - loss: 0.4506 - accuracy: 0.8077\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 972us/step - loss: 0.4471 - accuracy: 0.8067\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4404 - accuracy: 0.8086\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 971us/step - loss: 0.4281 - accuracy: 0.8184\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8243\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8121\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 969us/step - loss: 0.4128 - accuracy: 0.8219\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 911us/step - loss: 0.4331 - accuracy: 0.8131\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 951us/step - loss: 0.4205 - accuracy: 0.8140\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4050 - accuracy: 0.8253\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 994us/step - loss: 0.4112 - accuracy: 0.8175\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 906us/step - loss: 0.4074 - accuracy: 0.8194\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.4211 - accuracy: 0.8204\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 963us/step - loss: 0.4164 - accuracy: 0.8224\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 926us/step - loss: 0.4216 - accuracy: 0.8106\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 914us/step - loss: 0.4040 - accuracy: 0.8175\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.4076 - accuracy: 0.8209\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4036 - accuracy: 0.8288\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 910us/step - loss: 0.3847 - accuracy: 0.8425\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 961us/step - loss: 0.4052 - accuracy: 0.8234\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3964 - accuracy: 0.8356\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.4112 - accuracy: 0.8145\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.3848 - accuracy: 0.8400\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.4146 - accuracy: 0.8126\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.4228 - accuracy: 0.8194\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.3917 - accuracy: 0.8327\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.3946 - accuracy: 0.8263\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.3985 - accuracy: 0.8253\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.3741 - accuracy: 0.8405\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 978us/step - loss: 0.3842 - accuracy: 0.8346\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 916us/step - loss: 0.3905 - accuracy: 0.8342\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 918us/step - loss: 0.3886 - accuracy: 0.8322\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.3725 - accuracy: 0.8449\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8454\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 961us/step - loss: 0.3842 - accuracy: 0.8317\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.3740 - accuracy: 0.8327\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.3653 - accuracy: 0.8508\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8479\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8479\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 934us/step - loss: 0.3678 - accuracy: 0.8464\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 995us/step - loss: 0.3516 - accuracy: 0.8606\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.3703 - accuracy: 0.8435\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3602 - accuracy: 0.8464\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8474\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8440\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8366\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8479\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 927us/step - loss: 0.3721 - accuracy: 0.8445\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.3520 - accuracy: 0.8440\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.3635 - accuracy: 0.8503\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.3751 - accuracy: 0.8395\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 888us/step - loss: 0.3474 - accuracy: 0.8513\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 949us/step - loss: 0.3405 - accuracy: 0.8533\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.3560 - accuracy: 0.8587\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3442 - accuracy: 0.8582\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8577\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8651\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3555 - accuracy: 0.8528\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 992us/step - loss: 0.3525 - accuracy: 0.8459\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 974us/step - loss: 0.3507 - accuracy: 0.8548\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 922us/step - loss: 0.3371 - accuracy: 0.8557\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 910us/step - loss: 0.3378 - accuracy: 0.8611\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.3552 - accuracy: 0.8420\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.3332 - accuracy: 0.8631\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.3277 - accuracy: 0.8675\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.3354 - accuracy: 0.8562\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 958us/step - loss: 0.3403 - accuracy: 0.8557\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3347 - accuracy: 0.8503\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 897us/step - loss: 0.3224 - accuracy: 0.8636\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.3352 - accuracy: 0.8523\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 956us/step - loss: 0.3445 - accuracy: 0.8553\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8577\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8587\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8484\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.3299 - accuracy: 0.8685\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.3324 - accuracy: 0.8616\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.3150 - accuracy: 0.8646\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.3250 - accuracy: 0.8705\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 0.3370 - accuracy: 0.8592\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 945us/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 915us/step - loss: 0.3180 - accuracy: 0.8606\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.3255 - accuracy: 0.8626\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.3269 - accuracy: 0.8685\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.3261 - accuracy: 0.8680\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.3128 - accuracy: 0.8651\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3133 - accuracy: 0.8700\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3258 - accuracy: 0.8621\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3100 - accuracy: 0.8759\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8705\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.8646\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8739\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3214 - accuracy: 0.8670\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 921us/step - loss: 0.3183 - accuracy: 0.8700\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 947us/step - loss: 0.3134 - accuracy: 0.8734\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 952us/step - loss: 0.3132 - accuracy: 0.8602\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.3039 - accuracy: 0.8749\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.3103 - accuracy: 0.8754\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.3341 - accuracy: 0.8543\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 969us/step - loss: 0.3103 - accuracy: 0.8739\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 948us/step - loss: 0.3008 - accuracy: 0.8729\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 908us/step - loss: 0.3101 - accuracy: 0.8714\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 906us/step - loss: 0.3075 - accuracy: 0.8724\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 957us/step - loss: 0.3094 - accuracy: 0.8631\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 991us/step - loss: 0.3225 - accuracy: 0.8641\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 998us/step - loss: 0.3189 - accuracy: 0.8665\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.2979 - accuracy: 0.8749\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 930us/step - loss: 0.2990 - accuracy: 0.8763\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.3116 - accuracy: 0.8651\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.2848 - accuracy: 0.8817\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 974us/step - loss: 0.2913 - accuracy: 0.8808\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8744\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.8906\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 931us/step - loss: 0.3030 - accuracy: 0.8754\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2895 - accuracy: 0.8734\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.2935 - accuracy: 0.8768\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 993us/step - loss: 0.2831 - accuracy: 0.8832\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 978us/step - loss: 0.3010 - accuracy: 0.8710\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 979us/step - loss: 0.2945 - accuracy: 0.8768\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8768\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2982 - accuracy: 0.8778\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.8778\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8896\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 935us/step - loss: 0.2836 - accuracy: 0.8803\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 911us/step - loss: 0.2862 - accuracy: 0.8724\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.2753 - accuracy: 0.8862\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.2756 - accuracy: 0.8896\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 962us/step - loss: 0.2976 - accuracy: 0.8734\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 973us/step - loss: 0.2848 - accuracy: 0.8724\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.2951 - accuracy: 0.8808\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2747 - accuracy: 0.8876\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.8955\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 996us/step - loss: 0.2775 - accuracy: 0.8886\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.2835 - accuracy: 0.8749\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8832\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 927us/step - loss: 0.2763 - accuracy: 0.8871\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 909us/step - loss: 0.2895 - accuracy: 0.8778\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2746 - accuracy: 0.8847\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.2776 - accuracy: 0.8822\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.2684 - accuracy: 0.8930\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.2784 - accuracy: 0.8813\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2843 - accuracy: 0.8881\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.2752 - accuracy: 0.8808\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.2811 - accuracy: 0.8925\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 889us/step - loss: 0.2703 - accuracy: 0.8906\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.2631 - accuracy: 0.8906\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 873us/step - loss: 0.2737 - accuracy: 0.8827\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.2572 - accuracy: 0.8955\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.2614 - accuracy: 0.8970\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.2672 - accuracy: 0.8896\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 912us/step - loss: 0.2779 - accuracy: 0.8847\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2643 - accuracy: 0.8974\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.2703 - accuracy: 0.8930\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.2758 - accuracy: 0.8793\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2690 - accuracy: 0.8867\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2589 - accuracy: 0.8901\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.2656 - accuracy: 0.8955\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2656 - accuracy: 0.8970\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 932us/step - loss: 0.2539 - accuracy: 0.8965\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 942us/step - loss: 0.2779 - accuracy: 0.8857\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 888us/step - loss: 0.2610 - accuracy: 0.8930\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 883us/step - loss: 0.2624 - accuracy: 0.8911\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2437 - accuracy: 0.9033\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 897us/step - loss: 0.2528 - accuracy: 0.9009\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 924us/step - loss: 0.2589 - accuracy: 0.8955\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.2497 - accuracy: 0.8979\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.2637 - accuracy: 0.8847\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 0.2582 - accuracy: 0.8935\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.2477 - accuracy: 0.9019\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 905us/step - loss: 0.2593 - accuracy: 0.8901\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.2710 - accuracy: 0.8886\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.2622 - accuracy: 0.8940\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8930\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 906us/step - loss: 0.2622 - accuracy: 0.8876\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.2400 - accuracy: 0.9038\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.2626 - accuracy: 0.8970\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2575 - accuracy: 0.8940\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2533 - accuracy: 0.8950\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.2528 - accuracy: 0.9004\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2492 - accuracy: 0.9038\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.2529 - accuracy: 0.8984\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.2443 - accuracy: 0.8984\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2488 - accuracy: 0.8950\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8945\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8955\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9014\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9028\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2549 - accuracy: 0.8965\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.2415 - accuracy: 0.8974\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 917us/step - loss: 0.2504 - accuracy: 0.8994\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 887us/step - loss: 0.2346 - accuracy: 0.9038\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 0.2381 - accuracy: 0.9082\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2320 - accuracy: 0.9038\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.2299 - accuracy: 0.9048\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.2454 - accuracy: 0.9004\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.2433 - accuracy: 0.9092\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.2276 - accuracy: 0.9078\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2358 - accuracy: 0.9117\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.2446 - accuracy: 0.9038\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2395 - accuracy: 0.9014\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.2485 - accuracy: 0.8955\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.2515 - accuracy: 0.9014\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.2646 - accuracy: 0.8950\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2375 - accuracy: 0.9014\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2406 - accuracy: 0.9024\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2405 - accuracy: 0.9078\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2467 - accuracy: 0.9082\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2436 - accuracy: 0.8984\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.2292 - accuracy: 0.9014\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.2335 - accuracy: 0.9048\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.2529 - accuracy: 0.8965\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.2377 - accuracy: 0.9048\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.2348 - accuracy: 0.9043\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.2499 - accuracy: 0.9004\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.2269 - accuracy: 0.9092\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.2184 - accuracy: 0.9210\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2421 - accuracy: 0.8950\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.2343 - accuracy: 0.9078\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9102\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2260 - accuracy: 0.9068\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.2558 - accuracy: 0.8930\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 878us/step - loss: 0.2248 - accuracy: 0.9112\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.2448 - accuracy: 0.8940\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.2301 - accuracy: 0.9151\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.2252 - accuracy: 0.9112\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2313 - accuracy: 0.9092\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2349 - accuracy: 0.9097\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.2202 - accuracy: 0.9112\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 874us/step - loss: 0.2116 - accuracy: 0.9102\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2305 - accuracy: 0.9078\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.2264 - accuracy: 0.9107\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.2289 - accuracy: 0.9068\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2086 - accuracy: 0.9269\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.2438 - accuracy: 0.9019\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.2297 - accuracy: 0.9097\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2230 - accuracy: 0.9156\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.2170 - accuracy: 0.9161\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.2180 - accuracy: 0.9166\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.2405 - accuracy: 0.9033\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.2218 - accuracy: 0.9058\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.2062 - accuracy: 0.9092\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2297 - accuracy: 0.9112\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2156 - accuracy: 0.9230\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2334 - accuracy: 0.9078\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2345 - accuracy: 0.8965\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2173 - accuracy: 0.9181\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.2247 - accuracy: 0.9127\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2146 - accuracy: 0.9122\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2249 - accuracy: 0.9063\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.2118 - accuracy: 0.9190\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.2286 - accuracy: 0.9019\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2334 - accuracy: 0.9033\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2119 - accuracy: 0.9151\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2058 - accuracy: 0.9156\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2221 - accuracy: 0.9082\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.2192 - accuracy: 0.9082\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2078 - accuracy: 0.9171\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.2179 - accuracy: 0.9146\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.2174 - accuracy: 0.9136\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2141 - accuracy: 0.9141\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.2182 - accuracy: 0.9185\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.2299 - accuracy: 0.9107\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.2165 - accuracy: 0.9156\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.2166 - accuracy: 0.9132\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2345 - accuracy: 0.9058\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 818us/step - loss: 0.2252 - accuracy: 0.9078\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.2058 - accuracy: 0.9176\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.2111 - accuracy: 0.9161\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.2171 - accuracy: 0.9087\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.2199 - accuracy: 0.9102\n",
      "Epoch 331/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2012 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 301.\n",
      "32/32 [==============================] - 0s 897us/step - loss: 0.2178 - accuracy: 0.9136\n",
      "Epoch 331: early stopping\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.4890 - accuracy: 0.8033\n",
      "10/10 [==============================] - 0s 566us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "After appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Results - Loss: 0.4889756441116333, Accuracy: 0.8032786846160889, Precision: 0.7668690180786956, Recall: 0.772167154410145, F1 Score: 0.7687456343086808\n",
      "Confusion Matrix:\n",
      " [[181   3  30]\n",
      " [  4  32   0]\n",
      " [ 23   0  32]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 84, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'037A'}\n",
      "Moved to Test Set:\n",
      "{'037A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "782\n",
      "Length of y_train_val:\n",
      "782\n",
      "Length of groups_train_val:\n",
      "782\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     501\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     87\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1002, 2: 825, 1: 580})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.2620 - accuracy: 0.4724\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.0146 - accuracy: 0.5550\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.9256 - accuracy: 0.6132\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.8901 - accuracy: 0.6327\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8495 - accuracy: 0.6510\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.8082 - accuracy: 0.6602\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.7919 - accuracy: 0.6622\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.7738 - accuracy: 0.6668\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.7429 - accuracy: 0.7025\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.7444 - accuracy: 0.6847\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.7093 - accuracy: 0.7029\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.7100 - accuracy: 0.7017\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.6777 - accuracy: 0.7092\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.6824 - accuracy: 0.7113\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.6517 - accuracy: 0.7250\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.6648 - accuracy: 0.7158\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.6411 - accuracy: 0.7345\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.6330 - accuracy: 0.7387\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.6161 - accuracy: 0.7374\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.6113 - accuracy: 0.7420\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.6199 - accuracy: 0.7462\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.6174 - accuracy: 0.7354\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.5740 - accuracy: 0.7524\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.5880 - accuracy: 0.7491\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.5888 - accuracy: 0.7432\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.5823 - accuracy: 0.7570\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.5607 - accuracy: 0.7640\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.5605 - accuracy: 0.7549\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.5617 - accuracy: 0.7673\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.5579 - accuracy: 0.7673\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.5356 - accuracy: 0.7640\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.5573 - accuracy: 0.7628\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.5206 - accuracy: 0.7786\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.5343 - accuracy: 0.7665\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5542 - accuracy: 0.7673\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.5104 - accuracy: 0.7889\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.5102 - accuracy: 0.7910\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.5207 - accuracy: 0.7682\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.5030 - accuracy: 0.7835\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.5019 - accuracy: 0.7877\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.5049 - accuracy: 0.7877\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.5045 - accuracy: 0.7819\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.7811\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.4976 - accuracy: 0.7873\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.4870 - accuracy: 0.7956\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7877\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.7989\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.8022\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4803 - accuracy: 0.7977\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 980us/step - loss: 0.4844 - accuracy: 0.7881\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 965us/step - loss: 0.4584 - accuracy: 0.8064\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.4638 - accuracy: 0.8039\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.4554 - accuracy: 0.7973\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 976us/step - loss: 0.4647 - accuracy: 0.8056\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.4608 - accuracy: 0.7977\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4500 - accuracy: 0.8010\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.4601 - accuracy: 0.8072\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.4506 - accuracy: 0.8031\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4512 - accuracy: 0.8031\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.4419 - accuracy: 0.8022\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.4371 - accuracy: 0.8222\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.8064\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.8164\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.4406 - accuracy: 0.8180\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.4230 - accuracy: 0.8238\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.4477 - accuracy: 0.8130\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.4306 - accuracy: 0.8126\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.4239 - accuracy: 0.8168\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 958us/step - loss: 0.4166 - accuracy: 0.8226\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 977us/step - loss: 0.4266 - accuracy: 0.8106\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.4165 - accuracy: 0.8122\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.4070 - accuracy: 0.8297\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4089 - accuracy: 0.8226\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.4158 - accuracy: 0.8234\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4019 - accuracy: 0.8317\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.4103 - accuracy: 0.8326\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.4063 - accuracy: 0.8309\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8309\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8276\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4068 - accuracy: 0.8272\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8334\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.3886 - accuracy: 0.8413\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3932 - accuracy: 0.8367\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.4052 - accuracy: 0.8326\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.4033 - accuracy: 0.8288\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3778 - accuracy: 0.8421\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3822 - accuracy: 0.8334\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3883 - accuracy: 0.8380\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3818 - accuracy: 0.8392\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.3815 - accuracy: 0.8388\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3833 - accuracy: 0.8430\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3784 - accuracy: 0.8450\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3967 - accuracy: 0.8338\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3727 - accuracy: 0.8442\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.3852 - accuracy: 0.8342\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3725 - accuracy: 0.8459\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3766 - accuracy: 0.8425\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.3769 - accuracy: 0.8409\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8471\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 928us/step - loss: 0.3530 - accuracy: 0.8538\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.3559 - accuracy: 0.8521\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.3631 - accuracy: 0.8538\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.3740 - accuracy: 0.8371\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3618 - accuracy: 0.8525\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3649 - accuracy: 0.8484\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.3490 - accuracy: 0.8496\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3568 - accuracy: 0.8604\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3644 - accuracy: 0.8430\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.3540 - accuracy: 0.8513\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3510 - accuracy: 0.8467\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.3503 - accuracy: 0.8575\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.3476 - accuracy: 0.8550\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3308 - accuracy: 0.8716\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3365 - accuracy: 0.8650\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3406 - accuracy: 0.8637\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3387 - accuracy: 0.8604\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.3540 - accuracy: 0.8587\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.3322 - accuracy: 0.8650\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8521\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8571\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8617\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8637\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.3316 - accuracy: 0.8650\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.3365 - accuracy: 0.8550\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8542\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8654\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.3276 - accuracy: 0.8691\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.3268 - accuracy: 0.8625\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.3358 - accuracy: 0.8650\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3326 - accuracy: 0.8600\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.3189 - accuracy: 0.8725\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3249 - accuracy: 0.8662\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 987us/step - loss: 0.3155 - accuracy: 0.8683\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.3355 - accuracy: 0.8612\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8799\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.3211 - accuracy: 0.8683\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.3239 - accuracy: 0.8695\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3189 - accuracy: 0.8695\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3179 - accuracy: 0.8671\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3144 - accuracy: 0.8633\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3028 - accuracy: 0.8833\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.3231 - accuracy: 0.8720\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3239 - accuracy: 0.8608\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3129 - accuracy: 0.8695\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2919 - accuracy: 0.8833\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3200 - accuracy: 0.8695\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3161 - accuracy: 0.8671\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3053 - accuracy: 0.8733\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3007 - accuracy: 0.8795\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3103 - accuracy: 0.8762\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3058 - accuracy: 0.8787\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2945 - accuracy: 0.8816\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.3077 - accuracy: 0.8803\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3086 - accuracy: 0.8779\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2943 - accuracy: 0.8754\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2928 - accuracy: 0.8816\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3161 - accuracy: 0.8737\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3070 - accuracy: 0.8766\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2932 - accuracy: 0.8766\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2992 - accuracy: 0.8779\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2903 - accuracy: 0.8783\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2808 - accuracy: 0.8874\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.3061 - accuracy: 0.8787\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2961 - accuracy: 0.8733\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2931 - accuracy: 0.8779\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2966 - accuracy: 0.8866\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.2883 - accuracy: 0.8837\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.8882\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2920 - accuracy: 0.8754\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2981 - accuracy: 0.8745\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8808\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.8895\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8853\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8824\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2842 - accuracy: 0.8828\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.2734 - accuracy: 0.8837\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 992us/step - loss: 0.2711 - accuracy: 0.8874\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.2957 - accuracy: 0.8774\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.2683 - accuracy: 0.8912\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2732 - accuracy: 0.8916\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.2826 - accuracy: 0.8862\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 992us/step - loss: 0.2864 - accuracy: 0.8837\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.2778 - accuracy: 0.8820\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.2689 - accuracy: 0.8945\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.2644 - accuracy: 0.8966\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 928us/step - loss: 0.2801 - accuracy: 0.8824\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.8945\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8891\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2770 - accuracy: 0.8841\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 945us/step - loss: 0.2918 - accuracy: 0.8766\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.2560 - accuracy: 0.8986\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8970\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.2805 - accuracy: 0.8912\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.2801 - accuracy: 0.8841\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2683 - accuracy: 0.8903\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.2625 - accuracy: 0.8953\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.2519 - accuracy: 0.8982\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2563 - accuracy: 0.9044\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.2534 - accuracy: 0.8949\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.2520 - accuracy: 0.8928\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 991us/step - loss: 0.2589 - accuracy: 0.8995\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.8903\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.8882\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8891\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.8949\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.9003\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 994us/step - loss: 0.2589 - accuracy: 0.8995\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.8999\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.8999\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2518 - accuracy: 0.9036\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.2520 - accuracy: 0.8995\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2561 - accuracy: 0.8986\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2578 - accuracy: 0.8945\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.2513 - accuracy: 0.8949\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2493 - accuracy: 0.8957\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2482 - accuracy: 0.9053\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2527 - accuracy: 0.9024\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.2629 - accuracy: 0.8982\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.8912\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.9061\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.8957\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9069\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2379 - accuracy: 0.9032\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.8936\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.9020\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9028\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.9036\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.8995\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2319 - accuracy: 0.9107\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9065\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2454 - accuracy: 0.9044\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2471 - accuracy: 0.9074\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2389 - accuracy: 0.9036\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2443 - accuracy: 0.9011\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2390 - accuracy: 0.9036\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.2444 - accuracy: 0.9036\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2429 - accuracy: 0.9074\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2152 - accuracy: 0.9182\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2411 - accuracy: 0.9115\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2400 - accuracy: 0.9049\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2441 - accuracy: 0.9053\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2209 - accuracy: 0.9157\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2555 - accuracy: 0.8936\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2274 - accuracy: 0.9057\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2297 - accuracy: 0.9082\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2402 - accuracy: 0.8949\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2267 - accuracy: 0.9074\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2288 - accuracy: 0.9111\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2385 - accuracy: 0.9053\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2380 - accuracy: 0.9119\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2370 - accuracy: 0.9032\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2298 - accuracy: 0.9086\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.2381 - accuracy: 0.9015\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2345 - accuracy: 0.9082\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2141 - accuracy: 0.9173\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2307 - accuracy: 0.9003\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2304 - accuracy: 0.9082\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2077 - accuracy: 0.9173\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2274 - accuracy: 0.9057\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2248 - accuracy: 0.9103\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9144\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.9036\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.2331 - accuracy: 0.9078\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2190 - accuracy: 0.9090\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2104 - accuracy: 0.9161\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2417 - accuracy: 0.9090\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2218 - accuracy: 0.9103\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2233 - accuracy: 0.9098\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2178 - accuracy: 0.9177\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2226 - accuracy: 0.9044\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2172 - accuracy: 0.9123\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2127 - accuracy: 0.9169\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2160 - accuracy: 0.9128\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2107 - accuracy: 0.9136\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2162 - accuracy: 0.9169\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2126 - accuracy: 0.9206\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2286 - accuracy: 0.9053\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2195 - accuracy: 0.9107\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2195 - accuracy: 0.9094\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2205 - accuracy: 0.9152\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2138 - accuracy: 0.9107\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2282 - accuracy: 0.9082\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2048 - accuracy: 0.9186\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2134 - accuracy: 0.9169\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2092 - accuracy: 0.9132\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2059 - accuracy: 0.9190\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2101 - accuracy: 0.9190\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2138 - accuracy: 0.9165\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2249 - accuracy: 0.9111\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2276 - accuracy: 0.9132\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2324 - accuracy: 0.9057\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2060 - accuracy: 0.9194\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2029 - accuracy: 0.9198\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2112 - accuracy: 0.9132\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2143 - accuracy: 0.9173\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1967 - accuracy: 0.9219\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1985 - accuracy: 0.9190\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2131 - accuracy: 0.9115\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2091 - accuracy: 0.9161\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2150 - accuracy: 0.9144\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2028 - accuracy: 0.9186\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2088 - accuracy: 0.9194\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2057 - accuracy: 0.9211\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.2078 - accuracy: 0.9169\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2137 - accuracy: 0.9169\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.2046 - accuracy: 0.9161\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2093 - accuracy: 0.9165\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9194\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9190\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9198\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.2126 - accuracy: 0.9165\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2033 - accuracy: 0.9256\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.1929 - accuracy: 0.9227\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9219\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9236\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 980us/step - loss: 0.2104 - accuracy: 0.9157\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 995us/step - loss: 0.1968 - accuracy: 0.9194\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2099 - accuracy: 0.9136\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1844 - accuracy: 0.9277\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.2050 - accuracy: 0.9190\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9169\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.1994 - accuracy: 0.9136\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1938 - accuracy: 0.9244\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1990 - accuracy: 0.9215\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1871 - accuracy: 0.9215\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.1995 - accuracy: 0.9198\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9190\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9219\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9219\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9194\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9182\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9198\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9244\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.1902 - accuracy: 0.9236\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.1967 - accuracy: 0.9211\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.1828 - accuracy: 0.9344\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.1876 - accuracy: 0.9244\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.1984 - accuracy: 0.9281\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2073 - accuracy: 0.9144\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1970 - accuracy: 0.9202\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1767 - accuracy: 0.9260\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.1938 - accuracy: 0.9265\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1942 - accuracy: 0.9248\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2161 - accuracy: 0.9148\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.2229 - accuracy: 0.9148\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1876 - accuracy: 0.9290\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1765 - accuracy: 0.9344\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1762 - accuracy: 0.9285\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1976 - accuracy: 0.9206\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1861 - accuracy: 0.9269\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1973 - accuracy: 0.9211\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1881 - accuracy: 0.9227\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1931 - accuracy: 0.9231\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1986 - accuracy: 0.9169\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1907 - accuracy: 0.9290\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1830 - accuracy: 0.9314\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1805 - accuracy: 0.9256\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2035 - accuracy: 0.9152\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1918 - accuracy: 0.9260\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1819 - accuracy: 0.9339\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1797 - accuracy: 0.9277\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1841 - accuracy: 0.9290\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1970 - accuracy: 0.9202\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1854 - accuracy: 0.9290\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1928 - accuracy: 0.9182\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1898 - accuracy: 0.9186\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.1719 - accuracy: 0.9319\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1940 - accuracy: 0.9194\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.1669 - accuracy: 0.9377\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1857 - accuracy: 0.9294\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1930 - accuracy: 0.9294\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1947 - accuracy: 0.9211\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1746 - accuracy: 0.9344\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1868 - accuracy: 0.9252\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1843 - accuracy: 0.9273\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.1818 - accuracy: 0.9269\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.1853 - accuracy: 0.9290\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.1693 - accuracy: 0.9331\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9294\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.1809 - accuracy: 0.9236\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.1922 - accuracy: 0.9223\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1693 - accuracy: 0.9314\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1808 - accuracy: 0.9298\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1835 - accuracy: 0.9277\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1830 - accuracy: 0.9277\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1741 - accuracy: 0.9398\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1707 - accuracy: 0.9302\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1743 - accuracy: 0.9310\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 965us/step - loss: 0.1773 - accuracy: 0.9331\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.1776 - accuracy: 0.9260\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.1807 - accuracy: 0.9290\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1830 - accuracy: 0.9240\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.1741 - accuracy: 0.9335\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.1729 - accuracy: 0.9335\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.1815 - accuracy: 0.9339\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9298\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9319\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.1853 - accuracy: 0.9265\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1588 - accuracy: 0.9335\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1790 - accuracy: 0.9269\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1741 - accuracy: 0.9281\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1746 - accuracy: 0.9319\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1642 - accuracy: 0.9369\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1803 - accuracy: 0.9323\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1829 - accuracy: 0.9277\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1657 - accuracy: 0.9348\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1670 - accuracy: 0.9348\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1598 - accuracy: 0.9423\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1700 - accuracy: 0.9327\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1642 - accuracy: 0.9348\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1890 - accuracy: 0.9273\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1774 - accuracy: 0.9319\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1832 - accuracy: 0.9294\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1749 - accuracy: 0.9331\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1684 - accuracy: 0.9269\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1731 - accuracy: 0.9331\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1926 - accuracy: 0.9173\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1816 - accuracy: 0.9244\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1699 - accuracy: 0.9306\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1578 - accuracy: 0.9406\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.1742 - accuracy: 0.9298\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1690 - accuracy: 0.9302\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1652 - accuracy: 0.9356\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1621 - accuracy: 0.9356\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1581 - accuracy: 0.9406\n",
      "Epoch 426/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1593 - accuracy: 0.9360\n",
      "Epoch 427/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.1673 - accuracy: 0.9360\n",
      "Epoch 428/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1761 - accuracy: 0.9314\n",
      "Epoch 429/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1233 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 399.\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.1728 - accuracy: 0.9294\n",
      "Epoch 429: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7935\n",
      "5/5 [==============================] - 0s 714us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "After appending - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n",
      "Final Test Results - Loss: 0.5299147963523865, Accuracy: 0.7935484051704407, Precision: 0.7109225874867445, Recall: 0.7678643195884577, F1 Score: 0.7245703014933785\n",
      "Confusion Matrix:\n",
      " [[69  5 13]\n",
      " [ 9 45  1]\n",
      " [ 4  0  9]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7007187547686139\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7290340512990952\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7354828268289566\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6949106387436289\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7213971677855934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.77 (85/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, kitten]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, adult, kitten, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, adult...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, kitten, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, adult, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, adult]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, kitten, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, senior, adult, senior, senior,...        senior           senior                   True\n",
       "64    058A                           [senior, senior, kitten]        senior           senior                   True\n",
       "63    057A  [adult, adult, senior, senior, senior, senior,...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, senior, adult, senior, ...        senior           senior                   True\n",
       "59    053A       [adult, senior, adult, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "1     001A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, adult, kitten, kitten, adult, ...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, adult...        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, adult, senior, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, senior, adult, adult, kitten, ...         adult            adult                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "50    044A           [kitten, senior, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "48    042A  [adult, kitten, kitten, adult, kitten, adult, ...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A        [adult, adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A  [senior, senior, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "42    036A  [adult, senior, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "40    034A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "109   117A  [senior, senior, adult, adult, senior, adult, ...        senior           senior                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "66    060A                            [kitten, kitten, adult]        kitten            adult                  False\n",
       "57    051B  [senior, adult, kitten, adult, adult, adult, a...         adult           senior                  False\n",
       "30    025C             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "101   106A  [adult, adult, senior, senior, adult, adult, a...         adult           senior                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "56    051A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False\n",
       "34    027A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "12    011A                                    [adult, senior]         adult           senior                  False\n",
       "69    063A  [adult, senior, senior, kitten, senior, senior...        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "74    068A  [adult, senior, adult, senior, senior, adult, ...        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, adult, adult, ad...         adult           senior                  False\n",
       "89    094A  [senior, senior, adult, adult, senior, adult, ...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "19    018A                                   [senior, senior]        senior            adult                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     59\n",
      "kitten    13\n",
      "senior    13\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             59  80.821918\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             13  59.090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            436  78.558559\n",
      "1           kitten          136            110  80.882353\n",
      "2           senior          178             96  53.932584\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      168     78.87\n",
      "1          M    337      230     68.25\n",
      "2          X    319      244     76.49\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 2: 715, 1: 490})\n",
      "Epoch 1/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.1703 - accuracy: 0.4852\n",
      "Epoch 2/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0019 - accuracy: 0.5682\n",
      "Epoch 3/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.9426 - accuracy: 0.5958\n",
      "Epoch 4/1500\n",
      "33/33 [==============================] - 0s 900us/step - loss: 0.9068 - accuracy: 0.5978\n",
      "Epoch 5/1500\n",
      "33/33 [==============================] - 0s 821us/step - loss: 0.8613 - accuracy: 0.6342\n",
      "Epoch 6/1500\n",
      "33/33 [==============================] - 0s 933us/step - loss: 0.8306 - accuracy: 0.6434\n",
      "Epoch 7/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.8124 - accuracy: 0.6482\n",
      "Epoch 8/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.7593 - accuracy: 0.6846\n",
      "Epoch 9/1500\n",
      "33/33 [==============================] - 0s 906us/step - loss: 0.7557 - accuracy: 0.6691\n",
      "Epoch 10/1500\n",
      "33/33 [==============================] - 0s 912us/step - loss: 0.7347 - accuracy: 0.6778\n",
      "Epoch 11/1500\n",
      "33/33 [==============================] - 0s 899us/step - loss: 0.7268 - accuracy: 0.6938\n",
      "Epoch 12/1500\n",
      "33/33 [==============================] - 0s 944us/step - loss: 0.7286 - accuracy: 0.6987\n",
      "Epoch 13/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.7155 - accuracy: 0.6929\n",
      "Epoch 14/1500\n",
      "33/33 [==============================] - 0s 989us/step - loss: 0.6996 - accuracy: 0.6972\n",
      "Epoch 15/1500\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.6661 - accuracy: 0.7215\n",
      "Epoch 16/1500\n",
      "33/33 [==============================] - 0s 956us/step - loss: 0.6650 - accuracy: 0.7060\n",
      "Epoch 17/1500\n",
      "33/33 [==============================] - 0s 986us/step - loss: 0.6836 - accuracy: 0.7089\n",
      "Epoch 18/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.6496 - accuracy: 0.7171\n",
      "Epoch 19/1500\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.6627 - accuracy: 0.7249\n",
      "Epoch 20/1500\n",
      "33/33 [==============================] - 0s 868us/step - loss: 0.6402 - accuracy: 0.7230\n",
      "Epoch 21/1500\n",
      "33/33 [==============================] - 0s 856us/step - loss: 0.6353 - accuracy: 0.7317\n",
      "Epoch 22/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.6359 - accuracy: 0.7273\n",
      "Epoch 23/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.6064 - accuracy: 0.7356\n",
      "Epoch 24/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.6078 - accuracy: 0.7370\n",
      "Epoch 25/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.5959 - accuracy: 0.7487\n",
      "Epoch 26/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.5834 - accuracy: 0.7501\n",
      "Epoch 27/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.5780 - accuracy: 0.7525\n",
      "Epoch 28/1500\n",
      "33/33 [==============================] - 0s 827us/step - loss: 0.5843 - accuracy: 0.7462\n",
      "Epoch 29/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.5838 - accuracy: 0.7589\n",
      "Epoch 30/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.5752 - accuracy: 0.7540\n",
      "Epoch 31/1500\n",
      "33/33 [==============================] - 0s 806us/step - loss: 0.5407 - accuracy: 0.7715\n",
      "Epoch 32/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5777 - accuracy: 0.7579\n",
      "Epoch 33/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5498 - accuracy: 0.7686\n",
      "Epoch 34/1500\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.5424 - accuracy: 0.7676\n",
      "Epoch 35/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7715\n",
      "Epoch 36/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.7569\n",
      "Epoch 37/1500\n",
      "33/33 [==============================] - 0s 923us/step - loss: 0.5436 - accuracy: 0.7642\n",
      "Epoch 38/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.5262 - accuracy: 0.7754\n",
      "Epoch 39/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.5379 - accuracy: 0.7724\n",
      "Epoch 40/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.5121 - accuracy: 0.7763\n",
      "Epoch 41/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.5228 - accuracy: 0.7686\n",
      "Epoch 42/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.5240 - accuracy: 0.7734\n",
      "Epoch 43/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.5183 - accuracy: 0.7821\n",
      "Epoch 44/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.5156 - accuracy: 0.7758\n",
      "Epoch 45/1500\n",
      "33/33 [==============================] - 0s 846us/step - loss: 0.5111 - accuracy: 0.7855\n",
      "Epoch 46/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.5294 - accuracy: 0.7758\n",
      "Epoch 47/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.4948 - accuracy: 0.7807\n",
      "Epoch 48/1500\n",
      "33/33 [==============================] - 0s 833us/step - loss: 0.4998 - accuracy: 0.7865\n",
      "Epoch 49/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.5126 - accuracy: 0.7773\n",
      "Epoch 50/1500\n",
      "33/33 [==============================] - 0s 840us/step - loss: 0.4983 - accuracy: 0.7865\n",
      "Epoch 51/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.4823 - accuracy: 0.7923\n",
      "Epoch 52/1500\n",
      "33/33 [==============================] - 0s 819us/step - loss: 0.4907 - accuracy: 0.7899\n",
      "Epoch 53/1500\n",
      "33/33 [==============================] - 0s 843us/step - loss: 0.4955 - accuracy: 0.7841\n",
      "Epoch 54/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.4743 - accuracy: 0.7957\n",
      "Epoch 55/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.4788 - accuracy: 0.7957\n",
      "Epoch 56/1500\n",
      "33/33 [==============================] - 0s 862us/step - loss: 0.4884 - accuracy: 0.7773\n",
      "Epoch 57/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.4769 - accuracy: 0.8025\n",
      "Epoch 58/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.4687 - accuracy: 0.8045\n",
      "Epoch 59/1500\n",
      "33/33 [==============================] - 0s 843us/step - loss: 0.4617 - accuracy: 0.7977\n",
      "Epoch 60/1500\n",
      "33/33 [==============================] - 0s 834us/step - loss: 0.4777 - accuracy: 0.8011\n",
      "Epoch 61/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.4570 - accuracy: 0.8006\n",
      "Epoch 62/1500\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.4774 - accuracy: 0.7957\n",
      "Epoch 63/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.4714 - accuracy: 0.7904\n",
      "Epoch 64/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.4528 - accuracy: 0.8093\n",
      "Epoch 65/1500\n",
      "33/33 [==============================] - 0s 840us/step - loss: 0.4588 - accuracy: 0.7967\n",
      "Epoch 66/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.4658 - accuracy: 0.8035\n",
      "Epoch 67/1500\n",
      "33/33 [==============================] - 0s 932us/step - loss: 0.4487 - accuracy: 0.8069\n",
      "Epoch 68/1500\n",
      "33/33 [==============================] - 0s 996us/step - loss: 0.4392 - accuracy: 0.8074\n",
      "Epoch 69/1500\n",
      "33/33 [==============================] - 0s 978us/step - loss: 0.4529 - accuracy: 0.8035\n",
      "Epoch 70/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.8040\n",
      "Epoch 71/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8142\n",
      "Epoch 72/1500\n",
      "33/33 [==============================] - 0s 895us/step - loss: 0.4533 - accuracy: 0.8088\n",
      "Epoch 73/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.4174 - accuracy: 0.8292\n",
      "Epoch 74/1500\n",
      "33/33 [==============================] - 0s 929us/step - loss: 0.4305 - accuracy: 0.8219\n",
      "Epoch 75/1500\n",
      "33/33 [==============================] - 0s 866us/step - loss: 0.4251 - accuracy: 0.8205\n",
      "Epoch 76/1500\n",
      "33/33 [==============================] - 0s 916us/step - loss: 0.4380 - accuracy: 0.8117\n",
      "Epoch 77/1500\n",
      "33/33 [==============================] - 0s 906us/step - loss: 0.4103 - accuracy: 0.8224\n",
      "Epoch 78/1500\n",
      "33/33 [==============================] - 0s 920us/step - loss: 0.4221 - accuracy: 0.8161\n",
      "Epoch 79/1500\n",
      "33/33 [==============================] - 0s 986us/step - loss: 0.4394 - accuracy: 0.8142\n",
      "Epoch 80/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4197 - accuracy: 0.8195\n",
      "Epoch 81/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8195\n",
      "Epoch 82/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8219\n",
      "Epoch 83/1500\n",
      "33/33 [==============================] - 0s 959us/step - loss: 0.4197 - accuracy: 0.8253\n",
      "Epoch 84/1500\n",
      "33/33 [==============================] - 0s 909us/step - loss: 0.4151 - accuracy: 0.8180\n",
      "Epoch 85/1500\n",
      "33/33 [==============================] - 0s 886us/step - loss: 0.4265 - accuracy: 0.8244\n",
      "Epoch 86/1500\n",
      "33/33 [==============================] - 0s 917us/step - loss: 0.4109 - accuracy: 0.8248\n",
      "Epoch 87/1500\n",
      "33/33 [==============================] - 0s 903us/step - loss: 0.3972 - accuracy: 0.8341\n",
      "Epoch 88/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.4025 - accuracy: 0.8292\n",
      "Epoch 89/1500\n",
      "33/33 [==============================] - 0s 954us/step - loss: 0.4199 - accuracy: 0.8151\n",
      "Epoch 90/1500\n",
      "33/33 [==============================] - 0s 905us/step - loss: 0.4016 - accuracy: 0.8326\n",
      "Epoch 91/1500\n",
      "33/33 [==============================] - 0s 878us/step - loss: 0.3924 - accuracy: 0.8331\n",
      "Epoch 92/1500\n",
      "33/33 [==============================] - 0s 876us/step - loss: 0.3883 - accuracy: 0.8345\n",
      "Epoch 93/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3982 - accuracy: 0.8389\n",
      "Epoch 94/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.4132 - accuracy: 0.8248\n",
      "Epoch 95/1500\n",
      "33/33 [==============================] - 0s 915us/step - loss: 0.3961 - accuracy: 0.8273\n",
      "Epoch 96/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8379\n",
      "Epoch 97/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8360\n",
      "Epoch 98/1500\n",
      "33/33 [==============================] - 0s 950us/step - loss: 0.3952 - accuracy: 0.8336\n",
      "Epoch 99/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.3975 - accuracy: 0.8316\n",
      "Epoch 100/1500\n",
      "33/33 [==============================] - 0s 912us/step - loss: 0.3937 - accuracy: 0.8287\n",
      "Epoch 101/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3807 - accuracy: 0.8472\n",
      "Epoch 102/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.3976 - accuracy: 0.8282\n",
      "Epoch 103/1500\n",
      "33/33 [==============================] - 0s 893us/step - loss: 0.3795 - accuracy: 0.8491\n",
      "Epoch 104/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8316\n",
      "Epoch 105/1500\n",
      "33/33 [==============================] - 0s 929us/step - loss: 0.3794 - accuracy: 0.8438\n",
      "Epoch 106/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.3665 - accuracy: 0.8394\n",
      "Epoch 107/1500\n",
      "33/33 [==============================] - 0s 864us/step - loss: 0.3797 - accuracy: 0.8365\n",
      "Epoch 108/1500\n",
      "33/33 [==============================] - 0s 846us/step - loss: 0.3676 - accuracy: 0.8428\n",
      "Epoch 109/1500\n",
      "33/33 [==============================] - 0s 859us/step - loss: 0.3750 - accuracy: 0.8409\n",
      "Epoch 110/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.3504 - accuracy: 0.8569\n",
      "Epoch 111/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3722 - accuracy: 0.8486\n",
      "Epoch 112/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.3730 - accuracy: 0.8462\n",
      "Epoch 113/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3754 - accuracy: 0.8384\n",
      "Epoch 114/1500\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.3589 - accuracy: 0.8462\n",
      "Epoch 115/1500\n",
      "33/33 [==============================] - 0s 858us/step - loss: 0.3571 - accuracy: 0.8525\n",
      "Epoch 116/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.3645 - accuracy: 0.8428\n",
      "Epoch 117/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.3903 - accuracy: 0.8336\n",
      "Epoch 118/1500\n",
      "33/33 [==============================] - 0s 854us/step - loss: 0.3582 - accuracy: 0.8467\n",
      "Epoch 119/1500\n",
      "33/33 [==============================] - 0s 872us/step - loss: 0.3586 - accuracy: 0.8612\n",
      "Epoch 120/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3629 - accuracy: 0.8423\n",
      "Epoch 121/1500\n",
      "33/33 [==============================] - 0s 864us/step - loss: 0.3560 - accuracy: 0.8496\n",
      "Epoch 122/1500\n",
      "33/33 [==============================] - 0s 859us/step - loss: 0.3628 - accuracy: 0.8525\n",
      "Epoch 123/1500\n",
      "33/33 [==============================] - 0s 820us/step - loss: 0.3619 - accuracy: 0.8428\n",
      "Epoch 124/1500\n",
      "33/33 [==============================] - 0s 930us/step - loss: 0.3596 - accuracy: 0.8525\n",
      "Epoch 125/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.3437 - accuracy: 0.8496\n",
      "Epoch 126/1500\n",
      "33/33 [==============================] - 0s 984us/step - loss: 0.3541 - accuracy: 0.8515\n",
      "Epoch 127/1500\n",
      "33/33 [==============================] - 0s 899us/step - loss: 0.3677 - accuracy: 0.8491\n",
      "Epoch 128/1500\n",
      "33/33 [==============================] - 0s 936us/step - loss: 0.3495 - accuracy: 0.8462\n",
      "Epoch 129/1500\n",
      "33/33 [==============================] - 0s 910us/step - loss: 0.3509 - accuracy: 0.8632\n",
      "Epoch 130/1500\n",
      "33/33 [==============================] - 0s 937us/step - loss: 0.3403 - accuracy: 0.8622\n",
      "Epoch 131/1500\n",
      "33/33 [==============================] - 0s 905us/step - loss: 0.3415 - accuracy: 0.8588\n",
      "Epoch 132/1500\n",
      "33/33 [==============================] - 0s 916us/step - loss: 0.3236 - accuracy: 0.8685\n",
      "Epoch 133/1500\n",
      "33/33 [==============================] - 0s 955us/step - loss: 0.3362 - accuracy: 0.8569\n",
      "Epoch 134/1500\n",
      "33/33 [==============================] - 0s 902us/step - loss: 0.3439 - accuracy: 0.8520\n",
      "Epoch 135/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.3460 - accuracy: 0.8578\n",
      "Epoch 136/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8680\n",
      "Epoch 137/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8656\n",
      "Epoch 138/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8593\n",
      "Epoch 139/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8656\n",
      "Epoch 140/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8627\n",
      "Epoch 141/1500\n",
      "33/33 [==============================] - 0s 960us/step - loss: 0.3243 - accuracy: 0.8675\n",
      "Epoch 142/1500\n",
      "33/33 [==============================] - 0s 968us/step - loss: 0.3425 - accuracy: 0.8554\n",
      "Epoch 143/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8700\n",
      "Epoch 144/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.8641\n",
      "Epoch 145/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3286 - accuracy: 0.8729\n",
      "Epoch 146/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8637\n",
      "Epoch 147/1500\n",
      "33/33 [==============================] - 0s 938us/step - loss: 0.3150 - accuracy: 0.8695\n",
      "Epoch 148/1500\n",
      "33/33 [==============================] - 0s 963us/step - loss: 0.3263 - accuracy: 0.8724\n",
      "Epoch 149/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.3296 - accuracy: 0.8549\n",
      "Epoch 150/1500\n",
      "33/33 [==============================] - 0s 885us/step - loss: 0.3106 - accuracy: 0.8772\n",
      "Epoch 151/1500\n",
      "33/33 [==============================] - 0s 941us/step - loss: 0.3457 - accuracy: 0.8481\n",
      "Epoch 152/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3302 - accuracy: 0.8593\n",
      "Epoch 153/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8651\n",
      "Epoch 154/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8690\n",
      "Epoch 155/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8782\n",
      "Epoch 156/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3038 - accuracy: 0.8738\n",
      "Epoch 157/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8753\n",
      "Epoch 158/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8714\n",
      "Epoch 159/1500\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8690\n",
      "Epoch 160/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3178 - accuracy: 0.8748\n",
      "Epoch 161/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8719\n",
      "Epoch 162/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8743\n",
      "Epoch 163/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3129 - accuracy: 0.8705\n",
      "Epoch 164/1500\n",
      "33/33 [==============================] - 0s 919us/step - loss: 0.3128 - accuracy: 0.8777\n",
      "Epoch 165/1500\n",
      "33/33 [==============================] - 0s 925us/step - loss: 0.3215 - accuracy: 0.8714\n",
      "Epoch 166/1500\n",
      "33/33 [==============================] - 0s 991us/step - loss: 0.2969 - accuracy: 0.8772\n",
      "Epoch 167/1500\n",
      "33/33 [==============================] - 0s 919us/step - loss: 0.3143 - accuracy: 0.8680\n",
      "Epoch 168/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8772\n",
      "Epoch 169/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8869\n",
      "Epoch 170/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.8705\n",
      "Epoch 171/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8753\n",
      "Epoch 172/1500\n",
      "33/33 [==============================] - 0s 985us/step - loss: 0.3035 - accuracy: 0.8729\n",
      "Epoch 173/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2982 - accuracy: 0.8836\n",
      "Epoch 174/1500\n",
      "33/33 [==============================] - 0s 879us/step - loss: 0.2972 - accuracy: 0.8836\n",
      "Epoch 175/1500\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.2814 - accuracy: 0.8928\n",
      "Epoch 176/1500\n",
      "33/33 [==============================] - 0s 805us/step - loss: 0.3003 - accuracy: 0.8802\n",
      "Epoch 177/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.2982 - accuracy: 0.8797\n",
      "Epoch 178/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.3087 - accuracy: 0.8758\n",
      "Epoch 179/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.3002 - accuracy: 0.8806\n",
      "Epoch 180/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.3001 - accuracy: 0.8802\n",
      "Epoch 181/1500\n",
      "33/33 [==============================] - 0s 843us/step - loss: 0.3072 - accuracy: 0.8705\n",
      "Epoch 182/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.2910 - accuracy: 0.8884\n",
      "Epoch 183/1500\n",
      "33/33 [==============================] - 0s 900us/step - loss: 0.2868 - accuracy: 0.8826\n",
      "Epoch 184/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.8869\n",
      "Epoch 185/1500\n",
      "33/33 [==============================] - 0s 918us/step - loss: 0.2794 - accuracy: 0.8889\n",
      "Epoch 186/1500\n",
      "33/33 [==============================] - 0s 907us/step - loss: 0.2740 - accuracy: 0.8860\n",
      "Epoch 187/1500\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.3067 - accuracy: 0.8865\n",
      "Epoch 188/1500\n",
      "33/33 [==============================] - 0s 945us/step - loss: 0.2795 - accuracy: 0.8933\n",
      "Epoch 189/1500\n",
      "33/33 [==============================] - 0s 995us/step - loss: 0.2762 - accuracy: 0.8869\n",
      "Epoch 190/1500\n",
      "33/33 [==============================] - 0s 930us/step - loss: 0.3046 - accuracy: 0.8821\n",
      "Epoch 191/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8869\n",
      "Epoch 192/1500\n",
      "33/33 [==============================] - 0s 926us/step - loss: 0.2715 - accuracy: 0.8879\n",
      "Epoch 193/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2783 - accuracy: 0.8884\n",
      "Epoch 194/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.2938 - accuracy: 0.8826\n",
      "Epoch 195/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.2873 - accuracy: 0.8850\n",
      "Epoch 196/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.2851 - accuracy: 0.8845\n",
      "Epoch 197/1500\n",
      "33/33 [==============================] - 0s 898us/step - loss: 0.2763 - accuracy: 0.8845\n",
      "Epoch 198/1500\n",
      "33/33 [==============================] - 0s 946us/step - loss: 0.2896 - accuracy: 0.8763\n",
      "Epoch 199/1500\n",
      "33/33 [==============================] - 0s 985us/step - loss: 0.2838 - accuracy: 0.8874\n",
      "Epoch 200/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.8952\n",
      "Epoch 201/1500\n",
      "33/33 [==============================] - 0s 970us/step - loss: 0.2794 - accuracy: 0.8884\n",
      "Epoch 202/1500\n",
      "33/33 [==============================] - 0s 961us/step - loss: 0.2591 - accuracy: 0.8937\n",
      "Epoch 203/1500\n",
      "33/33 [==============================] - 0s 932us/step - loss: 0.2741 - accuracy: 0.8971\n",
      "Epoch 204/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.8933\n",
      "Epoch 205/1500\n",
      "33/33 [==============================] - 0s 900us/step - loss: 0.2928 - accuracy: 0.8787\n",
      "Epoch 206/1500\n",
      "33/33 [==============================] - 0s 938us/step - loss: 0.2800 - accuracy: 0.8836\n",
      "Epoch 207/1500\n",
      "33/33 [==============================] - 0s 986us/step - loss: 0.2607 - accuracy: 0.8962\n",
      "Epoch 208/1500\n",
      "33/33 [==============================] - 0s 939us/step - loss: 0.2803 - accuracy: 0.8952\n",
      "Epoch 209/1500\n",
      "33/33 [==============================] - 0s 952us/step - loss: 0.2567 - accuracy: 0.8981\n",
      "Epoch 210/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2671 - accuracy: 0.8942\n",
      "Epoch 211/1500\n",
      "33/33 [==============================] - 0s 882us/step - loss: 0.2608 - accuracy: 0.9049\n",
      "Epoch 212/1500\n",
      "33/33 [==============================] - 0s 847us/step - loss: 0.2696 - accuracy: 0.8884\n",
      "Epoch 213/1500\n",
      "33/33 [==============================] - 0s 829us/step - loss: 0.2789 - accuracy: 0.8967\n",
      "Epoch 214/1500\n",
      "33/33 [==============================] - 0s 863us/step - loss: 0.2706 - accuracy: 0.8874\n",
      "Epoch 215/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2750 - accuracy: 0.8840\n",
      "Epoch 216/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.2618 - accuracy: 0.8962\n",
      "Epoch 217/1500\n",
      "33/33 [==============================] - 0s 866us/step - loss: 0.2581 - accuracy: 0.9020\n",
      "Epoch 218/1500\n",
      "33/33 [==============================] - 0s 880us/step - loss: 0.2605 - accuracy: 0.8981\n",
      "Epoch 219/1500\n",
      "33/33 [==============================] - 0s 821us/step - loss: 0.2701 - accuracy: 0.8952\n",
      "Epoch 220/1500\n",
      "33/33 [==============================] - 0s 888us/step - loss: 0.2627 - accuracy: 0.9020\n",
      "Epoch 221/1500\n",
      "33/33 [==============================] - 0s 963us/step - loss: 0.2547 - accuracy: 0.9044\n",
      "Epoch 222/1500\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.2680 - accuracy: 0.8928\n",
      "Epoch 223/1500\n",
      "33/33 [==============================] - 0s 859us/step - loss: 0.2701 - accuracy: 0.8879\n",
      "Epoch 224/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.2726 - accuracy: 0.8908\n",
      "Epoch 225/1500\n",
      "33/33 [==============================] - 0s 808us/step - loss: 0.2569 - accuracy: 0.8957\n",
      "Epoch 226/1500\n",
      "33/33 [==============================] - 0s 867us/step - loss: 0.2636 - accuracy: 0.8865\n",
      "Epoch 227/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.2490 - accuracy: 0.8986\n",
      "Epoch 228/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2581 - accuracy: 0.8991\n",
      "Epoch 229/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2516 - accuracy: 0.8976\n",
      "Epoch 230/1500\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8923\n",
      "Epoch 231/1500\n",
      "33/33 [==============================] - 0s 918us/step - loss: 0.2663 - accuracy: 0.9000\n",
      "Epoch 232/1500\n",
      "33/33 [==============================] - 0s 922us/step - loss: 0.2523 - accuracy: 0.8971\n",
      "Epoch 233/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.2684 - accuracy: 0.8908\n",
      "Epoch 234/1500\n",
      "33/33 [==============================] - 0s 865us/step - loss: 0.2628 - accuracy: 0.8942\n",
      "Epoch 235/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.2609 - accuracy: 0.8962\n",
      "Epoch 236/1500\n",
      "33/33 [==============================] - 0s 817us/step - loss: 0.2508 - accuracy: 0.9015\n",
      "Epoch 237/1500\n",
      "33/33 [==============================] - 0s 920us/step - loss: 0.2544 - accuracy: 0.8991\n",
      "Epoch 238/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.2602 - accuracy: 0.9030\n",
      "Epoch 239/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9015\n",
      "Epoch 240/1500\n",
      "33/33 [==============================] - 0s 928us/step - loss: 0.2737 - accuracy: 0.8908\n",
      "Epoch 241/1500\n",
      "33/33 [==============================] - 0s 895us/step - loss: 0.2339 - accuracy: 0.9059\n",
      "Epoch 242/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.2747 - accuracy: 0.8884\n",
      "Epoch 243/1500\n",
      "33/33 [==============================] - 0s 907us/step - loss: 0.2586 - accuracy: 0.8947\n",
      "Epoch 244/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.2665 - accuracy: 0.8918\n",
      "Epoch 245/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2260 - accuracy: 0.9199\n",
      "Epoch 246/1500\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.2323 - accuracy: 0.9068\n",
      "Epoch 247/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.2414 - accuracy: 0.9034\n",
      "Epoch 248/1500\n",
      "33/33 [==============================] - 0s 854us/step - loss: 0.2536 - accuracy: 0.9049\n",
      "Epoch 249/1500\n",
      "33/33 [==============================] - 0s 827us/step - loss: 0.2395 - accuracy: 0.9039\n",
      "Epoch 250/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.2417 - accuracy: 0.9039\n",
      "Epoch 251/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.2613 - accuracy: 0.8947\n",
      "Epoch 252/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.2464 - accuracy: 0.9078\n",
      "Epoch 253/1500\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.2359 - accuracy: 0.9064\n",
      "Epoch 254/1500\n",
      "33/33 [==============================] - 0s 889us/step - loss: 0.2361 - accuracy: 0.9034\n",
      "Epoch 255/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2475 - accuracy: 0.9020\n",
      "Epoch 256/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.2431 - accuracy: 0.8996\n",
      "Epoch 257/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.2351 - accuracy: 0.9165\n",
      "Epoch 258/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.2604 - accuracy: 0.9025\n",
      "Epoch 259/1500\n",
      "33/33 [==============================] - 0s 866us/step - loss: 0.2344 - accuracy: 0.9098\n",
      "Epoch 260/1500\n",
      "33/33 [==============================] - 0s 803us/step - loss: 0.2338 - accuracy: 0.9127\n",
      "Epoch 261/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.2374 - accuracy: 0.9098\n",
      "Epoch 262/1500\n",
      "33/33 [==============================] - 0s 832us/step - loss: 0.2394 - accuracy: 0.9073\n",
      "Epoch 263/1500\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.2242 - accuracy: 0.9107\n",
      "Epoch 264/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.2305 - accuracy: 0.9098\n",
      "Epoch 265/1500\n",
      "33/33 [==============================] - 0s 987us/step - loss: 0.2442 - accuracy: 0.9054\n",
      "Epoch 266/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.2371 - accuracy: 0.9102\n",
      "Epoch 267/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.2176 - accuracy: 0.9141\n",
      "Epoch 268/1500\n",
      "33/33 [==============================] - 0s 886us/step - loss: 0.2237 - accuracy: 0.9093\n",
      "Epoch 269/1500\n",
      "33/33 [==============================] - 0s 851us/step - loss: 0.2267 - accuracy: 0.9122\n",
      "Epoch 270/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.2272 - accuracy: 0.9131\n",
      "Epoch 271/1500\n",
      "33/33 [==============================] - 0s 818us/step - loss: 0.2242 - accuracy: 0.9102\n",
      "Epoch 272/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.2366 - accuracy: 0.9068\n",
      "Epoch 273/1500\n",
      "33/33 [==============================] - 0s 823us/step - loss: 0.2278 - accuracy: 0.9098\n",
      "Epoch 274/1500\n",
      "33/33 [==============================] - 0s 845us/step - loss: 0.2470 - accuracy: 0.8967\n",
      "Epoch 275/1500\n",
      "33/33 [==============================] - 0s 852us/step - loss: 0.2262 - accuracy: 0.9044\n",
      "Epoch 276/1500\n",
      "33/33 [==============================] - 0s 944us/step - loss: 0.2383 - accuracy: 0.9030\n",
      "Epoch 277/1500\n",
      "33/33 [==============================] - 0s 972us/step - loss: 0.2225 - accuracy: 0.9088\n",
      "Epoch 278/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.2360 - accuracy: 0.9088\n",
      "Epoch 279/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.2318 - accuracy: 0.9098\n",
      "Epoch 280/1500\n",
      "33/33 [==============================] - 0s 937us/step - loss: 0.2242 - accuracy: 0.9190\n",
      "Epoch 281/1500\n",
      "33/33 [==============================] - 0s 925us/step - loss: 0.2130 - accuracy: 0.9204\n",
      "Epoch 282/1500\n",
      "33/33 [==============================] - 0s 880us/step - loss: 0.2206 - accuracy: 0.9127\n",
      "Epoch 283/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.2273 - accuracy: 0.9098\n",
      "Epoch 284/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2380 - accuracy: 0.9034\n",
      "Epoch 285/1500\n",
      "33/33 [==============================] - 0s 832us/step - loss: 0.2266 - accuracy: 0.9098\n",
      "Epoch 286/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.2185 - accuracy: 0.9131\n",
      "Epoch 287/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2049 - accuracy: 0.9204\n",
      "Epoch 288/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.2382 - accuracy: 0.8981\n",
      "Epoch 289/1500\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.2235 - accuracy: 0.9195\n",
      "Epoch 290/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.2293 - accuracy: 0.9146\n",
      "Epoch 291/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2269 - accuracy: 0.9102\n",
      "Epoch 292/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.2119 - accuracy: 0.9127\n",
      "Epoch 293/1500\n",
      "33/33 [==============================] - 0s 917us/step - loss: 0.2198 - accuracy: 0.9117\n",
      "Epoch 294/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.2374 - accuracy: 0.9025\n",
      "Epoch 295/1500\n",
      "33/33 [==============================] - 0s 852us/step - loss: 0.2081 - accuracy: 0.9195\n",
      "Epoch 296/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.2324 - accuracy: 0.9117\n",
      "Epoch 297/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2122 - accuracy: 0.9146\n",
      "Epoch 298/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.2153 - accuracy: 0.9161\n",
      "Epoch 299/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.2312 - accuracy: 0.9146\n",
      "Epoch 300/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2208 - accuracy: 0.9122\n",
      "Epoch 301/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.2377 - accuracy: 0.8962\n",
      "Epoch 302/1500\n",
      "33/33 [==============================] - 0s 828us/step - loss: 0.2216 - accuracy: 0.9093\n",
      "Epoch 303/1500\n",
      "33/33 [==============================] - 0s 819us/step - loss: 0.2147 - accuracy: 0.9170\n",
      "Epoch 304/1500\n",
      "33/33 [==============================] - 0s 879us/step - loss: 0.2257 - accuracy: 0.9117\n",
      "Epoch 305/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.2047 - accuracy: 0.9175\n",
      "Epoch 306/1500\n",
      "33/33 [==============================] - 0s 864us/step - loss: 0.2133 - accuracy: 0.9199\n",
      "Epoch 307/1500\n",
      "33/33 [==============================] - 0s 870us/step - loss: 0.2197 - accuracy: 0.9073\n",
      "Epoch 308/1500\n",
      "33/33 [==============================] - 0s 897us/step - loss: 0.2098 - accuracy: 0.9136\n",
      "Epoch 309/1500\n",
      "33/33 [==============================] - 0s 909us/step - loss: 0.1928 - accuracy: 0.9292\n",
      "Epoch 310/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.2421 - accuracy: 0.9044\n",
      "Epoch 311/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.1976 - accuracy: 0.9253\n",
      "Epoch 312/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.2043 - accuracy: 0.9204\n",
      "Epoch 313/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.2203 - accuracy: 0.9122\n",
      "Epoch 314/1500\n",
      "33/33 [==============================] - 0s 832us/step - loss: 0.2242 - accuracy: 0.9122\n",
      "Epoch 315/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.2041 - accuracy: 0.9224\n",
      "Epoch 316/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.2136 - accuracy: 0.9127\n",
      "Epoch 317/1500\n",
      "33/33 [==============================] - 0s 845us/step - loss: 0.2088 - accuracy: 0.9190\n",
      "Epoch 318/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.2162 - accuracy: 0.9175\n",
      "Epoch 319/1500\n",
      "33/33 [==============================] - 0s 803us/step - loss: 0.2183 - accuracy: 0.9136\n",
      "Epoch 320/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.1979 - accuracy: 0.9238\n",
      "Epoch 321/1500\n",
      "33/33 [==============================] - 0s 858us/step - loss: 0.2110 - accuracy: 0.9229\n",
      "Epoch 322/1500\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.1958 - accuracy: 0.9262\n",
      "Epoch 323/1500\n",
      "33/33 [==============================] - 0s 870us/step - loss: 0.2132 - accuracy: 0.9175\n",
      "Epoch 324/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2193 - accuracy: 0.9161\n",
      "Epoch 325/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2058 - accuracy: 0.9209\n",
      "Epoch 326/1500\n",
      "33/33 [==============================] - 0s 851us/step - loss: 0.2104 - accuracy: 0.9161\n",
      "Epoch 327/1500\n",
      "33/33 [==============================] - 0s 856us/step - loss: 0.2087 - accuracy: 0.9165\n",
      "Epoch 328/1500\n",
      "33/33 [==============================] - 0s 828us/step - loss: 0.1989 - accuracy: 0.9219\n",
      "Epoch 329/1500\n",
      "33/33 [==============================] - 0s 840us/step - loss: 0.1943 - accuracy: 0.9253\n",
      "Epoch 330/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1945 - accuracy: 0.9262\n",
      "Epoch 331/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2001 - accuracy: 0.9267\n",
      "Epoch 332/1500\n",
      "33/33 [==============================] - 0s 825us/step - loss: 0.1953 - accuracy: 0.9204\n",
      "Epoch 333/1500\n",
      "33/33 [==============================] - 0s 889us/step - loss: 0.2091 - accuracy: 0.9156\n",
      "Epoch 334/1500\n",
      "33/33 [==============================] - 0s 959us/step - loss: 0.2084 - accuracy: 0.9165\n",
      "Epoch 335/1500\n",
      "33/33 [==============================] - 0s 994us/step - loss: 0.2272 - accuracy: 0.9127\n",
      "Epoch 336/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.2004 - accuracy: 0.9229\n",
      "Epoch 337/1500\n",
      "33/33 [==============================] - 0s 847us/step - loss: 0.2038 - accuracy: 0.9170\n",
      "Epoch 338/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.1939 - accuracy: 0.9287\n",
      "Epoch 339/1500\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.2211 - accuracy: 0.8594Restoring model weights from the end of the best epoch: 309.\n",
      "33/33 [==============================] - 0s 954us/step - loss: 0.1980 - accuracy: 0.9190\n",
      "Epoch 339: early stopping\n",
      "9/9 [==============================] - 0s 748us/step - loss: 0.7026 - accuracy: 0.7015\n",
      "9/9 [==============================] - 0s 665us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.702578067779541, Accuracy: 0.7014925479888916, Precision: 0.7014230979748222, Recall: 0.669553163731246, F1 Score: 0.6559595890489059\n",
      "Confusion Matrix:\n",
      " [[124   2  34]\n",
      " [ 33  40   0]\n",
      " [ 11   0  24]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '041A' '042A'\n",
      " '044A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '040A'\n",
      " '043A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "741\n",
      "Length of y_train_val:\n",
      "741\n",
      "Length of groups_train_val:\n",
      "741\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     26\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 866, 2: 815, 1: 725})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1701 - accuracy: 0.4896\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8790 - accuracy: 0.6197\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.8037 - accuracy: 0.6604\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.7724 - accuracy: 0.6775\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.7278 - accuracy: 0.7070\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.6879 - accuracy: 0.7178\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.6838 - accuracy: 0.7207\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.6496 - accuracy: 0.7344\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.6250 - accuracy: 0.7481\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.6131 - accuracy: 0.7531\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.6243 - accuracy: 0.7481\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.5852 - accuracy: 0.7581\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.5738 - accuracy: 0.7614\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.5839 - accuracy: 0.7556\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5638 - accuracy: 0.7681\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.5491 - accuracy: 0.7664\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.5602 - accuracy: 0.7747\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5466 - accuracy: 0.7760\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.5183 - accuracy: 0.7889\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.4921 - accuracy: 0.8071\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.5171 - accuracy: 0.7930\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.5073 - accuracy: 0.7868\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.4953 - accuracy: 0.7909\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 984us/step - loss: 0.4826 - accuracy: 0.7993\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.5066 - accuracy: 0.7880\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.4847 - accuracy: 0.8009\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.4880 - accuracy: 0.8009\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.4815 - accuracy: 0.8059\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.4686 - accuracy: 0.8059\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4583 - accuracy: 0.8059\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.4597 - accuracy: 0.8080\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.4720 - accuracy: 0.8134\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.4660 - accuracy: 0.8096\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.4535 - accuracy: 0.8159\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.4542 - accuracy: 0.8096\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.4530 - accuracy: 0.8084\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.4319 - accuracy: 0.8300\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4451 - accuracy: 0.8117\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.4438 - accuracy: 0.8184\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.4397 - accuracy: 0.8192\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4249 - accuracy: 0.8221\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.4162 - accuracy: 0.8275\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.4286 - accuracy: 0.8238\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.4308 - accuracy: 0.8246\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.4135 - accuracy: 0.8329\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.4099 - accuracy: 0.8292\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.4080 - accuracy: 0.8292\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.4166 - accuracy: 0.8325\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.4158 - accuracy: 0.8283\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 961us/step - loss: 0.3936 - accuracy: 0.8392\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4075 - accuracy: 0.8288\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8350\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8358\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.3969 - accuracy: 0.8441\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.3969 - accuracy: 0.8325\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.3932 - accuracy: 0.8313\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.4083 - accuracy: 0.8250\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.3779 - accuracy: 0.8458\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.3827 - accuracy: 0.8412\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.3834 - accuracy: 0.8446\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.3734 - accuracy: 0.8466\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.3991 - accuracy: 0.8396\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3798 - accuracy: 0.8416\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3780 - accuracy: 0.8504\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3869 - accuracy: 0.8450\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3771 - accuracy: 0.8483\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3595 - accuracy: 0.8579\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3662 - accuracy: 0.8458\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.3717 - accuracy: 0.8466\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.3527 - accuracy: 0.8599\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3611 - accuracy: 0.8487\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.3655 - accuracy: 0.8433\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3383 - accuracy: 0.8637\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.3576 - accuracy: 0.8487\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.3594 - accuracy: 0.8554\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3615 - accuracy: 0.8533\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.3389 - accuracy: 0.8633\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.3515 - accuracy: 0.8533\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.3403 - accuracy: 0.8628\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.3409 - accuracy: 0.8608\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.3301 - accuracy: 0.8658\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.3300 - accuracy: 0.8624\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8628\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3386 - accuracy: 0.8587\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.3477 - accuracy: 0.8529\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.3343 - accuracy: 0.8583\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.3347 - accuracy: 0.8703\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.3320 - accuracy: 0.8637\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8658\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8678\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.3268 - accuracy: 0.8599\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.3271 - accuracy: 0.8753\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.3309 - accuracy: 0.8583\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3209 - accuracy: 0.8695\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3240 - accuracy: 0.8720\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3284 - accuracy: 0.8624\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3221 - accuracy: 0.8670\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3186 - accuracy: 0.8724\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.3052 - accuracy: 0.8815\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3020 - accuracy: 0.8757\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3137 - accuracy: 0.8712\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3019 - accuracy: 0.8728\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3278 - accuracy: 0.8691\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3025 - accuracy: 0.8782\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3157 - accuracy: 0.8728\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3188 - accuracy: 0.8749\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3179 - accuracy: 0.8707\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.3092 - accuracy: 0.8662\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.3100 - accuracy: 0.8803\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3041 - accuracy: 0.8820\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3046 - accuracy: 0.8795\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.3018 - accuracy: 0.8828\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3088 - accuracy: 0.8728\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3056 - accuracy: 0.8699\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3102 - accuracy: 0.8736\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3035 - accuracy: 0.8791\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3015 - accuracy: 0.8774\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3039 - accuracy: 0.8757\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2937 - accuracy: 0.8836\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2948 - accuracy: 0.8857\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2879 - accuracy: 0.8849\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3021 - accuracy: 0.8761\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2775 - accuracy: 0.8857\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2749 - accuracy: 0.8878\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3034 - accuracy: 0.8716\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2771 - accuracy: 0.8832\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2703 - accuracy: 0.8948\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2751 - accuracy: 0.8882\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.2897 - accuracy: 0.8753\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.2790 - accuracy: 0.8845\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2793 - accuracy: 0.8878\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2682 - accuracy: 0.8924\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.2911 - accuracy: 0.8840\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2817 - accuracy: 0.8886\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2860 - accuracy: 0.8849\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.2814 - accuracy: 0.8915\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2721 - accuracy: 0.8878\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2768 - accuracy: 0.8903\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.2847 - accuracy: 0.8828\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8965\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8886\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.2713 - accuracy: 0.8928\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8919\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8936\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2665 - accuracy: 0.8911\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.9007\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8878\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.8953\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2595 - accuracy: 0.8973\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.2666 - accuracy: 0.8978\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.2604 - accuracy: 0.8915\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2650 - accuracy: 0.8965\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.2624 - accuracy: 0.8944\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2624 - accuracy: 0.8940\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2468 - accuracy: 0.8961\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2698 - accuracy: 0.8845\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.2682 - accuracy: 0.8882\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2515 - accuracy: 0.8990\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2633 - accuracy: 0.8932\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.2578 - accuracy: 0.8982\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2529 - accuracy: 0.8953\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2461 - accuracy: 0.9081\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.2602 - accuracy: 0.8957\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2410 - accuracy: 0.9027\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2441 - accuracy: 0.8957\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2457 - accuracy: 0.8932\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2505 - accuracy: 0.9015\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2363 - accuracy: 0.9073\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2580 - accuracy: 0.9040\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2518 - accuracy: 0.8986\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2625 - accuracy: 0.8944\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2701 - accuracy: 0.8907\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2371 - accuracy: 0.9081\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2498 - accuracy: 0.9002\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2459 - accuracy: 0.8924\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2510 - accuracy: 0.8973\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2226 - accuracy: 0.9065\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2427 - accuracy: 0.9044\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.2424 - accuracy: 0.8957\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.2600 - accuracy: 0.8932\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.2522 - accuracy: 0.9002\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2465 - accuracy: 0.8924\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2281 - accuracy: 0.9061\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2319 - accuracy: 0.9077\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2475 - accuracy: 0.9044\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2366 - accuracy: 0.9048\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2262 - accuracy: 0.9094\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2374 - accuracy: 0.9036\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2358 - accuracy: 0.9131\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2416 - accuracy: 0.9002\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.2154 - accuracy: 0.9185\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2380 - accuracy: 0.9086\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2120 - accuracy: 0.9165\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2452 - accuracy: 0.9007\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2328 - accuracy: 0.9061\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.2277 - accuracy: 0.9065\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.2527 - accuracy: 0.8969\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.2332 - accuracy: 0.9148\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.2338 - accuracy: 0.9015\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.2182 - accuracy: 0.9144\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.2231 - accuracy: 0.9119\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2245 - accuracy: 0.9094\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2164 - accuracy: 0.9152\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2234 - accuracy: 0.9065\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2171 - accuracy: 0.9152\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2175 - accuracy: 0.9156\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2208 - accuracy: 0.9098\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.2289 - accuracy: 0.9106\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2217 - accuracy: 0.9144\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2095 - accuracy: 0.9140\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2217 - accuracy: 0.9106\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2185 - accuracy: 0.9123\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2066 - accuracy: 0.9169\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2293 - accuracy: 0.9073\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2230 - accuracy: 0.9135\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2181 - accuracy: 0.9152\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2218 - accuracy: 0.9152\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2315 - accuracy: 0.9044\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2250 - accuracy: 0.9073\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2127 - accuracy: 0.9198\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2298 - accuracy: 0.9094\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2232 - accuracy: 0.9123\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2091 - accuracy: 0.9165\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2335 - accuracy: 0.9019\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2094 - accuracy: 0.9214\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2061 - accuracy: 0.9210\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2038 - accuracy: 0.9231\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2070 - accuracy: 0.9181\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2254 - accuracy: 0.9131\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2027 - accuracy: 0.9202\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9231\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.2076 - accuracy: 0.9194\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2129 - accuracy: 0.9090\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.2079 - accuracy: 0.9148\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2090 - accuracy: 0.9115\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2036 - accuracy: 0.9219\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2068 - accuracy: 0.9190\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2185 - accuracy: 0.9144\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2074 - accuracy: 0.9235\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2136 - accuracy: 0.9177\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1972 - accuracy: 0.9244\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1901 - accuracy: 0.9239\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1927 - accuracy: 0.9227\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2065 - accuracy: 0.9190\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2035 - accuracy: 0.9181\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2102 - accuracy: 0.9202\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1947 - accuracy: 0.9219\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1834 - accuracy: 0.9244\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2035 - accuracy: 0.9210\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.2007 - accuracy: 0.9231\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1976 - accuracy: 0.9206\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2073 - accuracy: 0.9173\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1928 - accuracy: 0.9273\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1893 - accuracy: 0.9252\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2002 - accuracy: 0.9206\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 988us/step - loss: 0.1882 - accuracy: 0.9293\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.1922 - accuracy: 0.9256\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1934 - accuracy: 0.9235\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1884 - accuracy: 0.9293\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1966 - accuracy: 0.9148\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1913 - accuracy: 0.9244\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.1834 - accuracy: 0.9206\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1970 - accuracy: 0.9231\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1831 - accuracy: 0.9339\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2015 - accuracy: 0.9210\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.1940 - accuracy: 0.9264\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.1899 - accuracy: 0.9285\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1844 - accuracy: 0.9335\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1918 - accuracy: 0.9181\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.1986 - accuracy: 0.9219\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1946 - accuracy: 0.9206\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1901 - accuracy: 0.9252\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1849 - accuracy: 0.9281\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2194 - accuracy: 0.9073\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.1874 - accuracy: 0.9314\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1920 - accuracy: 0.9227\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.1914 - accuracy: 0.9239\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1743 - accuracy: 0.9343\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.1852 - accuracy: 0.9227\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.1698 - accuracy: 0.9298\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.1858 - accuracy: 0.9268\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1822 - accuracy: 0.9260\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.1727 - accuracy: 0.9318\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.1928 - accuracy: 0.9281\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1969 - accuracy: 0.9264\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 963us/step - loss: 0.1862 - accuracy: 0.9252\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.1805 - accuracy: 0.9306\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1886 - accuracy: 0.9285\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.1647 - accuracy: 0.9360\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1853 - accuracy: 0.9293\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1836 - accuracy: 0.9273\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1934 - accuracy: 0.9214\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1875 - accuracy: 0.9231\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.1768 - accuracy: 0.9314\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.1704 - accuracy: 0.9364\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1816 - accuracy: 0.9306\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.1876 - accuracy: 0.9235\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1742 - accuracy: 0.9331\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1780 - accuracy: 0.9277\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1787 - accuracy: 0.9343\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1685 - accuracy: 0.9364\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1633 - accuracy: 0.9356\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1793 - accuracy: 0.9277\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1787 - accuracy: 0.9306\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1674 - accuracy: 0.9314\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1843 - accuracy: 0.9298\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1676 - accuracy: 0.9352\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1724 - accuracy: 0.9331\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1731 - accuracy: 0.9364\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1759 - accuracy: 0.9323\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1761 - accuracy: 0.9323\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1696 - accuracy: 0.9327\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1654 - accuracy: 0.9356\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1687 - accuracy: 0.9318\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.1904 - accuracy: 0.9306\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1730 - accuracy: 0.9289\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9293\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9335\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9331\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9285\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9314\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9260\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9335\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.1704 - accuracy: 0.9372\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9306\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 989us/step - loss: 0.1666 - accuracy: 0.9410\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1609 - accuracy: 0.9360\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.1659 - accuracy: 0.9364\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1633 - accuracy: 0.9368\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.1748 - accuracy: 0.9323\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9323\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 989us/step - loss: 0.1724 - accuracy: 0.9327\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9406\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.1687 - accuracy: 0.9331\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.1658 - accuracy: 0.9360\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.1509 - accuracy: 0.9422\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.1737 - accuracy: 0.9285\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1689 - accuracy: 0.9331\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.1705 - accuracy: 0.9327\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.1601 - accuracy: 0.9414\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.1675 - accuracy: 0.9339\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.1687 - accuracy: 0.9381\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1651 - accuracy: 0.9343\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.1438 - accuracy: 0.9497\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.1579 - accuracy: 0.9439\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1640 - accuracy: 0.9410\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.1721 - accuracy: 0.9360\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1586 - accuracy: 0.9393\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1678 - accuracy: 0.9372\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1586 - accuracy: 0.9393\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1564 - accuracy: 0.9414\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1475 - accuracy: 0.9451\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1736 - accuracy: 0.9323\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1596 - accuracy: 0.9372\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1516 - accuracy: 0.9401\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1620 - accuracy: 0.9397\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1535 - accuracy: 0.9377\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1674 - accuracy: 0.9335\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1449 - accuracy: 0.9468\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1467 - accuracy: 0.9497\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1623 - accuracy: 0.9385\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1513 - accuracy: 0.9431\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1482 - accuracy: 0.9406\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1543 - accuracy: 0.9426\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.1684 - accuracy: 0.9331\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.1456 - accuracy: 0.9422\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1530 - accuracy: 0.9389\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1548 - accuracy: 0.9414\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1624 - accuracy: 0.9360\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1624 - accuracy: 0.9401\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1423 - accuracy: 0.9460\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.1693 - accuracy: 0.9310\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 969us/step - loss: 0.1508 - accuracy: 0.9414\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1462 - accuracy: 0.9468\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.1586 - accuracy: 0.9418\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1689 - accuracy: 0.9364\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1619 - accuracy: 0.9431\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1531 - accuracy: 0.9418\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.1469 - accuracy: 0.9422\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.1593 - accuracy: 0.9410\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.1623 - accuracy: 0.9389\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1649 - accuracy: 0.9422\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.1541 - accuracy: 0.9410\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1584 - accuracy: 0.9368\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 961us/step - loss: 0.1363 - accuracy: 0.9505\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 999us/step - loss: 0.1477 - accuracy: 0.9368\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9422\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1474 - accuracy: 0.9431\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1487 - accuracy: 0.9385\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1441 - accuracy: 0.9439\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1492 - accuracy: 0.9389\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.1497 - accuracy: 0.9426\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1490 - accuracy: 0.9435\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.1531 - accuracy: 0.9385\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.1655 - accuracy: 0.9372\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.1526 - accuracy: 0.9385\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1424 - accuracy: 0.9518\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1407 - accuracy: 0.9468\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.1425 - accuracy: 0.9443\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.1710 - accuracy: 0.9347\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1448 - accuracy: 0.9456\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1408 - accuracy: 0.9468\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1500 - accuracy: 0.9422\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1738 - accuracy: 0.9281\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1335 - accuracy: 0.9493\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9381\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1551 - accuracy: 0.9414\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.1302 - accuracy: 0.9514\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.1599 - accuracy: 0.9377\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.1459 - accuracy: 0.9489\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1419 - accuracy: 0.9472\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1437 - accuracy: 0.9472\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.1470 - accuracy: 0.9489\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1477 - accuracy: 0.9476\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1587 - accuracy: 0.9389\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1544 - accuracy: 0.9456\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1392 - accuracy: 0.9480\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.1490 - accuracy: 0.9464\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.1456 - accuracy: 0.9460\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.1438 - accuracy: 0.9480\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1451 - accuracy: 0.9447\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1425 - accuracy: 0.9426\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1395 - accuracy: 0.9472\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1451 - accuracy: 0.9426\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1310 - accuracy: 0.9518\n",
      "Epoch 426/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1483 - accuracy: 0.9401\n",
      "Epoch 427/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1636 - accuracy: 0.9360\n",
      "Epoch 428/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.1476 - accuracy: 0.9439\n",
      "Epoch 429/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1387 - accuracy: 0.9472\n",
      "Epoch 430/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1686 - accuracy: 0.9389\n",
      "Epoch 431/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1337 - accuracy: 0.9522\n",
      "Epoch 432/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1439 - accuracy: 0.9476\n",
      "Epoch 433/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1409 - accuracy: 0.9443\n",
      "Epoch 434/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1558 - accuracy: 0.9381\n",
      "Epoch 435/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1406 - accuracy: 0.9472\n",
      "Epoch 436/1500\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.1322 - accuracy: 0.9472\n",
      "Epoch 437/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1437 - accuracy: 0.9476\n",
      "Epoch 438/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.2965 - accuracy: 0.8594Restoring model weights from the end of the best epoch: 408.\n",
      "38/38 [==============================] - 0s 985us/step - loss: 0.1411 - accuracy: 0.9489\n",
      "Epoch 438: early stopping\n",
      "7/7 [==============================] - 0s 806us/step - loss: 0.6602 - accuracy: 0.7347\n",
      "7/7 [==============================] - 0s 728us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "Final Test Results - Loss: 0.6602287888526917, Accuracy: 0.7346938848495483, Precision: 0.534623889101501, Recall: 0.5952302178108629, F1 Score: 0.5543419570818099\n",
      "Confusion Matrix:\n",
      " [[119  12  24]\n",
      " [  3  23   0]\n",
      " [ 12   1   2]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 874, 1: 820, 2: 605})\n",
      "Epoch 1/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.1603 - accuracy: 0.4976\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.9071 - accuracy: 0.6133\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 956us/step - loss: 0.8324 - accuracy: 0.6425\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 890us/step - loss: 0.7554 - accuracy: 0.6807\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.7401 - accuracy: 0.6903\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.6972 - accuracy: 0.6999\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.7147 - accuracy: 0.6925\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.6788 - accuracy: 0.7173\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.6610 - accuracy: 0.7381\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.6543 - accuracy: 0.7264\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.6356 - accuracy: 0.7316\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.6247 - accuracy: 0.7364\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.6132 - accuracy: 0.7316\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.6069 - accuracy: 0.7360\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.5967 - accuracy: 0.7551\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.5693 - accuracy: 0.7551\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.5480 - accuracy: 0.7616\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.5514 - accuracy: 0.7621\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.5467 - accuracy: 0.7638\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.5369 - accuracy: 0.7799\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.5253 - accuracy: 0.7734\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.5096 - accuracy: 0.7834\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.5164 - accuracy: 0.7834\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.4888 - accuracy: 0.7960\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.4968 - accuracy: 0.7856\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.5050 - accuracy: 0.7921\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.4954 - accuracy: 0.7856\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.4924 - accuracy: 0.7999\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4822 - accuracy: 0.8034\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4551 - accuracy: 0.8034\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 926us/step - loss: 0.4778 - accuracy: 0.7947\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 921us/step - loss: 0.4728 - accuracy: 0.7990\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.4785 - accuracy: 0.7969\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.4580 - accuracy: 0.8073\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.4554 - accuracy: 0.8017\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.4503 - accuracy: 0.8108\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.4635 - accuracy: 0.8077\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 0.4452 - accuracy: 0.8086\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.4388 - accuracy: 0.8086\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.4332 - accuracy: 0.8090\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.4206 - accuracy: 0.8177\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.4295 - accuracy: 0.8212\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.4444 - accuracy: 0.8043\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.4377 - accuracy: 0.8177\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.4161 - accuracy: 0.8264\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.4174 - accuracy: 0.8221\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.4209 - accuracy: 0.8138\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.4109 - accuracy: 0.8299\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.4049 - accuracy: 0.8351\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.4104 - accuracy: 0.8282\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.4068 - accuracy: 0.8243\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.4130 - accuracy: 0.8273\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.4008 - accuracy: 0.8273\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.3985 - accuracy: 0.8282\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3928 - accuracy: 0.8269\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.3933 - accuracy: 0.8378\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.4051 - accuracy: 0.8256\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.3869 - accuracy: 0.8373\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.3884 - accuracy: 0.8391\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.3803 - accuracy: 0.8369\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.3764 - accuracy: 0.8443\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3924 - accuracy: 0.8351\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3835 - accuracy: 0.8408\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.3928 - accuracy: 0.8343\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.3798 - accuracy: 0.8443\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.3660 - accuracy: 0.8447\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 758us/step - loss: 0.3664 - accuracy: 0.8460\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.3764 - accuracy: 0.8369\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.3775 - accuracy: 0.8330\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.3613 - accuracy: 0.8438\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.3743 - accuracy: 0.8399\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.3678 - accuracy: 0.8378\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.3586 - accuracy: 0.8521\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8456\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 0.3524 - accuracy: 0.8517\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 930us/step - loss: 0.3564 - accuracy: 0.8556\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 968us/step - loss: 0.3576 - accuracy: 0.8504\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.3473 - accuracy: 0.8504\n",
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.3648 - accuracy: 0.8447\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3623 - accuracy: 0.8456\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.3428 - accuracy: 0.8582\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 984us/step - loss: 0.3484 - accuracy: 0.8547\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 993us/step - loss: 0.3566 - accuracy: 0.8495\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8652\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 931us/step - loss: 0.3413 - accuracy: 0.8495\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.3420 - accuracy: 0.8569\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.3341 - accuracy: 0.8599\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.3392 - accuracy: 0.8599\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 747us/step - loss: 0.3352 - accuracy: 0.8612\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.3482 - accuracy: 0.8621\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.3354 - accuracy: 0.8625\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.3321 - accuracy: 0.8630\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.3315 - accuracy: 0.8652\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.3222 - accuracy: 0.8634\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.3258 - accuracy: 0.8591\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.3215 - accuracy: 0.8678\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.3158 - accuracy: 0.8708\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.3302 - accuracy: 0.8660\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3190 - accuracy: 0.8669\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.3446 - accuracy: 0.8560\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.3345 - accuracy: 0.8504\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.3100 - accuracy: 0.8704\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.3106 - accuracy: 0.8817\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.3130 - accuracy: 0.8621\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.3268 - accuracy: 0.8660\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.3033 - accuracy: 0.8730\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.3017 - accuracy: 0.8786\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.3094 - accuracy: 0.8708\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.3155 - accuracy: 0.8673\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.3048 - accuracy: 0.8739\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.3071 - accuracy: 0.8704\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.3024 - accuracy: 0.8821\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.2906 - accuracy: 0.8847\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.3102 - accuracy: 0.8682\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.3069 - accuracy: 0.8717\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.3053 - accuracy: 0.8769\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.3189 - accuracy: 0.8625\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.3043 - accuracy: 0.8739\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.2916 - accuracy: 0.8773\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.2989 - accuracy: 0.8799\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.2942 - accuracy: 0.8752\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.2900 - accuracy: 0.8786\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 947us/step - loss: 0.2861 - accuracy: 0.8808\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.3120 - accuracy: 0.8708\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 942us/step - loss: 0.2931 - accuracy: 0.8782\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.2974 - accuracy: 0.8734\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2841 - accuracy: 0.8882\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.2868 - accuracy: 0.8856\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2965 - accuracy: 0.8786\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2780 - accuracy: 0.8847\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2873 - accuracy: 0.8791\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2785 - accuracy: 0.8834\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.2940 - accuracy: 0.8873\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2899 - accuracy: 0.8795\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.2797 - accuracy: 0.8839\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2736 - accuracy: 0.8839\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.2807 - accuracy: 0.8895\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 949us/step - loss: 0.2903 - accuracy: 0.8747\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.2788 - accuracy: 0.8830\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.2883 - accuracy: 0.8847\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.2681 - accuracy: 0.8930\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2726 - accuracy: 0.8895\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.2873 - accuracy: 0.8795\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.2841 - accuracy: 0.8804\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.2885 - accuracy: 0.8756\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2857 - accuracy: 0.8878\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.2657 - accuracy: 0.8926\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.2731 - accuracy: 0.8891\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2648 - accuracy: 0.8917\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2781 - accuracy: 0.8921\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.2572 - accuracy: 0.8952\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.2777 - accuracy: 0.8873\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.2601 - accuracy: 0.8965\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.2608 - accuracy: 0.8973\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2717 - accuracy: 0.8834\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 968us/step - loss: 0.2551 - accuracy: 0.8965\n",
      "Epoch 157/1500\n",
      "36/36 [==============================] - 0s 977us/step - loss: 0.2547 - accuracy: 0.9030\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 975us/step - loss: 0.2689 - accuracy: 0.8930\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.2700 - accuracy: 0.8839\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 0.2597 - accuracy: 0.8991\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.9004\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.8991\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.9100\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8900\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 932us/step - loss: 0.2620 - accuracy: 0.8921\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.2604 - accuracy: 0.8987\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 887us/step - loss: 0.2512 - accuracy: 0.8947\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.2490 - accuracy: 0.8978\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.2589 - accuracy: 0.8917\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.2492 - accuracy: 0.8965\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.2566 - accuracy: 0.8995\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.2596 - accuracy: 0.8965\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.2500 - accuracy: 0.8969\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.2483 - accuracy: 0.8978\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 911us/step - loss: 0.2635 - accuracy: 0.8960\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.2431 - accuracy: 0.9026\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.2439 - accuracy: 0.9078\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2340 - accuracy: 0.9021\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.2517 - accuracy: 0.8943\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2364 - accuracy: 0.9065\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.2500 - accuracy: 0.8991\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.2512 - accuracy: 0.8973\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.2495 - accuracy: 0.8982\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.2345 - accuracy: 0.9017\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2426 - accuracy: 0.9047\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.2469 - accuracy: 0.9030\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2417 - accuracy: 0.9060\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.2308 - accuracy: 0.9039\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.2689 - accuracy: 0.8930\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2319 - accuracy: 0.9121\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 923us/step - loss: 0.2216 - accuracy: 0.9104\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.2364 - accuracy: 0.9060\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.2173 - accuracy: 0.9187\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.2443 - accuracy: 0.9078\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.2387 - accuracy: 0.9021\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.2384 - accuracy: 0.9039\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.2373 - accuracy: 0.9082\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.2266 - accuracy: 0.9208\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2457 - accuracy: 0.9043\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.2323 - accuracy: 0.9069\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.2205 - accuracy: 0.9187\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2268 - accuracy: 0.9104\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.2283 - accuracy: 0.9091\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 0.2275 - accuracy: 0.9130\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 910us/step - loss: 0.2265 - accuracy: 0.9082\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 931us/step - loss: 0.2211 - accuracy: 0.9126\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 937us/step - loss: 0.2088 - accuracy: 0.9247\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9074\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 955us/step - loss: 0.2235 - accuracy: 0.9104\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 979us/step - loss: 0.2219 - accuracy: 0.9143\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.2196 - accuracy: 0.9143\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.2264 - accuracy: 0.9082\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2235 - accuracy: 0.9143\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.2289 - accuracy: 0.9100\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2242 - accuracy: 0.9069\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.2196 - accuracy: 0.9143\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2072 - accuracy: 0.9152\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.2082 - accuracy: 0.9226\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.2299 - accuracy: 0.9056\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.2062 - accuracy: 0.9165\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.2260 - accuracy: 0.9130\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2204 - accuracy: 0.9134\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.2083 - accuracy: 0.9113\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.2025 - accuracy: 0.9256\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2188 - accuracy: 0.9191\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.2219 - accuracy: 0.9130\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.2095 - accuracy: 0.9208\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.2224 - accuracy: 0.9147\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2126 - accuracy: 0.9230\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2121 - accuracy: 0.9165\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2198 - accuracy: 0.9087\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.1965 - accuracy: 0.9256\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 896us/step - loss: 0.2158 - accuracy: 0.9147\n",
      "Epoch 234/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9034\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.2071 - accuracy: 0.9195\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.2027 - accuracy: 0.9213\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.2158 - accuracy: 0.9134\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2183 - accuracy: 0.9134\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.2049 - accuracy: 0.9178\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.2078 - accuracy: 0.9104\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.2216 - accuracy: 0.9161\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.2121 - accuracy: 0.9200\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.2025 - accuracy: 0.9208\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.2032 - accuracy: 0.9204\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.2060 - accuracy: 0.9169\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.2113 - accuracy: 0.9126\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.1932 - accuracy: 0.9217\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2023 - accuracy: 0.9208\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.1969 - accuracy: 0.9247\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.2013 - accuracy: 0.9191\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2037 - accuracy: 0.9252\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.2004 - accuracy: 0.9256\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1981 - accuracy: 0.9243\n",
      "Epoch 254/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.2023 - accuracy: 0.9182\n",
      "Epoch 255/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1853 - accuracy: 0.9252\n",
      "Epoch 256/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.1981 - accuracy: 0.9252\n",
      "Epoch 257/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2129 - accuracy: 0.9182\n",
      "Epoch 258/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2141 - accuracy: 0.9156\n",
      "Epoch 259/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.2038 - accuracy: 0.9221\n",
      "Epoch 260/1500\n",
      "36/36 [==============================] - 0s 990us/step - loss: 0.2010 - accuracy: 0.9230\n",
      "Epoch 261/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.2008 - accuracy: 0.9217\n",
      "Epoch 262/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1960 - accuracy: 0.9243\n",
      "Epoch 263/1500\n",
      "36/36 [==============================] - 0s 901us/step - loss: 0.1995 - accuracy: 0.9287\n",
      "Epoch 264/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1991 - accuracy: 0.9252\n",
      "Epoch 265/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1996 - accuracy: 0.9269\n",
      "Epoch 266/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2003 - accuracy: 0.9234\n",
      "Epoch 267/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1920 - accuracy: 0.9278\n",
      "Epoch 268/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.1890 - accuracy: 0.9300\n",
      "Epoch 269/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.1891 - accuracy: 0.9269\n",
      "Epoch 270/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.1965 - accuracy: 0.9213\n",
      "Epoch 271/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.1851 - accuracy: 0.9265\n",
      "Epoch 272/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.2042 - accuracy: 0.9191\n",
      "Epoch 273/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.1797 - accuracy: 0.9321\n",
      "Epoch 274/1500\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.1902 - accuracy: 0.9295\n",
      "Epoch 275/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.2103 - accuracy: 0.9121\n",
      "Epoch 276/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.2023 - accuracy: 0.9226\n",
      "Epoch 277/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.1877 - accuracy: 0.9239\n",
      "Epoch 278/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1805 - accuracy: 0.9295\n",
      "Epoch 279/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.1905 - accuracy: 0.9230\n",
      "Epoch 280/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 0.1881 - accuracy: 0.9226\n",
      "Epoch 281/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1869 - accuracy: 0.9287\n",
      "Epoch 282/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.1904 - accuracy: 0.9256\n",
      "Epoch 283/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.2062 - accuracy: 0.9274\n",
      "Epoch 284/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.1751 - accuracy: 0.9313\n",
      "Epoch 285/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.1819 - accuracy: 0.9282\n",
      "Epoch 286/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1858 - accuracy: 0.9230\n",
      "Epoch 287/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1906 - accuracy: 0.9217\n",
      "Epoch 288/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.1830 - accuracy: 0.9239\n",
      "Epoch 289/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1993 - accuracy: 0.9234\n",
      "Epoch 290/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1944 - accuracy: 0.9213\n",
      "Epoch 291/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.1832 - accuracy: 0.9308\n",
      "Epoch 292/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.1893 - accuracy: 0.9282\n",
      "Epoch 293/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1902 - accuracy: 0.9261\n",
      "Epoch 294/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1744 - accuracy: 0.9308\n",
      "Epoch 295/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1776 - accuracy: 0.9387\n",
      "Epoch 296/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1897 - accuracy: 0.9239\n",
      "Epoch 297/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1801 - accuracy: 0.9313\n",
      "Epoch 298/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9330\n",
      "Epoch 299/1500\n",
      "36/36 [==============================] - 0s 919us/step - loss: 0.1900 - accuracy: 0.9261\n",
      "Epoch 300/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.1837 - accuracy: 0.9326\n",
      "Epoch 301/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1939 - accuracy: 0.9187\n",
      "Epoch 302/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.1704 - accuracy: 0.9343\n",
      "Epoch 303/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.1796 - accuracy: 0.9326\n",
      "Epoch 304/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1887 - accuracy: 0.9304\n",
      "Epoch 305/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1838 - accuracy: 0.9278\n",
      "Epoch 306/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1704 - accuracy: 0.9326\n",
      "Epoch 307/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.1953 - accuracy: 0.9234\n",
      "Epoch 308/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1828 - accuracy: 0.9282\n",
      "Epoch 309/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1740 - accuracy: 0.9300\n",
      "Epoch 310/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1598 - accuracy: 0.9439\n",
      "Epoch 311/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2104 - accuracy: 0.9208\n",
      "Epoch 312/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.1677 - accuracy: 0.9400\n",
      "Epoch 313/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.1789 - accuracy: 0.9326\n",
      "Epoch 314/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.1759 - accuracy: 0.9295\n",
      "Epoch 315/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.1685 - accuracy: 0.9348\n",
      "Epoch 316/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.1844 - accuracy: 0.9247\n",
      "Epoch 317/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1728 - accuracy: 0.9317\n",
      "Epoch 318/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1677 - accuracy: 0.9291\n",
      "Epoch 319/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.1723 - accuracy: 0.9313\n",
      "Epoch 320/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.1629 - accuracy: 0.9395\n",
      "Epoch 321/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.1787 - accuracy: 0.9321\n",
      "Epoch 322/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.1744 - accuracy: 0.9295\n",
      "Epoch 323/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.1712 - accuracy: 0.9378\n",
      "Epoch 324/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.1797 - accuracy: 0.9300\n",
      "Epoch 325/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.1776 - accuracy: 0.9269\n",
      "Epoch 326/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.1852 - accuracy: 0.9317\n",
      "Epoch 327/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.1896 - accuracy: 0.9247\n",
      "Epoch 328/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.1767 - accuracy: 0.9291\n",
      "Epoch 329/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1830 - accuracy: 0.9261\n",
      "Epoch 330/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.1661 - accuracy: 0.9330\n",
      "Epoch 331/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.1666 - accuracy: 0.9378\n",
      "Epoch 332/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.1745 - accuracy: 0.9317\n",
      "Epoch 333/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.1526 - accuracy: 0.9413\n",
      "Epoch 334/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.1720 - accuracy: 0.9278\n",
      "Epoch 335/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.1841 - accuracy: 0.9304\n",
      "Epoch 336/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.1704 - accuracy: 0.9321\n",
      "Epoch 337/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.1771 - accuracy: 0.9343\n",
      "Epoch 338/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1586 - accuracy: 0.9369\n",
      "Epoch 339/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1560 - accuracy: 0.9387\n",
      "Epoch 340/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.1588 - accuracy: 0.9408\n",
      "Epoch 341/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1693 - accuracy: 0.9387\n",
      "Epoch 342/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.1490 - accuracy: 0.9465\n",
      "Epoch 343/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.1767 - accuracy: 0.9226\n",
      "Epoch 344/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1577 - accuracy: 0.9378\n",
      "Epoch 345/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1583 - accuracy: 0.9387\n",
      "Epoch 346/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1693 - accuracy: 0.9348\n",
      "Epoch 347/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.1610 - accuracy: 0.9413\n",
      "Epoch 348/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.1645 - accuracy: 0.9369\n",
      "Epoch 349/1500\n",
      "36/36 [==============================] - 0s 941us/step - loss: 0.1727 - accuracy: 0.9317\n",
      "Epoch 350/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.1669 - accuracy: 0.9369\n",
      "Epoch 351/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.1589 - accuracy: 0.9400\n",
      "Epoch 352/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.1652 - accuracy: 0.9369\n",
      "Epoch 353/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1699 - accuracy: 0.9343\n",
      "Epoch 354/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1629 - accuracy: 0.9361\n",
      "Epoch 355/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.1698 - accuracy: 0.9365\n",
      "Epoch 356/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.1580 - accuracy: 0.9339\n",
      "Epoch 357/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1566 - accuracy: 0.9387\n",
      "Epoch 358/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1679 - accuracy: 0.9365\n",
      "Epoch 359/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.1686 - accuracy: 0.9304\n",
      "Epoch 360/1500\n",
      "36/36 [==============================] - 0s 818us/step - loss: 0.1741 - accuracy: 0.9330\n",
      "Epoch 361/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1480 - accuracy: 0.9400\n",
      "Epoch 362/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.1564 - accuracy: 0.9400\n",
      "Epoch 363/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.1509 - accuracy: 0.9435\n",
      "Epoch 364/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.1607 - accuracy: 0.9382\n",
      "Epoch 365/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1624 - accuracy: 0.9391\n",
      "Epoch 366/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.1565 - accuracy: 0.9421\n",
      "Epoch 367/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.1734 - accuracy: 0.9308\n",
      "Epoch 368/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.1539 - accuracy: 0.9391\n",
      "Epoch 369/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1678 - accuracy: 0.9304\n",
      "Epoch 370/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.1536 - accuracy: 0.9365\n",
      "Epoch 371/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1596 - accuracy: 0.9387\n",
      "Epoch 372/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.1618 - accuracy: 0.9369\n",
      "Epoch 373/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1656 - accuracy: 0.9339\n",
      "Epoch 374/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.1585 - accuracy: 0.9369\n",
      "Epoch 375/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1586 - accuracy: 0.9387\n",
      "Epoch 376/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.1626 - accuracy: 0.9374\n",
      "Epoch 377/1500\n",
      "36/36 [==============================] - 0s 981us/step - loss: 0.1511 - accuracy: 0.9391\n",
      "Epoch 378/1500\n",
      "36/36 [==============================] - 0s 912us/step - loss: 0.1517 - accuracy: 0.9408\n",
      "Epoch 379/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1520 - accuracy: 0.9408\n",
      "Epoch 380/1500\n",
      "36/36 [==============================] - 0s 965us/step - loss: 0.1629 - accuracy: 0.9387\n",
      "Epoch 381/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.1538 - accuracy: 0.9439\n",
      "Epoch 382/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.1437 - accuracy: 0.9439\n",
      "Epoch 383/1500\n",
      "36/36 [==============================] - 0s 955us/step - loss: 0.1596 - accuracy: 0.9369\n",
      "Epoch 384/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.1420 - accuracy: 0.9443\n",
      "Epoch 385/1500\n",
      "36/36 [==============================] - 0s 910us/step - loss: 0.1532 - accuracy: 0.9391\n",
      "Epoch 386/1500\n",
      "36/36 [==============================] - 0s 951us/step - loss: 0.1524 - accuracy: 0.9356\n",
      "Epoch 387/1500\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.1550 - accuracy: 0.9408\n",
      "Epoch 388/1500\n",
      "36/36 [==============================] - 0s 917us/step - loss: 0.1498 - accuracy: 0.9430\n",
      "Epoch 389/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1572 - accuracy: 0.9387\n",
      "Epoch 390/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.1565 - accuracy: 0.9400\n",
      "Epoch 391/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.1510 - accuracy: 0.9469\n",
      "Epoch 392/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.1720 - accuracy: 0.9378\n",
      "Epoch 393/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1598 - accuracy: 0.9435\n",
      "Epoch 394/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1515 - accuracy: 0.9413\n",
      "Epoch 395/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.1600 - accuracy: 0.9387\n",
      "Epoch 396/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.1497 - accuracy: 0.9356\n",
      "Epoch 397/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1570 - accuracy: 0.9435\n",
      "Epoch 398/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.1470 - accuracy: 0.9387\n",
      "Epoch 399/1500\n",
      "36/36 [==============================] - 0s 896us/step - loss: 0.1500 - accuracy: 0.9408\n",
      "Epoch 400/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.1534 - accuracy: 0.9504\n",
      "Epoch 401/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.1449 - accuracy: 0.9461\n",
      "Epoch 402/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.1478 - accuracy: 0.9469\n",
      "Epoch 403/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.1508 - accuracy: 0.9430\n",
      "Epoch 404/1500\n",
      "36/36 [==============================] - 0s 943us/step - loss: 0.1383 - accuracy: 0.9482\n",
      "Epoch 405/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.1591 - accuracy: 0.9321\n",
      "Epoch 406/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.1412 - accuracy: 0.9448\n",
      "Epoch 407/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.1608 - accuracy: 0.9435\n",
      "Epoch 408/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1448 - accuracy: 0.9426\n",
      "Epoch 409/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.1408 - accuracy: 0.9461\n",
      "Epoch 410/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.1596 - accuracy: 0.9426\n",
      "Epoch 411/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.1538 - accuracy: 0.9369\n",
      "Epoch 412/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.1614 - accuracy: 0.9348\n",
      "Epoch 413/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.1400 - accuracy: 0.9482\n",
      "Epoch 414/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 0.1561 - accuracy: 0.9395\n",
      "Epoch 415/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.1387 - accuracy: 0.9443\n",
      "Epoch 416/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.1543 - accuracy: 0.9421\n",
      "Epoch 417/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.1411 - accuracy: 0.9469\n",
      "Epoch 418/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1510 - accuracy: 0.9378\n",
      "Epoch 419/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1434 - accuracy: 0.9461\n",
      "Epoch 420/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1442 - accuracy: 0.9430\n",
      "Epoch 421/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.1404 - accuracy: 0.9461\n",
      "Epoch 422/1500\n",
      "36/36 [==============================] - 0s 941us/step - loss: 0.1430 - accuracy: 0.9474\n",
      "Epoch 423/1500\n",
      "36/36 [==============================] - 0s 981us/step - loss: 0.1367 - accuracy: 0.9478\n",
      "Epoch 424/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9452\n",
      "Epoch 425/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9482\n",
      "Epoch 426/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9387\n",
      "Epoch 427/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9461\n",
      "Epoch 428/1500\n",
      "36/36 [==============================] - 0s 985us/step - loss: 0.1398 - accuracy: 0.9500\n",
      "Epoch 429/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9482\n",
      "Epoch 430/1500\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.1456 - accuracy: 0.9443\n",
      "Epoch 431/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.1422 - accuracy: 0.9487\n",
      "Epoch 432/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1545 - accuracy: 0.9417\n",
      "Epoch 433/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.1326 - accuracy: 0.9504\n",
      "Epoch 434/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.1461 - accuracy: 0.9452\n",
      "Epoch 435/1500\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.1465 - accuracy: 0.9417\n",
      "Epoch 436/1500\n",
      "36/36 [==============================] - 0s 932us/step - loss: 0.1439 - accuracy: 0.9465\n",
      "Epoch 437/1500\n",
      "36/36 [==============================] - 0s 946us/step - loss: 0.1347 - accuracy: 0.9508\n",
      "Epoch 438/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.1384 - accuracy: 0.9487\n",
      "Epoch 439/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1361 - accuracy: 0.9500\n",
      "Epoch 440/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.1546 - accuracy: 0.9356\n",
      "Epoch 441/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1444 - accuracy: 0.9452\n",
      "Epoch 442/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1523 - accuracy: 0.9474\n",
      "Epoch 443/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1533 - accuracy: 0.9421\n",
      "Epoch 444/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.1301 - accuracy: 0.9526\n",
      "Epoch 445/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.1430 - accuracy: 0.9474\n",
      "Epoch 446/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.1420 - accuracy: 0.9443\n",
      "Epoch 447/1500\n",
      "36/36 [==============================] - 0s 816us/step - loss: 0.1427 - accuracy: 0.9508\n",
      "Epoch 448/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1435 - accuracy: 0.9443\n",
      "Epoch 449/1500\n",
      "36/36 [==============================] - 0s 961us/step - loss: 0.1432 - accuracy: 0.9452\n",
      "Epoch 450/1500\n",
      "36/36 [==============================] - 0s 972us/step - loss: 0.1399 - accuracy: 0.9500\n",
      "Epoch 451/1500\n",
      "36/36 [==============================] - 0s 928us/step - loss: 0.1324 - accuracy: 0.9465\n",
      "Epoch 452/1500\n",
      "36/36 [==============================] - 0s 944us/step - loss: 0.1393 - accuracy: 0.9508\n",
      "Epoch 453/1500\n",
      "36/36 [==============================] - 0s 972us/step - loss: 0.1179 - accuracy: 0.9561\n",
      "Epoch 454/1500\n",
      "36/36 [==============================] - 0s 941us/step - loss: 0.1361 - accuracy: 0.9513\n",
      "Epoch 455/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1334 - accuracy: 0.9530\n",
      "Epoch 456/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1432 - accuracy: 0.9456\n",
      "Epoch 457/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.1375 - accuracy: 0.9448\n",
      "Epoch 458/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1265 - accuracy: 0.9522\n",
      "Epoch 459/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.1251 - accuracy: 0.9530\n",
      "Epoch 460/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.1513 - accuracy: 0.9478\n",
      "Epoch 461/1500\n",
      "36/36 [==============================] - 0s 916us/step - loss: 0.1320 - accuracy: 0.9504\n",
      "Epoch 462/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1366 - accuracy: 0.9474\n",
      "Epoch 463/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9561\n",
      "Epoch 464/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9382\n",
      "Epoch 465/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1459 - accuracy: 0.9421\n",
      "Epoch 466/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.1375 - accuracy: 0.9474\n",
      "Epoch 467/1500\n",
      "36/36 [==============================] - 0s 953us/step - loss: 0.1469 - accuracy: 0.9404\n",
      "Epoch 468/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.1392 - accuracy: 0.9435\n",
      "Epoch 469/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.1359 - accuracy: 0.9491\n",
      "Epoch 470/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.1337 - accuracy: 0.9487\n",
      "Epoch 471/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.1231 - accuracy: 0.9500\n",
      "Epoch 472/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.1267 - accuracy: 0.9565\n",
      "Epoch 473/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.1211 - accuracy: 0.9522\n",
      "Epoch 474/1500\n",
      "36/36 [==============================] - 0s 926us/step - loss: 0.1448 - accuracy: 0.9482\n",
      "Epoch 475/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.1291 - accuracy: 0.9487\n",
      "Epoch 476/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.1303 - accuracy: 0.9530\n",
      "Epoch 477/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.1404 - accuracy: 0.9469\n",
      "Epoch 478/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.1294 - accuracy: 0.9474\n",
      "Epoch 479/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1318 - accuracy: 0.9504\n",
      "Epoch 480/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1235 - accuracy: 0.9513\n",
      "Epoch 481/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.1140 - accuracy: 0.9556\n",
      "Epoch 482/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.1431 - accuracy: 0.9465\n",
      "Epoch 483/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1381 - accuracy: 0.9491\n",
      "Epoch 484/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.1266 - accuracy: 0.9535\n",
      "Epoch 485/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.1368 - accuracy: 0.9508\n",
      "Epoch 486/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.1356 - accuracy: 0.9504\n",
      "Epoch 487/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1385 - accuracy: 0.9452\n",
      "Epoch 488/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1326 - accuracy: 0.9530\n",
      "Epoch 489/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1358 - accuracy: 0.9461\n",
      "Epoch 490/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1295 - accuracy: 0.9474\n",
      "Epoch 491/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.1191 - accuracy: 0.9504\n",
      "Epoch 492/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.1309 - accuracy: 0.9482\n",
      "Epoch 493/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.1298 - accuracy: 0.9482\n",
      "Epoch 494/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9548\n",
      "Epoch 495/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.1250 - accuracy: 0.9587\n",
      "Epoch 496/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.1325 - accuracy: 0.9487\n",
      "Epoch 497/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1350 - accuracy: 0.9504\n",
      "Epoch 498/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1156 - accuracy: 0.9539\n",
      "Epoch 499/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1333 - accuracy: 0.9474\n",
      "Epoch 500/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.1190 - accuracy: 0.9565\n",
      "Epoch 501/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.1369 - accuracy: 0.9421\n",
      "Epoch 502/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.1187 - accuracy: 0.9556\n",
      "Epoch 503/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1259 - accuracy: 0.9526\n",
      "Epoch 504/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1215 - accuracy: 0.9526\n",
      "Epoch 505/1500\n",
      "36/36 [==============================] - 0s 930us/step - loss: 0.1279 - accuracy: 0.9526\n",
      "Epoch 506/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.1288 - accuracy: 0.9504\n",
      "Epoch 507/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1281 - accuracy: 0.9448\n",
      "Epoch 508/1500\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.1258 - accuracy: 0.9487\n",
      "Epoch 509/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.1171 - accuracy: 0.9539\n",
      "Epoch 510/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.1395 - accuracy: 0.9443\n",
      "Epoch 511/1500\n",
      " 1/36 [..............................] - ETA: 0s - loss: 0.1042 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 481.\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.1403 - accuracy: 0.9452\n",
      "Epoch 511: early stopping\n",
      "7/7 [==============================] - 0s 901us/step - loss: 0.9043 - accuracy: 0.7721\n",
      "7/7 [==============================] - 0s 699us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "After appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "Final Test Results - Loss: 0.904304563999176, Accuracy: 0.7720929980278015, Precision: 0.7640184541592991, Recall: 0.7772521812256912, F1 Score: 0.7692645137080332\n",
      "Confusion Matrix:\n",
      " [[122   1  28]\n",
      " [  1   6   0]\n",
      " [ 19   0  38]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'022A'}\n",
      "Moved to Test Set:\n",
      "{'022A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '022A' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "762\n",
      "Length of y_train_val:\n",
      "762\n",
      "Length of groups_train_val:\n",
      "762\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     496\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     92\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 992, 1: 795, 2: 535})\n",
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.2329 - accuracy: 0.4660\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.9477 - accuracy: 0.5857\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.8854 - accuracy: 0.6180\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.8058 - accuracy: 0.6434\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.7588 - accuracy: 0.6761\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.7315 - accuracy: 0.6908\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.7027 - accuracy: 0.6921\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.6978 - accuracy: 0.6934\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.6822 - accuracy: 0.7110\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.6784 - accuracy: 0.7080\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.6453 - accuracy: 0.7321\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.6353 - accuracy: 0.7278\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.6076 - accuracy: 0.7407\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.5979 - accuracy: 0.7550\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.5868 - accuracy: 0.7593\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.5590 - accuracy: 0.7606\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.5766 - accuracy: 0.7562\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.5630 - accuracy: 0.7644\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.5558 - accuracy: 0.7636\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.5356 - accuracy: 0.7821\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.5309 - accuracy: 0.7808\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.4986 - accuracy: 0.7868\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.5020 - accuracy: 0.7911\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.5122 - accuracy: 0.7877\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.5141 - accuracy: 0.7842\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.5165 - accuracy: 0.7808\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.4894 - accuracy: 0.7924\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.4844 - accuracy: 0.7933\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.4830 - accuracy: 0.7959\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.4784 - accuracy: 0.7989\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.4832 - accuracy: 0.8015\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.4627 - accuracy: 0.8040\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 900us/step - loss: 0.4608 - accuracy: 0.8006\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.4859 - accuracy: 0.8028\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.4505 - accuracy: 0.8144\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.4474 - accuracy: 0.8062\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.4641 - accuracy: 0.8079\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 999us/step - loss: 0.4456 - accuracy: 0.8161\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 929us/step - loss: 0.4436 - accuracy: 0.8170\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8096\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4256 - accuracy: 0.8187\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 979us/step - loss: 0.4502 - accuracy: 0.8170\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8165\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 944us/step - loss: 0.4322 - accuracy: 0.8165\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.4333 - accuracy: 0.8217\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8165\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 922us/step - loss: 0.4261 - accuracy: 0.8221\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.4226 - accuracy: 0.8260\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.4474 - accuracy: 0.8243\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.4095 - accuracy: 0.8295\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.4064 - accuracy: 0.8307\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.3831 - accuracy: 0.8424\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.4213 - accuracy: 0.8273\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.4178 - accuracy: 0.8200\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.4133 - accuracy: 0.8273\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.3978 - accuracy: 0.8368\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.3982 - accuracy: 0.8385\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.4013 - accuracy: 0.8394\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.4000 - accuracy: 0.8320\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.3884 - accuracy: 0.8359\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.4175 - accuracy: 0.8187\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.4010 - accuracy: 0.8290\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 938us/step - loss: 0.3869 - accuracy: 0.8389\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.3672 - accuracy: 0.8527\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.3962 - accuracy: 0.8286\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.3869 - accuracy: 0.8411\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.3930 - accuracy: 0.8329\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.3945 - accuracy: 0.8471\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.3743 - accuracy: 0.8463\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3917 - accuracy: 0.8217\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 945us/step - loss: 0.3754 - accuracy: 0.8424\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 910us/step - loss: 0.3601 - accuracy: 0.8497\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.3857 - accuracy: 0.8402\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.3693 - accuracy: 0.8441\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.3830 - accuracy: 0.8415\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.3928 - accuracy: 0.8411\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.3579 - accuracy: 0.8562\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.3622 - accuracy: 0.8501\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.3746 - accuracy: 0.8424\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.3513 - accuracy: 0.8523\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.3672 - accuracy: 0.8506\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.3701 - accuracy: 0.8424\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.3581 - accuracy: 0.8540\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 938us/step - loss: 0.3558 - accuracy: 0.8575\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.3654 - accuracy: 0.8493\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.3505 - accuracy: 0.8553\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 904us/step - loss: 0.3430 - accuracy: 0.8609\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 924us/step - loss: 0.3423 - accuracy: 0.8609\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 910us/step - loss: 0.3477 - accuracy: 0.8540\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.3585 - accuracy: 0.8557\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.3474 - accuracy: 0.8605\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 927us/step - loss: 0.3520 - accuracy: 0.8510\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 935us/step - loss: 0.3546 - accuracy: 0.8514\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 948us/step - loss: 0.3414 - accuracy: 0.8549\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 886us/step - loss: 0.3441 - accuracy: 0.8575\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.3545 - accuracy: 0.8575\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8613\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8648\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3435 - accuracy: 0.8514\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.3304 - accuracy: 0.8566\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.3397 - accuracy: 0.8699\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.3432 - accuracy: 0.8592\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.3266 - accuracy: 0.8695\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 939us/step - loss: 0.3286 - accuracy: 0.8635\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 915us/step - loss: 0.3326 - accuracy: 0.8609\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 926us/step - loss: 0.3516 - accuracy: 0.8570\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 905us/step - loss: 0.3335 - accuracy: 0.8566\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.3296 - accuracy: 0.8669\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 884us/step - loss: 0.3195 - accuracy: 0.8652\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.3134 - accuracy: 0.8730\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.3220 - accuracy: 0.8717\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.3077 - accuracy: 0.8738\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.3235 - accuracy: 0.8665\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.3251 - accuracy: 0.8596\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.3314 - accuracy: 0.8635\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.3252 - accuracy: 0.8665\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.3092 - accuracy: 0.8661\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.3255 - accuracy: 0.8686\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.3314 - accuracy: 0.8639\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.3164 - accuracy: 0.8674\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.3157 - accuracy: 0.8747\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.3213 - accuracy: 0.8579\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3167 - accuracy: 0.8678\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3114 - accuracy: 0.8717\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.3186 - accuracy: 0.8665\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.2968 - accuracy: 0.8811\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.3244 - accuracy: 0.8695\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3163 - accuracy: 0.8708\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 974us/step - loss: 0.3059 - accuracy: 0.8773\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.3080 - accuracy: 0.8777\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 918us/step - loss: 0.3061 - accuracy: 0.8712\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 917us/step - loss: 0.2998 - accuracy: 0.8820\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 918us/step - loss: 0.2971 - accuracy: 0.8829\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 980us/step - loss: 0.3089 - accuracy: 0.8773\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 888us/step - loss: 0.2905 - accuracy: 0.8837\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2832 - accuracy: 0.8893\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2937 - accuracy: 0.8816\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2903 - accuracy: 0.8820\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.2927 - accuracy: 0.8798\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.3007 - accuracy: 0.8712\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2929 - accuracy: 0.8824\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.2880 - accuracy: 0.8824\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.2934 - accuracy: 0.8781\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 872us/step - loss: 0.2940 - accuracy: 0.8773\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 891us/step - loss: 0.2943 - accuracy: 0.8811\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.2951 - accuracy: 0.8803\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.2846 - accuracy: 0.8842\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 900us/step - loss: 0.2793 - accuracy: 0.8824\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.2836 - accuracy: 0.8807\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 902us/step - loss: 0.2726 - accuracy: 0.8872\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2838 - accuracy: 0.8846\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.2848 - accuracy: 0.8794\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.2798 - accuracy: 0.8824\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2895 - accuracy: 0.8872\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.2784 - accuracy: 0.8906\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.2833 - accuracy: 0.8820\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 915us/step - loss: 0.2889 - accuracy: 0.8837\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.2802 - accuracy: 0.8880\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 871us/step - loss: 0.2807 - accuracy: 0.8807\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2892 - accuracy: 0.8790\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.2724 - accuracy: 0.8837\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2718 - accuracy: 0.8880\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.2629 - accuracy: 0.8941\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.2760 - accuracy: 0.8893\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2811 - accuracy: 0.8906\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.2687 - accuracy: 0.8919\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2621 - accuracy: 0.8984\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.2650 - accuracy: 0.8880\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.2716 - accuracy: 0.8979\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2602 - accuracy: 0.8885\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2555 - accuracy: 0.9014\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2574 - accuracy: 0.8997\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.2743 - accuracy: 0.8919\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 892us/step - loss: 0.2753 - accuracy: 0.8859\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 946us/step - loss: 0.2737 - accuracy: 0.8854\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.2676 - accuracy: 0.8898\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 951us/step - loss: 0.2783 - accuracy: 0.8876\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8915\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8997\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 974us/step - loss: 0.2800 - accuracy: 0.8876\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2621 - accuracy: 0.8928\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2648 - accuracy: 0.8975\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2634 - accuracy: 0.8962\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2610 - accuracy: 0.8984\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.2581 - accuracy: 0.8997\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 882us/step - loss: 0.2506 - accuracy: 0.9005\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 965us/step - loss: 0.2706 - accuracy: 0.8941\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 936us/step - loss: 0.2501 - accuracy: 0.8971\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2582 - accuracy: 0.8910\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2487 - accuracy: 0.9005\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.2517 - accuracy: 0.8936\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 957us/step - loss: 0.2477 - accuracy: 0.9031\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 978us/step - loss: 0.2648 - accuracy: 0.8906\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 979us/step - loss: 0.2508 - accuracy: 0.9057\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 937us/step - loss: 0.2400 - accuracy: 0.9027\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2610 - accuracy: 0.8962\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.2369 - accuracy: 0.9109\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.2534 - accuracy: 0.8962\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.2443 - accuracy: 0.9040\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.2411 - accuracy: 0.9040\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2463 - accuracy: 0.8971\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.2563 - accuracy: 0.8910\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2428 - accuracy: 0.9074\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 871us/step - loss: 0.2231 - accuracy: 0.9139\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 950us/step - loss: 0.2549 - accuracy: 0.9044\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.2414 - accuracy: 0.9091\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 919us/step - loss: 0.2337 - accuracy: 0.9078\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 954us/step - loss: 0.2405 - accuracy: 0.9065\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2207 - accuracy: 0.9096\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2362 - accuracy: 0.9053\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2362 - accuracy: 0.9048\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.2342 - accuracy: 0.9027\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2178 - accuracy: 0.9190\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 926us/step - loss: 0.2188 - accuracy: 0.9109\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 944us/step - loss: 0.2293 - accuracy: 0.9117\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 934us/step - loss: 0.2333 - accuracy: 0.9087\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 930us/step - loss: 0.2379 - accuracy: 0.9031\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2468 - accuracy: 0.8928\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.2465 - accuracy: 0.9044\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.2201 - accuracy: 0.9134\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.2290 - accuracy: 0.9096\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 939us/step - loss: 0.2271 - accuracy: 0.9143\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 923us/step - loss: 0.2349 - accuracy: 0.9040\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 891us/step - loss: 0.2314 - accuracy: 0.9044\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 898us/step - loss: 0.2308 - accuracy: 0.9083\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.2231 - accuracy: 0.9109\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.2242 - accuracy: 0.9096\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.2221 - accuracy: 0.9121\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2303 - accuracy: 0.9053\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2362 - accuracy: 0.9070\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 917us/step - loss: 0.2278 - accuracy: 0.9074\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.2237 - accuracy: 0.9057\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 900us/step - loss: 0.2599 - accuracy: 0.8941\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2237 - accuracy: 0.9121\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2354 - accuracy: 0.9074\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2377 - accuracy: 0.9061\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 924us/step - loss: 0.2142 - accuracy: 0.9160\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2271 - accuracy: 0.9126\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2052 - accuracy: 0.9169\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2271 - accuracy: 0.9091\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.2160 - accuracy: 0.9134\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2342 - accuracy: 0.9035\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2278 - accuracy: 0.9078\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2355 - accuracy: 0.9035\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9096\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2127 - accuracy: 0.9165\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 913us/step - loss: 0.2145 - accuracy: 0.9152\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 969us/step - loss: 0.2190 - accuracy: 0.9126\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 903us/step - loss: 0.1960 - accuracy: 0.9216\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 886us/step - loss: 0.2221 - accuracy: 0.9156\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 898us/step - loss: 0.2291 - accuracy: 0.9091\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.2079 - accuracy: 0.9152\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2143 - accuracy: 0.9152\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 918us/step - loss: 0.2126 - accuracy: 0.9134\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 911us/step - loss: 0.2020 - accuracy: 0.9233\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 934us/step - loss: 0.2062 - accuracy: 0.9165\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2293 - accuracy: 0.9134\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2102 - accuracy: 0.9199\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 905us/step - loss: 0.2097 - accuracy: 0.9186\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 896us/step - loss: 0.2068 - accuracy: 0.9143\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2050 - accuracy: 0.9147\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 963us/step - loss: 0.2107 - accuracy: 0.9203\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 989us/step - loss: 0.2160 - accuracy: 0.9100\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2269 - accuracy: 0.9044\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2115 - accuracy: 0.9152\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2044 - accuracy: 0.9182\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2121 - accuracy: 0.9147\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.2123 - accuracy: 0.9165\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.2035 - accuracy: 0.9242\n",
      "Epoch 271/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.1983 - accuracy: 0.9233\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2013 - accuracy: 0.9156\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2104 - accuracy: 0.9169\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.2192 - accuracy: 0.9057\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2085 - accuracy: 0.9152\n",
      "Epoch 276/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1973 - accuracy: 0.9208\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2314 - accuracy: 0.9109\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.1792 - accuracy: 0.9320\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2014 - accuracy: 0.9242\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.1932 - accuracy: 0.9195\n",
      "Epoch 281/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.2018 - accuracy: 0.9251\n",
      "Epoch 282/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2062 - accuracy: 0.9152\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.1964 - accuracy: 0.9276\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.1865 - accuracy: 0.9268\n",
      "Epoch 285/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.1953 - accuracy: 0.9233\n",
      "Epoch 286/1500\n",
      "37/37 [==============================] - 0s 948us/step - loss: 0.1970 - accuracy: 0.9225\n",
      "Epoch 287/1500\n",
      "37/37 [==============================] - 0s 903us/step - loss: 0.1979 - accuracy: 0.9229\n",
      "Epoch 288/1500\n",
      "37/37 [==============================] - 0s 993us/step - loss: 0.1973 - accuracy: 0.9225\n",
      "Epoch 289/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.2013 - accuracy: 0.9251\n",
      "Epoch 290/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2055 - accuracy: 0.9242\n",
      "Epoch 291/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.2083 - accuracy: 0.9147\n",
      "Epoch 292/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.1899 - accuracy: 0.9264\n",
      "Epoch 293/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.1891 - accuracy: 0.9251\n",
      "Epoch 294/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.2195 - accuracy: 0.9096\n",
      "Epoch 295/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.1976 - accuracy: 0.9212\n",
      "Epoch 296/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.1974 - accuracy: 0.9229\n",
      "Epoch 297/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2101 - accuracy: 0.9139\n",
      "Epoch 298/1500\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.1844 - accuracy: 0.9242\n",
      "Epoch 299/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.1908 - accuracy: 0.9251\n",
      "Epoch 300/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.1822 - accuracy: 0.9268\n",
      "Epoch 301/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.2003 - accuracy: 0.9220\n",
      "Epoch 302/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.1954 - accuracy: 0.9216\n",
      "Epoch 303/1500\n",
      "37/37 [==============================] - 0s 922us/step - loss: 0.1988 - accuracy: 0.9182\n",
      "Epoch 304/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.1957 - accuracy: 0.9255\n",
      "Epoch 305/1500\n",
      "37/37 [==============================] - 0s 872us/step - loss: 0.1910 - accuracy: 0.9242\n",
      "Epoch 306/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.1913 - accuracy: 0.9268\n",
      "Epoch 307/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1805 - accuracy: 0.9307\n",
      "Epoch 308/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.3174 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 278.\n",
      "37/37 [==============================] - 0s 971us/step - loss: 0.1912 - accuracy: 0.9259\n",
      "Epoch 308: early stopping\n",
      "6/6 [==============================] - 0s 913us/step - loss: 0.9880 - accuracy: 0.6629\n",
      "6/6 [==============================] - 0s 709us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "After appending - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n",
      "Final Test Results - Loss: 0.9880028367042542, Accuracy: 0.6628571152687073, Precision: 0.6426675094816688, Recall: 0.7181397564128734, F1 Score: 0.6500602561335347\n",
      "Confusion Matrix:\n",
      " [[75  5 12]\n",
      " [ 1 11  0]\n",
      " [37  4 30]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.657406578993071\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8137785643339157\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7177841365337372\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6606832376793228\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6900438297951684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (82/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, kitten, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, senior, adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, kitten, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, adult, senior, senior, senior, adult,...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [adult, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, senior, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, senior, ...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C              [senior, senior, adult, adult, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                             [senior, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [kitten, adult, adult, kitten, adult, adult, k...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "48    042A  [kitten, adult, kitten, adult, adult, adult, a...         adult           kitten                  False\n",
       "69    063A  [adult, senior, senior, adult, senior, senior,...        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, adult,...        senior            adult                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "103   109A      [adult, adult, kitten, kitten, kitten, adult]         adult           kitten                  False\n",
       "102   108A     [kitten, adult, kitten, kitten, adult, kitten]        kitten           senior                  False\n",
       "101   106A  [adult, senior, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, kitten, ad...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "76    070A             [adult, senior, adult, senior, senior]        senior            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "57    051B  [adult, adult, kitten, adult, senior, adult, a...         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "109   117A  [adult, adult, adult, adult, adult, adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     61\n",
      "kitten    10\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             61  83.561644\n",
      "1           kitten           15             10  66.666667\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          558            440  78.853047\n",
      "1           kitten          118             80  67.796610\n",
      "2           senior          178             94  52.808989\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    222      162     72.97\n",
      "1          M    337      259     76.85\n",
      "2          X    295      193     65.42\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 846, 2: 765, 1: 715})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.0576 - accuracy: 0.5310\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.8775 - accuracy: 0.6165\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.7899 - accuracy: 0.6720\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 922us/step - loss: 0.7558 - accuracy: 0.6801\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 914us/step - loss: 0.7112 - accuracy: 0.7085\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 926us/step - loss: 0.6921 - accuracy: 0.7158\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.6745 - accuracy: 0.7180\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.6432 - accuracy: 0.7377\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.6519 - accuracy: 0.7347\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.6141 - accuracy: 0.7575\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.6166 - accuracy: 0.7524\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.6092 - accuracy: 0.7438\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.5681 - accuracy: 0.7739\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.5733 - accuracy: 0.7709\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.5538 - accuracy: 0.7704\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.5627 - accuracy: 0.7691\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.5542 - accuracy: 0.7640\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.5403 - accuracy: 0.7726\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 875us/step - loss: 0.5361 - accuracy: 0.7799\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.5278 - accuracy: 0.7825\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.5098 - accuracy: 0.7872\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.4995 - accuracy: 0.7949\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 929us/step - loss: 0.4910 - accuracy: 0.7966\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 982us/step - loss: 0.5054 - accuracy: 0.7880\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 954us/step - loss: 0.4894 - accuracy: 0.8005\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.4815 - accuracy: 0.8070\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 902us/step - loss: 0.4802 - accuracy: 0.8009\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 928us/step - loss: 0.4683 - accuracy: 0.8186\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.4779 - accuracy: 0.8022\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 888us/step - loss: 0.4545 - accuracy: 0.8104\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.4570 - accuracy: 0.8173\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.4489 - accuracy: 0.8147\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.4618 - accuracy: 0.8078\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.4630 - accuracy: 0.8048\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 977us/step - loss: 0.4475 - accuracy: 0.8160\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 941us/step - loss: 0.4317 - accuracy: 0.8207\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.4384 - accuracy: 0.8181\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 930us/step - loss: 0.4242 - accuracy: 0.8276\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.4407 - accuracy: 0.8138\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.4221 - accuracy: 0.8328\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 903us/step - loss: 0.4280 - accuracy: 0.8203\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.4189 - accuracy: 0.8306\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 871us/step - loss: 0.4212 - accuracy: 0.8263\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.4177 - accuracy: 0.8336\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.4035 - accuracy: 0.8302\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.4065 - accuracy: 0.8353\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 913us/step - loss: 0.4017 - accuracy: 0.8405\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4062 - accuracy: 0.8388\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4029 - accuracy: 0.8388\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 964us/step - loss: 0.3873 - accuracy: 0.8465\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 905us/step - loss: 0.3829 - accuracy: 0.8469\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 945us/step - loss: 0.3824 - accuracy: 0.8461\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 952us/step - loss: 0.3976 - accuracy: 0.8340\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 946us/step - loss: 0.3873 - accuracy: 0.8418\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8426\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 957us/step - loss: 0.3833 - accuracy: 0.8383\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 983us/step - loss: 0.3937 - accuracy: 0.8401\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3754 - accuracy: 0.8560\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3854 - accuracy: 0.8392\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3888 - accuracy: 0.8396\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.3737 - accuracy: 0.8512\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.3679 - accuracy: 0.8426\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.3780 - accuracy: 0.8491\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 882us/step - loss: 0.3605 - accuracy: 0.8577\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3605 - accuracy: 0.8534\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.3620 - accuracy: 0.8525\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.3528 - accuracy: 0.8607\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.3740 - accuracy: 0.8444\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 952us/step - loss: 0.3494 - accuracy: 0.8616\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.3525 - accuracy: 0.8534\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.3432 - accuracy: 0.8620\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.3534 - accuracy: 0.8508\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 967us/step - loss: 0.3343 - accuracy: 0.8577\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.3580 - accuracy: 0.8530\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.3398 - accuracy: 0.8663\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.3444 - accuracy: 0.8538\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.3273 - accuracy: 0.8702\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.3434 - accuracy: 0.8564\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.3439 - accuracy: 0.8573\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.3442 - accuracy: 0.8551\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.3459 - accuracy: 0.8598\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.3248 - accuracy: 0.8672\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3232 - accuracy: 0.8706\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.3174 - accuracy: 0.8792\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.3463 - accuracy: 0.8555\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.3404 - accuracy: 0.8629\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3412 - accuracy: 0.8581\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.3213 - accuracy: 0.8732\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.3316 - accuracy: 0.8680\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.3305 - accuracy: 0.8581\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.3147 - accuracy: 0.8680\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.3213 - accuracy: 0.8753\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.3310 - accuracy: 0.8598\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.3111 - accuracy: 0.8689\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.3034 - accuracy: 0.8723\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.3188 - accuracy: 0.8736\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.3235 - accuracy: 0.8693\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 895us/step - loss: 0.3322 - accuracy: 0.8616\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.3221 - accuracy: 0.8650\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 886us/step - loss: 0.3136 - accuracy: 0.8770\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.3133 - accuracy: 0.8710\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.3119 - accuracy: 0.8745\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.3021 - accuracy: 0.8826\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.3085 - accuracy: 0.8775\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3055 - accuracy: 0.8732\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2999 - accuracy: 0.8801\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.2858 - accuracy: 0.8917\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.3020 - accuracy: 0.8809\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2923 - accuracy: 0.8895\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.3085 - accuracy: 0.8783\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.2838 - accuracy: 0.8831\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2939 - accuracy: 0.8809\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.2918 - accuracy: 0.8826\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.2850 - accuracy: 0.8887\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2927 - accuracy: 0.8908\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2887 - accuracy: 0.8869\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2900 - accuracy: 0.8887\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.2925 - accuracy: 0.8878\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2976 - accuracy: 0.8839\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2829 - accuracy: 0.8869\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.2971 - accuracy: 0.8826\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.2874 - accuracy: 0.8882\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2834 - accuracy: 0.8801\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2960 - accuracy: 0.8809\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2720 - accuracy: 0.8891\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2907 - accuracy: 0.8861\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2907 - accuracy: 0.8809\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2701 - accuracy: 0.8960\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2896 - accuracy: 0.8844\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.2733 - accuracy: 0.8887\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.2772 - accuracy: 0.8887\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.2714 - accuracy: 0.8925\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2796 - accuracy: 0.8852\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.2700 - accuracy: 0.8934\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2745 - accuracy: 0.8951\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.8934\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 976us/step - loss: 0.2888 - accuracy: 0.8921\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 935us/step - loss: 0.2827 - accuracy: 0.8826\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2771 - accuracy: 0.8856\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.2607 - accuracy: 0.8934\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2610 - accuracy: 0.8938\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.2615 - accuracy: 0.8955\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.2626 - accuracy: 0.8981\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2604 - accuracy: 0.8960\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2609 - accuracy: 0.8929\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.2706 - accuracy: 0.8891\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2677 - accuracy: 0.8955\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.2607 - accuracy: 0.8938\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 914us/step - loss: 0.2608 - accuracy: 0.8960\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2671 - accuracy: 0.8955\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2540 - accuracy: 0.9054\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2615 - accuracy: 0.8985\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2507 - accuracy: 0.9046\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.2531 - accuracy: 0.9024\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.2735 - accuracy: 0.8813\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.2448 - accuracy: 0.9011\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.2466 - accuracy: 0.8968\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2488 - accuracy: 0.9020\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.2507 - accuracy: 0.9024\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.2487 - accuracy: 0.9020\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 888us/step - loss: 0.2480 - accuracy: 0.9003\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2433 - accuracy: 0.9093\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2386 - accuracy: 0.9063\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2404 - accuracy: 0.9084\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2514 - accuracy: 0.8977\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2386 - accuracy: 0.9089\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2471 - accuracy: 0.9097\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.2379 - accuracy: 0.9093\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.2590 - accuracy: 0.9007\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2356 - accuracy: 0.9050\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.2315 - accuracy: 0.9063\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2577 - accuracy: 0.8981\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2367 - accuracy: 0.9101\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2461 - accuracy: 0.9007\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2474 - accuracy: 0.9063\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.2306 - accuracy: 0.9033\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2335 - accuracy: 0.9084\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2240 - accuracy: 0.9106\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.2243 - accuracy: 0.9110\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2327 - accuracy: 0.9089\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2384 - accuracy: 0.9063\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 971us/step - loss: 0.2394 - accuracy: 0.9071\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9114\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 909us/step - loss: 0.2377 - accuracy: 0.9106\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2421 - accuracy: 0.9101\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2307 - accuracy: 0.9084\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.2198 - accuracy: 0.9144\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2249 - accuracy: 0.9119\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2241 - accuracy: 0.9140\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2316 - accuracy: 0.9050\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.2349 - accuracy: 0.9067\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2355 - accuracy: 0.9093\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.2395 - accuracy: 0.9011\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.2159 - accuracy: 0.9166\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.2204 - accuracy: 0.9110\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.2365 - accuracy: 0.9097\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.2211 - accuracy: 0.9110\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.2320 - accuracy: 0.9093\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.2270 - accuracy: 0.9149\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.2212 - accuracy: 0.9123\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2314 - accuracy: 0.9106\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2265 - accuracy: 0.9127\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.2256 - accuracy: 0.9127\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.2307 - accuracy: 0.9132\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.2179 - accuracy: 0.9119\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2028 - accuracy: 0.9170\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.2295 - accuracy: 0.9093\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2163 - accuracy: 0.9132\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2271 - accuracy: 0.9114\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2242 - accuracy: 0.9123\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2155 - accuracy: 0.9157\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2354 - accuracy: 0.9093\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2223 - accuracy: 0.9144\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.2131 - accuracy: 0.9166\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.2060 - accuracy: 0.9261\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2109 - accuracy: 0.9166\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2160 - accuracy: 0.9162\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2239 - accuracy: 0.9132\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.2292 - accuracy: 0.9071\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2172 - accuracy: 0.9162\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.2144 - accuracy: 0.9200\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2162 - accuracy: 0.9127\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2057 - accuracy: 0.9196\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2154 - accuracy: 0.9123\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2044 - accuracy: 0.9205\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.1984 - accuracy: 0.9261\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.1973 - accuracy: 0.9273\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.2194 - accuracy: 0.9097\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.2071 - accuracy: 0.9218\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2205 - accuracy: 0.9119\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2104 - accuracy: 0.9230\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2225 - accuracy: 0.9149\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.2079 - accuracy: 0.9114\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2240 - accuracy: 0.9110\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.1972 - accuracy: 0.9252\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2280 - accuracy: 0.9071\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.1931 - accuracy: 0.9252\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.2078 - accuracy: 0.9166\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1942 - accuracy: 0.9243\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.2017 - accuracy: 0.9205\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.1979 - accuracy: 0.9213\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.2018 - accuracy: 0.9239\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2180 - accuracy: 0.9101\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.1970 - accuracy: 0.9230\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2020 - accuracy: 0.9218\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.1909 - accuracy: 0.9295\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.2077 - accuracy: 0.9248\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.1958 - accuracy: 0.9179\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.1975 - accuracy: 0.9175\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2067 - accuracy: 0.9192\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.1915 - accuracy: 0.9291\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.1989 - accuracy: 0.9265\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.1825 - accuracy: 0.9325\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.1879 - accuracy: 0.9278\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.1672 - accuracy: 0.9347\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2088 - accuracy: 0.9200\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.1969 - accuracy: 0.9235\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1919 - accuracy: 0.9321\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1857 - accuracy: 0.9278\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1816 - accuracy: 0.9295\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.1827 - accuracy: 0.9243\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.1812 - accuracy: 0.9338\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.1988 - accuracy: 0.9235\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.1839 - accuracy: 0.9209\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2023 - accuracy: 0.9200\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.1955 - accuracy: 0.9222\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.1732 - accuracy: 0.9390\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.1891 - accuracy: 0.9261\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.2174 - accuracy: 0.9114\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2002 - accuracy: 0.9179\n",
      "Epoch 271/1500\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.1849 - accuracy: 0.9342\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2014 - accuracy: 0.9218\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.1887 - accuracy: 0.9282\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.1834 - accuracy: 0.9321\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.1896 - accuracy: 0.9273\n",
      "Epoch 276/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.1787 - accuracy: 0.9321\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.1827 - accuracy: 0.9291\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.1965 - accuracy: 0.9222\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1903 - accuracy: 0.9269\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1835 - accuracy: 0.9338\n",
      "Epoch 281/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.1969 - accuracy: 0.9222\n",
      "Epoch 282/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.1906 - accuracy: 0.9278\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.1763 - accuracy: 0.9312\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9261\n",
      "Epoch 285/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.2078 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 255.\n",
      "37/37 [==============================] - 0s 973us/step - loss: 0.1710 - accuracy: 0.9342\n",
      "Epoch 285: early stopping\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.0085 - accuracy: 0.6284\n",
      "7/7 [==============================] - 0s 638us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.60 (15/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 1.0085262060165405, Accuracy: 0.6284403800964355, Precision: 0.5704861111111111, Recall: 0.596046176046176, F1 Score: 0.5646449611973775\n",
      "Confusion Matrix:\n",
      " [[106   8  51]\n",
      " [  6  22   0]\n",
      " [ 16   0   9]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'020A'}\n",
      "Moved to Test Set:\n",
      "{'020A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '018A' '019A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '011A' '014B' '015A' '016A' '019B' '020A' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "711\n",
      "Length of y_train_val:\n",
      "711\n",
      "Length of groups_train_val:\n",
      "711\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     446\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     142\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 892, 1: 680, 2: 645})\n",
      "Epoch 1/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 1.1216 - accuracy: 0.4867\n",
      "Epoch 2/1500\n",
      "35/35 [==============================] - 0s 992us/step - loss: 0.9112 - accuracy: 0.5900\n",
      "Epoch 3/1500\n",
      "35/35 [==============================] - 0s 942us/step - loss: 0.8576 - accuracy: 0.6247\n",
      "Epoch 4/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.8044 - accuracy: 0.6581\n",
      "Epoch 5/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.7711 - accuracy: 0.6631\n",
      "Epoch 6/1500\n",
      "35/35 [==============================] - 0s 796us/step - loss: 0.7270 - accuracy: 0.6955\n",
      "Epoch 7/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.7294 - accuracy: 0.6861\n",
      "Epoch 8/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.6901 - accuracy: 0.7104\n",
      "Epoch 9/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.6587 - accuracy: 0.7163\n",
      "Epoch 10/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.6676 - accuracy: 0.7068\n",
      "Epoch 11/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.6638 - accuracy: 0.7145\n",
      "Epoch 12/1500\n",
      "35/35 [==============================] - 0s 878us/step - loss: 0.6245 - accuracy: 0.7226\n",
      "Epoch 13/1500\n",
      "35/35 [==============================] - 0s 911us/step - loss: 0.6031 - accuracy: 0.7510\n",
      "Epoch 14/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.5994 - accuracy: 0.7415\n",
      "Epoch 15/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.5923 - accuracy: 0.7501\n",
      "Epoch 16/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.5878 - accuracy: 0.7524\n",
      "Epoch 17/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.5728 - accuracy: 0.7569\n",
      "Epoch 18/1500\n",
      "35/35 [==============================] - 0s 884us/step - loss: 0.5551 - accuracy: 0.7659\n",
      "Epoch 19/1500\n",
      "35/35 [==============================] - 0s 845us/step - loss: 0.5523 - accuracy: 0.7673\n",
      "Epoch 20/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.5677 - accuracy: 0.7600\n",
      "Epoch 21/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.5333 - accuracy: 0.7668\n",
      "Epoch 22/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.5393 - accuracy: 0.7686\n",
      "Epoch 23/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.5275 - accuracy: 0.7718\n",
      "Epoch 24/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.5089 - accuracy: 0.7826\n",
      "Epoch 25/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.5218 - accuracy: 0.7794\n",
      "Epoch 26/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.4968 - accuracy: 0.7903\n",
      "Epoch 27/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.5042 - accuracy: 0.7939\n",
      "Epoch 28/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.4916 - accuracy: 0.7866\n",
      "Epoch 29/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.4819 - accuracy: 0.8060\n",
      "Epoch 30/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.4844 - accuracy: 0.7948\n",
      "Epoch 31/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.4633 - accuracy: 0.8056\n",
      "Epoch 32/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.4741 - accuracy: 0.7943\n",
      "Epoch 33/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.4675 - accuracy: 0.8056\n",
      "Epoch 34/1500\n",
      "35/35 [==============================] - 0s 840us/step - loss: 0.4654 - accuracy: 0.8088\n",
      "Epoch 35/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.4609 - accuracy: 0.8024\n",
      "Epoch 36/1500\n",
      "35/35 [==============================] - 0s 892us/step - loss: 0.4442 - accuracy: 0.8173\n",
      "Epoch 37/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.4638 - accuracy: 0.8069\n",
      "Epoch 38/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.4464 - accuracy: 0.8088\n",
      "Epoch 39/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.4330 - accuracy: 0.8214\n",
      "Epoch 40/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.4276 - accuracy: 0.8173\n",
      "Epoch 41/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.4203 - accuracy: 0.8182\n",
      "Epoch 42/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.4298 - accuracy: 0.8137\n",
      "Epoch 43/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.4357 - accuracy: 0.8263\n",
      "Epoch 44/1500\n",
      "35/35 [==============================] - 0s 855us/step - loss: 0.4109 - accuracy: 0.8259\n",
      "Epoch 45/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.4121 - accuracy: 0.8236\n",
      "Epoch 46/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.4243 - accuracy: 0.8223\n",
      "Epoch 47/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.4131 - accuracy: 0.8295\n",
      "Epoch 48/1500\n",
      "35/35 [==============================] - 0s 923us/step - loss: 0.4085 - accuracy: 0.8372\n",
      "Epoch 49/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.3952 - accuracy: 0.8286\n",
      "Epoch 50/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.3933 - accuracy: 0.8358\n",
      "Epoch 51/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.3882 - accuracy: 0.8358\n",
      "Epoch 52/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.3860 - accuracy: 0.8435\n",
      "Epoch 53/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.3916 - accuracy: 0.8403\n",
      "Epoch 54/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.3756 - accuracy: 0.8462\n",
      "Epoch 55/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3831 - accuracy: 0.8358\n",
      "Epoch 56/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.3739 - accuracy: 0.8498\n",
      "Epoch 57/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.3803 - accuracy: 0.8439\n",
      "Epoch 58/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.3749 - accuracy: 0.8426\n",
      "Epoch 59/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.3880 - accuracy: 0.8376\n",
      "Epoch 60/1500\n",
      "35/35 [==============================] - 0s 940us/step - loss: 0.3687 - accuracy: 0.8484\n",
      "Epoch 61/1500\n",
      "35/35 [==============================] - 0s 950us/step - loss: 0.3909 - accuracy: 0.8408\n",
      "Epoch 62/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3636 - accuracy: 0.8466\n",
      "Epoch 63/1500\n",
      "35/35 [==============================] - 0s 988us/step - loss: 0.3620 - accuracy: 0.8471\n",
      "Epoch 64/1500\n",
      "35/35 [==============================] - 0s 922us/step - loss: 0.3581 - accuracy: 0.8507\n",
      "Epoch 65/1500\n",
      "35/35 [==============================] - 0s 954us/step - loss: 0.3695 - accuracy: 0.8412\n",
      "Epoch 66/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.3577 - accuracy: 0.8512\n",
      "Epoch 67/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.3585 - accuracy: 0.8525\n",
      "Epoch 68/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.3507 - accuracy: 0.8548\n",
      "Epoch 69/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3553 - accuracy: 0.8475\n",
      "Epoch 70/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.3483 - accuracy: 0.8471\n",
      "Epoch 71/1500\n",
      "35/35 [==============================] - 0s 982us/step - loss: 0.3418 - accuracy: 0.8557\n",
      "Epoch 72/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.3466 - accuracy: 0.8566\n",
      "Epoch 73/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.3441 - accuracy: 0.8539\n",
      "Epoch 74/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.3451 - accuracy: 0.8579\n",
      "Epoch 75/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.3428 - accuracy: 0.8588\n",
      "Epoch 76/1500\n",
      "35/35 [==============================] - 0s 947us/step - loss: 0.3456 - accuracy: 0.8548\n",
      "Epoch 77/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.3434 - accuracy: 0.8552\n",
      "Epoch 78/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.3513 - accuracy: 0.8548\n",
      "Epoch 79/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.3241 - accuracy: 0.8629\n",
      "Epoch 80/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.3365 - accuracy: 0.8611\n",
      "Epoch 81/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.3193 - accuracy: 0.8751\n",
      "Epoch 82/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.3369 - accuracy: 0.8687\n",
      "Epoch 83/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.3179 - accuracy: 0.8746\n",
      "Epoch 84/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.3278 - accuracy: 0.8602\n",
      "Epoch 85/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.3284 - accuracy: 0.8665\n",
      "Epoch 86/1500\n",
      "35/35 [==============================] - 0s 923us/step - loss: 0.3132 - accuracy: 0.8755\n",
      "Epoch 87/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.3350 - accuracy: 0.8624\n",
      "Epoch 88/1500\n",
      "35/35 [==============================] - 0s 889us/step - loss: 0.3109 - accuracy: 0.8764\n",
      "Epoch 89/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.3267 - accuracy: 0.8584\n",
      "Epoch 90/1500\n",
      "35/35 [==============================] - 0s 910us/step - loss: 0.3217 - accuracy: 0.8606\n",
      "Epoch 91/1500\n",
      "35/35 [==============================] - 0s 878us/step - loss: 0.3132 - accuracy: 0.8678\n",
      "Epoch 92/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.3113 - accuracy: 0.8769\n",
      "Epoch 93/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.3094 - accuracy: 0.8683\n",
      "Epoch 94/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.3102 - accuracy: 0.8773\n",
      "Epoch 95/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.3175 - accuracy: 0.8660\n",
      "Epoch 96/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.2935 - accuracy: 0.8809\n",
      "Epoch 97/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.3016 - accuracy: 0.8773\n",
      "Epoch 98/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.3102 - accuracy: 0.8719\n",
      "Epoch 99/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.3148 - accuracy: 0.8728\n",
      "Epoch 100/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.3099 - accuracy: 0.8791\n",
      "Epoch 101/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.2889 - accuracy: 0.8886\n",
      "Epoch 102/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.3159 - accuracy: 0.8651\n",
      "Epoch 103/1500\n",
      "35/35 [==============================] - 0s 904us/step - loss: 0.3044 - accuracy: 0.8719\n",
      "Epoch 104/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2984 - accuracy: 0.8809\n",
      "Epoch 105/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.3007 - accuracy: 0.8710\n",
      "Epoch 106/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.2915 - accuracy: 0.8868\n",
      "Epoch 107/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.3017 - accuracy: 0.8841\n",
      "Epoch 108/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.3006 - accuracy: 0.8782\n",
      "Epoch 109/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3034 - accuracy: 0.8710\n",
      "Epoch 110/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.2995 - accuracy: 0.8827\n",
      "Epoch 111/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.2817 - accuracy: 0.8945\n",
      "Epoch 112/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.2944 - accuracy: 0.8796\n",
      "Epoch 113/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2917 - accuracy: 0.8877\n",
      "Epoch 114/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.2607 - accuracy: 0.8949\n",
      "Epoch 115/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.2795 - accuracy: 0.8913\n",
      "Epoch 116/1500\n",
      "35/35 [==============================] - 0s 855us/step - loss: 0.2958 - accuracy: 0.8769\n",
      "Epoch 117/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.2687 - accuracy: 0.8931\n",
      "Epoch 118/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.2825 - accuracy: 0.8872\n",
      "Epoch 119/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.2719 - accuracy: 0.8935\n",
      "Epoch 120/1500\n",
      "35/35 [==============================] - 0s 824us/step - loss: 0.2770 - accuracy: 0.8868\n",
      "Epoch 121/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.2846 - accuracy: 0.8832\n",
      "Epoch 122/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2767 - accuracy: 0.8832\n",
      "Epoch 123/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2656 - accuracy: 0.8899\n",
      "Epoch 124/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2676 - accuracy: 0.8899\n",
      "Epoch 125/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.2702 - accuracy: 0.8926\n",
      "Epoch 126/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.2836 - accuracy: 0.8755\n",
      "Epoch 127/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2776 - accuracy: 0.8908\n",
      "Epoch 128/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.2762 - accuracy: 0.8926\n",
      "Epoch 129/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2691 - accuracy: 0.8913\n",
      "Epoch 130/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2699 - accuracy: 0.8913\n",
      "Epoch 131/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.2629 - accuracy: 0.8922\n",
      "Epoch 132/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2756 - accuracy: 0.8922\n",
      "Epoch 133/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.2690 - accuracy: 0.8972\n",
      "Epoch 134/1500\n",
      "35/35 [==============================] - 0s 845us/step - loss: 0.2632 - accuracy: 0.8913\n",
      "Epoch 135/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.2707 - accuracy: 0.8945\n",
      "Epoch 136/1500\n",
      "35/35 [==============================] - 0s 911us/step - loss: 0.2609 - accuracy: 0.8940\n",
      "Epoch 137/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.2597 - accuracy: 0.8999\n",
      "Epoch 138/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.2458 - accuracy: 0.9021\n",
      "Epoch 139/1500\n",
      "35/35 [==============================] - 0s 852us/step - loss: 0.2583 - accuracy: 0.8985\n",
      "Epoch 140/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.2547 - accuracy: 0.8940\n",
      "Epoch 141/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.2514 - accuracy: 0.9026\n",
      "Epoch 142/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2486 - accuracy: 0.9026\n",
      "Epoch 143/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.9026\n",
      "Epoch 144/1500\n",
      "35/35 [==============================] - 0s 974us/step - loss: 0.2454 - accuracy: 0.9048\n",
      "Epoch 145/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.2595 - accuracy: 0.8963\n",
      "Epoch 146/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2384 - accuracy: 0.9134\n",
      "Epoch 147/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2494 - accuracy: 0.8990\n",
      "Epoch 148/1500\n",
      "35/35 [==============================] - 0s 912us/step - loss: 0.2551 - accuracy: 0.9008\n",
      "Epoch 149/1500\n",
      "35/35 [==============================] - 0s 959us/step - loss: 0.2395 - accuracy: 0.9053\n",
      "Epoch 150/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.2438 - accuracy: 0.9003\n",
      "Epoch 151/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2438 - accuracy: 0.8985\n",
      "Epoch 152/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2438 - accuracy: 0.9084\n",
      "Epoch 153/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2404 - accuracy: 0.9012\n",
      "Epoch 154/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2515 - accuracy: 0.9044\n",
      "Epoch 155/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2419 - accuracy: 0.9053\n",
      "Epoch 156/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.2481 - accuracy: 0.9089\n",
      "Epoch 157/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.2370 - accuracy: 0.9044\n",
      "Epoch 158/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2434 - accuracy: 0.9008\n",
      "Epoch 159/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2368 - accuracy: 0.9017\n",
      "Epoch 160/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.2433 - accuracy: 0.9026\n",
      "Epoch 161/1500\n",
      "35/35 [==============================] - 0s 855us/step - loss: 0.2388 - accuracy: 0.9035\n",
      "Epoch 162/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2403 - accuracy: 0.9080\n",
      "Epoch 163/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.2339 - accuracy: 0.9053\n",
      "Epoch 164/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.2307 - accuracy: 0.9166\n",
      "Epoch 165/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2503 - accuracy: 0.8999\n",
      "Epoch 166/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.2300 - accuracy: 0.9111\n",
      "Epoch 167/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.2262 - accuracy: 0.9111\n",
      "Epoch 168/1500\n",
      "35/35 [==============================] - 0s 938us/step - loss: 0.2496 - accuracy: 0.9008\n",
      "Epoch 169/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2285 - accuracy: 0.9080\n",
      "Epoch 170/1500\n",
      "35/35 [==============================] - 0s 810us/step - loss: 0.2358 - accuracy: 0.9075\n",
      "Epoch 171/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.2364 - accuracy: 0.9030\n",
      "Epoch 172/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2290 - accuracy: 0.9017\n",
      "Epoch 173/1500\n",
      "35/35 [==============================] - 0s 922us/step - loss: 0.2477 - accuracy: 0.9017\n",
      "Epoch 174/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.2388 - accuracy: 0.9080\n",
      "Epoch 175/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2312 - accuracy: 0.9098\n",
      "Epoch 176/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.2303 - accuracy: 0.9066\n",
      "Epoch 177/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2266 - accuracy: 0.9093\n",
      "Epoch 178/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.2358 - accuracy: 0.8994\n",
      "Epoch 179/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.2163 - accuracy: 0.9184\n",
      "Epoch 180/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.2259 - accuracy: 0.9044\n",
      "Epoch 181/1500\n",
      "35/35 [==============================] - 0s 858us/step - loss: 0.2271 - accuracy: 0.9048\n",
      "Epoch 182/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2249 - accuracy: 0.9107\n",
      "Epoch 183/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.2130 - accuracy: 0.9116\n",
      "Epoch 184/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.2230 - accuracy: 0.9120\n",
      "Epoch 185/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.2215 - accuracy: 0.9206\n",
      "Epoch 186/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.2196 - accuracy: 0.9179\n",
      "Epoch 187/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.2309 - accuracy: 0.9075\n",
      "Epoch 188/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2281 - accuracy: 0.9107\n",
      "Epoch 189/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.2204 - accuracy: 0.9129\n",
      "Epoch 190/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.1992 - accuracy: 0.9197\n",
      "Epoch 191/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.2208 - accuracy: 0.9116\n",
      "Epoch 192/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2135 - accuracy: 0.9161\n",
      "Epoch 193/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.2183 - accuracy: 0.9161\n",
      "Epoch 194/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.2268 - accuracy: 0.9039\n",
      "Epoch 195/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.2288 - accuracy: 0.9057\n",
      "Epoch 196/1500\n",
      "35/35 [==============================] - 0s 969us/step - loss: 0.2125 - accuracy: 0.9175\n",
      "Epoch 197/1500\n",
      "35/35 [==============================] - 0s 942us/step - loss: 0.2041 - accuracy: 0.9197\n",
      "Epoch 198/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.2243 - accuracy: 0.9107\n",
      "Epoch 199/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.2008 - accuracy: 0.9211\n",
      "Epoch 200/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2243 - accuracy: 0.9084\n",
      "Epoch 201/1500\n",
      "35/35 [==============================] - 0s 932us/step - loss: 0.2094 - accuracy: 0.9233\n",
      "Epoch 202/1500\n",
      "35/35 [==============================] - 0s 910us/step - loss: 0.2279 - accuracy: 0.9084\n",
      "Epoch 203/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9188\n",
      "Epoch 204/1500\n",
      "35/35 [==============================] - 0s 930us/step - loss: 0.2007 - accuracy: 0.9197\n",
      "Epoch 205/1500\n",
      "35/35 [==============================] - 0s 906us/step - loss: 0.2246 - accuracy: 0.9129\n",
      "Epoch 206/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2215 - accuracy: 0.9071\n",
      "Epoch 207/1500\n",
      "35/35 [==============================] - 0s 904us/step - loss: 0.2228 - accuracy: 0.9147\n",
      "Epoch 208/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.2064 - accuracy: 0.9184\n",
      "Epoch 209/1500\n",
      "35/35 [==============================] - 0s 912us/step - loss: 0.2022 - accuracy: 0.9224\n",
      "Epoch 210/1500\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.2327 - accuracy: 0.9129\n",
      "Epoch 211/1500\n",
      "35/35 [==============================] - 0s 972us/step - loss: 0.2003 - accuracy: 0.9188\n",
      "Epoch 212/1500\n",
      "35/35 [==============================] - 0s 914us/step - loss: 0.2053 - accuracy: 0.9138\n",
      "Epoch 213/1500\n",
      "35/35 [==============================] - 0s 917us/step - loss: 0.2076 - accuracy: 0.9175\n",
      "Epoch 214/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.2128 - accuracy: 0.9129\n",
      "Epoch 215/1500\n",
      "35/35 [==============================] - 0s 894us/step - loss: 0.1905 - accuracy: 0.9274\n",
      "Epoch 216/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.2004 - accuracy: 0.9260\n",
      "Epoch 217/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.2093 - accuracy: 0.9197\n",
      "Epoch 218/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.1953 - accuracy: 0.9238\n",
      "Epoch 219/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9238\n",
      "Epoch 220/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.1945 - accuracy: 0.9260\n",
      "Epoch 221/1500\n",
      "35/35 [==============================] - 0s 966us/step - loss: 0.1929 - accuracy: 0.9197\n",
      "Epoch 222/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9206\n",
      "Epoch 223/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9202\n",
      "Epoch 224/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9229\n",
      "Epoch 225/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9242\n",
      "Epoch 226/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9089\n",
      "Epoch 227/1500\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.2092 - accuracy: 0.9134\n",
      "Epoch 228/1500\n",
      "35/35 [==============================] - 0s 941us/step - loss: 0.1931 - accuracy: 0.9247\n",
      "Epoch 229/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9251\n",
      "Epoch 230/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9224\n",
      "Epoch 231/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9256\n",
      "Epoch 232/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9328\n",
      "Epoch 233/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9274\n",
      "Epoch 234/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9260\n",
      "Epoch 235/1500\n",
      "35/35 [==============================] - 0s 996us/step - loss: 0.1901 - accuracy: 0.9211\n",
      "Epoch 236/1500\n",
      "35/35 [==============================] - 0s 999us/step - loss: 0.1798 - accuracy: 0.9314\n",
      "Epoch 237/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9323\n",
      "Epoch 238/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9256\n",
      "Epoch 239/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9175\n",
      "Epoch 240/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.1881 - accuracy: 0.9238\n",
      "Epoch 241/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9224\n",
      "Epoch 242/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9314\n",
      "Epoch 243/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1883 - accuracy: 0.9283\n",
      "Epoch 244/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.1861 - accuracy: 0.9278\n",
      "Epoch 245/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.1891 - accuracy: 0.9274\n",
      "Epoch 246/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.1916 - accuracy: 0.9242\n",
      "Epoch 247/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2093 - accuracy: 0.9125\n",
      "Epoch 248/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.1926 - accuracy: 0.9256\n",
      "Epoch 249/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9242\n",
      "Epoch 250/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9206\n",
      "Epoch 251/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9278\n",
      "Epoch 252/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9251\n",
      "Epoch 253/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9247\n",
      "Epoch 254/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.1694 - accuracy: 0.9310\n",
      "Epoch 255/1500\n",
      "35/35 [==============================] - 0s 907us/step - loss: 0.1939 - accuracy: 0.9247\n",
      "Epoch 256/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9265\n",
      "Epoch 257/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9292\n",
      "Epoch 258/1500\n",
      "35/35 [==============================] - 0s 956us/step - loss: 0.1730 - accuracy: 0.9296\n",
      "Epoch 259/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9405\n",
      "Epoch 260/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9220\n",
      "Epoch 261/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9296\n",
      "Epoch 262/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9369\n",
      "Epoch 263/1500\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9310\n",
      "Epoch 264/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9319\n",
      "Epoch 265/1500\n",
      "35/35 [==============================] - 0s 908us/step - loss: 0.1848 - accuracy: 0.9319\n",
      "Epoch 266/1500\n",
      "35/35 [==============================] - 0s 900us/step - loss: 0.1741 - accuracy: 0.9332\n",
      "Epoch 267/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.1763 - accuracy: 0.9215\n",
      "Epoch 268/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.1746 - accuracy: 0.9359\n",
      "Epoch 269/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9260\n",
      "Epoch 270/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9314\n",
      "Epoch 271/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1801 - accuracy: 0.9292\n",
      "Epoch 272/1500\n",
      "35/35 [==============================] - 0s 873us/step - loss: 0.1873 - accuracy: 0.9242\n",
      "Epoch 273/1500\n",
      "35/35 [==============================] - 0s 985us/step - loss: 0.1825 - accuracy: 0.9337\n",
      "Epoch 274/1500\n",
      "35/35 [==============================] - 0s 919us/step - loss: 0.1763 - accuracy: 0.9283\n",
      "Epoch 275/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.1677 - accuracy: 0.9369\n",
      "Epoch 276/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.1652 - accuracy: 0.9328\n",
      "Epoch 277/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.1652 - accuracy: 0.9405\n",
      "Epoch 278/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.1740 - accuracy: 0.9341\n",
      "Epoch 279/1500\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.1627 - accuracy: 0.9391\n",
      "Epoch 280/1500\n",
      "35/35 [==============================] - 0s 946us/step - loss: 0.1754 - accuracy: 0.9305\n",
      "Epoch 281/1500\n",
      "35/35 [==============================] - 0s 930us/step - loss: 0.1717 - accuracy: 0.9350\n",
      "Epoch 282/1500\n",
      "35/35 [==============================] - 0s 974us/step - loss: 0.1695 - accuracy: 0.9323\n",
      "Epoch 283/1500\n",
      "35/35 [==============================] - 0s 941us/step - loss: 0.1840 - accuracy: 0.9310\n",
      "Epoch 284/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.1811 - accuracy: 0.9314\n",
      "Epoch 285/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.1629 - accuracy: 0.9405\n",
      "Epoch 286/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.1929 - accuracy: 0.9238\n",
      "Epoch 287/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.1740 - accuracy: 0.9337\n",
      "Epoch 288/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.1583 - accuracy: 0.9373\n",
      "Epoch 289/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.1692 - accuracy: 0.9328\n",
      "Epoch 290/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.1518 - accuracy: 0.9432\n",
      "Epoch 291/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.1670 - accuracy: 0.9323\n",
      "Epoch 292/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.1728 - accuracy: 0.9328\n",
      "Epoch 293/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.1636 - accuracy: 0.9364\n",
      "Epoch 294/1500\n",
      "35/35 [==============================] - 0s 883us/step - loss: 0.1612 - accuracy: 0.9400\n",
      "Epoch 295/1500\n",
      "35/35 [==============================] - 0s 885us/step - loss: 0.1667 - accuracy: 0.9414\n",
      "Epoch 296/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.1589 - accuracy: 0.9387\n",
      "Epoch 297/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1676 - accuracy: 0.9369\n",
      "Epoch 298/1500\n",
      "35/35 [==============================] - 0s 801us/step - loss: 0.1590 - accuracy: 0.9400\n",
      "Epoch 299/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1550 - accuracy: 0.9373\n",
      "Epoch 300/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.1591 - accuracy: 0.9409\n",
      "Epoch 301/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.1573 - accuracy: 0.9427\n",
      "Epoch 302/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.1554 - accuracy: 0.9432\n",
      "Epoch 303/1500\n",
      "35/35 [==============================] - 0s 824us/step - loss: 0.1756 - accuracy: 0.9310\n",
      "Epoch 304/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1545 - accuracy: 0.9409\n",
      "Epoch 305/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1718 - accuracy: 0.9359\n",
      "Epoch 306/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.1701 - accuracy: 0.9319\n",
      "Epoch 307/1500\n",
      "35/35 [==============================] - 0s 819us/step - loss: 0.1634 - accuracy: 0.9355\n",
      "Epoch 308/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1589 - accuracy: 0.9418\n",
      "Epoch 309/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.1527 - accuracy: 0.9414\n",
      "Epoch 310/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1588 - accuracy: 0.9423\n",
      "Epoch 311/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.1569 - accuracy: 0.9387\n",
      "Epoch 312/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.1612 - accuracy: 0.9364\n",
      "Epoch 313/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1665 - accuracy: 0.9387\n",
      "Epoch 314/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.1841 - accuracy: 0.9256\n",
      "Epoch 315/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1651 - accuracy: 0.9369\n",
      "Epoch 316/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.1565 - accuracy: 0.9414\n",
      "Epoch 317/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.1751 - accuracy: 0.9310\n",
      "Epoch 318/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.1658 - accuracy: 0.9337\n",
      "Epoch 319/1500\n",
      "35/35 [==============================] - 0s 824us/step - loss: 0.1637 - accuracy: 0.9319\n",
      "Epoch 320/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.1395 - accuracy: 0.9490\n",
      "Epoch 321/1500\n",
      "35/35 [==============================] - 0s 791us/step - loss: 0.1465 - accuracy: 0.9454\n",
      "Epoch 322/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.1611 - accuracy: 0.9414\n",
      "Epoch 323/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.1521 - accuracy: 0.9409\n",
      "Epoch 324/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.1602 - accuracy: 0.9369\n",
      "Epoch 325/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.1553 - accuracy: 0.9414\n",
      "Epoch 326/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1651 - accuracy: 0.9337\n",
      "Epoch 327/1500\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.1540 - accuracy: 0.9436\n",
      "Epoch 328/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.1565 - accuracy: 0.9468\n",
      "Epoch 329/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.1685 - accuracy: 0.9350\n",
      "Epoch 330/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.1583 - accuracy: 0.9391\n",
      "Epoch 331/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1484 - accuracy: 0.9450\n",
      "Epoch 332/1500\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.1529 - accuracy: 0.9427\n",
      "Epoch 333/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.1540 - accuracy: 0.9427\n",
      "Epoch 334/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1558 - accuracy: 0.9391\n",
      "Epoch 335/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.1700 - accuracy: 0.9337\n",
      "Epoch 336/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1704 - accuracy: 0.9364\n",
      "Epoch 337/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.1399 - accuracy: 0.9454\n",
      "Epoch 338/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.1480 - accuracy: 0.9418\n",
      "Epoch 339/1500\n",
      "35/35 [==============================] - 0s 806us/step - loss: 0.1419 - accuracy: 0.9522\n",
      "Epoch 340/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.1497 - accuracy: 0.9396\n",
      "Epoch 341/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.1498 - accuracy: 0.9409\n",
      "Epoch 342/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.1559 - accuracy: 0.9414\n",
      "Epoch 343/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.1305 - accuracy: 0.9504\n",
      "Epoch 344/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1492 - accuracy: 0.9432\n",
      "Epoch 345/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.1444 - accuracy: 0.9382\n",
      "Epoch 346/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1429 - accuracy: 0.9472\n",
      "Epoch 347/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.1549 - accuracy: 0.9409\n",
      "Epoch 348/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1433 - accuracy: 0.9454\n",
      "Epoch 349/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.1546 - accuracy: 0.9373\n",
      "Epoch 350/1500\n",
      "35/35 [==============================] - 0s 812us/step - loss: 0.1392 - accuracy: 0.9472\n",
      "Epoch 351/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1505 - accuracy: 0.9414\n",
      "Epoch 352/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.1549 - accuracy: 0.9418\n",
      "Epoch 353/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1506 - accuracy: 0.9445\n",
      "Epoch 354/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1422 - accuracy: 0.9432\n",
      "Epoch 355/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.1510 - accuracy: 0.9432\n",
      "Epoch 356/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1488 - accuracy: 0.9423\n",
      "Epoch 357/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.1373 - accuracy: 0.9423\n",
      "Epoch 358/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1439 - accuracy: 0.9454\n",
      "Epoch 359/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.1357 - accuracy: 0.9522\n",
      "Epoch 360/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.1424 - accuracy: 0.9427\n",
      "Epoch 361/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.1476 - accuracy: 0.9436\n",
      "Epoch 362/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.1521 - accuracy: 0.9364\n",
      "Epoch 363/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.1513 - accuracy: 0.9373\n",
      "Epoch 364/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.1454 - accuracy: 0.9463\n",
      "Epoch 365/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.1583 - accuracy: 0.9405\n",
      "Epoch 366/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.1558 - accuracy: 0.9423\n",
      "Epoch 367/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1437 - accuracy: 0.9409\n",
      "Epoch 368/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.1314 - accuracy: 0.9504\n",
      "Epoch 369/1500\n",
      "35/35 [==============================] - 0s 815us/step - loss: 0.1468 - accuracy: 0.9427\n",
      "Epoch 370/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1368 - accuracy: 0.9490\n",
      "Epoch 371/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1318 - accuracy: 0.9504\n",
      "Epoch 372/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1327 - accuracy: 0.9517\n",
      "Epoch 373/1500\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.1205 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 343.\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.1446 - accuracy: 0.9427\n",
      "Epoch 373: early stopping\n",
      "8/8 [==============================] - 0s 820us/step - loss: 0.9011 - accuracy: 0.7566\n",
      "8/8 [==============================] - 0s 594us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.71 (20/28)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "Final Test Results - Loss: 0.9010636210441589, Accuracy: 0.7566371560096741, Precision: 0.7305316170987813, Recall: 0.7513461722717256, F1 Score: 0.739904620339403\n",
      "Confusion Matrix:\n",
      " [[112   4  26]\n",
      " [  1  32   2]\n",
      " [ 21   1  27]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '045A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '044A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    150\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 940, 1: 750, 2: 680})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.0713 - accuracy: 0.4890\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.8646 - accuracy: 0.6203\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.7830 - accuracy: 0.6544\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.7730 - accuracy: 0.6519\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.7365 - accuracy: 0.6675\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.7154 - accuracy: 0.6751\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.7050 - accuracy: 0.7030\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.6658 - accuracy: 0.7110\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.6829 - accuracy: 0.7000\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.6785 - accuracy: 0.6958\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.6566 - accuracy: 0.7169\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.6415 - accuracy: 0.7190\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.6295 - accuracy: 0.7342\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.6223 - accuracy: 0.7359\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.5923 - accuracy: 0.7354\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.6045 - accuracy: 0.7262\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5866 - accuracy: 0.7380\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6037 - accuracy: 0.7295\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5943 - accuracy: 0.7430\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5822 - accuracy: 0.7443\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.5677 - accuracy: 0.7460\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5568 - accuracy: 0.7574\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.5552 - accuracy: 0.7603\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.5640 - accuracy: 0.7591\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.5614 - accuracy: 0.7460\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5307 - accuracy: 0.7684\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.5380 - accuracy: 0.7603\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.5310 - accuracy: 0.7591\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.5150 - accuracy: 0.7700\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.5256 - accuracy: 0.7637\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.5254 - accuracy: 0.7717\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.5168 - accuracy: 0.7738\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.5060 - accuracy: 0.7734\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.5171 - accuracy: 0.7684\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.5100 - accuracy: 0.7726\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.4922 - accuracy: 0.7861\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.4996 - accuracy: 0.7916\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4868 - accuracy: 0.8025\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4940 - accuracy: 0.7878\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.4782 - accuracy: 0.7873\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.4885 - accuracy: 0.7789\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4928 - accuracy: 0.7911\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.4943 - accuracy: 0.7785\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.4857 - accuracy: 0.7869\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.4832 - accuracy: 0.7802\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.4659 - accuracy: 0.7932\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.4860 - accuracy: 0.7852\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4676 - accuracy: 0.7954\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.4792 - accuracy: 0.7819\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.4674 - accuracy: 0.8034\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.4532 - accuracy: 0.7992\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4563 - accuracy: 0.7970\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4485 - accuracy: 0.8038\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.4575 - accuracy: 0.7954\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.4505 - accuracy: 0.7992\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.4493 - accuracy: 0.8042\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.4554 - accuracy: 0.7937\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.4364 - accuracy: 0.8068\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.4378 - accuracy: 0.8046\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4452 - accuracy: 0.7958\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.4409 - accuracy: 0.8021\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.4428 - accuracy: 0.7996\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.4301 - accuracy: 0.8068\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.4322 - accuracy: 0.8038\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4226 - accuracy: 0.8135\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.4251 - accuracy: 0.8194\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.4355 - accuracy: 0.8038\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.4353 - accuracy: 0.8101\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.4236 - accuracy: 0.8135\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.4268 - accuracy: 0.8122\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4227 - accuracy: 0.8207\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.4331 - accuracy: 0.8080\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 984us/step - loss: 0.4211 - accuracy: 0.8169\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.4191 - accuracy: 0.8203\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.4115 - accuracy: 0.8186\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.4211 - accuracy: 0.8181\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.4190 - accuracy: 0.8186\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.4253 - accuracy: 0.8160\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.4222 - accuracy: 0.8186\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.4044 - accuracy: 0.8224\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4137 - accuracy: 0.8165\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.4088 - accuracy: 0.8262\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.3925 - accuracy: 0.8304\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.4205 - accuracy: 0.8131\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.4094 - accuracy: 0.8215\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3970 - accuracy: 0.8342\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.4078 - accuracy: 0.8241\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.3994 - accuracy: 0.8198\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8316\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 986us/step - loss: 0.4003 - accuracy: 0.8287\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3847 - accuracy: 0.8295\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8350\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.3889 - accuracy: 0.8325\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.3904 - accuracy: 0.8316\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 934us/step - loss: 0.3961 - accuracy: 0.8198\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.3708 - accuracy: 0.8443\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3902 - accuracy: 0.8257\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3752 - accuracy: 0.8405\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3798 - accuracy: 0.8333\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.3804 - accuracy: 0.8338\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 986us/step - loss: 0.3845 - accuracy: 0.8333\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8371\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3802 - accuracy: 0.8439\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8329\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.3777 - accuracy: 0.8262\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.3903 - accuracy: 0.8338\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.3681 - accuracy: 0.8401\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 989us/step - loss: 0.3596 - accuracy: 0.8502\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.3734 - accuracy: 0.8388\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3702 - accuracy: 0.8363\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3727 - accuracy: 0.8380\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.3791 - accuracy: 0.8392\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3789 - accuracy: 0.8359\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3726 - accuracy: 0.8371\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3509 - accuracy: 0.8447\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3678 - accuracy: 0.8384\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.3650 - accuracy: 0.8397\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.3816 - accuracy: 0.8295\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3606 - accuracy: 0.8447\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3572 - accuracy: 0.8435\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.3601 - accuracy: 0.8451\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 980us/step - loss: 0.3654 - accuracy: 0.8388\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3752 - accuracy: 0.8392\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3604 - accuracy: 0.8405\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3518 - accuracy: 0.8506\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3554 - accuracy: 0.8451\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8481\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.8544\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.3647 - accuracy: 0.8392\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3478 - accuracy: 0.8494\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.3551 - accuracy: 0.8502\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3514 - accuracy: 0.8494\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3501 - accuracy: 0.8549\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.3551 - accuracy: 0.8477\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3466 - accuracy: 0.8620\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.3398 - accuracy: 0.8603\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.3397 - accuracy: 0.8553\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.3374 - accuracy: 0.8654\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.3336 - accuracy: 0.8506\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3470 - accuracy: 0.8515\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.3369 - accuracy: 0.8565\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3394 - accuracy: 0.8582\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.3533 - accuracy: 0.8380\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3578 - accuracy: 0.8519\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8515\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8540\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3395 - accuracy: 0.8536\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8633\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3337 - accuracy: 0.8603\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3348 - accuracy: 0.8553\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8603\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8561\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3462 - accuracy: 0.8494\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3343 - accuracy: 0.8549\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.3221 - accuracy: 0.8675\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.3225 - accuracy: 0.8667\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.3384 - accuracy: 0.8561\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3261 - accuracy: 0.8722\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8586\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.8646\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.3290 - accuracy: 0.8650\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.3274 - accuracy: 0.8662\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3285 - accuracy: 0.8654\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.3241 - accuracy: 0.8650\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.3283 - accuracy: 0.8565\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3300 - accuracy: 0.8641\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3081 - accuracy: 0.8751\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.3249 - accuracy: 0.8595\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8633\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3088 - accuracy: 0.8658\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3072 - accuracy: 0.8671\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 996us/step - loss: 0.3141 - accuracy: 0.8755\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8654\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8675\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8637\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.3225 - accuracy: 0.8658\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.3169 - accuracy: 0.8637\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3123 - accuracy: 0.8662\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.3111 - accuracy: 0.8705\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.3128 - accuracy: 0.8633\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3055 - accuracy: 0.8662\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3131 - accuracy: 0.8726\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.3047 - accuracy: 0.8684\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3053 - accuracy: 0.8785\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3124 - accuracy: 0.8688\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3157 - accuracy: 0.8734\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.3174 - accuracy: 0.8722\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.3041 - accuracy: 0.8776\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.3214 - accuracy: 0.8620\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.3100 - accuracy: 0.8688\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3321 - accuracy: 0.8684\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3176 - accuracy: 0.8654\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3047 - accuracy: 0.8722\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3045 - accuracy: 0.8781\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2970 - accuracy: 0.8705\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3038 - accuracy: 0.8764\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3056 - accuracy: 0.8722\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2907 - accuracy: 0.8814\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.3080 - accuracy: 0.8705\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2998 - accuracy: 0.8781\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.2914 - accuracy: 0.8730\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3005 - accuracy: 0.8772\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.3067 - accuracy: 0.8705\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2943 - accuracy: 0.8738\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2932 - accuracy: 0.8776\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2889 - accuracy: 0.8785\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2986 - accuracy: 0.8776\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3001 - accuracy: 0.8709\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3104 - accuracy: 0.8717\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2891 - accuracy: 0.8814\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2905 - accuracy: 0.8776\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2875 - accuracy: 0.8831\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 985us/step - loss: 0.2964 - accuracy: 0.8747\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2858 - accuracy: 0.8793\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2919 - accuracy: 0.8768\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2918 - accuracy: 0.8768\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.2847 - accuracy: 0.8781\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.3019 - accuracy: 0.8734\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2786 - accuracy: 0.8827\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3024 - accuracy: 0.8759\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2810 - accuracy: 0.8861\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.2825 - accuracy: 0.8747\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.2770 - accuracy: 0.8928\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2717 - accuracy: 0.8911\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2761 - accuracy: 0.8899\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2845 - accuracy: 0.8831\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2907 - accuracy: 0.8755\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2884 - accuracy: 0.8840\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2891 - accuracy: 0.8827\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2755 - accuracy: 0.8844\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2852 - accuracy: 0.8802\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2762 - accuracy: 0.8831\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2738 - accuracy: 0.8895\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.2742 - accuracy: 0.8903\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2799 - accuracy: 0.8907\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2767 - accuracy: 0.8819\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2668 - accuracy: 0.8911\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2831 - accuracy: 0.8831\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2878 - accuracy: 0.8802\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2863 - accuracy: 0.8776\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2753 - accuracy: 0.8937\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2879 - accuracy: 0.8852\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.2679 - accuracy: 0.8924\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2882 - accuracy: 0.8810\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.8869\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.8907\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2759 - accuracy: 0.8852\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.2620 - accuracy: 0.8970\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2707 - accuracy: 0.8848\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.2736 - accuracy: 0.8861\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2725 - accuracy: 0.8911\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2728 - accuracy: 0.8819\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2759 - accuracy: 0.8928\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2862 - accuracy: 0.8747\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2702 - accuracy: 0.8835\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2744 - accuracy: 0.8895\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.2600 - accuracy: 0.9004\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2606 - accuracy: 0.8895\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2632 - accuracy: 0.8844\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 928us/step - loss: 0.2731 - accuracy: 0.8920\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2452 - accuracy: 0.8983\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2640 - accuracy: 0.8970\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2703 - accuracy: 0.8949\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2529 - accuracy: 0.9004\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2597 - accuracy: 0.8970\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2589 - accuracy: 0.8932\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2742 - accuracy: 0.8890\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2646 - accuracy: 0.8924\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2684 - accuracy: 0.8945\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2622 - accuracy: 0.8954\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2641 - accuracy: 0.8890\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2616 - accuracy: 0.8954\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.2595 - accuracy: 0.8924\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2582 - accuracy: 0.8920\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2572 - accuracy: 0.8983\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2663 - accuracy: 0.8916\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2667 - accuracy: 0.8869\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2537 - accuracy: 0.8992\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2520 - accuracy: 0.9038\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2611 - accuracy: 0.8932\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2502 - accuracy: 0.8966\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2724 - accuracy: 0.8873\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2559 - accuracy: 0.8979\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2536 - accuracy: 0.8983\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2631 - accuracy: 0.8983\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2420 - accuracy: 0.8979\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2644 - accuracy: 0.8882\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2604 - accuracy: 0.8996\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2575 - accuracy: 0.8949\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2482 - accuracy: 0.8987\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2575 - accuracy: 0.8886\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2475 - accuracy: 0.8992\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2627 - accuracy: 0.8869\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9008\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2625 - accuracy: 0.8873\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2438 - accuracy: 0.9008\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2429 - accuracy: 0.9008\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2541 - accuracy: 0.9004\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2499 - accuracy: 0.8979\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2399 - accuracy: 0.9046\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2271 - accuracy: 0.9097\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2454 - accuracy: 0.9025\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2417 - accuracy: 0.9017\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2496 - accuracy: 0.9017\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2633 - accuracy: 0.8932\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2585 - accuracy: 0.8924\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8954\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2474 - accuracy: 0.9042\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2406 - accuracy: 0.8983\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2159 - accuracy: 0.9203\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2454 - accuracy: 0.9004\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2472 - accuracy: 0.8996\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2395 - accuracy: 0.8996\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2472 - accuracy: 0.9025\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2386 - accuracy: 0.9042\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2375 - accuracy: 0.9051\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2458 - accuracy: 0.8975\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2413 - accuracy: 0.9021\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2344 - accuracy: 0.9089\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2289 - accuracy: 0.9055\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2287 - accuracy: 0.9093\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2473 - accuracy: 0.8970\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2455 - accuracy: 0.9025\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2344 - accuracy: 0.9004\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2418 - accuracy: 0.9084\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2334 - accuracy: 0.9068\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2323 - accuracy: 0.9030\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2391 - accuracy: 0.9017\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2318 - accuracy: 0.9114\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2311 - accuracy: 0.9051\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2183 - accuracy: 0.9118\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.2357 - accuracy: 0.9072\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2466 - accuracy: 0.9084\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2341 - accuracy: 0.9038\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2392 - accuracy: 0.9038\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2217 - accuracy: 0.9089\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2454 - accuracy: 0.8996\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2445 - accuracy: 0.9076\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2314 - accuracy: 0.9068\n",
      "Epoch 340/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1973 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 310.\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.2269 - accuracy: 0.9068\n",
      "Epoch 340: early stopping\n",
      "6/6 [==============================] - 0s 963us/step - loss: 0.4490 - accuracy: 0.8011\n",
      "6/6 [==============================] - 0s 689us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.87 (26/30)\n",
      "Before appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "After appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "Final Test Results - Loss: 0.44897574186325073, Accuracy: 0.8011049628257751, Precision: 0.7783575975294638, Recall: 0.7195318805488298, F1 Score: 0.7424402679467361\n",
      "Confusion Matrix:\n",
      " [[103   3  12]\n",
      " [  8  12   1]\n",
      " [ 12   0  30]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 882, 1: 710, 2: 580})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.3169 - accuracy: 0.3867\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.0442 - accuracy: 0.5249\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9227 - accuracy: 0.5824\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.8587 - accuracy: 0.6294\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.8157 - accuracy: 0.6464\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.7947 - accuracy: 0.6510\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 935us/step - loss: 0.7575 - accuracy: 0.6731\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.7394 - accuracy: 0.6828\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.7135 - accuracy: 0.6934\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.6892 - accuracy: 0.7113\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.6705 - accuracy: 0.7099\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.6680 - accuracy: 0.7136\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.6502 - accuracy: 0.7247\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.6270 - accuracy: 0.7284\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.6130 - accuracy: 0.7330\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.5747 - accuracy: 0.7569\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.5791 - accuracy: 0.7551\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.5895 - accuracy: 0.7569\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.7620\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5426 - accuracy: 0.7703\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.5582 - accuracy: 0.7592\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.5560 - accuracy: 0.7657\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5362 - accuracy: 0.7753\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 962us/step - loss: 0.5507 - accuracy: 0.7716\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.5341 - accuracy: 0.7799\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.5197 - accuracy: 0.7735\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.5299 - accuracy: 0.7767\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.5213 - accuracy: 0.7680\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7799\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.5260 - accuracy: 0.7753\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.5098 - accuracy: 0.7868\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.4956 - accuracy: 0.7896\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4841 - accuracy: 0.8006\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.5048 - accuracy: 0.7831\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.4832 - accuracy: 0.7901\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.4595 - accuracy: 0.8020\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.4653 - accuracy: 0.8039\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.4838 - accuracy: 0.7993\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.4641 - accuracy: 0.8103\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.4558 - accuracy: 0.8085\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.4505 - accuracy: 0.8071\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.4521 - accuracy: 0.8071\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4397 - accuracy: 0.8154\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.4599 - accuracy: 0.8089\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.4492 - accuracy: 0.8172\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.4555 - accuracy: 0.8099\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.4427 - accuracy: 0.8099\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.4258 - accuracy: 0.8223\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.4443 - accuracy: 0.8080\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.4441 - accuracy: 0.8177\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.4142 - accuracy: 0.8292\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.4289 - accuracy: 0.8200\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4318 - accuracy: 0.8191\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.4427 - accuracy: 0.8131\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.4264 - accuracy: 0.8241\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.4243 - accuracy: 0.8181\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.4106 - accuracy: 0.8352\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.3974 - accuracy: 0.8264\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4016 - accuracy: 0.8356\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.3998 - accuracy: 0.8260\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.3936 - accuracy: 0.8352\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.4214 - accuracy: 0.8186\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4028 - accuracy: 0.8283\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4189 - accuracy: 0.8191\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.4000 - accuracy: 0.8297\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.4093 - accuracy: 0.8375\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.3870 - accuracy: 0.8471\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3955 - accuracy: 0.8237\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3843 - accuracy: 0.8366\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3822 - accuracy: 0.8435\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8352\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3798 - accuracy: 0.8393\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3764 - accuracy: 0.8476\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.3895 - accuracy: 0.8347\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.3908 - accuracy: 0.8389\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3755 - accuracy: 0.8476\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3724 - accuracy: 0.8458\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 970us/step - loss: 0.3760 - accuracy: 0.8416\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3799 - accuracy: 0.8379\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3638 - accuracy: 0.8490\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.3662 - accuracy: 0.8448\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.3585 - accuracy: 0.8587\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3564 - accuracy: 0.8504\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.3684 - accuracy: 0.8536\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.3751 - accuracy: 0.8435\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.3565 - accuracy: 0.8541\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.3764 - accuracy: 0.8430\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.3627 - accuracy: 0.8490\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.3680 - accuracy: 0.8462\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3525 - accuracy: 0.8499\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 982us/step - loss: 0.3481 - accuracy: 0.8504\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.3640 - accuracy: 0.8416\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.3469 - accuracy: 0.8476\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.3608 - accuracy: 0.8490\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.3486 - accuracy: 0.8591\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.3407 - accuracy: 0.8531\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.3376 - accuracy: 0.8646\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3411 - accuracy: 0.8536\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3426 - accuracy: 0.8564\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3420 - accuracy: 0.8554\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.3482 - accuracy: 0.8582\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3329 - accuracy: 0.8669\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.3408 - accuracy: 0.8522\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3346 - accuracy: 0.8610\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3389 - accuracy: 0.8656\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.3356 - accuracy: 0.8591\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.3492 - accuracy: 0.8531\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3301 - accuracy: 0.8665\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.3310 - accuracy: 0.8651\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.3388 - accuracy: 0.8605\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3271 - accuracy: 0.8651\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.3275 - accuracy: 0.8651\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.3234 - accuracy: 0.8692\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.3262 - accuracy: 0.8669\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.3269 - accuracy: 0.8660\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3159 - accuracy: 0.8738\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3098 - accuracy: 0.8692\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.8669\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3225 - accuracy: 0.8646\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8623\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3182 - accuracy: 0.8674\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.3144 - accuracy: 0.8757\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.3195 - accuracy: 0.8656\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.3267 - accuracy: 0.8734\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3165 - accuracy: 0.8683\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.3066 - accuracy: 0.8771\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.3159 - accuracy: 0.8720\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.3116 - accuracy: 0.8738\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3059 - accuracy: 0.8697\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.3059 - accuracy: 0.8752\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3123 - accuracy: 0.8725\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.3020 - accuracy: 0.8757\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.3125 - accuracy: 0.8762\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2852 - accuracy: 0.8854\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.3202 - accuracy: 0.8752\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.3159 - accuracy: 0.8725\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2973 - accuracy: 0.8762\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2903 - accuracy: 0.8785\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.2920 - accuracy: 0.8775\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.3000 - accuracy: 0.8757\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2903 - accuracy: 0.8844\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.2882 - accuracy: 0.8803\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2883 - accuracy: 0.8789\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2892 - accuracy: 0.8877\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2985 - accuracy: 0.8738\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.3046 - accuracy: 0.8780\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2956 - accuracy: 0.8720\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2909 - accuracy: 0.8803\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2882 - accuracy: 0.8803\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2955 - accuracy: 0.8854\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2948 - accuracy: 0.8775\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.2962 - accuracy: 0.8780\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2712 - accuracy: 0.8900\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2971 - accuracy: 0.8738\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2744 - accuracy: 0.8840\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2964 - accuracy: 0.8697\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2751 - accuracy: 0.8849\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2933 - accuracy: 0.8798\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2795 - accuracy: 0.8877\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 715us/step - loss: 0.2911 - accuracy: 0.8821\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2861 - accuracy: 0.8789\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2912 - accuracy: 0.8812\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2661 - accuracy: 0.8904\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2773 - accuracy: 0.8858\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2810 - accuracy: 0.8785\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2684 - accuracy: 0.8913\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2813 - accuracy: 0.8780\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2706 - accuracy: 0.8849\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2811 - accuracy: 0.8886\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2770 - accuracy: 0.8867\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 713us/step - loss: 0.2761 - accuracy: 0.8881\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2695 - accuracy: 0.8923\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2740 - accuracy: 0.8890\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 974us/step - loss: 0.2589 - accuracy: 0.9015\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.2670 - accuracy: 0.8867\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2680 - accuracy: 0.8918\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2800 - accuracy: 0.8886\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2700 - accuracy: 0.8886\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2748 - accuracy: 0.8835\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 706us/step - loss: 0.2730 - accuracy: 0.8932\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2496 - accuracy: 0.9001\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2734 - accuracy: 0.8895\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2583 - accuracy: 0.9001\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2687 - accuracy: 0.8918\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2705 - accuracy: 0.8877\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2515 - accuracy: 0.8959\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.2679 - accuracy: 0.8890\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2665 - accuracy: 0.8955\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2664 - accuracy: 0.8881\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2565 - accuracy: 0.8983\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.2419 - accuracy: 0.9047\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.2627 - accuracy: 0.8936\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2370 - accuracy: 0.9047\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2656 - accuracy: 0.8881\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2534 - accuracy: 0.8904\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2609 - accuracy: 0.8909\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2546 - accuracy: 0.9010\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2457 - accuracy: 0.8983\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2509 - accuracy: 0.8983\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2471 - accuracy: 0.8987\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2554 - accuracy: 0.8950\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2610 - accuracy: 0.8890\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2441 - accuracy: 0.9047\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2567 - accuracy: 0.9006\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2586 - accuracy: 0.8932\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2482 - accuracy: 0.8950\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2552 - accuracy: 0.9015\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2400 - accuracy: 0.9075\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.2499 - accuracy: 0.8955\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2469 - accuracy: 0.9029\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2347 - accuracy: 0.9042\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 705us/step - loss: 0.2443 - accuracy: 0.9047\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2452 - accuracy: 0.8946\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2512 - accuracy: 0.8946\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2345 - accuracy: 0.9102\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2361 - accuracy: 0.8978\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2217 - accuracy: 0.9125\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2403 - accuracy: 0.9038\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2240 - accuracy: 0.9134\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2486 - accuracy: 0.8946\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2490 - accuracy: 0.9001\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2204 - accuracy: 0.9056\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2297 - accuracy: 0.9093\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2336 - accuracy: 0.9065\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2371 - accuracy: 0.9079\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.2309 - accuracy: 0.9102\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 721us/step - loss: 0.2258 - accuracy: 0.9047\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2280 - accuracy: 0.9111\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.2215 - accuracy: 0.9088\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 727us/step - loss: 0.2382 - accuracy: 0.9024\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.2350 - accuracy: 0.9079\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2244 - accuracy: 0.9088\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2284 - accuracy: 0.9134\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.2449 - accuracy: 0.9015\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2240 - accuracy: 0.9157\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2342 - accuracy: 0.9088\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2215 - accuracy: 0.9111\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2419 - accuracy: 0.8969\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2293 - accuracy: 0.9079\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2174 - accuracy: 0.9116\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2125 - accuracy: 0.9185\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2289 - accuracy: 0.9065\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2172 - accuracy: 0.9148\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2164 - accuracy: 0.9121\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2234 - accuracy: 0.9111\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2303 - accuracy: 0.9019\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 718us/step - loss: 0.2312 - accuracy: 0.9148\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 723us/step - loss: 0.2264 - accuracy: 0.9116\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 727us/step - loss: 0.2317 - accuracy: 0.9075\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2141 - accuracy: 0.9185\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.2231 - accuracy: 0.9102\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2074 - accuracy: 0.9194\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2345 - accuracy: 0.9070\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2267 - accuracy: 0.9084\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2110 - accuracy: 0.9134\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2077 - accuracy: 0.9190\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2082 - accuracy: 0.9176\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2152 - accuracy: 0.9116\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2168 - accuracy: 0.9121\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2193 - accuracy: 0.9148\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.2053 - accuracy: 0.9222\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2060 - accuracy: 0.9162\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2209 - accuracy: 0.9144\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2039 - accuracy: 0.9245\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2186 - accuracy: 0.9199\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.2109 - accuracy: 0.9185\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2125 - accuracy: 0.9121\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.2292 - accuracy: 0.9121\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2225 - accuracy: 0.9130\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2056 - accuracy: 0.9180\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2023 - accuracy: 0.9208\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.2070 - accuracy: 0.9199\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 708us/step - loss: 0.2022 - accuracy: 0.9185\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.2037 - accuracy: 0.9176\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1948 - accuracy: 0.9217\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2021 - accuracy: 0.9162\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2125 - accuracy: 0.9153\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1990 - accuracy: 0.9227\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2178 - accuracy: 0.9144\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1925 - accuracy: 0.9185\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.2191 - accuracy: 0.9121\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2080 - accuracy: 0.9227\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1956 - accuracy: 0.9148\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2027 - accuracy: 0.9199\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2141 - accuracy: 0.9153\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9144\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9273\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 965us/step - loss: 0.2049 - accuracy: 0.9162\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 960us/step - loss: 0.2127 - accuracy: 0.9125\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.2109 - accuracy: 0.9130\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.1997 - accuracy: 0.9139\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2025 - accuracy: 0.9217\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.2238 - accuracy: 0.9121\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2113 - accuracy: 0.9180\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1932 - accuracy: 0.9250\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.2012 - accuracy: 0.9231\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2045 - accuracy: 0.9194\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1986 - accuracy: 0.9231\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.2065 - accuracy: 0.9217\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2004 - accuracy: 0.9171\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.1962 - accuracy: 0.9213\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.2035 - accuracy: 0.9185\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2069 - accuracy: 0.9171\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.1929 - accuracy: 0.9203\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2080 - accuracy: 0.9167\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.1757 - accuracy: 0.9314\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 707us/step - loss: 0.1804 - accuracy: 0.9314\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2053 - accuracy: 0.9213\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1870 - accuracy: 0.9296\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.2001 - accuracy: 0.9185\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.1852 - accuracy: 0.9309\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.1899 - accuracy: 0.9259\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 991us/step - loss: 0.1893 - accuracy: 0.9254\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 998us/step - loss: 0.1953 - accuracy: 0.9199\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.1819 - accuracy: 0.9346\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2162 - accuracy: 0.9134\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9314\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9240\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 995us/step - loss: 0.1806 - accuracy: 0.9282\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1822 - accuracy: 0.9254\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1629 - accuracy: 0.9351\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.1942 - accuracy: 0.9222\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.1859 - accuracy: 0.9245\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2014 - accuracy: 0.9185\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1830 - accuracy: 0.9273\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1876 - accuracy: 0.9263\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1751 - accuracy: 0.9282\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1905 - accuracy: 0.9203\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1956 - accuracy: 0.9332\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.1746 - accuracy: 0.9282\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1772 - accuracy: 0.9365\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.1882 - accuracy: 0.9263\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.1744 - accuracy: 0.9369\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1827 - accuracy: 0.9286\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1868 - accuracy: 0.9240\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1803 - accuracy: 0.9300\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1898 - accuracy: 0.9227\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1834 - accuracy: 0.9309\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1856 - accuracy: 0.9282\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1702 - accuracy: 0.9401\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1817 - accuracy: 0.9319\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.1981 - accuracy: 0.9199\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1854 - accuracy: 0.9277\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1758 - accuracy: 0.9300\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.1824 - accuracy: 0.9314\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1704 - accuracy: 0.9355\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.1648 - accuracy: 0.9351\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1719 - accuracy: 0.9360\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.1655 - accuracy: 0.9406\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 723us/step - loss: 0.1744 - accuracy: 0.9309\n",
      "Epoch 351/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1664 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 321.\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1777 - accuracy: 0.9282\n",
      "Epoch 351: early stopping\n",
      "8/8 [==============================] - 0s 777us/step - loss: 0.9520 - accuracy: 0.6597\n",
      "8/8 [==============================] - 0s 584us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (23/29)\n",
      "Before appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "After appending - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n",
      "Final Test Results - Loss: 0.951958179473877, Accuracy: 0.6596638560295105, Precision: 0.6781842818428184, Recall: 0.6456430551456771, F1 Score: 0.6586378251973107\n",
      "Confusion Matrix:\n",
      " [[115   0  32]\n",
      " [  3  26   0]\n",
      " [ 46   0  16]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6764069186702067\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8276309370994568\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7114615887403488\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6893899018955436\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6781418210031022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, kitten, adult, kitten, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "64    058A                           [senior, senior, senior]        senior           senior                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, senior, adult, adult, senior, ...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "59    053A       [adult, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "51    045A  [adult, adult, kitten, kitten, kitten, adult, ...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, adult, senior, adult, se...         adult            adult                   True\n",
       "92    097A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, senior, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, senior, senior, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "99    104A                       [adult, adult, adult, adult]         adult           senior                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "50    044A  [adult, adult, kitten, adult, kitten, adult, a...         adult           kitten                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "60    054A                                    [senior, adult]         adult           senior                  False\n",
       "90    095A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, ad...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "65    059A  [senior, senior, senior, senior, adult, adult,...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [kitten, adult]         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    13\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          572            436  76.223776\n",
      "1           kitten          113             92  81.415929\n",
      "2           senior          178             82  46.067416\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      146     68.54\n",
      "1          M    360      263     73.06\n",
      "2          X    290      201     69.31\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      146     68.54\n",
      "1          M    360      263     73.06\n",
      "2          X    290      201     69.31\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 936, 1: 815, 2: 795})\n",
      "Epoch 1/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.1782 - accuracy: 0.5012\n",
      "Epoch 2/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.8925 - accuracy: 0.6033\n",
      "Epoch 3/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.8146 - accuracy: 0.6445\n",
      "Epoch 4/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.7843 - accuracy: 0.6579\n",
      "Epoch 5/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.7307 - accuracy: 0.6732\n",
      "Epoch 6/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.6969 - accuracy: 0.6991\n",
      "Epoch 7/1500\n",
      "40/40 [==============================] - 0s 851us/step - loss: 0.6692 - accuracy: 0.7003\n",
      "Epoch 8/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.6687 - accuracy: 0.7011\n",
      "Epoch 9/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6333 - accuracy: 0.7235\n",
      "Epoch 10/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6102 - accuracy: 0.7290\n",
      "Epoch 11/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.5984 - accuracy: 0.7333\n",
      "Epoch 12/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.5957 - accuracy: 0.7357\n",
      "Epoch 13/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.6027 - accuracy: 0.7451\n",
      "Epoch 14/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.5753 - accuracy: 0.7482\n",
      "Epoch 15/1500\n",
      "40/40 [==============================] - 0s 917us/step - loss: 0.5626 - accuracy: 0.7533\n",
      "Epoch 16/1500\n",
      "40/40 [==============================] - 0s 929us/step - loss: 0.5594 - accuracy: 0.7581\n",
      "Epoch 17/1500\n",
      "40/40 [==============================] - 0s 884us/step - loss: 0.5592 - accuracy: 0.7651\n",
      "Epoch 18/1500\n",
      "40/40 [==============================] - 0s 837us/step - loss: 0.5296 - accuracy: 0.7710\n",
      "Epoch 19/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.5277 - accuracy: 0.7604\n",
      "Epoch 20/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.5283 - accuracy: 0.7734\n",
      "Epoch 21/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.5187 - accuracy: 0.7722\n",
      "Epoch 22/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.5134 - accuracy: 0.7730\n",
      "Epoch 23/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.5200 - accuracy: 0.7738\n",
      "Epoch 24/1500\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.4929 - accuracy: 0.7887\n",
      "Epoch 25/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.5015 - accuracy: 0.7804\n",
      "Epoch 26/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.4822 - accuracy: 0.7828\n",
      "Epoch 27/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.5136 - accuracy: 0.7702\n",
      "Epoch 28/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.4869 - accuracy: 0.7926\n",
      "Epoch 29/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.4896 - accuracy: 0.7852\n",
      "Epoch 30/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.4792 - accuracy: 0.7895\n",
      "Epoch 31/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.4776 - accuracy: 0.8056\n",
      "Epoch 32/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.4690 - accuracy: 0.7950\n",
      "Epoch 33/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4655 - accuracy: 0.7981\n",
      "Epoch 34/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.4804 - accuracy: 0.7887\n",
      "Epoch 35/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.4503 - accuracy: 0.8052\n",
      "Epoch 36/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.4464 - accuracy: 0.8020\n",
      "Epoch 37/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.4494 - accuracy: 0.8119\n",
      "Epoch 38/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.4467 - accuracy: 0.8064\n",
      "Epoch 39/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.4616 - accuracy: 0.7938\n",
      "Epoch 40/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.4526 - accuracy: 0.7977\n",
      "Epoch 41/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4327 - accuracy: 0.8052\n",
      "Epoch 42/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.4377 - accuracy: 0.8123\n",
      "Epoch 43/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.4348 - accuracy: 0.8087\n",
      "Epoch 44/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.4280 - accuracy: 0.8213\n",
      "Epoch 45/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4312 - accuracy: 0.8170\n",
      "Epoch 46/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.4360 - accuracy: 0.8154\n",
      "Epoch 47/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.4196 - accuracy: 0.8236\n",
      "Epoch 48/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4357 - accuracy: 0.8099\n",
      "Epoch 49/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.4243 - accuracy: 0.8107\n",
      "Epoch 50/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.4377 - accuracy: 0.8028\n",
      "Epoch 51/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.4136 - accuracy: 0.8217\n",
      "Epoch 52/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.4108 - accuracy: 0.8197\n",
      "Epoch 53/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.4041 - accuracy: 0.8244\n",
      "Epoch 54/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.4126 - accuracy: 0.8291\n",
      "Epoch 55/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.4191 - accuracy: 0.8213\n",
      "Epoch 56/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.4158 - accuracy: 0.8209\n",
      "Epoch 57/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.4036 - accuracy: 0.8311\n",
      "Epoch 58/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.4028 - accuracy: 0.8252\n",
      "Epoch 59/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.3988 - accuracy: 0.8319\n",
      "Epoch 60/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.4000 - accuracy: 0.8350\n",
      "Epoch 61/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.3891 - accuracy: 0.8394\n",
      "Epoch 62/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.4052 - accuracy: 0.8221\n",
      "Epoch 63/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.3986 - accuracy: 0.8319\n",
      "Epoch 64/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.3846 - accuracy: 0.8346\n",
      "Epoch 65/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.3850 - accuracy: 0.8362\n",
      "Epoch 66/1500\n",
      "40/40 [==============================] - 0s 927us/step - loss: 0.4090 - accuracy: 0.8229\n",
      "Epoch 67/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.3893 - accuracy: 0.8374\n",
      "Epoch 68/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.3722 - accuracy: 0.8449\n",
      "Epoch 69/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.3946 - accuracy: 0.8339\n",
      "Epoch 70/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.3812 - accuracy: 0.8386\n",
      "Epoch 71/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.3783 - accuracy: 0.8386\n",
      "Epoch 72/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3807 - accuracy: 0.8390\n",
      "Epoch 73/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.3699 - accuracy: 0.8397\n",
      "Epoch 74/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.3729 - accuracy: 0.8429\n",
      "Epoch 75/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.3751 - accuracy: 0.8374\n",
      "Epoch 76/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3751 - accuracy: 0.8409\n",
      "Epoch 77/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.3775 - accuracy: 0.8397\n",
      "Epoch 78/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3688 - accuracy: 0.8456\n",
      "Epoch 79/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.3665 - accuracy: 0.8429\n",
      "Epoch 80/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3656 - accuracy: 0.8413\n",
      "Epoch 81/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.3670 - accuracy: 0.8429\n",
      "Epoch 82/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.3651 - accuracy: 0.8488\n",
      "Epoch 83/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.3573 - accuracy: 0.8492\n",
      "Epoch 84/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.3625 - accuracy: 0.8456\n",
      "Epoch 85/1500\n",
      "40/40 [==============================] - 0s 889us/step - loss: 0.3633 - accuracy: 0.8476\n",
      "Epoch 86/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.3683 - accuracy: 0.8429\n",
      "Epoch 87/1500\n",
      "40/40 [==============================] - 0s 878us/step - loss: 0.3602 - accuracy: 0.8555\n",
      "Epoch 88/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.3533 - accuracy: 0.8531\n",
      "Epoch 89/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.3521 - accuracy: 0.8531\n",
      "Epoch 90/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.3589 - accuracy: 0.8441\n",
      "Epoch 91/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.3609 - accuracy: 0.8445\n",
      "Epoch 92/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.3410 - accuracy: 0.8476\n",
      "Epoch 93/1500\n",
      "40/40 [==============================] - 0s 884us/step - loss: 0.3473 - accuracy: 0.8547\n",
      "Epoch 94/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.3416 - accuracy: 0.8551\n",
      "Epoch 95/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.3405 - accuracy: 0.8602\n",
      "Epoch 96/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.3268 - accuracy: 0.8559\n",
      "Epoch 97/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.3427 - accuracy: 0.8555\n",
      "Epoch 98/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.3523 - accuracy: 0.8598\n",
      "Epoch 99/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.3457 - accuracy: 0.8551\n",
      "Epoch 100/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.3403 - accuracy: 0.8582\n",
      "Epoch 101/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3446 - accuracy: 0.8539\n",
      "Epoch 102/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3423 - accuracy: 0.8555\n",
      "Epoch 103/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3431 - accuracy: 0.8582\n",
      "Epoch 104/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.3379 - accuracy: 0.8641\n",
      "Epoch 105/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.3313 - accuracy: 0.8610\n",
      "Epoch 106/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3377 - accuracy: 0.8586\n",
      "Epoch 107/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3300 - accuracy: 0.8578\n",
      "Epoch 108/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.3187 - accuracy: 0.8665\n",
      "Epoch 109/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.3202 - accuracy: 0.8641\n",
      "Epoch 110/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3270 - accuracy: 0.8653\n",
      "Epoch 111/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3373 - accuracy: 0.8574\n",
      "Epoch 112/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3457 - accuracy: 0.8590\n",
      "Epoch 113/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.3266 - accuracy: 0.8610\n",
      "Epoch 114/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3201 - accuracy: 0.8676\n",
      "Epoch 115/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.3213 - accuracy: 0.8606\n",
      "Epoch 116/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.3378 - accuracy: 0.8566\n",
      "Epoch 117/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3137 - accuracy: 0.8712\n",
      "Epoch 118/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.3104 - accuracy: 0.8708\n",
      "Epoch 119/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.3060 - accuracy: 0.8723\n",
      "Epoch 120/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.3306 - accuracy: 0.8680\n",
      "Epoch 121/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.3245 - accuracy: 0.8676\n",
      "Epoch 122/1500\n",
      "40/40 [==============================] - 0s 909us/step - loss: 0.3427 - accuracy: 0.8606\n",
      "Epoch 123/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.3104 - accuracy: 0.8751\n",
      "Epoch 124/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.3183 - accuracy: 0.8727\n",
      "Epoch 125/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.3103 - accuracy: 0.8731\n",
      "Epoch 126/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.3188 - accuracy: 0.8712\n",
      "Epoch 127/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3252 - accuracy: 0.8657\n",
      "Epoch 128/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.3249 - accuracy: 0.8598\n",
      "Epoch 129/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.3225 - accuracy: 0.8625\n",
      "Epoch 130/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.3216 - accuracy: 0.8641\n",
      "Epoch 131/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.3005 - accuracy: 0.8802\n",
      "Epoch 132/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.3153 - accuracy: 0.8700\n",
      "Epoch 133/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3206 - accuracy: 0.8684\n",
      "Epoch 134/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.3137 - accuracy: 0.8731\n",
      "Epoch 135/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2955 - accuracy: 0.8865\n",
      "Epoch 136/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2978 - accuracy: 0.8798\n",
      "Epoch 137/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.3078 - accuracy: 0.8763\n",
      "Epoch 138/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.2951 - accuracy: 0.8830\n",
      "Epoch 139/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.3015 - accuracy: 0.8743\n",
      "Epoch 140/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2949 - accuracy: 0.8794\n",
      "Epoch 141/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.2987 - accuracy: 0.8830\n",
      "Epoch 142/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2957 - accuracy: 0.8794\n",
      "Epoch 143/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.3091 - accuracy: 0.8704\n",
      "Epoch 144/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.3058 - accuracy: 0.8672\n",
      "Epoch 145/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2927 - accuracy: 0.8771\n",
      "Epoch 146/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.2881 - accuracy: 0.8830\n",
      "Epoch 147/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3023 - accuracy: 0.8767\n",
      "Epoch 148/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2985 - accuracy: 0.8747\n",
      "Epoch 149/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.3005 - accuracy: 0.8731\n",
      "Epoch 150/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2852 - accuracy: 0.8861\n",
      "Epoch 151/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.2914 - accuracy: 0.8845\n",
      "Epoch 152/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.2691 - accuracy: 0.8892\n",
      "Epoch 153/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2833 - accuracy: 0.8861\n",
      "Epoch 154/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.2883 - accuracy: 0.8755\n",
      "Epoch 155/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.2777 - accuracy: 0.8888\n",
      "Epoch 156/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2883 - accuracy: 0.8849\n",
      "Epoch 157/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.2763 - accuracy: 0.8814\n",
      "Epoch 158/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2791 - accuracy: 0.8892\n",
      "Epoch 159/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2813 - accuracy: 0.8841\n",
      "Epoch 160/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2817 - accuracy: 0.8830\n",
      "Epoch 161/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2987 - accuracy: 0.8700\n",
      "Epoch 162/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2814 - accuracy: 0.8830\n",
      "Epoch 163/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.2586 - accuracy: 0.8979\n",
      "Epoch 164/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.2719 - accuracy: 0.8885\n",
      "Epoch 165/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2723 - accuracy: 0.8916\n",
      "Epoch 166/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2845 - accuracy: 0.8869\n",
      "Epoch 167/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.2637 - accuracy: 0.8857\n",
      "Epoch 168/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2767 - accuracy: 0.8865\n",
      "Epoch 169/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.2823 - accuracy: 0.8849\n",
      "Epoch 170/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2715 - accuracy: 0.8869\n",
      "Epoch 171/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2795 - accuracy: 0.8849\n",
      "Epoch 172/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2664 - accuracy: 0.8885\n",
      "Epoch 173/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.2679 - accuracy: 0.8888\n",
      "Epoch 174/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.2741 - accuracy: 0.8885\n",
      "Epoch 175/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.2685 - accuracy: 0.8940\n",
      "Epoch 176/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2706 - accuracy: 0.8833\n",
      "Epoch 177/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.2735 - accuracy: 0.8873\n",
      "Epoch 178/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2677 - accuracy: 0.8830\n",
      "Epoch 179/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2791 - accuracy: 0.8908\n",
      "Epoch 180/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2731 - accuracy: 0.8900\n",
      "Epoch 181/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2722 - accuracy: 0.8936\n",
      "Epoch 182/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2685 - accuracy: 0.8881\n",
      "Epoch 183/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2662 - accuracy: 0.8896\n",
      "Epoch 184/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.2570 - accuracy: 0.9014\n",
      "Epoch 185/1500\n",
      "40/40 [==============================] - 0s 947us/step - loss: 0.2660 - accuracy: 0.8881\n",
      "Epoch 186/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.2614 - accuracy: 0.8963\n",
      "Epoch 187/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2701 - accuracy: 0.8936\n",
      "Epoch 188/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2590 - accuracy: 0.8928\n",
      "Epoch 189/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.2731 - accuracy: 0.8853\n",
      "Epoch 190/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.2631 - accuracy: 0.8943\n",
      "Epoch 191/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.2561 - accuracy: 0.8963\n",
      "Epoch 192/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.2730 - accuracy: 0.8873\n",
      "Epoch 193/1500\n",
      "40/40 [==============================] - 0s 934us/step - loss: 0.2671 - accuracy: 0.8943\n",
      "Epoch 194/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2581 - accuracy: 0.8908\n",
      "Epoch 195/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8936\n",
      "Epoch 196/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.8971\n",
      "Epoch 197/1500\n",
      "40/40 [==============================] - 0s 926us/step - loss: 0.2555 - accuracy: 0.8983\n",
      "Epoch 198/1500\n",
      "40/40 [==============================] - 0s 837us/step - loss: 0.2550 - accuracy: 0.9034\n",
      "Epoch 199/1500\n",
      "40/40 [==============================] - 0s 912us/step - loss: 0.2572 - accuracy: 0.8908\n",
      "Epoch 200/1500\n",
      "40/40 [==============================] - 0s 880us/step - loss: 0.2590 - accuracy: 0.8975\n",
      "Epoch 201/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.2584 - accuracy: 0.8943\n",
      "Epoch 202/1500\n",
      "40/40 [==============================] - 0s 919us/step - loss: 0.2500 - accuracy: 0.8995\n",
      "Epoch 203/1500\n",
      "40/40 [==============================] - 0s 903us/step - loss: 0.2603 - accuracy: 0.8936\n",
      "Epoch 204/1500\n",
      "40/40 [==============================] - 0s 922us/step - loss: 0.2390 - accuracy: 0.9053\n",
      "Epoch 205/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.2439 - accuracy: 0.8987\n",
      "Epoch 206/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.2391 - accuracy: 0.9046\n",
      "Epoch 207/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.2515 - accuracy: 0.9026\n",
      "Epoch 208/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2492 - accuracy: 0.8991\n",
      "Epoch 209/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2405 - accuracy: 0.9057\n",
      "Epoch 210/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2447 - accuracy: 0.8943\n",
      "Epoch 211/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.2417 - accuracy: 0.9018\n",
      "Epoch 212/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.2496 - accuracy: 0.8967\n",
      "Epoch 213/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.2454 - accuracy: 0.8998\n",
      "Epoch 214/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2449 - accuracy: 0.8924\n",
      "Epoch 215/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.2499 - accuracy: 0.8963\n",
      "Epoch 216/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2583 - accuracy: 0.8955\n",
      "Epoch 217/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2455 - accuracy: 0.8983\n",
      "Epoch 218/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.2403 - accuracy: 0.9034\n",
      "Epoch 219/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2326 - accuracy: 0.9065\n",
      "Epoch 220/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.2393 - accuracy: 0.9053\n",
      "Epoch 221/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2401 - accuracy: 0.9057\n",
      "Epoch 222/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.2341 - accuracy: 0.9069\n",
      "Epoch 223/1500\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.2483 - accuracy: 0.9038\n",
      "Epoch 224/1500\n",
      "40/40 [==============================] - 0s 902us/step - loss: 0.2464 - accuracy: 0.9042\n",
      "Epoch 225/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.2349 - accuracy: 0.9038\n",
      "Epoch 226/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.2424 - accuracy: 0.8971\n",
      "Epoch 227/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2366 - accuracy: 0.9057\n",
      "Epoch 228/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2336 - accuracy: 0.9018\n",
      "Epoch 229/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2466 - accuracy: 0.9002\n",
      "Epoch 230/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2338 - accuracy: 0.9097\n",
      "Epoch 231/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.2443 - accuracy: 0.9026\n",
      "Epoch 232/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2389 - accuracy: 0.9085\n",
      "Epoch 233/1500\n",
      "40/40 [==============================] - 0s 903us/step - loss: 0.2265 - accuracy: 0.9038\n",
      "Epoch 234/1500\n",
      "40/40 [==============================] - 0s 907us/step - loss: 0.2409 - accuracy: 0.9030\n",
      "Epoch 235/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2328 - accuracy: 0.9112\n",
      "Epoch 236/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2424 - accuracy: 0.9014\n",
      "Epoch 237/1500\n",
      "40/40 [==============================] - 0s 846us/step - loss: 0.2348 - accuracy: 0.9010\n",
      "Epoch 238/1500\n",
      "40/40 [==============================] - 0s 847us/step - loss: 0.2372 - accuracy: 0.9053\n",
      "Epoch 239/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2316 - accuracy: 0.9049\n",
      "Epoch 240/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2351 - accuracy: 0.9093\n",
      "Epoch 241/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.2337 - accuracy: 0.9077\n",
      "Epoch 242/1500\n",
      "40/40 [==============================] - 0s 888us/step - loss: 0.2301 - accuracy: 0.9136\n",
      "Epoch 243/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.2030 - accuracy: 0.9203\n",
      "Epoch 244/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.2360 - accuracy: 0.9057\n",
      "Epoch 245/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.2435 - accuracy: 0.9010\n",
      "Epoch 246/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.2182 - accuracy: 0.9120\n",
      "Epoch 247/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.2237 - accuracy: 0.9108\n",
      "Epoch 248/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2328 - accuracy: 0.9038\n",
      "Epoch 249/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2283 - accuracy: 0.9093\n",
      "Epoch 250/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2208 - accuracy: 0.9140\n",
      "Epoch 251/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2203 - accuracy: 0.9089\n",
      "Epoch 252/1500\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.2305 - accuracy: 0.9108\n",
      "Epoch 253/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2230 - accuracy: 0.9120\n",
      "Epoch 254/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2186 - accuracy: 0.9132\n",
      "Epoch 255/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2234 - accuracy: 0.9144\n",
      "Epoch 256/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2165 - accuracy: 0.9152\n",
      "Epoch 257/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2268 - accuracy: 0.9108\n",
      "Epoch 258/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.2323 - accuracy: 0.9049\n",
      "Epoch 259/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2228 - accuracy: 0.9124\n",
      "Epoch 260/1500\n",
      "40/40 [==============================] - 0s 891us/step - loss: 0.2290 - accuracy: 0.9034\n",
      "Epoch 261/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2288 - accuracy: 0.9077\n",
      "Epoch 262/1500\n",
      "40/40 [==============================] - 0s 897us/step - loss: 0.2180 - accuracy: 0.9195\n",
      "Epoch 263/1500\n",
      "40/40 [==============================] - 0s 883us/step - loss: 0.2120 - accuracy: 0.9159\n",
      "Epoch 264/1500\n",
      "40/40 [==============================] - 0s 892us/step - loss: 0.2073 - accuracy: 0.9183\n",
      "Epoch 265/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.2108 - accuracy: 0.9218\n",
      "Epoch 266/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2092 - accuracy: 0.9171\n",
      "Epoch 267/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2205 - accuracy: 0.9148\n",
      "Epoch 268/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.2190 - accuracy: 0.9116\n",
      "Epoch 269/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.2157 - accuracy: 0.9191\n",
      "Epoch 270/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.2095 - accuracy: 0.9195\n",
      "Epoch 271/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2008 - accuracy: 0.9195\n",
      "Epoch 272/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2127 - accuracy: 0.9120\n",
      "Epoch 273/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.2093 - accuracy: 0.9226\n",
      "Epoch 274/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.2198 - accuracy: 0.9140\n",
      "Epoch 275/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.2069 - accuracy: 0.9156\n",
      "Epoch 276/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2073 - accuracy: 0.9144\n",
      "Epoch 277/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2175 - accuracy: 0.9116\n",
      "Epoch 278/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.2060 - accuracy: 0.9199\n",
      "Epoch 279/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2167 - accuracy: 0.9159\n",
      "Epoch 280/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.2037 - accuracy: 0.9222\n",
      "Epoch 281/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2123 - accuracy: 0.9167\n",
      "Epoch 282/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2122 - accuracy: 0.9148\n",
      "Epoch 283/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.2009 - accuracy: 0.9179\n",
      "Epoch 284/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2094 - accuracy: 0.9230\n",
      "Epoch 285/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2121 - accuracy: 0.9156\n",
      "Epoch 286/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2040 - accuracy: 0.9187\n",
      "Epoch 287/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.2164 - accuracy: 0.9112\n",
      "Epoch 288/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2055 - accuracy: 0.9183\n",
      "Epoch 289/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.2032 - accuracy: 0.9226\n",
      "Epoch 290/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.2058 - accuracy: 0.9163\n",
      "Epoch 291/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2062 - accuracy: 0.9156\n",
      "Epoch 292/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.2080 - accuracy: 0.9159\n",
      "Epoch 293/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.1957 - accuracy: 0.9238\n",
      "Epoch 294/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.1973 - accuracy: 0.9203\n",
      "Epoch 295/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2167 - accuracy: 0.9120\n",
      "Epoch 296/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.1995 - accuracy: 0.9195\n",
      "Epoch 297/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.1968 - accuracy: 0.9199\n",
      "Epoch 298/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2028 - accuracy: 0.9187\n",
      "Epoch 299/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.1960 - accuracy: 0.9226\n",
      "Epoch 300/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2064 - accuracy: 0.9171\n",
      "Epoch 301/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2065 - accuracy: 0.9171\n",
      "Epoch 302/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.1939 - accuracy: 0.9218\n",
      "Epoch 303/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.1856 - accuracy: 0.9321\n",
      "Epoch 304/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2024 - accuracy: 0.9254\n",
      "Epoch 305/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.1910 - accuracy: 0.9222\n",
      "Epoch 306/1500\n",
      "40/40 [==============================] - 0s 868us/step - loss: 0.2062 - accuracy: 0.9179\n",
      "Epoch 307/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2094 - accuracy: 0.9124\n",
      "Epoch 308/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.2043 - accuracy: 0.9207\n",
      "Epoch 309/1500\n",
      "40/40 [==============================] - 0s 893us/step - loss: 0.1867 - accuracy: 0.9321\n",
      "Epoch 310/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.2101 - accuracy: 0.9132\n",
      "Epoch 311/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.2018 - accuracy: 0.9191\n",
      "Epoch 312/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.2043 - accuracy: 0.9250\n",
      "Epoch 313/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2012 - accuracy: 0.9230\n",
      "Epoch 314/1500\n",
      "40/40 [==============================] - 0s 860us/step - loss: 0.1990 - accuracy: 0.9211\n",
      "Epoch 315/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.1930 - accuracy: 0.9199\n",
      "Epoch 316/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.1848 - accuracy: 0.9332\n",
      "Epoch 317/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2084 - accuracy: 0.9144\n",
      "Epoch 318/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.1930 - accuracy: 0.9207\n",
      "Epoch 319/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2204 - accuracy: 0.9167\n",
      "Epoch 320/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2001 - accuracy: 0.9222\n",
      "Epoch 321/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.1909 - accuracy: 0.9266\n",
      "Epoch 322/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2116 - accuracy: 0.9167\n",
      "Epoch 323/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.1946 - accuracy: 0.9246\n",
      "Epoch 324/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.1999 - accuracy: 0.9226\n",
      "Epoch 325/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1914 - accuracy: 0.9273\n",
      "Epoch 326/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.1945 - accuracy: 0.9246\n",
      "Epoch 327/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.1972 - accuracy: 0.9183\n",
      "Epoch 328/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.1957 - accuracy: 0.9226\n",
      "Epoch 329/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.1756 - accuracy: 0.9321\n",
      "Epoch 330/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.1951 - accuracy: 0.9230\n",
      "Epoch 331/1500\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.1938 - accuracy: 0.9211\n",
      "Epoch 332/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.1971 - accuracy: 0.9258\n",
      "Epoch 333/1500\n",
      "40/40 [==============================] - 0s 902us/step - loss: 0.1977 - accuracy: 0.9234\n",
      "Epoch 334/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.1937 - accuracy: 0.9281\n",
      "Epoch 335/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.1985 - accuracy: 0.9250\n",
      "Epoch 336/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.1793 - accuracy: 0.9222\n",
      "Epoch 337/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.1763 - accuracy: 0.9317\n",
      "Epoch 338/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.1914 - accuracy: 0.9277\n",
      "Epoch 339/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.1937 - accuracy: 0.9293\n",
      "Epoch 340/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.1764 - accuracy: 0.9273\n",
      "Epoch 341/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.1960 - accuracy: 0.9207\n",
      "Epoch 342/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.1920 - accuracy: 0.9262\n",
      "Epoch 343/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.1812 - accuracy: 0.9285\n",
      "Epoch 344/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.1866 - accuracy: 0.9250\n",
      "Epoch 345/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.1814 - accuracy: 0.9309\n",
      "Epoch 346/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.1850 - accuracy: 0.9297\n",
      "Epoch 347/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.1851 - accuracy: 0.9246\n",
      "Epoch 348/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.1887 - accuracy: 0.9246\n",
      "Epoch 349/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.1831 - accuracy: 0.9293\n",
      "Epoch 350/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.1776 - accuracy: 0.9328\n",
      "Epoch 351/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.1821 - accuracy: 0.9273\n",
      "Epoch 352/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2025 - accuracy: 0.9214\n",
      "Epoch 353/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.1777 - accuracy: 0.9321\n",
      "Epoch 354/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.1858 - accuracy: 0.9250\n",
      "Epoch 355/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.1830 - accuracy: 0.9297\n",
      "Epoch 356/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.1792 - accuracy: 0.9368\n",
      "Epoch 357/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1838 - accuracy: 0.9313\n",
      "Epoch 358/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.1843 - accuracy: 0.9281\n",
      "Epoch 359/1500\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.1264 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 329.\n",
      "40/40 [==============================] - 0s 900us/step - loss: 0.1908 - accuracy: 0.9207\n",
      "Epoch 359: early stopping\n",
      "5/5 [==============================] - 0s 960us/step - loss: 0.6918 - accuracy: 0.7347\n",
      "5/5 [==============================] - 0s 873us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.6918318271636963, Accuracy: 0.7346938848495483, Precision: 0.5873205741626794, Recall: 0.8326023391812866, F1 Score: 0.6394582870571571\n",
      "Confusion Matrix:\n",
      " [[85 10 25]\n",
      " [ 0  8  0]\n",
      " [ 3  1 15]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 842, 1: 770, 2: 770})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1792 - accuracy: 0.4887\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.9641 - accuracy: 0.5684\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.8832 - accuracy: 0.6096\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.8350 - accuracy: 0.6352\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.7809 - accuracy: 0.6553\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.7504 - accuracy: 0.6721\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.7154 - accuracy: 0.6919\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.6978 - accuracy: 0.6919\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.6714 - accuracy: 0.7120\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.6822 - accuracy: 0.7120\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.6562 - accuracy: 0.7242\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.6420 - accuracy: 0.7280\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.6482 - accuracy: 0.7296\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.6109 - accuracy: 0.7477\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.6013 - accuracy: 0.7506\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.5963 - accuracy: 0.7540\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.6035 - accuracy: 0.7385\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.5850 - accuracy: 0.7448\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.5648 - accuracy: 0.7494\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.5805 - accuracy: 0.7473\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.5623 - accuracy: 0.7590\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.5352 - accuracy: 0.7641\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.5432 - accuracy: 0.7636\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.5445 - accuracy: 0.7548\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.5235 - accuracy: 0.7708\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5153 - accuracy: 0.7750\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5195 - accuracy: 0.7699\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7842\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7825\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.5187 - accuracy: 0.7750\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.5011 - accuracy: 0.7775\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.5041 - accuracy: 0.7914\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.5001 - accuracy: 0.7838\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.5022 - accuracy: 0.7863\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.4839 - accuracy: 0.7955\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.4907 - accuracy: 0.7876\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.4672 - accuracy: 0.7955\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.5002 - accuracy: 0.7842\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.4890 - accuracy: 0.7779\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.4745 - accuracy: 0.7985\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.4713 - accuracy: 0.7964\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.4732 - accuracy: 0.7939\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.4509 - accuracy: 0.7997\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.4650 - accuracy: 0.8006\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.4651 - accuracy: 0.7968\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.4570 - accuracy: 0.8039\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.4478 - accuracy: 0.8094\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.4536 - accuracy: 0.8006\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4366 - accuracy: 0.8073\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.4350 - accuracy: 0.8073\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.4439 - accuracy: 0.8073\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.4484 - accuracy: 0.8065\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4398 - accuracy: 0.8056\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.4515 - accuracy: 0.8039\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.4403 - accuracy: 0.8044\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.4236 - accuracy: 0.8233\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.4336 - accuracy: 0.8140\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.4298 - accuracy: 0.8178\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.4381 - accuracy: 0.8065\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.4231 - accuracy: 0.8216\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.4325 - accuracy: 0.8119\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.4367 - accuracy: 0.8044\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4183 - accuracy: 0.8115\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.4303 - accuracy: 0.8132\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.3977 - accuracy: 0.8233\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.4118 - accuracy: 0.8174\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4079 - accuracy: 0.8321\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.4126 - accuracy: 0.8178\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4112 - accuracy: 0.8237\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.4145 - accuracy: 0.8182\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.4099 - accuracy: 0.8212\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.4021 - accuracy: 0.8233\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.4006 - accuracy: 0.8195\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.4031 - accuracy: 0.8249\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3882 - accuracy: 0.8262\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4006 - accuracy: 0.8262\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3966 - accuracy: 0.8283\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.4063 - accuracy: 0.8254\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.3791 - accuracy: 0.8396\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.4033 - accuracy: 0.8291\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3949 - accuracy: 0.8266\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3713 - accuracy: 0.8380\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.3874 - accuracy: 0.8338\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3781 - accuracy: 0.8396\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3885 - accuracy: 0.8262\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3806 - accuracy: 0.8296\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3765 - accuracy: 0.8312\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.3815 - accuracy: 0.8342\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.3724 - accuracy: 0.8321\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3714 - accuracy: 0.8380\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.3635 - accuracy: 0.8384\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3768 - accuracy: 0.8371\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.3614 - accuracy: 0.8476\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.3790 - accuracy: 0.8312\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3543 - accuracy: 0.8396\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3572 - accuracy: 0.8463\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.3614 - accuracy: 0.8505\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3604 - accuracy: 0.8434\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.3584 - accuracy: 0.8396\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3448 - accuracy: 0.8472\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.3621 - accuracy: 0.8396\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3552 - accuracy: 0.8438\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3518 - accuracy: 0.8392\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3491 - accuracy: 0.8459\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3505 - accuracy: 0.8447\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.3542 - accuracy: 0.8463\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3366 - accuracy: 0.8552\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3514 - accuracy: 0.8463\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.3516 - accuracy: 0.8493\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3390 - accuracy: 0.8543\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3422 - accuracy: 0.8493\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3435 - accuracy: 0.8526\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3450 - accuracy: 0.8543\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3431 - accuracy: 0.8463\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3465 - accuracy: 0.8589\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3478 - accuracy: 0.8493\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3459 - accuracy: 0.8518\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.3442 - accuracy: 0.8426\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3354 - accuracy: 0.8535\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3381 - accuracy: 0.8535\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3311 - accuracy: 0.8577\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3487 - accuracy: 0.8510\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3289 - accuracy: 0.8568\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3410 - accuracy: 0.8497\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.3069 - accuracy: 0.8640\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3416 - accuracy: 0.8535\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3354 - accuracy: 0.8594\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3222 - accuracy: 0.8623\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3136 - accuracy: 0.8694\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3234 - accuracy: 0.8640\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3249 - accuracy: 0.8615\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.3327 - accuracy: 0.8652\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.3236 - accuracy: 0.8631\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.3283 - accuracy: 0.8552\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3254 - accuracy: 0.8673\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3146 - accuracy: 0.8652\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3107 - accuracy: 0.8715\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.3165 - accuracy: 0.8669\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.3167 - accuracy: 0.8669\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3070 - accuracy: 0.8707\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.3090 - accuracy: 0.8661\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.3249 - accuracy: 0.8631\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.3065 - accuracy: 0.8694\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3174 - accuracy: 0.8589\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3107 - accuracy: 0.8640\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.3038 - accuracy: 0.8728\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2992 - accuracy: 0.8757\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2976 - accuracy: 0.8766\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2980 - accuracy: 0.8732\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2941 - accuracy: 0.8753\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3004 - accuracy: 0.8724\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3045 - accuracy: 0.8715\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.3033 - accuracy: 0.8715\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3037 - accuracy: 0.8715\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.3010 - accuracy: 0.8745\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.3109 - accuracy: 0.8732\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2961 - accuracy: 0.8720\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.3123 - accuracy: 0.8699\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2844 - accuracy: 0.8816\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.3054 - accuracy: 0.8694\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2940 - accuracy: 0.8778\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2807 - accuracy: 0.8866\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2873 - accuracy: 0.8783\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2833 - accuracy: 0.8753\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2924 - accuracy: 0.8707\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2874 - accuracy: 0.8745\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3005 - accuracy: 0.8741\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2852 - accuracy: 0.8795\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3005 - accuracy: 0.8728\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2796 - accuracy: 0.8816\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2899 - accuracy: 0.8736\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2893 - accuracy: 0.8753\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2817 - accuracy: 0.8808\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2792 - accuracy: 0.8862\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2967 - accuracy: 0.8778\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2719 - accuracy: 0.8778\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2862 - accuracy: 0.8804\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2775 - accuracy: 0.8795\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2781 - accuracy: 0.8866\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2784 - accuracy: 0.8829\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2768 - accuracy: 0.8917\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2716 - accuracy: 0.8841\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2794 - accuracy: 0.8766\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2857 - accuracy: 0.8728\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2792 - accuracy: 0.8837\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2745 - accuracy: 0.8850\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2672 - accuracy: 0.8829\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2727 - accuracy: 0.8816\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2576 - accuracy: 0.8934\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2666 - accuracy: 0.8908\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2744 - accuracy: 0.8766\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2666 - accuracy: 0.8913\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2650 - accuracy: 0.8934\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2535 - accuracy: 0.8934\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2681 - accuracy: 0.8854\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2939 - accuracy: 0.8745\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2658 - accuracy: 0.8917\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2601 - accuracy: 0.8929\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2698 - accuracy: 0.8862\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2572 - accuracy: 0.8980\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2547 - accuracy: 0.8971\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2615 - accuracy: 0.8925\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2681 - accuracy: 0.8925\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2654 - accuracy: 0.8908\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2674 - accuracy: 0.8854\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2614 - accuracy: 0.8904\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2639 - accuracy: 0.8883\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2606 - accuracy: 0.8921\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2512 - accuracy: 0.8955\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.2506 - accuracy: 0.8984\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2565 - accuracy: 0.8946\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2476 - accuracy: 0.8938\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2490 - accuracy: 0.8929\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2486 - accuracy: 0.8976\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2557 - accuracy: 0.8913\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2577 - accuracy: 0.8963\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2454 - accuracy: 0.8988\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2614 - accuracy: 0.8913\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2344 - accuracy: 0.8971\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2517 - accuracy: 0.8959\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2451 - accuracy: 0.8984\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2498 - accuracy: 0.8976\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2417 - accuracy: 0.8984\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2460 - accuracy: 0.8980\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2464 - accuracy: 0.8988\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2441 - accuracy: 0.9018\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2486 - accuracy: 0.8929\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2416 - accuracy: 0.8980\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2427 - accuracy: 0.9051\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2454 - accuracy: 0.9026\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2324 - accuracy: 0.9026\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2396 - accuracy: 0.9055\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2419 - accuracy: 0.9009\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2341 - accuracy: 0.9060\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2264 - accuracy: 0.9043\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2318 - accuracy: 0.9106\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2401 - accuracy: 0.9026\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2357 - accuracy: 0.9085\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2225 - accuracy: 0.9093\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2324 - accuracy: 0.9034\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2292 - accuracy: 0.9034\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2466 - accuracy: 0.8967\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2228 - accuracy: 0.9106\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2428 - accuracy: 0.9051\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2306 - accuracy: 0.9051\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2297 - accuracy: 0.9060\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2417 - accuracy: 0.9005\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.2158 - accuracy: 0.9144\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9022\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.2287 - accuracy: 0.9068\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2294 - accuracy: 0.9139\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2307 - accuracy: 0.9026\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2367 - accuracy: 0.9022\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2236 - accuracy: 0.9081\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2358 - accuracy: 0.9085\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2262 - accuracy: 0.9097\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2322 - accuracy: 0.9034\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2289 - accuracy: 0.8988\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.2263 - accuracy: 0.9097\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 945us/step - loss: 0.2190 - accuracy: 0.9076\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.2230 - accuracy: 0.9076\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9102\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.8955\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.9055\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 998us/step - loss: 0.2207 - accuracy: 0.9110\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9114\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2449 - accuracy: 0.8963\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2210 - accuracy: 0.9123\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2232 - accuracy: 0.9068\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2113 - accuracy: 0.9110\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2313 - accuracy: 0.9068\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.2097 - accuracy: 0.9118\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2203 - accuracy: 0.9144\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2210 - accuracy: 0.9055\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2277 - accuracy: 0.9102\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.2111 - accuracy: 0.9148\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.2107 - accuracy: 0.9186\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.2143 - accuracy: 0.9106\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.2184 - accuracy: 0.9085\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2219 - accuracy: 0.9072\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.2163 - accuracy: 0.9181\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2228 - accuracy: 0.9123\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2107 - accuracy: 0.9139\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2098 - accuracy: 0.9232\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2087 - accuracy: 0.9186\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2100 - accuracy: 0.9169\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2176 - accuracy: 0.9072\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2111 - accuracy: 0.9127\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2007 - accuracy: 0.9181\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2031 - accuracy: 0.9194\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2048 - accuracy: 0.9156\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.1977 - accuracy: 0.9232\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1952 - accuracy: 0.9274\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2034 - accuracy: 0.9211\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2233 - accuracy: 0.9106\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2137 - accuracy: 0.9106\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.1967 - accuracy: 0.9186\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1997 - accuracy: 0.9219\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.2016 - accuracy: 0.9156\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2144 - accuracy: 0.9131\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2096 - accuracy: 0.9135\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2148 - accuracy: 0.9144\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2063 - accuracy: 0.9148\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2129 - accuracy: 0.9207\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2054 - accuracy: 0.9228\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2109 - accuracy: 0.9198\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2050 - accuracy: 0.9156\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2145 - accuracy: 0.9085\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2021 - accuracy: 0.9211\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2149 - accuracy: 0.9089\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1944 - accuracy: 0.9211\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2018 - accuracy: 0.9173\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1990 - accuracy: 0.9211\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2056 - accuracy: 0.9186\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2002 - accuracy: 0.9198\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2133 - accuracy: 0.9181\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2104 - accuracy: 0.9160\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2034 - accuracy: 0.9160\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2009 - accuracy: 0.9186\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1984 - accuracy: 0.9215\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2036 - accuracy: 0.9228\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1966 - accuracy: 0.9223\n",
      "Epoch 323/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.2174 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 293.\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2077 - accuracy: 0.9194\n",
      "Epoch 323: early stopping\n",
      "7/7 [==============================] - 0s 749us/step - loss: 0.5779 - accuracy: 0.7596\n",
      "7/7 [==============================] - 0s 610us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.5778680443763733, Accuracy: 0.7596153616905212, Precision: 0.6506018893387314, Recall: 0.8469091229306094, F1 Score: 0.6971572294392726\n",
      "Confusion Matrix:\n",
      " [[121  10  36]\n",
      " [  1  16   0]\n",
      " [  3   0  21]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A', '000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'109A', '031A'}\n",
      "Moved to Test Set:\n",
      "{'109A', '031A'}\n",
      "Removed from Test Set\n",
      "{'046A', '000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A'\n",
      " '073A' '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A' '031A'\n",
      " '032A' '035A' '037A' '040A' '042A' '051B' '052A' '057A' '058A' '070A'\n",
      " '072A' '076A' '095A' '103A' '105A' '106A' '109A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "679\n",
      "Length of y_train_val:\n",
      "679\n",
      "Length of groups_train_val:\n",
      "679\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     449\n",
      "kitten    117\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     139\n",
      "senior     65\n",
      "kitten     54\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 898, 1: 585, 2: 565})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 969us/step - loss: 1.1738 - accuracy: 0.4521\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 935us/step - loss: 0.9742 - accuracy: 0.5566\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 0.8616 - accuracy: 0.6172\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.8320 - accuracy: 0.6416\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.7739 - accuracy: 0.6685\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.7315 - accuracy: 0.6870\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.7306 - accuracy: 0.6821\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.6864 - accuracy: 0.7124\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.6781 - accuracy: 0.7202\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 874us/step - loss: 0.6637 - accuracy: 0.7212\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.6449 - accuracy: 0.7153\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.6136 - accuracy: 0.7329\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.6290 - accuracy: 0.7407\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.6216 - accuracy: 0.7402\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.5706 - accuracy: 0.7515\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.5868 - accuracy: 0.7485\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.5741 - accuracy: 0.7656\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.5594 - accuracy: 0.7666\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.5404 - accuracy: 0.7783\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.5510 - accuracy: 0.7598\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.5263 - accuracy: 0.7837\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 818us/step - loss: 0.5167 - accuracy: 0.7793\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.5169 - accuracy: 0.7886\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.5353 - accuracy: 0.7812\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.5165 - accuracy: 0.7896\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.5034 - accuracy: 0.7905\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.5024 - accuracy: 0.7910\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.4999 - accuracy: 0.7983\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.4767 - accuracy: 0.8071\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4805 - accuracy: 0.8130\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.4845 - accuracy: 0.8086\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 691us/step - loss: 0.4795 - accuracy: 0.8057\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.4733 - accuracy: 0.8013\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.4799 - accuracy: 0.8071\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.4667 - accuracy: 0.7925\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.4660 - accuracy: 0.8062\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.4650 - accuracy: 0.8105\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.4460 - accuracy: 0.8218\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.4399 - accuracy: 0.8208\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.4546 - accuracy: 0.8086\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.4498 - accuracy: 0.8120\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.4371 - accuracy: 0.8223\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.4368 - accuracy: 0.8237\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.4262 - accuracy: 0.8213\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.4327 - accuracy: 0.8271\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.4227 - accuracy: 0.8228\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.4225 - accuracy: 0.8330\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.4208 - accuracy: 0.8306\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.4228 - accuracy: 0.8237\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.4195 - accuracy: 0.8301\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.4116 - accuracy: 0.8291\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.4295 - accuracy: 0.8169\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.4090 - accuracy: 0.8364\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.3919 - accuracy: 0.8345\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.3989 - accuracy: 0.8413\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4116 - accuracy: 0.8301\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.3867 - accuracy: 0.8413\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.3838 - accuracy: 0.8438\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.4164 - accuracy: 0.8291\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.4021 - accuracy: 0.8369\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.3987 - accuracy: 0.8369\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.3859 - accuracy: 0.8442\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 828us/step - loss: 0.4010 - accuracy: 0.8379\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.4080 - accuracy: 0.8379\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.3851 - accuracy: 0.8447\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.3876 - accuracy: 0.8408\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.3766 - accuracy: 0.8540\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.3737 - accuracy: 0.8423\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.3929 - accuracy: 0.8418\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.3740 - accuracy: 0.8540\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.3642 - accuracy: 0.8481\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.3819 - accuracy: 0.8457\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.3664 - accuracy: 0.8530\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3544 - accuracy: 0.8540\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.3586 - accuracy: 0.8535\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3585 - accuracy: 0.8442\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 917us/step - loss: 0.3635 - accuracy: 0.8545\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.3538 - accuracy: 0.8569\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 962us/step - loss: 0.3643 - accuracy: 0.8467\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8594\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 915us/step - loss: 0.3509 - accuracy: 0.8540\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.3554 - accuracy: 0.8579\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.3407 - accuracy: 0.8662\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.3517 - accuracy: 0.8555\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.3488 - accuracy: 0.8574\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 921us/step - loss: 0.3371 - accuracy: 0.8667\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 0.3447 - accuracy: 0.8579\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.3452 - accuracy: 0.8643\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.3641 - accuracy: 0.8486\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.3641 - accuracy: 0.8555\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.3336 - accuracy: 0.8594\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.3311 - accuracy: 0.8604\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.3281 - accuracy: 0.8633\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.3270 - accuracy: 0.8740\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.3302 - accuracy: 0.8657\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.3341 - accuracy: 0.8652\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.3588 - accuracy: 0.8555\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3286 - accuracy: 0.8677\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.3211 - accuracy: 0.8706\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.3261 - accuracy: 0.8662\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.3261 - accuracy: 0.8711\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.3378 - accuracy: 0.8643\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3073 - accuracy: 0.8730\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.3176 - accuracy: 0.8638\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.3179 - accuracy: 0.8740\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.2996 - accuracy: 0.8877\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3120 - accuracy: 0.8706\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.3144 - accuracy: 0.8726\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.3204 - accuracy: 0.8677\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3126 - accuracy: 0.8735\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.3051 - accuracy: 0.8794\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.3296 - accuracy: 0.8628\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.3191 - accuracy: 0.8765\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.2943 - accuracy: 0.8877\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.3032 - accuracy: 0.8740\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.3036 - accuracy: 0.8794\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.3086 - accuracy: 0.8735\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.3025 - accuracy: 0.8750\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.3063 - accuracy: 0.8779\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.3128 - accuracy: 0.8735\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.3163 - accuracy: 0.8696\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.3108 - accuracy: 0.8765\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2965 - accuracy: 0.8857\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.3005 - accuracy: 0.8735\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.2964 - accuracy: 0.8867\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2981 - accuracy: 0.8818\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2983 - accuracy: 0.8774\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.2924 - accuracy: 0.8823\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2872 - accuracy: 0.8838\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.2950 - accuracy: 0.8789\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.2969 - accuracy: 0.8760\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2786 - accuracy: 0.8936\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 882us/step - loss: 0.2872 - accuracy: 0.8906\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.2903 - accuracy: 0.8887\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2993 - accuracy: 0.8799\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.2835 - accuracy: 0.8838\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.2922 - accuracy: 0.8848\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2787 - accuracy: 0.8921\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.2825 - accuracy: 0.8911\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2754 - accuracy: 0.8950\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2864 - accuracy: 0.8823\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.2609 - accuracy: 0.9019\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.2845 - accuracy: 0.8901\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2800 - accuracy: 0.8906\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2715 - accuracy: 0.8950\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2593 - accuracy: 0.8950\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.2612 - accuracy: 0.8833\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2758 - accuracy: 0.8994\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2598 - accuracy: 0.8984\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.2644 - accuracy: 0.8843\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.2734 - accuracy: 0.8975\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.2657 - accuracy: 0.8931\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2560 - accuracy: 0.9038\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2533 - accuracy: 0.9111\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.2592 - accuracy: 0.8911\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.2583 - accuracy: 0.8970\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2668 - accuracy: 0.8911\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2477 - accuracy: 0.9014\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2711 - accuracy: 0.8955\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.2516 - accuracy: 0.9004\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.2539 - accuracy: 0.9097\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2569 - accuracy: 0.8984\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.2547 - accuracy: 0.9033\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.2543 - accuracy: 0.9058\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2449 - accuracy: 0.9019\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2604 - accuracy: 0.8921\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.2355 - accuracy: 0.9146\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2557 - accuracy: 0.9038\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2343 - accuracy: 0.9082\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2366 - accuracy: 0.9033\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2448 - accuracy: 0.9048\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.2523 - accuracy: 0.9004\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.2350 - accuracy: 0.9033\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2417 - accuracy: 0.9009\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.2516 - accuracy: 0.9023\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.2405 - accuracy: 0.9067\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2520 - accuracy: 0.8975\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.2307 - accuracy: 0.9058\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2267 - accuracy: 0.9092\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.2323 - accuracy: 0.9072\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2467 - accuracy: 0.8984\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2421 - accuracy: 0.9048\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.2431 - accuracy: 0.9023\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2408 - accuracy: 0.9102\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2308 - accuracy: 0.9155\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2405 - accuracy: 0.9082\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2389 - accuracy: 0.9058\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.2368 - accuracy: 0.9033\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2248 - accuracy: 0.9126\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.2461 - accuracy: 0.8994\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2404 - accuracy: 0.9033\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2348 - accuracy: 0.9072\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2324 - accuracy: 0.9082\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2370 - accuracy: 0.9038\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2258 - accuracy: 0.9102\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2319 - accuracy: 0.9111\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2402 - accuracy: 0.9058\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2247 - accuracy: 0.9106\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.2216 - accuracy: 0.9111\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.2170 - accuracy: 0.9214\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2110 - accuracy: 0.9194\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.2216 - accuracy: 0.9146\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.2282 - accuracy: 0.9106\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2299 - accuracy: 0.9067\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.2251 - accuracy: 0.9170\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2013 - accuracy: 0.9229\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.2260 - accuracy: 0.9126\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2204 - accuracy: 0.9141\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.2432 - accuracy: 0.9043\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.2183 - accuracy: 0.9180\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.2060 - accuracy: 0.9268\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2151 - accuracy: 0.9141\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2176 - accuracy: 0.9150\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2246 - accuracy: 0.9097\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.2119 - accuracy: 0.9097\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.2225 - accuracy: 0.9106\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2054 - accuracy: 0.9229\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.2133 - accuracy: 0.9175\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 687us/step - loss: 0.2089 - accuracy: 0.9189\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.2204 - accuracy: 0.9146\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2248 - accuracy: 0.9131\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.1952 - accuracy: 0.9229\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1990 - accuracy: 0.9233\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.2157 - accuracy: 0.9136\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.2147 - accuracy: 0.9165\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2081 - accuracy: 0.9136\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.1862 - accuracy: 0.9258\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2006 - accuracy: 0.9170\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.1886 - accuracy: 0.9287\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.1890 - accuracy: 0.9248\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1973 - accuracy: 0.9258\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1976 - accuracy: 0.9160\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.1910 - accuracy: 0.9287\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 874us/step - loss: 0.1944 - accuracy: 0.9248\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.2238 - accuracy: 0.9092\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.2089 - accuracy: 0.9160\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1996 - accuracy: 0.9258\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.1987 - accuracy: 0.9297\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.1939 - accuracy: 0.9268\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1978 - accuracy: 0.9253\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1985 - accuracy: 0.9238\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.1758 - accuracy: 0.9331\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.1933 - accuracy: 0.9263\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2010 - accuracy: 0.9204\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.1971 - accuracy: 0.9233\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.1862 - accuracy: 0.9253\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.2116 - accuracy: 0.9155\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1904 - accuracy: 0.9287\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.1912 - accuracy: 0.9268\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1913 - accuracy: 0.9199\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1859 - accuracy: 0.9365\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.1979 - accuracy: 0.9219\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.1931 - accuracy: 0.9224\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.1836 - accuracy: 0.9268\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.1875 - accuracy: 0.9282\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 694us/step - loss: 0.1866 - accuracy: 0.9287\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.1927 - accuracy: 0.9292\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.2039 - accuracy: 0.9185\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.1741 - accuracy: 0.9321\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.1788 - accuracy: 0.9355\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1835 - accuracy: 0.9258\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.1813 - accuracy: 0.9380\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1863 - accuracy: 0.9277\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.1836 - accuracy: 0.9272\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.1731 - accuracy: 0.9277\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.1906 - accuracy: 0.9253\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.1843 - accuracy: 0.9282\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.1765 - accuracy: 0.9341\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.1695 - accuracy: 0.9385\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.1712 - accuracy: 0.9385\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.1840 - accuracy: 0.9272\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.1901 - accuracy: 0.9238\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1748 - accuracy: 0.9302\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.1774 - accuracy: 0.9277\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.1912 - accuracy: 0.9312\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.1739 - accuracy: 0.9321\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1721 - accuracy: 0.9380\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.1701 - accuracy: 0.9399\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.1738 - accuracy: 0.9370\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.1701 - accuracy: 0.9390\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.1830 - accuracy: 0.9263\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.1811 - accuracy: 0.9312\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.1666 - accuracy: 0.9370\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.1776 - accuracy: 0.9312\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.1595 - accuracy: 0.9380\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.1782 - accuracy: 0.9258\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.1649 - accuracy: 0.9385\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.1920 - accuracy: 0.9233\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1864 - accuracy: 0.9277\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.1813 - accuracy: 0.9341\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1665 - accuracy: 0.9346\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1690 - accuracy: 0.9385\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.1929 - accuracy: 0.9243\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.1612 - accuracy: 0.9390\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1778 - accuracy: 0.9287\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1628 - accuracy: 0.9365\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1898 - accuracy: 0.9238\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.1640 - accuracy: 0.9365\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1702 - accuracy: 0.9355\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.1752 - accuracy: 0.9331\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1670 - accuracy: 0.9395\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1623 - accuracy: 0.9370\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.1547 - accuracy: 0.9438\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.1745 - accuracy: 0.9331\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.1632 - accuracy: 0.9380\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1610 - accuracy: 0.9409\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.1719 - accuracy: 0.9316\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.1906 - accuracy: 0.9272\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1646 - accuracy: 0.9395\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.1596 - accuracy: 0.9429\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.1554 - accuracy: 0.9375\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.1758 - accuracy: 0.9331\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 686us/step - loss: 0.1602 - accuracy: 0.9390\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1652 - accuracy: 0.9365\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.1668 - accuracy: 0.9316\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.1645 - accuracy: 0.9390\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1612 - accuracy: 0.9341\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1606 - accuracy: 0.9375\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1569 - accuracy: 0.9419\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.1566 - accuracy: 0.9390\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.1496 - accuracy: 0.9414\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.1525 - accuracy: 0.9414\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.1472 - accuracy: 0.9438\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.1659 - accuracy: 0.9297\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.1471 - accuracy: 0.9429\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.1501 - accuracy: 0.9355\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.1540 - accuracy: 0.9380\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1545 - accuracy: 0.9409\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.1639 - accuracy: 0.9346\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 696us/step - loss: 0.1654 - accuracy: 0.9370\n",
      "Epoch 331/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1517 - accuracy: 0.9390\n",
      "Epoch 332/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.1509 - accuracy: 0.9448\n",
      "Epoch 333/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.1540 - accuracy: 0.9404\n",
      "Epoch 334/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.1603 - accuracy: 0.9380\n",
      "Epoch 335/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1391 - accuracy: 0.9512\n",
      "Epoch 336/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.1517 - accuracy: 0.9414\n",
      "Epoch 337/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.1595 - accuracy: 0.9395\n",
      "Epoch 338/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.1623 - accuracy: 0.9385\n",
      "Epoch 339/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1417 - accuracy: 0.9463\n",
      "Epoch 340/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1648 - accuracy: 0.9331\n",
      "Epoch 341/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.1705 - accuracy: 0.9336\n",
      "Epoch 342/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1493 - accuracy: 0.9463\n",
      "Epoch 343/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.1674 - accuracy: 0.9429\n",
      "Epoch 344/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1455 - accuracy: 0.9443\n",
      "Epoch 345/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.1479 - accuracy: 0.9434\n",
      "Epoch 346/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1546 - accuracy: 0.9424\n",
      "Epoch 347/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1443 - accuracy: 0.9473\n",
      "Epoch 348/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1571 - accuracy: 0.9399\n",
      "Epoch 349/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1650 - accuracy: 0.9424\n",
      "Epoch 350/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1478 - accuracy: 0.9443\n",
      "Epoch 351/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1301 - accuracy: 0.9492\n",
      "Epoch 352/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1527 - accuracy: 0.9438\n",
      "Epoch 353/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.1477 - accuracy: 0.9443\n",
      "Epoch 354/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.1496 - accuracy: 0.9399\n",
      "Epoch 355/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1559 - accuracy: 0.9380\n",
      "Epoch 356/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1354 - accuracy: 0.9492\n",
      "Epoch 357/1500\n",
      "32/32 [==============================] - 0s 890us/step - loss: 0.1584 - accuracy: 0.9365\n",
      "Epoch 358/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.1543 - accuracy: 0.9370\n",
      "Epoch 359/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.1496 - accuracy: 0.9473\n",
      "Epoch 360/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1418 - accuracy: 0.9434\n",
      "Epoch 361/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.1515 - accuracy: 0.9341\n",
      "Epoch 362/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.1428 - accuracy: 0.9497\n",
      "Epoch 363/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.1445 - accuracy: 0.9419\n",
      "Epoch 364/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.1390 - accuracy: 0.9497\n",
      "Epoch 365/1500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.1511 - accuracy: 0.9385\n",
      "Epoch 366/1500\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.1374 - accuracy: 0.9424\n",
      "Epoch 367/1500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.1484 - accuracy: 0.9414\n",
      "Epoch 368/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.1572 - accuracy: 0.9414\n",
      "Epoch 369/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1328 - accuracy: 0.9502\n",
      "Epoch 370/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.1459 - accuracy: 0.9453\n",
      "Epoch 371/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.1475 - accuracy: 0.9473\n",
      "Epoch 372/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.1630 - accuracy: 0.9346\n",
      "Epoch 373/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.1532 - accuracy: 0.9438\n",
      "Epoch 374/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1393 - accuracy: 0.9463\n",
      "Epoch 375/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.1279 - accuracy: 0.9482\n",
      "Epoch 376/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1536 - accuracy: 0.9482\n",
      "Epoch 377/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1323 - accuracy: 0.9536\n",
      "Epoch 378/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1515 - accuracy: 0.9375\n",
      "Epoch 379/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1302 - accuracy: 0.9526\n",
      "Epoch 380/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.1290 - accuracy: 0.9492\n",
      "Epoch 381/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1461 - accuracy: 0.9448\n",
      "Epoch 382/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1293 - accuracy: 0.9487\n",
      "Epoch 383/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.1376 - accuracy: 0.9463\n",
      "Epoch 384/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.1324 - accuracy: 0.9521\n",
      "Epoch 385/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.1457 - accuracy: 0.9448\n",
      "Epoch 386/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1379 - accuracy: 0.9482\n",
      "Epoch 387/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1448 - accuracy: 0.9434\n",
      "Epoch 388/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.1379 - accuracy: 0.9531\n",
      "Epoch 389/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1385 - accuracy: 0.9487\n",
      "Epoch 390/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1317 - accuracy: 0.9482\n",
      "Epoch 391/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1320 - accuracy: 0.9448\n",
      "Epoch 392/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1421 - accuracy: 0.9468\n",
      "Epoch 393/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1427 - accuracy: 0.9478\n",
      "Epoch 394/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.1292 - accuracy: 0.9482\n",
      "Epoch 395/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1437 - accuracy: 0.9424\n",
      "Epoch 396/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1363 - accuracy: 0.9468\n",
      "Epoch 397/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1383 - accuracy: 0.9443\n",
      "Epoch 398/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1414 - accuracy: 0.9448\n",
      "Epoch 399/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1435 - accuracy: 0.9448\n",
      "Epoch 400/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.1503 - accuracy: 0.9453\n",
      "Epoch 401/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1298 - accuracy: 0.9487\n",
      "Epoch 402/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.1392 - accuracy: 0.9453\n",
      "Epoch 403/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.1259 - accuracy: 0.9536\n",
      "Epoch 404/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1282 - accuracy: 0.9487\n",
      "Epoch 405/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.1411 - accuracy: 0.9487\n",
      "Epoch 406/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1458 - accuracy: 0.9458\n",
      "Epoch 407/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.1396 - accuracy: 0.9468\n",
      "Epoch 408/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.1225 - accuracy: 0.9570\n",
      "Epoch 409/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.1341 - accuracy: 0.9463\n",
      "Epoch 410/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1367 - accuracy: 0.9526\n",
      "Epoch 411/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.1279 - accuracy: 0.9507\n",
      "Epoch 412/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1420 - accuracy: 0.9438\n",
      "Epoch 413/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1406 - accuracy: 0.9438\n",
      "Epoch 414/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1358 - accuracy: 0.9478\n",
      "Epoch 415/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1032 - accuracy: 0.9644\n",
      "Epoch 416/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.1340 - accuracy: 0.9487\n",
      "Epoch 417/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.1439 - accuracy: 0.9414\n",
      "Epoch 418/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.1271 - accuracy: 0.9551\n",
      "Epoch 419/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.1263 - accuracy: 0.9531\n",
      "Epoch 420/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.1233 - accuracy: 0.9492\n",
      "Epoch 421/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1375 - accuracy: 0.9478\n",
      "Epoch 422/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1347 - accuracy: 0.9434\n",
      "Epoch 423/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.1551 - accuracy: 0.9443\n",
      "Epoch 424/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1368 - accuracy: 0.9453\n",
      "Epoch 425/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1360 - accuracy: 0.9492\n",
      "Epoch 426/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1339 - accuracy: 0.9492\n",
      "Epoch 427/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1426 - accuracy: 0.9536\n",
      "Epoch 428/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1228 - accuracy: 0.9570\n",
      "Epoch 429/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.1468 - accuracy: 0.9404\n",
      "Epoch 430/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.1288 - accuracy: 0.9507\n",
      "Epoch 431/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1395 - accuracy: 0.9482\n",
      "Epoch 432/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1303 - accuracy: 0.9502\n",
      "Epoch 433/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.1234 - accuracy: 0.9561\n",
      "Epoch 434/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1366 - accuracy: 0.9502\n",
      "Epoch 435/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1283 - accuracy: 0.9478\n",
      "Epoch 436/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1239 - accuracy: 0.9546\n",
      "Epoch 437/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1245 - accuracy: 0.9546\n",
      "Epoch 438/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.1150 - accuracy: 0.9580\n",
      "Epoch 439/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1246 - accuracy: 0.9512\n",
      "Epoch 440/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.1167 - accuracy: 0.9634\n",
      "Epoch 441/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1252 - accuracy: 0.9565\n",
      "Epoch 442/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.1175 - accuracy: 0.9497\n",
      "Epoch 443/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.1254 - accuracy: 0.9526\n",
      "Epoch 444/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.1228 - accuracy: 0.9561\n",
      "Epoch 445/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.1306 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 415.\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.1249 - accuracy: 0.9473\n",
      "Epoch 445: early stopping\n",
      "9/9 [==============================] - 0s 740us/step - loss: 0.9752 - accuracy: 0.6977\n",
      "9/9 [==============================] - 0s 569us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 0.9751850366592407, Accuracy: 0.6976743936538696, Precision: 0.7169987546699875, Recall: 0.6792439552631399, F1 Score: 0.694951664876477\n",
      "Confusion Matrix:\n",
      " [[105   3  31]\n",
      " [ 10  41   3]\n",
      " [ 31   0  34]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 916, 1: 680, 2: 540})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.1177 - accuracy: 0.5070\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.9836 - accuracy: 0.5730\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.8872 - accuracy: 0.6189\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.8825 - accuracy: 0.6259\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 995us/step - loss: 0.8262 - accuracy: 0.6428\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.8024 - accuracy: 0.6512\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.7647 - accuracy: 0.6709\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.7216 - accuracy: 0.6868\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.7371 - accuracy: 0.6854\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.7052 - accuracy: 0.6845\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.6991 - accuracy: 0.6934\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 954us/step - loss: 0.6908 - accuracy: 0.6919\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.7016 - accuracy: 0.7022\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.6660 - accuracy: 0.7083\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.6443 - accuracy: 0.7228\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.6432 - accuracy: 0.7182\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.6385 - accuracy: 0.7322\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.6159 - accuracy: 0.7191\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.6212 - accuracy: 0.7360\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.6224 - accuracy: 0.7257\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.6039 - accuracy: 0.7336\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5997 - accuracy: 0.7360\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.5813 - accuracy: 0.7467\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.5966 - accuracy: 0.7444\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.5795 - accuracy: 0.7519\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.5781 - accuracy: 0.7519\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.5517 - accuracy: 0.7650\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.5354 - accuracy: 0.7664\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.5655 - accuracy: 0.7430\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.5525 - accuracy: 0.7669\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.5208 - accuracy: 0.7772\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.5238 - accuracy: 0.7814\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.5133 - accuracy: 0.7781\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.5137 - accuracy: 0.7711\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.5375 - accuracy: 0.7575\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.5117 - accuracy: 0.7706\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.5096 - accuracy: 0.7772\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.4962 - accuracy: 0.7860\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.5104 - accuracy: 0.7743\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.5048 - accuracy: 0.7720\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.4947 - accuracy: 0.7870\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.5028 - accuracy: 0.7832\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.4956 - accuracy: 0.7814\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.4863 - accuracy: 0.7832\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.4870 - accuracy: 0.7870\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.4878 - accuracy: 0.7884\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.4806 - accuracy: 0.7870\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.4669 - accuracy: 0.8010\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.4692 - accuracy: 0.8001\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.4675 - accuracy: 0.7954\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.4647 - accuracy: 0.7935\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4545 - accuracy: 0.7992\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.4682 - accuracy: 0.7903\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.4730 - accuracy: 0.7968\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.4438 - accuracy: 0.8020\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.4696 - accuracy: 0.8006\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.4329 - accuracy: 0.8057\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.4484 - accuracy: 0.8071\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.4630 - accuracy: 0.8066\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.4234 - accuracy: 0.8202\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.4287 - accuracy: 0.8230\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.4289 - accuracy: 0.8179\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.4450 - accuracy: 0.8048\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.4122 - accuracy: 0.8169\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.4054 - accuracy: 0.8268\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.4304 - accuracy: 0.8179\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.4399 - accuracy: 0.8048\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.4157 - accuracy: 0.8160\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.4158 - accuracy: 0.8151\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.4097 - accuracy: 0.8235\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.4184 - accuracy: 0.8230\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.4177 - accuracy: 0.8207\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.4222 - accuracy: 0.8141\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4199 - accuracy: 0.8202\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.4023 - accuracy: 0.8333\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.3956 - accuracy: 0.8291\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.4059 - accuracy: 0.8282\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.3852 - accuracy: 0.8357\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.3946 - accuracy: 0.8263\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3829 - accuracy: 0.8352\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.4084 - accuracy: 0.8268\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3920 - accuracy: 0.8329\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3881 - accuracy: 0.8272\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3875 - accuracy: 0.8385\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.3970 - accuracy: 0.8347\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3798 - accuracy: 0.8324\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.3809 - accuracy: 0.8333\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.4130 - accuracy: 0.8230\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.3785 - accuracy: 0.8422\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.3761 - accuracy: 0.8361\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.3787 - accuracy: 0.8385\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3710 - accuracy: 0.8488\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.3682 - accuracy: 0.8413\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.3695 - accuracy: 0.8282\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.3666 - accuracy: 0.8460\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3666 - accuracy: 0.8404\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.3819 - accuracy: 0.8296\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3793 - accuracy: 0.8394\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3672 - accuracy: 0.8432\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.3474 - accuracy: 0.8511\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.3611 - accuracy: 0.8474\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.3732 - accuracy: 0.8343\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.3502 - accuracy: 0.8581\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.3680 - accuracy: 0.8469\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3493 - accuracy: 0.8436\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.3589 - accuracy: 0.8474\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3478 - accuracy: 0.8460\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.3604 - accuracy: 0.8464\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.3669 - accuracy: 0.8455\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3592 - accuracy: 0.8390\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3581 - accuracy: 0.8366\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.3482 - accuracy: 0.8577\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3561 - accuracy: 0.8511\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.3484 - accuracy: 0.8563\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3390 - accuracy: 0.8586\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3490 - accuracy: 0.8511\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.3569 - accuracy: 0.8460\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.3620 - accuracy: 0.8441\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.3402 - accuracy: 0.8600\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.3481 - accuracy: 0.8446\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3397 - accuracy: 0.8553\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3458 - accuracy: 0.8525\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.3243 - accuracy: 0.8647\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3409 - accuracy: 0.8535\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.3299 - accuracy: 0.8619\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.3133 - accuracy: 0.8652\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.3305 - accuracy: 0.8535\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3451 - accuracy: 0.8497\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.3212 - accuracy: 0.8680\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.3344 - accuracy: 0.8614\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.3266 - accuracy: 0.8567\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.3199 - accuracy: 0.8600\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.3258 - accuracy: 0.8628\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.3195 - accuracy: 0.8680\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.3297 - accuracy: 0.8633\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.3181 - accuracy: 0.8596\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.3196 - accuracy: 0.8680\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.3107 - accuracy: 0.8713\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3142 - accuracy: 0.8647\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3056 - accuracy: 0.8755\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.3072 - accuracy: 0.8764\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3094 - accuracy: 0.8727\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3218 - accuracy: 0.8647\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3071 - accuracy: 0.8764\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3153 - accuracy: 0.8666\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.3083 - accuracy: 0.8661\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.3344 - accuracy: 0.8633\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.3132 - accuracy: 0.8684\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.3111 - accuracy: 0.8708\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3053 - accuracy: 0.8684\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3227 - accuracy: 0.8628\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3200 - accuracy: 0.8596\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3158 - accuracy: 0.8675\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.3044 - accuracy: 0.8722\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3188 - accuracy: 0.8624\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.3135 - accuracy: 0.8736\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2956 - accuracy: 0.8722\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3018 - accuracy: 0.8787\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2949 - accuracy: 0.8769\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 945us/step - loss: 0.3011 - accuracy: 0.8713\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.3056 - accuracy: 0.8745\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.2898 - accuracy: 0.8801\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.3074 - accuracy: 0.8684\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2944 - accuracy: 0.8825\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2841 - accuracy: 0.8787\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.2947 - accuracy: 0.8820\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2953 - accuracy: 0.8787\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2929 - accuracy: 0.8778\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.2741 - accuracy: 0.8886\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.2917 - accuracy: 0.8727\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2984 - accuracy: 0.8853\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2828 - accuracy: 0.8820\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2876 - accuracy: 0.8670\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2911 - accuracy: 0.8797\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2794 - accuracy: 0.8834\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2892 - accuracy: 0.8834\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2801 - accuracy: 0.8853\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2721 - accuracy: 0.8886\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2730 - accuracy: 0.8876\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2893 - accuracy: 0.8741\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2818 - accuracy: 0.8830\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2845 - accuracy: 0.8750\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2803 - accuracy: 0.8867\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2722 - accuracy: 0.8904\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.2642 - accuracy: 0.8867\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2872 - accuracy: 0.8797\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.2692 - accuracy: 0.8848\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.2568 - accuracy: 0.9012\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.2592 - accuracy: 0.8947\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.2778 - accuracy: 0.8900\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2724 - accuracy: 0.8773\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2790 - accuracy: 0.8909\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2556 - accuracy: 0.8965\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2728 - accuracy: 0.8914\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2751 - accuracy: 0.8858\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2614 - accuracy: 0.8984\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2758 - accuracy: 0.8736\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.2721 - accuracy: 0.8909\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2815 - accuracy: 0.8853\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2627 - accuracy: 0.8923\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2582 - accuracy: 0.8904\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2810 - accuracy: 0.8792\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2527 - accuracy: 0.8993\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.2720 - accuracy: 0.8844\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2663 - accuracy: 0.8965\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2705 - accuracy: 0.8900\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2686 - accuracy: 0.8928\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2715 - accuracy: 0.8895\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2710 - accuracy: 0.8942\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2584 - accuracy: 0.8909\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2709 - accuracy: 0.8881\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2623 - accuracy: 0.8928\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.2478 - accuracy: 0.9007\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2579 - accuracy: 0.8937\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.2487 - accuracy: 0.8998\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2681 - accuracy: 0.8825\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 947us/step - loss: 0.2713 - accuracy: 0.8801\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.8872\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.8965\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 988us/step - loss: 0.2474 - accuracy: 0.8979\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.2672 - accuracy: 0.8881\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.2582 - accuracy: 0.8961\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.2467 - accuracy: 0.9040\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2435 - accuracy: 0.9017\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2516 - accuracy: 0.9017\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2346 - accuracy: 0.9026\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2498 - accuracy: 0.8989\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2379 - accuracy: 0.9036\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2592 - accuracy: 0.8961\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2569 - accuracy: 0.8956\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.2404 - accuracy: 0.9022\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2376 - accuracy: 0.8989\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2495 - accuracy: 0.8984\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.2699 - accuracy: 0.8886\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2508 - accuracy: 0.8928\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.2417 - accuracy: 0.9026\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2690 - accuracy: 0.8919\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.2397 - accuracy: 0.9022\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2481 - accuracy: 0.8961\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2424 - accuracy: 0.8998\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2433 - accuracy: 0.9036\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2414 - accuracy: 0.9003\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.2457 - accuracy: 0.8956\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2465 - accuracy: 0.8956\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2526 - accuracy: 0.8970\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.2338 - accuracy: 0.9012\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2367 - accuracy: 0.9012\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2443 - accuracy: 0.9031\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.2499 - accuracy: 0.8961\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.2257 - accuracy: 0.9073\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 940us/step - loss: 0.2317 - accuracy: 0.9068\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2363 - accuracy: 0.8961\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2319 - accuracy: 0.9101\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2209 - accuracy: 0.9125\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2393 - accuracy: 0.9022\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.2304 - accuracy: 0.9022\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2229 - accuracy: 0.9134\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2287 - accuracy: 0.9125\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2344 - accuracy: 0.9059\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2290 - accuracy: 0.9101\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2238 - accuracy: 0.9087\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2473 - accuracy: 0.8956\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2376 - accuracy: 0.9036\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2447 - accuracy: 0.8942\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2373 - accuracy: 0.9031\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2512 - accuracy: 0.9040\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.2448 - accuracy: 0.9040\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2281 - accuracy: 0.9120\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2277 - accuracy: 0.9082\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2279 - accuracy: 0.9068\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2311 - accuracy: 0.9092\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.2116 - accuracy: 0.9129\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2171 - accuracy: 0.9167\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2283 - accuracy: 0.9073\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2365 - accuracy: 0.9064\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.2293 - accuracy: 0.9092\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2268 - accuracy: 0.9087\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.2274 - accuracy: 0.9101\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.2224 - accuracy: 0.9134\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2303 - accuracy: 0.9040\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2259 - accuracy: 0.9073\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2217 - accuracy: 0.9162\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2241 - accuracy: 0.9092\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.1980 - accuracy: 0.9242\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2061 - accuracy: 0.9185\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2259 - accuracy: 0.9068\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2161 - accuracy: 0.9139\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2162 - accuracy: 0.9110\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2131 - accuracy: 0.9120\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2173 - accuracy: 0.9125\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.2134 - accuracy: 0.9162\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2163 - accuracy: 0.9148\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2214 - accuracy: 0.9068\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2196 - accuracy: 0.9134\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2332 - accuracy: 0.9031\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2176 - accuracy: 0.9171\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.2148 - accuracy: 0.9129\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2157 - accuracy: 0.9092\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1995 - accuracy: 0.9185\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.2201 - accuracy: 0.9143\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2286 - accuracy: 0.9148\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2136 - accuracy: 0.9134\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.2139 - accuracy: 0.9148\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2162 - accuracy: 0.9148\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.2086 - accuracy: 0.9246\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1931 - accuracy: 0.9246\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2044 - accuracy: 0.9181\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2200 - accuracy: 0.9054\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1976 - accuracy: 0.9199\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2143 - accuracy: 0.9148\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.2015 - accuracy: 0.9171\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1973 - accuracy: 0.9260\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.1869 - accuracy: 0.9284\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2158 - accuracy: 0.9185\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2206 - accuracy: 0.9045\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2139 - accuracy: 0.9115\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1962 - accuracy: 0.9185\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2106 - accuracy: 0.9176\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.2126 - accuracy: 0.9153\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2133 - accuracy: 0.9129\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2043 - accuracy: 0.9209\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1961 - accuracy: 0.9251\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2229 - accuracy: 0.9199\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2228 - accuracy: 0.9110\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1977 - accuracy: 0.9242\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.1909 - accuracy: 0.9237\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2047 - accuracy: 0.9237\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.2066 - accuracy: 0.9153\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2056 - accuracy: 0.9129\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1916 - accuracy: 0.9274\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9143\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9185\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9185\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.2103 - accuracy: 0.9209\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 954us/step - loss: 0.1909 - accuracy: 0.9242\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 992us/step - loss: 0.2095 - accuracy: 0.9139\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1983 - accuracy: 0.9237\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.1768 - accuracy: 0.9321\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1923 - accuracy: 0.9199\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2008 - accuracy: 0.9199\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1886 - accuracy: 0.9251\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1991 - accuracy: 0.9143\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.1908 - accuracy: 0.9349\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1969 - accuracy: 0.9251\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.1898 - accuracy: 0.9237\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2023 - accuracy: 0.9265\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1802 - accuracy: 0.9302\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2019 - accuracy: 0.9237\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1903 - accuracy: 0.9195\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2108 - accuracy: 0.9106\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1935 - accuracy: 0.9232\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.2073 - accuracy: 0.9167\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.1893 - accuracy: 0.9284\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1867 - accuracy: 0.9256\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1877 - accuracy: 0.9270\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9204\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2016 - accuracy: 0.9204\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.1817 - accuracy: 0.9307\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 727us/step - loss: 0.1831 - accuracy: 0.9279\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1772 - accuracy: 0.9321\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1837 - accuracy: 0.9237\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.1965 - accuracy: 0.9190\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1908 - accuracy: 0.9242\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 959us/step - loss: 0.1896 - accuracy: 0.9284\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.2002 - accuracy: 0.9157\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 997us/step - loss: 0.1952 - accuracy: 0.9223\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9251\n",
      "Epoch 368/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1565 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 338.\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.1870 - accuracy: 0.9298\n",
      "Epoch 368: early stopping\n",
      "8/8 [==============================] - 0s 967us/step - loss: 0.7794 - accuracy: 0.6809\n",
      "8/8 [==============================] - 0s 639us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n",
      "Final Test Results - Loss: 0.7794369459152222, Accuracy: 0.6808510422706604, Precision: 0.7399610136452242, Recall: 0.5787545787545788, F1 Score: 0.6148163176363622\n",
      "Confusion Matrix:\n",
      " [[118   2  10]\n",
      " [ 19  16   0]\n",
      " [ 43   1  26]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6615958747523173\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7560804635286331\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7182086706161499\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6737205579541556\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7343774990324037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.76 (84/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, kitten]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, kitten, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, kitten,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, senior, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, kitten, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, adult...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[kitten, kitten, senior, kitten, senior, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "63    057A  [senior, adult, adult, senior, adult, adult, s...        senior           senior                   True\n",
       "78    072A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "80    074A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "59    053A        [adult, adult, adult, adult, kitten, adult]         adult            adult                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, adult,...        senior           senior                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A     [senior, senior, adult, senior, adult, kitten]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, senior, senior, adult, ...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "93    097B  [senior, adult, kitten, senior, adult, adult, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, senior, adult, senior, kitten,...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, senior, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                             [adult, adult, kitten]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                     [adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "39    033A  [adult, adult, kitten, kitten, adult, adult, k...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [adult, adult, adult, adult, adult, senior, ad...         adult           senior                  False\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...        senior            adult                  False\n",
       "74    068A  [adult, adult, senior, senior, senior, adult, ...        senior            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "42    036A  [senior, senior, adult, senior, adult, senior,...        senior            adult                  False\n",
       "103   109A  [adult, kitten, kitten, kitten, kitten, kitten...         adult           kitten                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "52    047A  [adult, adult, adult, kitten, kitten, kitten, ...         adult           kitten                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "60    054A                                    [senior, adult]         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "17    015A  [senior, senior, senior, senior, senior, adult...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "65    059A  [senior, adult, adult, adult, adult, senior, a...         adult           senior                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [kitten, adult]         adult           senior                  False\n",
       "69    063A  [kitten, kitten, senior, kitten, senior, kitte...        kitten            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     61\n",
      "kitten    12\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             61  83.561644\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnFUlEQVR4nO3dd3iN9//H8edJJCJDRAhib1Vfe8SqPWuWqrb6VWrVVvXVqtWiy6hVpZQqqmjtorTUTKgRoyJmCLFFyBAZ5/dHrty/HEmIJCRxXo/rcl3OPd/3nXOf8z6f+31/Piaz2WxGRERERMRK2GR0ACIiIiIiz5MSYBERERGxKkqARURERMSqKAEWEREREauiBFhERERErIoSYBERERGxKkqARURERMSqKAEWEREREauiBFhEJAuLjo7O6BDS3Yt4TCKSuWTL6ABEUioiIoKWLVsSFhYGQNmyZVm2bFkGRyVpce7cOb799luOHj1KWFgYuXPnpkGDBowcOTLZdapXr27xOmfOnPz555/Y2Fj+nv/qq69YtWqVxbRx48bRtm3bVMV68OBB+vXrB0CBAgXYsGFDqrbzNMaPH8/GjRsB6N27N3379rWYv3XrVlatWsX8+fPTdb8PHz6kRYsW3L9/H4B3332XgQMHJrt8mzZtuHbtGgC9evUyztPTun//Pt9//z25cuXivffeS9U20tuGDRv49NNPAahatSrff/99hsbz6aefWrz3li9fTunSpTMwopQLCQnh999/Z8eOHVy5coXg4GCyZctG3rx5qVChAm3atKFmzZoZHaZYCbUAS5axbds2I/kF8Pf3599//83AiCQtoqKi6N+/P7t27SIkJITo6Ghu3LjB9evXn2o79+7dw8/PL9H0AwcOpFeomc6tW7fo3bs3o0aNMhLP9GRvb0+TJk2M19u2bUt22RMnTljE0KpVq1Ttc8eOHbz22mssX75cLcDJCAsL488//7SYtnr16gyK5uns2bOHLl26MG3aNI4cOcKNGzeIiooiIiKCS5cusWnTJvr378+oUaN4+PBhRocrVkAtwJJlrFu3LtG0NWvW8PLLL2dANJJW586d4/bt28brVq1akStXLipWrPjU2zpw4IDF++DGjRtcvHgxXeKMlz9/frp37w6Ai4tLum47OfXq1cPd3R2AypUrG9MDAgI4cuTIM913y5YtWbt2LQBXrlzh33//TfJa++uvv4z/ly9fnqJFi6Zqfzt37iQ4ODhV61qLbdu2ERERYTFt8+bNDBkyBAcHhwyK6sm2b9/O//73P+O1o6MjtWrVokCBAty9e5f9+/cbnwVbt27FycmJTz75JKPCFSuhBFiyhICAAI4ePQrE3fK+d+8eEPdhOWzYMJycnDIyPEmFhK35Hh4eTJgw4am34eDgwIMHDzhw4AA9evQwpids/c2RI0eipCE1ChUqxKBBg9K8nafRtGlTmjZt+lz3Ga9atWrky5fPaJHftm1bkgnw9u3bjf+3bNnyucVnjRI2AsR/DoaGhrJ161batWuXgZEl7/Lly0YJCUDNmjWZNGkSbm5uxrSHDx8yYcIENm/eDMDatWvp1q1bqn9MiaSEEmDJEhJ+8L/++uv4+Pjw77//Eh4ezpYtW+jUqVOy6546dYolS5Zw+PBh7t69S+7cuSlZsiRdu3alTp06iZYPDQ1l2bJl7Nixg8uXL2NnZ4enpyfNmzfn9ddfx9HR0Vj2cTWaj6sZja9jdXd3Z/78+YwfPx4/Pz9y5szJ//73P5o0acLDhw9ZtmwZ27ZtIzAwkMjISJycnChevDidOnXi1VdfTXXsPXv25NixYwAMHTqUbt26WWxn+fLlTJ06FYhrhZw+fXqy5zdedHQ0GzZsYNOmTVy4cIGIiAjy5ctH3bp1eeedd/Dw8DCWbdu2LVevXjVe37hxwzgn69evx9PT84n7A6hYsSIHDhzg2LFjREZGkj17dgD++ecfY5lKlSrh4+OT5Pq3bt3ihx9+wNvbmxs3bhATE0OuXLkoX748PXr0sGiNTkkN8NatW1m/fj1nzpzh/v37uLu7U7NmTd555x2KFStmsey8efOM2t2PPvqIe/fu8fPPPxMREUH58uWN98Wj76+E0wCuXr1K9erVKVCgAJ988olRq+vq6soff/xBtmz//zEfHR1Ny5YtuXv3LgA//fQT5cuXT/LcmEwmWrRowU8//QTEJcBDhgzBZDIZy/j5+XHlyhUAbG1tad68uTHv7t27rFq1iu3btxMUFITZbKZo0aI0a9aMLl26WLRYPlrXPX/+fObPn5/omvrzzz9ZuXIl/v7+xMTEULhwYZo1a8Zbb72VqAU0PDycJUuWsHPnTgIDA3n48CHOzs6ULl2a9u3bp7pU49atW8ycOZM9e/YQFRVF2bJl6d69O/Xr1wcgNjaWtm3bGj8cvvrqK4tyEoCpU6eyfPlyIO7z7HE17/HOnTvH8ePHgf+/G/HVV18BcXfCHpcAX758mblz5+Lj40NERATlypWjd+/eODg40KtXLyCujnv8+PEW6z3N+U7O4sWLjR+7BQoUYMqUKRafoRBXcvPJJ59w584dPDw8KFmyJHZ2dsb8lFwr8Y4fP87KlSvx9fXl1q1buLi4UKFCBbp06YKXl5fFfp90TSf8nJo7d67xPk14DX7zzTe4uLjw/fffc+LECezs7KhZsyYDBgygUKFCKTpHkjGUAEumFx0dze+//268btu2Lfnz5zfqf9esWZNsArxx40YmTJhATEyMMe369etcv36dffv2MXDgQN59911j3rVr13j//fcJDAw0pj148AB/f3/8/f3566+/mDt3bqIP8NR68OABAwcOJCgoCIDbt29TpkwZYmNj+eSTT9ixY4fF8vfv3+fYsWMcO3aMy5cvWyQHTxN7u3btjAR469atiRLghDWfbdq0eeJx3L17l+HDhxut9PEuXbrEpUuX2LhxI5MnT06U6KRVtWrVOHDgAJGRkRw5csT4gjt48CAARYoUIU+ePEmuGxwcTJ8+fbh06ZLF9Nu3b7N792727dvHzJkzqVWr1hPjiIyMZNSoUezcudNi+tWrV1m3bh2bN29m3LhxtGjRIsn1V69ezenTp43X+fPnf+I+k1KzZk3y58/PtWvXCAkJwcfHh3r16hnzDx48aCS/JUqUSDb5jdeqVSsjAb5+/TrHjh2jUqVKxvyE5Q81atQwzrWfnx/Dhw/nxo0bFtvz8/PDz8+PjRs3MmvWLPLly5fiY0vqocYzZ85w5swZ/vzzT7777jtcXV2BuPd9r169LM4pxD2EdfDgQQ4ePMjly5fp3bt3ivcPce+N7t27W9Sp+/r64uvrywcffMBbb72FjY0Nbdq04YcffgDirq+ECbDZbLY4byl9KDNhI0CbNm1o1aoV06dPJzIykuPHj3P27FlKlSqVaL1Tp07x/vvvGw80Ahw9epRBgwbRsWPHZPf3NOc7ObGxsRZ3CDp16pTsZ6eDgwPffvvtY7cHj79WFi5cyNy5c4mNjTWm3blzh127drFr1y7efPNNhg8f/sR9PI1du3axfv16i++Ybdu2sX//fubOnUuZMmXSdX+SfvQQnGR6u3fv5s6dOwBUqVKFQoUK0bx5c3LkyAHEfcAn9RDU+fPnmTRpkvHBVLp0aV5//XWLVoDZs2fj7+9vvP7kk0+MBNLZ2Zk2bdrQvn17o8Ti5MmTfPfdd+l2bGFhYQQFBVG/fn06duxIrVq1KFy4MHv27DGSXycnJ9q3b0/Xrl0tPkx//vlnzGZzqmJv3ry58UV08uRJLl++bGzn2rVrRktTzpw5eeWVV554HJ9++qmR/GbLlo1GjRrRsWNHI8G5f/8+H374obGfTp06WSSDTk5OdO/ene7du+Ps7Jzi81etWjXj//GtvhcvXjQSlITzH/Xjjz8ayW/BggXp2rUrr732mpHExcTE8Msvv6QojpkzZxrJr8lkok6dOnTq1Mm4hfvw4UPGjRtnnNdHnT59mjx58tClSxeqVq2abKIMcS3ySZ27Tp06YWNjY5FQbd261WLdp/1hU7p0aUqWLJnk+pB0+cP9+/cZMWKEkfzmypWLtm3b0qJFC+M9d/78eT744APjYbfu3btb7KdSpUp0797dqHv+/fffjWTMZDLxyiuv0KlTJ+OuwunTp/n666+N9Tdt2mQkSW5ubrRr14633nrLooeB+fPnW7zvUyL+vVWvXj1ee+01iwR+xowZBAQEAHFJbXxL+Z49ewgPDzeWO3r0qHFuUvIjBOIeGN20aZNx/G3atMHZ2dkisU7qYbjY2FjGjBljJL/Zs2enVatWtG7dGkdHx2QfoHva852coKAgQkJCjNcJ69hTK7lrZfv27cyZM8dIfsuVK8frr79O1apVjXWXL1/O0qVL0xxDQmvWrMHOzo5WrVrRqlUr4y7UvXv3GD16tMVntGQuagGWTC9hy0f8l7uTkxNNmzY1blmtXr060UMTy5cvJyoqCoCGDRvy5ZdfGreDJ06cyNq1a3FycuLAgQOULVuWo0ePGkmck5MTS5cuNW5htW3bll69emFra8u///5LbGxsom63UqtRo0ZMnjzZYpq9vT0dOnTgzJkz9OvXj9q1awNxLVvNmjUjIiKCsLAw7t69i5ub21PH7ujoSNOmTVm/fj0Qlyj17NkTiLvtGf+h3bx5c+zt7R8b/9GjR9m9ezcQdxv8u+++o0qVKkBcSUb//v05efIkoaGhLFiwgPHjx/Puu+9y8OBB/vjjDyAu0U5NfW2FChUs6oDBsvyhWrVqyZY/FC5cmBYtWnDp0iVmzJhB7ty5gbhWz/iWwfjb+49z7do1i5ayCRMmGMngw4cPGTlyJLt37yY6OppZs2Yl243WrFmzUtSdVdOmTcmVK1ey565du3YsWLAAs9nMzp07jdKQ6Oho/v77byDu79S6desn7gvizsfs2bOBuPfGBx98gI2NDadPnzZ+QGTPnp1GjRoBsGrVKqNXCE9PTxYuXGj8qAgICKB79+6EhYXh7+/P5s2badu2LYMGDeL27ducO3cOiGvJTnh3Y/Hixcb/P/roI+OOz4ABA+jatSs3btxg27ZtDBo0iPz581v83QYMGECHDh2M199++y3Xrl2jePHiFq12KfW///2PLl26AHFJTs+ePQkICCAmJoZ169YxZMgQChUqRPXq1fnnn3+IjIxk165dxnsi4Y+IpMqYkrJz506j5T6+EQCgffv2RmK8efNmBg8ebFGacPDgQS5cuADE/c2///57o447ICCAt99+m8jIyET7e9rznZyED7kCxjUWb//+/QwYMCDJdZMqyYiX1LUS/x6FuB/YI0eOND6jFy1aZLQuz58/nw4dOjzVD+3HsbW1ZcGCBZQrVw6Azp0706tXL8xmM+fPn+fAgQMpuoskz59agCVTu3HjBt7e3kDcw0wJHwhq37698f+tW7datLLA/98GB+jSpYtFLeSAAQNYu3Ytf//9N++8806i5V955RWL+q3KlSuzdOlSdu3axcKFC9Mt+QWSbO3z8vJi9OjRLF68mNq1axMZGYmvry9LliyxaFGI//JKTeyPnr94CbtZSkkrYcLlmzdvbiS/ENcSnbD/2J07d1rcnkyrbNmyGXW6/v7+hISEWDwA97iSi86dOzNp0iSWLFlC7ty5CQkJYc+ePRblNkklB4/avn27cUyVK1e2eBDM3t7e4pbrkSNHjEQmoRIlSqRbX64FChQwWjrDwsLYu3cvEPdgYHxrXK1atZItDXlUy5YtjdbMW7ducfjwYcCy/OGVV14x7jQkfD/07NnTYj/FihWja9euxutHS3yScuvWLc6fPw+AnZ2dRTKbM2dOGjRoAMS1dsb/+IlPRgAmT57Mhx9+yIoVK4xygAkTJtCzZ8+nfsjK1dXVotwqZ86cvPbaa8brEydOGP9PeH3F/1hJWBJga2ub4gT40fKHeFWrVqVw4cJAXMv7o12kJSxJql27tsVDjMWKFUvyR1Bqzndy4ltD46XmB8ejkrpW/P39jR9jDg4ODB482OIz+r///S8FChQA4q6JJ8X9NBo1amTxfqtUqZLRYAEkKguTzEMtwJKpbdiwwfjQtLW15cMPP7SYbzKZMJvNhIWF8ccff1jUtCWsP4z/8Ivn5uZm8RTyk5YHyy/VlEjpra+k9gVxLYurV6/Gx8fHeAjlUfGJV2pir1SpEsWKFSMgIICzZ89y4cIFcuTIYXyJFytWjAoVKjwx/oQ1x0ntJ+G0+/fvExISkujcp0V8HXD8F/KhQ4cAKFq06BOTvBMnTrBu3ToOHTqUqBYYSFGy/qTjL1SoEE5OToSFhWE2m7ly5Qq5cuWyWCa590BqtW/fnv379wNxLY6NGzd+6vKHePnz56dKlSpG4rtt2zaqV69uUf6QMJF6mvdDSkoQEvYxHBUV9djWtPjWzqZNmxo/ZiIjI/n777+N1u+cOXPSsGFD3nnnHYoXL/7E/SdUsGBBbG1tLaYlfLgxYYtno0aNcHFx4f79+/j4+HD//n3OnDnDzZs3gZT/CLl27Zrxt4S4HhK2bNlivH7w4IHx/9WrV1v8beP3BSSZ7Cd1/Kk538l5tMb7+vXrFvv09PQ0uhaEuHKR+LsAyUnqWkn4nitcuHCiXoFsbW0pXbq08UBbwuUfJyXXf1LntVixYuzbtw9I3AoumYcSYMm0zGazcYse4m6nP25wgzVr1iT7UMfTtjykpqXi0YQ3vvziSZLqwi3+IZXw8HBMJhOVK1ematWqVKxYkYkTJ1p8sT3qaWJv3749M2bMAOJagRM+oJLSJClhy3pSHj0vCXsRSA8J63yXLl1qtHI+rv4X4kpkpk2bhtlsxsHBgQYNGlC5cmXy58/Pxx9/nOL9P+n4H5XU8ad3N34NGzbE1dWVkJAQdu/ezb1794waZRcXF6MVL6VatmxpJMDbt2+nU6dORvLj6upq0eL1tO+HJ0mYhNjY2Dz2x1P8tk0mE59++ikdO3Zk8+bNeHt7Gw+a3rt3j/Xr17N582bmzp1r8VDfkyQ1QEfC6y3hsWfPnp2WLVuyatUqoqKi2LFjh8WzCilt/d2wYYPFOYh/eDUpx44d49y5c0Y9dcJzndI7L6k538lxc3OjYMGCRknKwYMHLZ7BKFy4sEX5TsIymOQkda2k5BpMGGtS12BS5yclA7IkNWhHwh4s0vvzTtKPEmDJtA4dOpSiGsx4J0+exN/fn7JlywJxfcvG/9IPCAiwaKm5dOkSv/32GyVKlKBs2bKUK1fOopuupAZR+O6773BxcaFkyZJUqVIFBwcHi9tsCVtigCRvdScl4YdlvGnTphklHQlrSiHpD+XUxA5xX8Lffvst0dHRRgf0EPfFl9Ia0YQtMgkfKExqWs6cOZ/45PjTevnll4064IS3oB+XAN+7d49Zs2ZhNpuxs7Nj5cqVRtdr8bd/U+pJx3/58mWjGygbGxsKFiyYaJmk3gNpYW9vT6tWrfjll1948OABkydPNvrObtasWaJb00/StGlTJk+eTFRUFMHBwRYPQDVr1swiASlQoIDx0JW/v3+iVuCE56hIkSJP3HfC97adnR2bN2+2uO5iYmIStcrGK1asGCNGjCBbtmxcu3YNX19ffv31V3x9fYmKimLBggXMmjXriTHEu3z5Mg8ePLCos0145+DRFt327dsb9eFbtmwxkjtnZ2caNmz4xP2ZzeanHnJ7zZo1xp2yvHnzJhlnvLNnzyaalpbznZSWLVsaPWLE9+/76B2QeClJ0pO6VhJeg4GBgYSFhVkkyjExMRbHGl82kvA4Hv38jo2NNa6Zx0nqHCY81wn/BpK5qAZYMq34UagAunbtanRf9Oi/hE92J3yqOWECtHLlSosW2ZUrV7Js2TImTJhgfDgnXN7b29uiJeLUqVP88MMPTJ8+naFDhxq/+nPmzGks82jilLBG8nGSaiE4c+aM8f+EXxbe3t4Wo2XFf2GkJnaIeyglvv/SixcvcvLkSSDuIaSEX4SPk7CXiD/++ANfX1/jdVhYmEXXRg0bNkz3FhE7O7skR497XAJ88eJF4zzY2tpajOwW/1ARpOwLOeHxHzlyxKLUICoqim+++cYipqR+ADztOUn4xZ1cK1XCGtT4AQbg6cof4uXMmZO6desarxP+jR8d/CLh+Vi4cCG3bt0yXl+8eJEVK1YYr+MfnAMskqyEx5Q/f37jR0NkZCS//fabMS8iIoIOHTrQvn17hg0bZiQjY8aMoXnz5jRt2tT4TMifPz8tW7akc+fOxvpPO+x2fN/C8UJDQy0egHy0l4Ny5coZP8gPHDhg3A5P6Y+Q/fv3Gy3Xrq6u+Pj4JPkZmHAQmU2bNhm16wnr8b29vY3rG+J6U0hYShEvNef7cbp06WJ8ht29e5dhw4Yl6h7v4cOHLFq0KFGvJUlJ6lopU6aMkQQ/ePCA2bNnW7T4LlmyxCh/cHZ2pkaNGoDliI737t2zeK/u3LkzRXfx4v8m8c6ePWuUP4Dl30AyF7UAS6Z0//59iwdkHjcaVosWLYzSiC1btjB06FBy5MhB165d2bhxI9HR0Rw4cIA333yTGjVqcOXKFYsPqDfeeAOI+/KqWLGiMahCjx49aNCgAQ4ODhZJTevWrY3EN+HDGPv27eOLL76gbNmy7Ny503j4KDXy5MljfPGNGjWK5s2bc/v2bXbt2mWxXPwXXWpij9e+fftEDyM9TZJUrVo1qlSpwpEjR4iJiaFfv3688soruLq64u3tbdQUuri4PHW/qylVtWpVi/KYJ9X/Jpz34MEDevToQa1atfDz87O4xZySh+AKFSpEq1atjCRz1KhRbNy4kQIFCnDw4EGjayw7OzuLBwLTImHr1s2bNxk3bhyAxYhbpUuXpnz58hZJT5EiRVI11DTEJbrxdbTxChYsmCjp69y5M7/99hvBwcFcuXKFN998k3r16hEdHc3OnTuNOxvly5e3SJ4THtP69esJDQ2ldOnSvPbaa7z11ltGTylfffUVu3fvpkiRIuzfv99IbKKjo416zFKlShl/j6lTp+Lt7U3hwoWNPmHjPU35Q7x58+Zx7NgxChUqxL59+4y7VNmzZ09yMIr27dsn6jIspddXwoffGjZsmOyt/gYNGpA9e3YiIyO5d+8ef/75J6+++irVqlWjRIkSnD9/ntjYWPr06UPjxo0xm83s2LEjydv3wFOf78dxd3dn9OjRjBw5kpiYGI4fP07Hjh2pU6cOBQoUIDg4GG9v70R3zJ6mLMhkMvHee+8xceJEIK4nkhMnTlChQgXOnTtnlO8A9O3b19h2kSJFjPNmNpsZOnQoHTt2JCgoKMVdIJrNZgYNGkTDhg1xcHBg+/btxudGmTJlLLphk8xFLcCSKW3evNn4EMmbN+9jv6gaN25s3BaLfxgO4r4EP/74Y6O1LCAggFWrVlkkvz169LDoKWDixIlG60d4eDibN29mzZo1hIaGAnFPIA8dOtRi3wlvaf/22298/vnn7N27l9dffz3Vxx/fMwXEtUz8+uuv7Nixg5iYGIvuexI+zPG0scerXbu2xW06JyenFN2ejWdjY8MXX3zBSy+9BMR9MW7fvp01a9YYyW/OnDmZOnVquj/sFe/R3h6eVP9boEABix9VAQEBrFixgmPHjpEtWzbjFndISEiKboN+/PHHRm2j2Wxm7969/Prrr0bymz17diZMmJDkUMKpUbx4cYuW5N9//53Nmzcnag1+NCFLTetvvPr16ydKSpLqwSRPnjx8/fXXuLu7A3EDjmzYsIHNmzcbyW+pUqWYMmWKRUt2wkT69u3brFq1yniC/vXXX7fY1759+/jll1+MOmRnZ2e++uor43OgW7duNGvWDIi7/b17925+/vlntmzZYsRQrFgx+vfv/1TnoFmzZri7u+Pt7c2qVauM5NfGxoaPPvooyS7BEvYNC3FJV0oS75CQEIuBVR7XCODo6GjR8r5mzRojrgkTJhh/twcPHrBp0yY2b95MbGyscY7AsmX1ac/3kzRs2JBvv/3WeE9ERkayY8cOfv75ZzZv3myR/Lq4uNC3b1+GDRuWom3H69ChA++++65xHH5+fqxatcoi+X377bd58803jdf29vZGAwjE3S374osvWLx4Mfny5bO4u5ic6tWrY2Njw7Zt29iwYYNR7uTq6pqq4d3l+VECLJlSwpaPxo0bP/YWsYuLi8WQxvEf/hDX+rJo0SLji8vW1pacOXNSq1YtpkyZkqgPSk9PT5YsWULPnj0pXrw42bNnJ3v27JQsWZI+ffqwePFii8QjR44cLFiwgFatWpErVy4cHByoUKECEydOTDLZTKnXX3+dL7/8kvLly+Po6EiOHDmoUKECEyZMsNhuwjKLp409nq2trUVi1rRp0xQPcxovT548LFq0iI8//piqVavi6uqKvb09hQsX5s0332TFihXPtCUkvg443pMSYIDPPvuM/v37U6xYMezt7XF1daVevXosWLDAuDVvNpuN3g4efTgoIUdHR2bNmsXEiROpU6cO7u7u2NnZkT9/ftq3b8/PP//82ATmadnZ2TF58mTKly+PnZ0dOXPmpHr16olarBO29ppMphTXdScle/bsNG7c2GJacsMJV6lShV9++YXevXtTpkwZ4z380ksvMWTIEH788cdEJTaNGzemb9++eHh4kC1bNvLly2e0MNrY2DBx4kQmTJhAjRo1LN5fr732GsuWLbPoscTW1pZJkybx9ddf4+XlRYECBciWLRtOTk689NJL9OvXj59++umpeyPx9PRk2bJltG3b1rjeq1atyuzZs5Md0c3FxcWipTSlf4PNmzcbLbSurq7GbfvkJExYfX19jWS1bNmyLF68mEaNGpEzZ05y5MhBrVq1WLhwoUUiHj+wEDz9+U6J6tWr89tvvzF8+HBq1qxJ7ty5sbW1xcnJiSJFitCyZUvGjx/Ppk2b6N2791M/XAowcOBAFixYQOvWrSlQoAB2dna4ubnxyiuvMGfOnCST6kGDBjF06FCKFi2Kvb09BQoU4J133uGnn35K0fMKVapU4YcffqBGjRo4ODjg6upqDCGecHAXyXxMZg1TImLVLl26RNeuXY0v23nz5qUogbQ2P/74o9HZfsmSJS1qWTOrzz77zOhJpVq1asybNy+DI7I+hw8fpk+fPkDcj5B169YZD1w+a9euXWPz5s3kypULV1dXqlSpYpH0f/rpp8ZDdkOHDk00JLokbfz48WzcuBGA3r17WwzaIlmHaoBFrNDVq1dZuXIlMTExbNmyxUh+S5YsqeT3EVu2bGHy5MkWQ7o+q1KO9PDrr79y48YNTp06ZVHuk5aSHHk6p06dYtu2bYSHh1sMrFK3bt3nlvxC3B2MhA+hFi5cmDp16mBjY8PZs2eNASFMJhP16tV7bnGJZAaZNgG+fv06b7zxBlOmTLGo7wsMDGTatGkcOXIEW1tbmjZtyqBBgyzqIsPDw5k1axbbt28nPDycKlWq8MEHH1h0gyVizUwmk8XT7BB3W33EiBEZFFHm9e+//1okvxA34l1mdfLkSYv+syFuZMEmTZpkUETWJyIiwmI4YYirmx0yZMhzjaNAgQJ07NjRKAsLDAxM8s7FW2+9pe9HsTqZMgG+du0agwYNMh7eiXf//n369euHu7s748ePJzg4mJkzZxIUFGTRl+Mnn3zCiRMnGDx4ME5OTsyfP59+/fqxcuXKRE/Ai1ijvHnzUrhwYW7cuIGDgwNly5alZ8+ejx062Jq5uroSHh6Op6cnb7zxRppqaZ+1MmXKkCtXLiIiIsibNy9NmzalV69e6pD/OfL09CR//vzcuXMHFxcXKlSoQJ8+fZ565Ln0MGrUKCpVqsQff/zBmTNnjAfOXF1dKVu2LB06dEhU2y1iDTJVDXBsbCy///4706dPB+Kegp07d67xpbxo0SJ++OEHNm7caPQruHfvXoYMGcKCBQuoXLkyx44do2fPnsyYMcPotzI4OJh27drx7rvv8t5772XEoYmIiIhIJpGpeoE4c+YMX3zxBa+++qpFf5bxvL29qVKlisXAAF5eXjg5ORl9rnp7e5MjRw6L4Rbd3NyoWrVqmvplFREREZEXQ6ZKgPPnz8+aNWv44IMPkuyGKSAgINHQmba2tnh6ehrDvwYEBFCwYMFEQzUWLlw4ySFiRURERMS6ZKoaYFdX18f2uxcaGprk6DCOjo5G59MpWeZp+fv7G+umtONvEREREXm+oqKiMJlMTxyGOlMlwE+SsCP6R8V3TJ+SZVIjvlQ6uaEjRURERCRryFIJsLOzszGMZUJhYWHGqELOzs7cuXMnyWUSdpX2NMqWLcvx48cxm82UKlUqVdsQERERkWfr7NmzKer1JkslwEWLFiUwMNBiWkxMDEFBQcbQpUWLFsXHx4fY2FiLFt/AwMA093NoMplwdHRM0zZERERE5NlIaZePmeohuCfx8vLi8OHDBAcHG9N8fHwIDw83en3w8vIiLCwMb29vY5ng4GCOHDli0TOEiIiIiFinLJUAd+7cmezZszNgwAB27NjB2rVrGTNmDHXq1KFSpUoAVK1alWrVqjFmzBjWrl3Ljh076N+/Py4uLnTu3DmDj0BEREREMlqWKoFwc3Nj7ty5TJs2jdGjR+Pk5ESTJk0YOnSoxXKTJ0/mm2++YcaMGcTGxlKpUiW++OILjQInIiIiIplrJLjM7Pjx4wD85z//yeBIRERERCQpKc3XslQJhIiIiIhIWikBFhERERGrogRYRERERKyKEmARERERsSpKgEVERETEqigBFhERERGrogRYRERERKyKEmARERERsSpKgEVERETEqigBFhERERGrogRYRERERKyKEmARERERsSpKgEVERETEqigBFhERERGrogRYRERERKyKEmARERERsSpKgEVERETEqigBFhERERGrogRYRERERKyKEmARERERsSpKgEVERETEqigBFhERERGrogRYRERERKyKEmARERERsSrZMjoAkYTWrFnD8uXLCQoKIn/+/HTp0oXXX38dk8kEwJ49e/j+++85f/48uXLlom3btvTs2RM7O7vHbvf48ePMnj2bf//9F0dHR2rXrs2QIUPInTv38zgsERERyUTUAiyZxtq1a5k0aRI1atRg2rRpNGvWjMmTJ7Ns2TIAfHx8+OCDDyhZsiRTp07lnXfeYdmyZXz99deP3a6fnx/9+vXD0dGRKVOmMGjQIHx8fPjwww+fx2GJiIhIJqMWYMk01q9fT+XKlRkxYgQANWvW5OLFi6xcuZJu3bqxaNEiypUrx7hx4wCoVasWd+/eZeHChXzwwQfkyJEjye3OnDmTsmXLMnXqVGxs4n7zOTk5MXXqVK5cuULBggWfzwGKiIhIpqAEWDKNyMhI8uTJYzHN1dWVkJAQAMaMGUN0dLTFfDs7O2JjYxNNj3f37l0OHTrE+PHjjeQXoHHjxjRu3Didj0BERESyApVASKbx5ptv4uPjw6ZNmwgNDcXb25vff/+d1q1bA1CoUCGKFSsGQGhoKNu3b2fp0qW0aNECFxeXJLd59uxZYmNjcXNzY/To0bzyyivUr1+fsWPHcv/+/ed1aCIiIpKJqAVYMo0WLVpw6NAhxo4da0yrXbs2w4cPt1ju1q1btGzZEoCCBQvSv3//ZLcZHBwMwGeffUadOnWYMmUKly5d4ttvv+XKlSssWLDAeMBORERErINagCXTGD58OH/99ReDBw9m3rx5jBgxgpMnTzJy5EjMZrOxXPbs2fnuu+/48ssvsbe3p0ePHty4cSPJbUZFRQFQrlw5xowZQ82aNencuTMfffQRR48eZf/+/c/l2ERERCTzUAIsmcLRo0fZt28fH3zwAf/973+pVq0ab7zxBp9++ik7d+5kz549xrIuLi7UqFGDpk2bMmPGDO7cucO6deuS3K6joyMA9evXt5hep04dAE6dOvWMjkhEREQyK5VASKZw9epVACpVqmQxvWrVqgCcO3eOBw8eULhwYcqVK2fM9/T0JGfOnNy8eTPJ7RYpUgSAhw8fWkyPf2jOwcEhfQ5AREREsgy1AEumEP9w25EjRyymHz16FIh7AG727NnMnj3bYv6pU6cICQmhdOnSSW63ePHieHp6snXrVosyip07dwJQuXLldDoCERERySrUAiyZQrly5WjcuDHffPMN9+7do0KFCpw/f57vv/+el156iYYNG/LgwQPGjx/PF198QZMmTbhy5Qrz5s2jZMmStG3bFohr6fX398fDw4N8+fJhMpkYPHgwH3/8MaNGjaJDhw5cuHCBOXPm0LhxY4vWZBEREbEOJnPCZjFJ1vHjxwH4z3/+k8GRvLiioqL44Ycf2LRpEzdv3iR//vw0bNiQ3r17G7W8f/75J4sXL+bChQs4OjrSsGFDBg4cSM6cOQEICgqiXbt29O7dm759+xrb3r17N/Pnz+fs2bPkzJmTVq1a8f7772Nvb58hxyqS3p40jHhgYCDTpk3jyJEj2Nra0rRpUwYNGoSzs/Njt3vy5EmmT5+On58fTk5OtG3blj59+jxx+HERkYyQ0nxNCXAKKQEWkcxq7dq1TJw4kTfeeIMGDRpw5MgRFixYwJAhQ+jWrRv379+na9euuLu707NnT4KDg5k5cyYVKlRg1qxZyW738uXLdOvWjYoVK9KlSxcCAgKYM2cObdq0YdSoUc/xCEVEUial+ZpKIEREsrgnDSP+66+/EhISwrJly8iVKxcAHh4eDBkyBF9f32Rr4RcvXmwMG25nZ0e9evVwcHDg66+/pmfPnuTPn/85HaGISPrSQ3AiIllcZGQkTk5OFtMSDiPu7e1NlSpVjOQXwMvLCycnJ/bu3Zvsdn18fKhbt65FuUOTJk2IjY3F29s7fQ9CROQ5UgIsIpLFPWkY8YCAAKNLwHi2trZ4enpy8eLFJLf54MEDrl69mmg9Nzc3nJyckl1PRCQrUAmEiEgW96RhxENDQxO1EEPcQDFhYWFJbjM0NBQgyYfknJyckl1PRCQrUAuwiEgW96RhxGNjY5Nd18Ym6a+BJz0fHd+7hIhIVqQWYBGRLCx+GPHRo0fToUMHAKpVq0bBggUZOnQoe/bswdnZmfDw8ETrhoWF4eHhkeR241uMk2rpDQsLe2L3aSIimZlagEVEsrCUDCNetGhRAgMDLebHxMQQFBRkjML4KEdHRzw8PLh8+bLF9Dt37hAWFkbx4sXT6QhERJ4/JcBWKlbdP2dq+vtISqVkGHEvLy8OHz5McHCwMd/Hx4fw8HC8vLyS3XatWrXYvXs3Dx8+NKZt374dW1tbatSokY5HISLyfKkEwkrZmEz84nOaG/cS3xaVjOWR05GuXmUyOgzJIlIyjHi1atVYsWIFAwYMoHfv3oSEhDBz5kzq1Klj0XJ8/Phx3NzcKFSoEADdu3dn69atDB48mLfffpuLFy8yZ84cOnbsqD6ARSRL00hwKfQijgQ3c6svQcF6kjuz8XRzYnDzyhkdhmQhKRlG/OzZs0ybNo2jR4/i5OREgwYNGDp0qEXvENWrV6dNmzaMHz/emHbkyBFmzJjB6dOnyZUrF61bt6Zfv35ky6b2ExHJfDQUcjpTAizPixJgERGR1ElpvqYaYBERERGxKlnyHtaaNWtYvnw5QUFB5M+fny5duvD6668b/VIGBgYybdo0jhw5gq2tLU2bNmXQoEHqtkdEREREsl4CvHbtWiZNmsQbb7xBgwYNOHLkCJMnT+bhw4d069aN+/fv069fP9zd3Rk/fjzBwcHMnDmToKAgZs2aldHhi4iIiEgGy3IJ8Pr166lcuTIjRowAoGbNmly8eJGVK1fSrVs3fv31V0JCQli2bBm5cuUCwMPDgyFDhuDr60vlypUzLngRERERyXBZrgY4MjIy0Zj2rq6uhISEAODt7U2VKlWM5BfAy8sLJycn9u7d+zxDFREREZFMKMslwG+++SY+Pj5s2rSJ0NBQvL29+f3332ndujUAAQEBFClSxGIdW1tbPD09uXjxYkaELCIiIiKZSJYrgWjRogWHDh1i7NixxrTatWszfPhwAEJDQxO1EEPcsJ5JjWn/NMxmM+HhWX/gCJPJRI4cOTI6DHmCiIgI1Eth5hP/sK1kTrpmRKyb2WxO0ed0lkuAhw8fjq+vL4MHD+bll1/m7NmzfP/994wcOZIpU6YQGxub7Lo2Nmlr8I6KisLPzy9N28gMcuTIQfny5TM6DHmCCxcuEBERkdFhSAJ2dnaUf/llstnaZnQokoTomBhO/vsvUVFRGR2KiGQge3v7Jy6TpRLgo0ePsm/fPkaPHk2HDh0AqFatGgULFmTo0KHs2bMHZ2fnJFtpw8LC8PDwSNP+7ezsKFWqVJq2kRmoBStrKF68uFqzMhmTyUQ2W1sNI54JxQ8hXrp0aV03Ilbs7NmzKVouSyXAV69eBbAYux6gatWqAJw7d46iRYsSGBhoMT8mJoagoCAaNWqUpv2bTCZjWFGRZ01lKpnXjXvhGkUxk9J1I2LdUtrIl6UegitWrBgQNzZ9QkePHgWgUKFCeHl5cfjwYYKDg435Pj4+hIeH4+Xl9dxiFREREZHMKUu1AJcrV47GjRvzzTffcO/ePSpUqMD58+f5/vvveemll2jYsCHVqlVjxYoVDBgwgN69exMSEsLMmTOpU6dOopZjEREREbE+WSoBBpg0aRI//PADq1evZt68eeTPn5+2bdvSu3dvsmXLhpubG3PnzmXatGmMHj0aJycnmjRpwtChQzM6dBERERHJBLJcAmxnZ0e/fv3o169fssuUKlWKOXPmPMeoRERERCSryFI1wCIiIiIiaaUEWERERESsihJgEREREbEqSoBFRERExKooARYRERERq6IEWERERESsihJgEREREbEqSoBFRERExKooARYRERERq6IEWERERESsihJgEREREbEqSoBFRERExKooARYRERERq6IEWERERESsihJgEREREbEqSoBFRERExKooARYRERERq6IEWERERESsihJgEREREbEqSoBFRERExKooARYRERERq6IEWERERESsihJgEREREbEqSoBFRERExKooARYRERERq5ItLStfvnyZ69evExwcTLZs2ciVKxclSpQgZ86c6RWfiIiIiEi6euoE+MSJE6xZswYfHx9u3ryZ5DJFihShfv36tG3blhIlSqQ5SBERERGR9JLiBNjX15eZM2dy4sQJAMxmc7LLXrx4kUuXLrFs2TIqV67M0KFDKV++fNqjFRERERFJoxQlwJMmTWL9+vXExsYCUKxYMf7zn/9QunRp8ubNi5OTEwD37t3j5s2bnDlzhlOnTnH+/HmOHDlCjx49aN26NePGjXt2RyIiIiIikgIpSoDXrl2Lh4cHr732Gk2bNqVo0aIp2vjt27f5888/Wb16Nb///rsSYBERERHJcClKgL/++msaNGiAjc3TdRrh7u7OG2+8wRtvvIGPj0+qAhQRERERSU8pSoAbNWqU5h15eXmleRsiIiIiImmVpm7QAEJDQ/nuu+/Ys2cPt2/fxsPDg5YtW9KjRw/s7OzSI0YRERERkXST5gT4s88+Y8eOHcbrwMBAFixYQEREBEOGDEnr5kVERERE0lWaEuCoqCh27txJ48aNeeedd8iVKxehoaGsW7eOP/74QwmwiIiIiGQ6KXqqbdKkSdy6dSvR9MjISGJjYylRogQvv/wyhQoVoly5crz88stERkame7AiIiIiImmV4m7QNm/eTJcuXXj33XeNoY6dnZ0pXbo0P/zwA8uWLcPFxYXw8HDCwsJo0KDBMw1cRERERCQ1UtQC/Omnn+Lu7s6SJUto3749ixYt4sGDB8a8YsWKERERwY0bNwgNDaVixYqMGDHimQYuIiIiIpIaKWoBbt26Nc2bN2f16tUsXLiQOXPmsGLFCnr16kXHjh1ZsWIFV69e5c6dO3h4eODh4fGs4xYRERERSZUUj2yRLVs2unTpwtq1a3n//fd5+PAhX3/9NZ07d+aPP/7A09OTChUqKPkVERERkUzt6YZ2AxwcHOjZsyfr1q3jnXfe4ebNm4wdO5a33nqLvXv3PosYRURERETSTYoT4Nu3b/P777+zZMkS/vjjD0wmE4MGDWLt2rV07NiRCxcuMGzYMPr06cOxY8eeZcwiIiIiIqmWohrggwcPMnz4cCIiIoxpbm5uzJs3j2LFivHxxx/zzjvv8N1337Ft2zZ69epFvXr1mDZt2jMLXEREREQkNVLUAjxz5kyyZctG3bp1adGiBQ0aNCBbtmzMmTPHWKZQoUJMmjSJpUuXUrt2bfbs2fPMghYRERERSa0UtQAHBAQwc+ZMKleubEy7f/8+vXr1SrRsmTJlmDFjBr6+vukVo4iIiIhIuklRApw/f34mTJhAnTp1cHZ2JiIiAl9fXwoUKJDsOgmTZRERERGRzCJFCXDPnj0ZN24cv/zyCyaTCbPZjJ2dnUUJhIiIiIhIVpCiBLhly5YUL16cnTt3GoNdNG/enEKFCj3r+ERERERE0lWKEmCAsmXLUrZs2WcZi4iIiIjIM5eiXiCGDx/OgQMHUr2TkydPMnr06FSv/6jjx4/Tt29f6tWrR/PmzRk3bhx37twx5gcGBjJs2DAaNmxIkyZN+OKLLwgNDU23/YuIiIhI1pWiFuDdu3eze/duChUqRJMmTWjYsCEvvfQSNjZJ58/R0dEcPXqUAwcOsHv3bs6ePQvAxIkT0xywn58f/fr1o2bNmkyZMoWbN28ye/ZsAgMDWbhwIffv36dfv364u7szfvx4goODmTlzJkFBQcyaNSvN+xcRERGRrC1FCfD8+fP56quvOHPmDIsXL2bx4sXY2dlRvHhx8ubNi5OTEyaTifDwcK5du8alS5eIjIwEwGw2U65cOYYPH54uAc+cOZOyZcsydepUIwF3cnJi6tSpXLlyha1btxISEsKyZcvIlSsXAB4eHgwZMgRfX1/1TiEiIiJi5VKUAFeqVImlS5fy119/sWTJEvz8/Hj48CH+/v6cPn3aYlmz2QyAyWSiZs2adOrUiYYNG2IymdIc7N27dzl06BDjx4+3aH1u3LgxjRs3BsDb25sqVaoYyS+Al5cXTk5O7N27VwmwiIiIiJVL8UNwNjY2NGvWjGbNmhEUFMS+ffs4evQoN2/eNOpvc+fOTaFChahcuTI1atQgX7586Rrs2bNniY2Nxc3NjdGjR7Nr1y7MZjONGjVixIgRuLi4EBAQQLNmzSzWs7W1xdPTk4sXL6Zp/2azmfDw8DRtIzMwmUzkyJEjo8OQJ4iIiDB+UErmoGsn89N1I2LdzGZzihpdU5wAJ+Tp6Unnzp3p3LlzalZPteDgYAA+++wz6tSpw5QpU7h06RLffvstV65cYcGCBYSGhuLk5JRoXUdHR8LCwtK0/6ioKPz8/NK0jcwgR44clC9fPqPDkCe4cOECERERGR2GJKBrJ/PTdSMi9vb2T1wmVQlwRomKigKgXLlyjBkzBoCaNWvi4uLCJ598wv79+4mNjU12/eQe2kspOzs7SpUqlaZtZAbpUY4iz17x4sXVkpXJ6NrJ/HTdiFi3+I4XniRLJcCOjo4A1K9f32J6nTp1ADh16hTOzs5JlimEhYXh4eGRpv2bTCYjBpFnTbfaRZ6erhsR65bShoq0NYk+Z0WKFAHg4cOHFtOjo6MBcHBwoGjRogQGBlrMj4mJISgoiGLFij2XOEVEREQk88pSCXDx4sXx9PRk69atFre4du7cCUDlypXx8vLi8OHDRr0wgI+PD+Hh4Xh5eT33mEVEREQkc8lSCbDJZGLw4MEcP36cUaNGsX//fn755RemTZtG48aNKVeuHJ07dyZ79uwMGDCAHTt2sHbtWsaMGUOdOnWoVKlSRh+CiIiIiGSwVNUAnzhxggoVKqR3LCnStGlTsmfPzvz58xk2bBg5c+akU6dOvP/++wC4ubkxd+5cpk2bxujRo3FycqJJkyYMHTo0Q+IVERERkcwlVQlwjx49KF68OK+++iqtW7cmb9686R3XY9WvXz/Rg3AJlSpVijlz5jzHiEREREQkq0h1CURAQADffvstbdq0YeDAgfzxxx/G8MciIiIiIplVqlqAu3fvzl9//cXly5cxm80cOHCAAwcO4OjoSLNmzXj11Vc15LCIiIiIZEqpSoAHDhzIwIED8ff3588//+Svv/4iMDCQsLAw1q1bx7p16/D09KRNmza0adOG/Pnzp3fcIiIiIiKpkqaBMMqWLUvZsmUZMGAAp0+fZuXKlaxbtw6AoKAgvv/+exYsWECnTp0YPnx4mkdiExEREUkvkZGRvPLKK8TExFhMz5EjB7t37wbg5MmTTJ8+HT8/P5ycnGjbti19+vTBzs7usdv28fFhzpw5nDt3Dnd3d15//XW6deumESUziTSPBHf//n3++usvtm3bxqFDhzCZTJjNZqOf3piYGFatWkXOnDnp27dvmgMWERERSQ/nzp0jJiaGCRMmUKhQIWN6fIPd5cuX6d+/PxUrVuSLL74gICCAOXPmEBISwqhRo5Ld7vHjxxk6dCjNmjWjX79++Pr6MnPmTGJiYnj33Xef9WFJCqQqAQ4PD+fvv/9m69atHDhwwBiJzWw2Y2NjQ61atWjXrh0mk4lZs2YRFBTEli1blACLiIhIpnH69GlsbW1p0qQJ9vb2ieYvXrwYJycnpk6dip2dHfXq1cPBwYGvv/6anj17JlviOW/ePMqWLcuECRMAqFOnDtHR0SxatIiuXbvi4ODwTI9LnixVCXCzZs2IiooCMFp6PT09adu2baKaXw8PD9577z1u3LiRDuGKiIiIpA9/f3+KFSuWZPILcWUMdevWtSh3aNKkCV9++SXe3t507Ngx0ToPHz7k0KFDiRr9mjRpwk8//YSvr69Gps0EUpUAP3z4EAB7e3saN25M+/btqV69epLLenp6AuDi4pLKEEVERETSX3wL8IABAzh69Cj29vbG4Fm2trZcvXqVIkWKWKzj5uaGk5MTFy9eTHKbV65cISoqKtF6hQsXBuDixYtKgDOBVCXAL730Eu3ataNly5Y4Ozs/dtkcOXLw7bffUrBgwVQFKCIiIpLezGYzZ8+exWw206FDB9577z1OnjzJ/PnzuXDhAl988QVAknmOk5MTYWFhSW43NDTUWCYhR0dHgGTXk+crVQnwTz/9BMTVAkdFRRm3Bi5evEiePHks/uhOTk7UrFkzHUIVERERSR9ms5mpU6fi5uZGyZIlAahatSru7u6MGTOGgwcPPnb95HpziI2Nfex66hErc0j1X2HdunW0adOG48ePG9OWLl1Kq1atWL9+fboEJyIiIvIs2NjYUL16dSP5jVevXj0grpQBkm6xDQsLS/YOePz08PDwROsknC8ZK1UJ8N69e5k4cSKhoaGcPXvWmB4QEEBERAQTJ07kwIED6RakiIiISHq6efMma9as4dq1axbTIyMjAciTJw8eHh5cvnzZYv6dO3cICwujePHiSW63UKFC2NraEhgYaDE9/nWxYsXS6QgkLVKVAC9btgyAAgUKWPxyevvttylcuDBms5klS5akT4QiIiIi6SwmJoZJkybx22+/WUzfunUrtra2VKlShVq1arF7927j4X+A7du3Y2trS40aNZLcbvbs2alSpQo7duwwesqKX8/Z2ZkKFSo8mwOSp5KqGuBz585hMpkYO3Ys1apVM6Y3bNgQV1dX+vTpw5kzZ9ItSBEREZH0lD9/ftq2bcuSJUvInj07FStWxNfXl0WLFtGlSxeKFi1K9+7d2bp1K4MHD+btt9/m4sWLzJkzh44dOxpdvj58+BB/f388PDzIly8fAO+99x79+/fno48+ol27dhw7dowlS5YwcOBA9QGcSaSqBTj+CUc3N7dE8+K7O7t//34awhIRERF5tj7++GN69erFpk2bGDp0KJs2baJv374MGzYMiCtXmD17Ng8ePGDkyJH8/PPPvPXWW3z44YfGNm7dukWPHj1Yu3atMa1GjRp8/fXXXLx4kQ8//JAtW7YwZMgQunfv/rwPUZKRqhbgfPnycfnyZVavXm3xJjCbzfzyyy/GMiIiIiKZlb29Pb169aJXr17JLlOlShV+/PHHZOd7enom2WNEo0aNaNSoUXqEKc9AqhLghg0bsmTJElauXImPjw+lS5cmOjqa06dPc/XqVUwmEw0aNEjvWEVERERE0ixVCXDPnj35+++/CQwM5NKlS1y6dMmYZzabKVy4MO+99166BSkiIiIikl5SVQPs7OzMokWL6NChA87OzpjNZsxmM05OTnTo0IGFCxeqnzsRERERyZRS1QIM4OrqyieffMKoUaO4e/cuZrMZNze3ZEdGERERERHJDNI8Hp/JZMLNzY3cuXMbyW9sbCz79u1Lc3AiIiIiIuktVS3AZrOZhQsXsmvXLu7du2cx7nV0dDR3794lOjqa/fv3p1ugIiIiIiLpIVUJ8IoVK5g7dy4mk8lilBPAmKZSCBERERHJjFJVAvH7778DkCNHDgoXLozJZOLll1+mePHiRvI7cuTIdA1UREREsq7YRxrMJPOwxr9NqlqAL1++jMlk4quvvsLNzY1u3brRt29fateuzTfffMPPP/9MQEBAOocqIiIiWZWNycQvPqe5cS88o0ORBDxyOtLVq0xGh/HcpSoBjoyMBKBIkSIUKFAAR0dHTpw4Qe3atenYsSM///wze/fuZfjw4ekarIiIiGRdN+6FExQcltFhiKSuBCJ37twA+Pv7YzKZKF26NHv37gXiWocBbty4kU4hioiIiIikn1QlwJUqVcJsNjNmzBgCAwOpUqUKJ0+epEuXLowaNQr4/yRZRERERCQzSVUC3KtXL3LmzElUVBR58+alRYsWmEwmAgICiIiIwGQy0bRp0/SOVUREREQkzVKVABcvXpwlS5bQu3dvHBwcKFWqFOPGjSNfvnzkzJmT9u3b07dv3/SOVUREREQkzVL1ENzevXupWLEivXr1Mqa1bt2a1q1bp1tgIiIiIiLPQqpagMeOHUvLli3ZtWtXescjIiIiIvJMpSoBfvDgAVFRURQrViydwxERERERebZSlQA3adIEgB07dqRrMCIiIiIiz1qqaoDLlCnDnj17+Pbbb1m9ejUlSpTA2dmZbNn+f3Mmk4mxY8emW6AiIiIiIukhVQnwjBkzMJlMAFy9epWrV68muZwSYBERERHJbFKVAAOYzebHzo9PkEVEREREMpNUJcDr169P7zhERERERJ6LVCXABQoUSO84RERERESei1QlwIcPH07RclWrVk3N5kVEREREnplUJcB9+/Z9Yo2vyWRi//79qQpKRERERORZeWYPwYmIiIiIZEapSoB79+5t8dpsNvPw4UOuXbvGjh07KFeuHD179kyXAEVERERE0lOqEuA+ffokO+/PP/9k1KhR3L9/P9VBiYiIiIg8K6kaCvlxGjduDMDy5cvTe9MiIiIiImmW7gnwP//8g9ls5ty5c+m9aRERERGRNEtVCUS/fv0STYuNjSU0NJTz588DkDt37rRFJiIiIiLyDKQqAT506FCy3aDF9w7Rpk2b1EclIiIiIvKMpGs3aHZ2duTNm5cWLVrQq1evNAWWUiNGjODUqVNs2LDBmBYYGMi0adM4cuQItra2NG3alEGDBuHs7PxcYhIRERGRzCtVCfA///yT3nGkyqZNm9ixY4fF0Mz379+nX79+uLu7M378eIKDg5k5cyZBQUHMmjUrA6MVERERkcwg1S3ASYmKisLOzi49N5msmzdvMmXKFPLly2cx/ddffyUkJIRly5aRK1cuADw8PBgyZAi+vr5Urlz5ucQnIiIiIplTqnuB8Pf3p3///pw6dcqYNnPmTHr16sWZM2fSJbjHmTBhArVq1aJGjRoW0729valSpYqR/AJ4eXnh5OTE3r17n3lcIiIiIpK5pSoBPn/+PH379uXgwYMWyW5AQABHjx6lT58+BAQEpFeMiaxdu5ZTp04xcuTIRPMCAgIoUqSIxTRbW1s8PT25ePHiM4tJRERERLKGVJVALFy4kLCwMOzt7S16g3jppZc4fPgwYWFh/Pjjj4wfPz694jRcvXqVb775hrFjx1q08sYLDQ3Fyckp0XRHR0fCwsLStG+z2Ux4eHiatpEZmEwmcuTIkdFhyBNEREQk+bCpZBxdO5mfrpvMSddO5veiXDtmsznZnsoSSlUC7Ovri8lkYvTo0bRq1cqY3r9/f0qVKsUnn3zCkSNHUrPpxzKbzXz22WfUqVOHJk2aJLlMbGxssuvb2KRt3I+oqCj8/PzStI3MIEeOHJQvXz6jw5AnuHDhAhERERkdhiSgayfz03WTOenayfxepGvH3t7+icukKgG+c+cOABUqVEg0r2zZsgDcunUrNZt+rJUrV3LmzBl++eUXoqOjgf/vji06OhobGxucnZ2TbKUNCwvDw8MjTfu3s7OjVKlSadpGZpCSX0aS8YoXL/5C/Bp/kejayfx03WROunYyvxfl2jl79myKlktVAuzq6srt27f5559/KFy4sMW8ffv2AeDi4pKaTT/WX3/9xd27d2nZsmWieV5eXvTu3ZuiRYsSGBhoMS8mJoagoCAaNWqUpv2bTCYcHR3TtA2RlNLtQpGnp+tGJHVelGsnpT+2UpUAV69enS1btjB16lT8/PwoW7Ys0dHRnDx5km3btmEymRL1zpAeRo0alah1d/78+fj5+TFt2jTy5s2LjY0NP/30E8HBwbi5uQHg4+NDeHg4Xl5e6R6TiIiIiGQtqUqAe/Xqxa5du4iIiGDdunUW88xmMzly5OC9995LlwATKlasWKJprq6u2NnZGbVFnTt3ZsWKFQwYMIDevXsTEhLCzJkzqVOnDpUqVUr3mEREREQka0nVU2FFixZl1qxZFClSBLPZbPGvSJEizJo1K8lk9Xlwc3Nj7ty55MqVi9GjRzNnzhyaNGnCF198kSHxiIiIiEjmkuqR4CpWrMivv/6Kv78/gYGBmM1mChcuTNmyZZ9rsXtSXa2VKlWKOXPmPLcYRERERCTrSNNQyOHh4ZQoUcLo+eHixYuEh4cn2Q+viIiIiEhmkOqOcdetW0ebNm04fvy4MW3p0qW0atWK9evXp0twIiIiIiLpLVUJ8N69e5k4cSKhoaEW/a0FBAQQERHBxIkTOXDgQLoFKSIiIiKSXlKVAC9btgyAAgUKULJkSWP622+/TeHChTGbzSxZsiR9IhQRERERSUepqgE+d+4cJpOJsWPHUq1aNWN6w4YNcXV1pU+fPpw5cybdghQRERERSS+pagEODQ0FMAaaSCh+BLj79++nISwRERERkWcjVQlwvnz5AFi9erXFdLPZzC+//GKxjIiIiIhIZpKqEoiGDRuyZMkSVq5ciY+PD6VLlyY6OprTp09z9epVTCYTDRo0SO9YRURERETSLFUJcM+ePfn7778JDAzk0qVLXLp0yZgXPyDGsxgKWUREREQkrVJVAuHs7MyiRYvo0KEDzs7OxjDITk5OdOjQgYULF+Ls7JzesYqIiIiIpFmqR4JzdXXlk08+YdSoUdy9exez2Yybm9tzHQZZRERERORppXokuHgmkwk3Nzdy586NyWQiIiKCNWvW8N///jc94hMRERERSVepbgF+lJ+fH6tXr2br1q1ERESk12ZFRERERNJVmhLg8PBwNm/ezNq1a/H39zemm81mlUKIiIiISKaUqgT433//Zc2aNWzbts1o7TWbzQDY2trSoEEDOnXqlH5RioiIiIikkxQnwGFhYWzevJk1a9YYwxzHJ73xTCYTGzduJE+ePOkbpYiIiIhIOklRAvzZZ5/x559/8uDBA4uk19HRkcaNG5M/f34WLFgAoORXRERERDK1FCXAGzZswGQyYTabyZYtG15eXrRq1YoGDRqQPXt2vL29n3WcIiIiIiLp4qm6QTOZTHh4eFChQgXKly9P9uzZn1VcIiIiIiLPRIpagCtXroyvry8AV69eZd68ecybN4/y5cvTsmVLjfomIiIiIllGihLg+fPnc+nSJdauXcumTZu4ffs2ACdPnuTkyZMWy8bExGBra5v+kYqIiIiIpIMUl0AUKVKEwYMH8/vvvzN58mTq1atn1AUn7Pe3ZcuWTJ8+nXPnzj2zoEVEREREUuup+wG2tbWlYcOGNGzYkFu3brF+/Xo2bNjA5cuXAQgJCeHnn39m+fLl7N+/P90DFhERERFJi6d6CO5RefLkoWfPnqxZs4bvvvuOli1bYmdnZ7QKi4iIiIhkNmkaCjmh6tWrU716dUaOHMmmTZtYv359em1aRERERCTdpFsCHM/Z2ZkuXbrQpUuX9N60iIiIiEiapakEQkREREQkq1ECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFiVbBkdwNOKjY1l9erV/Prrr1y5coXcuXPzyiuv0LdvX5ydnQEIDAxk2rRpHDlyBFtbW5o2bcqgQYOM+SIiIiJivbJcAvzTTz/x3Xff8c4771CjRg0uXbrE3LlzOXfuHN9++y2hoaH069cPd3d3xo8fT3BwMDNnziQoKIhZs2ZldPgiIiIiksGyVAIcGxvL4sWLee211xg4cCAAtWrVwtXVlVGjRuHn58f+/fsJCQlh2bJl5MqVCwAPDw+GDBmCr68vlStXzrgDEBEREZEMl6VqgMPCwmjdujUtWrSwmF6sWDEALl++jLe3N1WqVDGSXwAvLy+cnJzYu3fvc4xWRERERDKjLNUC7OLiwogRIxJN//vvvwEoUaIEAQEBNGvWzGK+ra0tnp6eXLx48XmEKSIiIiKZWJZKgJNy4sQJFi9eTP369SlVqhShoaE4OTklWs7R0ZGwsLA07ctsNhMeHp6mbWQGJpOJHDlyZHQY8gQRERGYzeaMDkMS0LWT+em6yZx07WR+L8q1YzabMZlMT1wuSyfAvr6+DBs2DE9PT8aNGwfE1Qknx8YmbRUfUVFR+Pn5pWkbmUGOHDkoX758RochT3DhwgUiIiIyOgxJQNdO5qfrJnPStZP5vUjXjr29/ROXybIJ8NatW/n0008pUqQIs2bNMmp+nZ2dk2ylDQsLw8PDI037tLOzo1SpUmnaRmaQkl9GkvGKFy/+Qvwaf5Ho2sn8dN1kTrp2Mr8X5do5e/ZsipbLkgnwkiVLmDlzJtWqVWPKlCkW/fsWLVqUwMBAi+VjYmIICgqiUaNGadqvyWTC0dExTdsQSSndLhR5erpuRFLnRbl2UvpjK0v1AgHw22+/MWPGDJo2bcqsWbMSDW7h5eXF4cOHCQ4ONqb5+PgQHh6Ol5fX8w5XRERERDKZLNUCfOvWLaZNm4anpydvvPEGp06dsphfqFAhOnfuzIoVKxgwYAC9e/cmJCSEmTNnUqdOHSpVqpRBkYuIiIhIZpGlEuC9e/cSGRlJUFAQvXr1SjR/3LhxtG3blrlz5zJt2jRGjx6Nk5MTTZo0YejQoc8/YBERERHJdLJUAty+fXvat2//xOVKlSrFnDlznkNEIiIiIpLVZLkaYBERERGRtFACLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlWUAIuIiIiIVVECLCIiIiJWRQmwiIiIiFgVJcAiIiIiYlVe6ATYx8eH//73v9StW5d27dqxZMkSzGZzRoclIiIiIhnohU2Ajx8/ztChQylatCiTJ0+mZcuWzJw5k8WLF2d0aCIiIiKSgbJldADPyrx58yhbtiwTJkwAoE6dOkRHR7No0SK6du2Kg4NDBkcoIiIiIhnhhWwBfvjwIYcOHaJRo0YW05s0aUJYWBi+vr4ZE5iIiIiIZLgXMgG+cuUKUVFRFClSxGJ64cKFAbh48WJGhCUiIiIimcALWQIRGhoKgJOTk8V0R0dHAMLCwp5qe/7+/jx8+BCAY8eOpUOEGc9kMlEzdywxuVQKktnY2sRy/PhxPbCZSenayZx03WR+unYypxft2omKisJkMj1xuRcyAY6NjX3sfBubp2/4jj+ZKTmpWYVTdruMDkEe40V6r71odO1kXrpuMjddO5nXi3LtmEwm602AnZ2dAQgPD7eYHt/yGz8/pcqWLZs+gYmIiIhIhnsha4ALFSqEra0tgYGBFtPjXxcrViwDohIRERGRzOCFTICzZ89OlSpV2LFjh0VNy/bt23F2dqZChQoZGJ2IiIiIZKQXMgEGeO+99zhx4gQfffQRe/fu5bvvvmPJkiX06NFDfQCLiIiIWDGT+UV57C8JO3bsYN68eVy8eBEPDw9ef/11unXrltFhiYiIiEgGeqETYBERERGRR72wJRAiIiIiIklRAiwiIiIiVkUJsIiIiIhYFSXAIiIiImJVlACLiIiIiFVRAiwiIiIiVkUJsFg99QQoL7qk3uN634uINVMCLFlSUFAQ1atXZ8OGDale5/79+4wdO5YjR448qzBFnom2bdsyfvz4JOfNmzeP6tWrG699fX0ZMmSIxTILFixgyZIlzzJEEauSmu8kyVhKgMVq+fv7s2nTJmJjYzM6FJF006FDBxYtWmS8Xrt2LRcuXLBYZu7cuURERDzv0EReWHny5GHRokXUq1cvo0ORFMqW0QGIiEj6yZcvH/ny5cvoMESsir29Pf/5z38yOgx5CmoBlgz34MEDZs+eTceOHalduzYNGjSgf//++Pv7G8ts376dN998k7p16/L2229z+vRpi21s2LCB6tWrExQUZDE9uVvFBw8epF+/fgD069ePPn36pP+BiTwn69ato0aNGixYsMCiBGL8+PFs3LiRq1evGrdn4+fNnz/folTi7NmzDB06lAYNGtCgQQM+/PBDLl++bMw/ePAg1atX58CBAwwYMIC6devSokULZs6cSUxMzPM9YJGn4Ofnx/vvv0+DBg145ZVX6N+/P8ePHzfmHzlyhD59+lC3bl0aN27MuHHjCA4ONuZv2LCBWrVqceLECXr06EGdOnVo06aNRRlRUiUQly5d4n//+x8tWrSgXr169O3bF19f30TrLF26lE6dOlG3bl3Wr1//bE+GGJQAS4YbN24c69ev591332X27NkMGzaM8+fPM3r0aMxmM7t27WLkyJGUKlWKKVOm0KxZM8aMGZOmfZYrV46RI0cCMHLkSD766KP0OBSR527r1q1MmjSJXr160atXL4t5vXr1om7duri7uxu3Z+PLI9q3b2/8/+LFi7z33nvcuXOH8ePHM2bMGK5cuWJMS2jMmDFUqVKF6dOn06JFC3766SfWrl37XI5V5GmFhoYyaNAgcuXKxddff83nn39OREQEAwcOJDQ0lMOHD/P+++/j4ODAl19+yQcffMChQ4fo27cvDx48MLYTGxvLRx99RPPmzZkxYwaVK1dmxowZeHt7J7nf8+fP884773D16lVGjBjBxIkTMZlM9OvXj0OHDlksO3/+fLp3785nn31GrVq1nun5kP+nEgjJUFFRUYSHhzNixAiaNWsGQLVq1QgNDWX69Oncvn2bBQsW8PLLLzNhwgQAateuDcDs2bNTvV9nZ2eKFy8OQPHixSlRokQaj0Tk+du9ezdjx47l3XffpW/fvonmFypUCDc3N4vbs25ubgB4eHgY0+bPn4+DgwNz5szB2dkZgBo1atC+fXuWLFli8RBdhw4djES7Ro0a7Ny5kz179tCpU6dneqwiqXHhwgXu3r1L165dqVSpEgDFihVj9erVhIWFMXv2bIoWLco333yDra0tAP/5z3/o0qUL69evp0uXLkBcrym9evWiQ4cOAFSqVIkdO3awe/du4zspofnz52NnZ8fcuXNxcnICoF69erzxxhvMmDGDn376yVi2adOmtGvX7lmeBkmCWoAlQ9nZ2TFr1iyaNWvGjRs3OHjwIL/99ht79uwB4hJkPz8/6tevb7FefLIsYq38/Pz46KOP8PDwMMp5Uuuff/6hatWqODg4EB0dTXR0NE5OTlSpUoX9+/dbLPtonaOHh4ceqJNMq2TJkri5uTFs2DA+//xzduzYgbu7O4MHD8bV1ZUTJ05Qr149zGaz8d4vWLAgxYoVS/Ter1ixovF/e3t7cuXKlex7/9ChQ9SvX99IfgGyZctG8+bN8fPzIzw83JhepkyZdD5qSQm1AEuG8/b2ZurUqQQEBODk5ETp0qVxdHQE4MaNG5jNZnLlymWxTp48eTIgUpHM49y5c9SrV489e/awcuVKunbtmupt3b17l23btrFt27ZE8+JbjOM5ODhYvDaZTOpJRTItR0dH5s+fzw8//MC2bdtYvXo12bNn59VXX6VHjx7ExsayePFiFi9enGjd7NmzW7x+9L1vY2OTbH/aISEhuLu7J5ru7u6O2WwmLCzMIkZ5/pQAS4a6fPkyH374IQ0aNGD69OkULFgQk8nEqlWr2LdvH66urtjY2CSqQwwJCbF4bTKZABJ9ESf8lS3yIqlTpw7Tp0/n448/Zs6cOTRs2JD8+fOnalsuLi7UrFmTbt26JZoXf1tYJKsqVqwYEyZMICYmhn///ZdNmzbx66+/4uHhgclk4q233qJFixaJ1ns04X0arq6u3L59O9H0+Gmurq7cunUr1duXtFMJhGQoPz8/IiMjeffddylUqJCRyO7btw+Iu2VUsWJFtm/fbvFLe9euXRbbib/NdP36dWNaQEBAokQ5IX2xS1aWO3duAIYPH46NjQ1ffvllksvZ2CT+mH90WtWqVblw4QJlypShfPnylC9fnpdeeolly5bx999/p3vsIs/Ln3/+SdOmTbl16xa2trZUrFiRjz76CBcXF27fvk25cuUICAgw3vfly5enRIkSzJs3L9HDak+jatWq7N6926KlNyYmhj/++IPy5ctjb2+fHocnaaAEWDJUuXLlsLW1ZdasWfj4+LB7925GjBhh1AA/ePCAAQMGcP78eUaMGMG+fftYvnw58+bNs9hO9erVyZ49O9OnT2fv3r1s3bqV4cOH4+rqmuy+XVxcANi7d2+ibtVEsoo8efIwYMAA9uzZw5YtWxLNd3Fx4c6dO+zdu9docXJxceHo0aMcPnwYs9lM7969CQwMZNiwYfz99994e3vzv//9j61bt1K6dOnnfUgi6aZy5crExsby4Ycf8vfff/PPP/8wadIkQkNDadKkCQMGDMDHx4fRo0ezZ88edu3axeDBg/nnn38oV65cqvfbu3dvIiMj6devH3/++Sc7d+5k0KBBXLlyhQEDBqTjEUpqKQGWDFW4cGEmTZrE9evXGT58OJ9//jkQN5yryWTiyJEjVKlShZkzZ3Ljxg1GjBjB6tWrGTt2rMV2XFxcmDx5MjExMXz44YfMnTuX3r17U758+WT3XaJECVq0aMHKlSsZPXr0Mz1OkWepU6dOvPzyy0ydOjXRXY+2bdtSoEABhg8fzsaNGwHo0aMHfn5+DB48mOvXr1O6dGkWLFiAyWRi3LhxjBw5klu3bjFlyhQaN26cEYckki7y5MnDrFmzcHZ2ZsKECQwdOhR/f3++/vprqlevjpeXF7NmzeL69euMHDmSsWPHYmtry5w5c9I0sEXJkiVZsGABbm5ufPbZZ8Z31rx589TVWSZhMidXwS0iIiIi8gJSC7CIiIiIWBUlwCIiIiJiVZQAi4iIiIhVUQIsIiIiIlZFCbCIiIiIWBUlwCIiIiJiVZQAi4iIiIhVyZbRAYiIvAh69+7NkSNHgLjBJ8aNG5fBESV29uxZfvvtNw4cOMCtW7d4+PAhbm5uvPTSS7Rr144GDRpkdIgiIs+FBsIQEUmjixcv0qlTJ+O1g4MDW7ZswdnZOQOjsvTjjz8yd+5coqOjk12mVatWfPrpp9jY6OagiLzY9CknIpJG69ats3j94MEDNm3alEHRJLZy5Upmz55NdHQ0+fLlY9SoUaxatYpffvmFoUOH4uTkBMDmzZv5+eefMzhaEZFnTy3AIiJpEB0dzauvvsrt27fx9PTk+vXrxMTEUKZMmUyRTN66dYu2bdsSFRVFvnz5+Omnn3B3d7dYZu/evQwZMgSAvHnzsmnTJkwmU0aEKyLyXKgGWEQkDfbs2cPt27cBaNeuHSdOnGDPnj2cPn2aEydOUKFChUTrBAUFMXv2bHx8fIiKiqJKlSp88MEHfP755xw+fJiqVavy/fffG8sHBAQwb948/vnnH8LDwylQoACtWrXinXfeIXv27I+Nb+PGjURFRQHQq1evRMkvQN26dRk6dCienp6UL1/eSH43bNjAp59+CsC0adNYvHgxJ0+exM3NjSVLluDu7k5UVBS//PILW7ZsITAwEICSJUvSoUMH2rVrZ5FI9+nTh8OHDwNw8OBBY/rBgwfp168fEFdL3bdvX4vly5Qpw1dffcWMGTP4559/MJlM1K5dm0GDBuHp6fnY4xcRSYoSYBGRNEhY/tCiRQsKFy7Mnj17AFi9enWiBPjq1at0796d4OBgY9q+ffs4efJkkjXD//77L/379ycsLMyYdvHiRebOncuBAweYM2cO2bIl/1Een3ACeHl5Jbtct27dHnOUMG7cOO7fvw+Au7s77u7uhIeH06dPH06dOmWx7PHjxzl+/Dh79+7liy++wNbW9rHbfpLg4GB69OjB3bt3jWnbtm3j8OHDLF68mPz586dp+yJifVQDLCKSSjdv3mTfvn0AlC9fnsKFC9OgQQOjpnbbtm2EhoZarDN79mwj+W3VqhXLly/nu+++I3fu3Fy+fNliWbPZzGeffUZYWBi5cuVi8uTJ/Pbbb4wYMQIbGxsOHz7MihUrHhvj9evXjf/nzZvXYt6tW7e4fv16on8PHz5MtJ2oqCimTZvGzz//zAcffADA9OnTjeS3efPmLF26lIULF1KrVi0Atm/fzpIlSx5/ElPg5s2b5MyZk9mzZ7N8+XJatWoFwO3bt5k1a1aaty8i1kcJsIhIKm3YsIGYmBgAWrZsCcT1ANGoUSMAIiIi2LJli7F8bGys0TqcL18+xo0bR+nSpalRowaTJk1KtP0zZ85w7tw5ANq0aUP58uVxcHCgYcOGVK1aFYDff//9sTEm7NHh0R4g/vvf//Lqq68m+nfs2LFE22natCmvvPIKZcqUoUqVKoSFhRn7LlmyJBMmTKBcuXJUrFiRKVOmGKUWT0rQU2rMmDF4eXlRunRpxo0bR4ECBQDYvXu38TcQEUkpJcAiIqlgNptZv3698drZ2Zl9+/axb98+i1vya9asMf4fHBxslDKUL1/eonShdOnSRstxvEuXLhn/X7p0qUWSGl9De+7cuSRbbOPly5fP+H9QUNDTHqahZMmSiWKLjIwEoHr16hZlDjly5KBixYpAXOttwtKF1DCZTBalJNmyZaN8+fIAhIeHp3n7ImJ9VAMsIpIKhw4dsihZ+Oyzz5Jczt/fn3///ZeXX34ZOzs7Y3pKOuBJSe1sTEwM9+7dI0+ePEnOr1mzptHqvGfPHkqUKGHMS9hV2/jx49m4cWOy+3m0PvlJsT3p+GJiYoxtxCfSj9tWdHR0sudPPVaIyNNSC7CISCo82vfv48S3AufMmRMXFxcA/Pz8LEoSTp06ZfGgG0DhwoWN//fv35+DBw8a/5YuXcqWLVs4ePBgsskvxNXmOjg4ALB48eJkW4Ef3fejHn3QrmDBgtjb2wNxvTjExsYa8yIiIjh+/DgQ1wKdK1cuAGP5R/d37dq1x+4b4n5wxIuJicHf3x+IS8zjty8iklJKgEVEntL9+/fZvn07AK6urnh7e1skpwcPHmTLli1GC+fWrVuNhK9FixZA3MNpn376KWfPnsXHx4dPPvkk0X5KlixJmTJlgLgSiD/++IPLly+zadMmunfvTsuWLRkxYsRjY82TJw/Dhg0DICQkhB49erBq1SoCAgIICAhgy5Yt9O3blx07djzVOXBycqJJkyZAXBnG2LFjOXXqFMePH+d///uf0TVcly5djHUSPoS3fPlyYmNj8ff3Z/HixU/c35dffsnu3bs5e/YsX375JVeuXAGgYcOGGrlORJ6aSiBERJ7S5s2bjdv2rVu3trg1Hy9Pnjw0aNCA7du3Ex4ezpYtW+jUqRM9e/Zkx44d3L59m82bN7N582YA8ufPT44cOYiIiDBu6ZtMJoYPH87gwYO5d+9eoiTZ1dXV6DP3cTp16kRUVBQzZszg9u3bfPXVV0kuZ2trS/v27Y362icZMWIEp0+f5ty5c2zZssXigT+Axo0bW3Sv1qJFCzZs2ADA/PnzWbBgAWazmf/85z9PrE82m81GIh8vb968DBw4MEWxiogkpJ/NIiJPKWH5Q/v27ZNdrlOnTsb/48sgPDw8+OGHH2jUqBFOTk44OTnRuHFjFixYYJQIJCwVqFatGj/++CPNmjXD3d0dOzs78uXLR9u2bfnxxx8pVapUimLu2rUrq1atokePHpQtWxZXV1fs7OzIkycPNWvWZODAgWzYsIFRo0bh6OiYom3mzJmTJUuWMGTIEF566SUcHR1xcHCgQoUKjB49mq+++sqiVtjLy4sJEyZQsmRJ7O3tKVCgAL179+abb7554r7iz1mOHDlwdnamefPmLFq06LHlHyIiydFQyCIiz5GPjw/29vZ4eHiQP39+o7Y2NjaW+vXrExkZSfPmzfn8888zONKMl9zIcSIiaaUSCBGR52jFihXs3r0bgA4dOtC9e3cePnzIxo0bjbKKlJYgiIhI6igBFhF5jt544w327t1LbGwsa9euZe3atRbz8+XLR7t27TImOBERK6EaYBGR58jLy4s5c+ZQv3593N3dsbW1xd7enkKFCtGpUyd+/PFHcubMmdFhioi80FQDLCIiIiJWRS3AIiIiImJVlACLiIiIiFVRAiwiIiIiVkUJsIiIiIhYFSXAIiIiImJVlACLiIiIiFVRAiwiIiIiVkUJsIiIiIhYFSXAIiIiImJV/g+JtiKzwazznwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            429  77.158273\n",
      "1           kitten          114             81  71.052632\n",
      "2           senior          178             96  53.932584\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    220      168     76.36\n",
      "1          M    337      236     70.03\n",
      "2          X    291      202     69.42\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 944, 2: 825, 1: 735})\n",
      "Epoch 1/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.1087 - accuracy: 0.5152\n",
      "Epoch 2/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.9644 - accuracy: 0.5791\n",
      "Epoch 3/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.8890 - accuracy: 0.6110\n",
      "Epoch 4/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.8289 - accuracy: 0.6374\n",
      "Epoch 5/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.8029 - accuracy: 0.6581\n",
      "Epoch 6/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.7770 - accuracy: 0.6601\n",
      "Epoch 7/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.7852 - accuracy: 0.6709\n",
      "Epoch 8/1500\n",
      "40/40 [==============================] - 0s 900us/step - loss: 0.7580 - accuracy: 0.6741\n",
      "Epoch 9/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.7468 - accuracy: 0.6645\n",
      "Epoch 10/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.7209 - accuracy: 0.6857\n",
      "Epoch 11/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.6793 - accuracy: 0.7105\n",
      "Epoch 12/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.6741 - accuracy: 0.7101\n",
      "Epoch 13/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.6749 - accuracy: 0.7081\n",
      "Epoch 14/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.6444 - accuracy: 0.7153\n",
      "Epoch 15/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.6387 - accuracy: 0.7284\n",
      "Epoch 16/1500\n",
      "40/40 [==============================] - 0s 719us/step - loss: 0.6189 - accuracy: 0.7388\n",
      "Epoch 17/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.6105 - accuracy: 0.7432\n",
      "Epoch 18/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.6157 - accuracy: 0.7408\n",
      "Epoch 19/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.5998 - accuracy: 0.7416\n",
      "Epoch 20/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.6041 - accuracy: 0.7460\n",
      "Epoch 21/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.5932 - accuracy: 0.7476\n",
      "Epoch 22/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.5863 - accuracy: 0.7528\n",
      "Epoch 23/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.5657 - accuracy: 0.7548\n",
      "Epoch 24/1500\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.5739 - accuracy: 0.7592\n",
      "Epoch 25/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.5562 - accuracy: 0.7604\n",
      "Epoch 26/1500\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.5539 - accuracy: 0.7544\n",
      "Epoch 27/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.5479 - accuracy: 0.7704\n",
      "Epoch 28/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.5380 - accuracy: 0.7664\n",
      "Epoch 29/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.5251 - accuracy: 0.7724\n",
      "Epoch 30/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.5305 - accuracy: 0.7656\n",
      "Epoch 31/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.5332 - accuracy: 0.7784\n",
      "Epoch 32/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.5127 - accuracy: 0.7867\n",
      "Epoch 33/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.5164 - accuracy: 0.7819\n",
      "Epoch 34/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.5093 - accuracy: 0.7883\n",
      "Epoch 35/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.5153 - accuracy: 0.7792\n",
      "Epoch 36/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.5276 - accuracy: 0.7839\n",
      "Epoch 37/1500\n",
      "40/40 [==============================] - -0s 850us/step - loss: 0.4999 - accuracy: 0.7879\n",
      "Epoch 38/1500\n",
      "40/40 [==============================] - 0s 979us/step - loss: 0.5019 - accuracy: 0.7871\n",
      "Epoch 39/1500\n",
      "40/40 [==============================] - 0s 852us/step - loss: 0.4892 - accuracy: 0.7979\n",
      "Epoch 40/1500\n",
      "40/40 [==============================] - 0s 858us/step - loss: 0.4855 - accuracy: 0.7983\n",
      "Epoch 41/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.4920 - accuracy: 0.7959\n",
      "Epoch 42/1500\n",
      "40/40 [==============================] - 0s 864us/step - loss: 0.4806 - accuracy: 0.7879\n",
      "Epoch 43/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.4883 - accuracy: 0.7859\n",
      "Epoch 44/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.4782 - accuracy: 0.7927\n",
      "Epoch 45/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.4772 - accuracy: 0.7955\n",
      "Epoch 46/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.4786 - accuracy: 0.7991\n",
      "Epoch 47/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.4820 - accuracy: 0.8007\n",
      "Epoch 48/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.4793 - accuracy: 0.7919\n",
      "Epoch 49/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.4519 - accuracy: 0.8155\n",
      "Epoch 50/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.4738 - accuracy: 0.7995\n",
      "Epoch 51/1500\n",
      "40/40 [==============================] - 0s 705us/step - loss: 0.4683 - accuracy: 0.8035\n",
      "Epoch 52/1500\n",
      "40/40 [==============================] - 0s 718us/step - loss: 0.4582 - accuracy: 0.8051\n",
      "Epoch 53/1500\n",
      "40/40 [==============================] - 0s 752us/step - loss: 0.4553 - accuracy: 0.8035\n",
      "Epoch 54/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.4674 - accuracy: 0.7935\n",
      "Epoch 55/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.4352 - accuracy: 0.8103\n",
      "Epoch 56/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.4454 - accuracy: 0.8095\n",
      "Epoch 57/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.4337 - accuracy: 0.8167\n",
      "Epoch 58/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.4404 - accuracy: 0.8151\n",
      "Epoch 59/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.4215 - accuracy: 0.8267\n",
      "Epoch 60/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.4149 - accuracy: 0.8231\n",
      "Epoch 61/1500\n",
      "40/40 [==============================] - 0s 720us/step - loss: 0.4369 - accuracy: 0.8171\n",
      "Epoch 62/1500\n",
      "40/40 [==============================] - 0s 736us/step - loss: 0.4419 - accuracy: 0.8107\n",
      "Epoch 63/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.4197 - accuracy: 0.8279\n",
      "Epoch 64/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.4337 - accuracy: 0.8127\n",
      "Epoch 65/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.4355 - accuracy: 0.8103\n",
      "Epoch 66/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.4274 - accuracy: 0.8247\n",
      "Epoch 67/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.4161 - accuracy: 0.8243\n",
      "Epoch 68/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.4048 - accuracy: 0.8295\n",
      "Epoch 69/1500\n",
      "40/40 [==============================] - 0s 935us/step - loss: 0.4159 - accuracy: 0.8207\n",
      "Epoch 70/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8223\n",
      "Epoch 71/1500\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.4153 - accuracy: 0.8263\n",
      "Epoch 72/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.8235\n",
      "Epoch 73/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3988 - accuracy: 0.8343\n",
      "Epoch 74/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8275\n",
      "Epoch 75/1500\n",
      "40/40 [==============================] - 0s 987us/step - loss: 0.3899 - accuracy: 0.8411\n",
      "Epoch 76/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4093 - accuracy: 0.8275\n",
      "Epoch 77/1500\n",
      "40/40 [==============================] - 0s 922us/step - loss: 0.4001 - accuracy: 0.8383\n",
      "Epoch 78/1500\n",
      "40/40 [==============================] - 0s 886us/step - loss: 0.4082 - accuracy: 0.8279\n",
      "Epoch 79/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3980 - accuracy: 0.8327\n",
      "Epoch 80/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.3919 - accuracy: 0.8327\n",
      "Epoch 81/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.8367\n",
      "Epoch 82/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.3842 - accuracy: 0.8395\n",
      "Epoch 83/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3970 - accuracy: 0.8291\n",
      "Epoch 84/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8558\n",
      "Epoch 85/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3844 - accuracy: 0.8391\n",
      "Epoch 86/1500\n",
      "40/40 [==============================] - 0s 945us/step - loss: 0.3963 - accuracy: 0.8315\n",
      "Epoch 87/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.3739 - accuracy: 0.8423\n",
      "Epoch 88/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.3723 - accuracy: 0.8407\n",
      "Epoch 89/1500\n",
      "40/40 [==============================] - 0s 894us/step - loss: 0.3784 - accuracy: 0.8363\n",
      "Epoch 90/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.3989 - accuracy: 0.8315\n",
      "Epoch 91/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.3749 - accuracy: 0.8466\n",
      "Epoch 92/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.3698 - accuracy: 0.8411\n",
      "Epoch 93/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.3649 - accuracy: 0.8462\n",
      "Epoch 94/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.3832 - accuracy: 0.8335\n",
      "Epoch 95/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.3820 - accuracy: 0.8379\n",
      "Epoch 96/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.3853 - accuracy: 0.8367\n",
      "Epoch 97/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.3662 - accuracy: 0.8486\n",
      "Epoch 98/1500\n",
      "40/40 [==============================] - 0s 724us/step - loss: 0.3624 - accuracy: 0.8494\n",
      "Epoch 99/1500\n",
      "40/40 [==============================] - 0s 720us/step - loss: 0.3639 - accuracy: 0.8522\n",
      "Epoch 100/1500\n",
      "40/40 [==============================] - 0s 713us/step - loss: 0.3870 - accuracy: 0.8395\n",
      "Epoch 101/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.3699 - accuracy: 0.8442\n",
      "Epoch 102/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.3556 - accuracy: 0.8534\n",
      "Epoch 103/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.3690 - accuracy: 0.8438\n",
      "Epoch 104/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.3561 - accuracy: 0.8514\n",
      "Epoch 105/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.3641 - accuracy: 0.8399\n",
      "Epoch 106/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8562\n",
      "Epoch 107/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3307 - accuracy: 0.8630\n",
      "Epoch 108/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3568 - accuracy: 0.8578\n",
      "Epoch 109/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8498\n",
      "Epoch 110/1500\n",
      "40/40 [==============================] - 0s 975us/step - loss: 0.3473 - accuracy: 0.8542\n",
      "Epoch 111/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.3335 - accuracy: 0.8618\n",
      "Epoch 112/1500\n",
      "40/40 [==============================] - 0s 883us/step - loss: 0.3385 - accuracy: 0.8638\n",
      "Epoch 113/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.3503 - accuracy: 0.8546\n",
      "Epoch 114/1500\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3341 - accuracy: 0.8662\n",
      "Epoch 115/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3506 - accuracy: 0.8458\n",
      "Epoch 116/1500\n",
      "40/40 [==============================] - 0s 949us/step - loss: 0.3533 - accuracy: 0.8438\n",
      "Epoch 117/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.3441 - accuracy: 0.8494\n",
      "Epoch 118/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3467 - accuracy: 0.8546\n",
      "Epoch 119/1500\n",
      "40/40 [==============================] - 0s 924us/step - loss: 0.3303 - accuracy: 0.8586\n",
      "Epoch 120/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.3369 - accuracy: 0.8634\n",
      "Epoch 121/1500\n",
      "40/40 [==============================] - 0s 982us/step - loss: 0.3345 - accuracy: 0.8666\n",
      "Epoch 122/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8546\n",
      "Epoch 123/1500\n",
      "40/40 [==============================] - 0s 971us/step - loss: 0.3281 - accuracy: 0.8602\n",
      "Epoch 124/1500\n",
      "40/40 [==============================] - 0s 893us/step - loss: 0.3350 - accuracy: 0.8590\n",
      "Epoch 125/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.3349 - accuracy: 0.8602\n",
      "Epoch 126/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.3348 - accuracy: 0.8626\n",
      "Epoch 127/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.3190 - accuracy: 0.8754\n",
      "Epoch 128/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.3373 - accuracy: 0.8658\n",
      "Epoch 129/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.3371 - accuracy: 0.8578\n",
      "Epoch 130/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.3257 - accuracy: 0.8614\n",
      "Epoch 131/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.3392 - accuracy: 0.8630\n",
      "Epoch 132/1500\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.3439 - accuracy: 0.8594\n",
      "Epoch 133/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.3346 - accuracy: 0.8638\n",
      "Epoch 134/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.3002 - accuracy: 0.8718\n",
      "Epoch 135/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.3344 - accuracy: 0.8570\n",
      "Epoch 136/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.3295 - accuracy: 0.8642\n",
      "Epoch 137/1500\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.3130 - accuracy: 0.8686\n",
      "Epoch 138/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.3140 - accuracy: 0.8666\n",
      "Epoch 139/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.3224 - accuracy: 0.8662\n",
      "Epoch 140/1500\n",
      "40/40 [==============================] - 0s 912us/step - loss: 0.3307 - accuracy: 0.8658\n",
      "Epoch 141/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.3103 - accuracy: 0.8766\n",
      "Epoch 142/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.3189 - accuracy: 0.8698\n",
      "Epoch 143/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.3110 - accuracy: 0.8746\n",
      "Epoch 144/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.3090 - accuracy: 0.8722\n",
      "Epoch 145/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.3030 - accuracy: 0.8754\n",
      "Epoch 146/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.3233 - accuracy: 0.8674\n",
      "Epoch 147/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3117 - accuracy: 0.8734\n",
      "Epoch 148/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.3174 - accuracy: 0.8702\n",
      "Epoch 149/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2893 - accuracy: 0.8830\n",
      "Epoch 150/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.3015 - accuracy: 0.8782\n",
      "Epoch 151/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3198 - accuracy: 0.8646\n",
      "Epoch 152/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.3113 - accuracy: 0.8690\n",
      "Epoch 153/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.3170 - accuracy: 0.8742\n",
      "Epoch 154/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.2926 - accuracy: 0.8822\n",
      "Epoch 155/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2942 - accuracy: 0.8830\n",
      "Epoch 156/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.3164 - accuracy: 0.8738\n",
      "Epoch 157/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.8826\n",
      "Epoch 158/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3023 - accuracy: 0.8738\n",
      "Epoch 159/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.3189 - accuracy: 0.8698\n",
      "Epoch 160/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.3010 - accuracy: 0.8718\n",
      "Epoch 161/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2965 - accuracy: 0.8794\n",
      "Epoch 162/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.3065 - accuracy: 0.8738\n",
      "Epoch 163/1500\n",
      "40/40 [==============================] - 0s 702us/step - loss: 0.3077 - accuracy: 0.8778\n",
      "Epoch 164/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.3003 - accuracy: 0.8718\n",
      "Epoch 165/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.2914 - accuracy: 0.8786\n",
      "Epoch 166/1500\n",
      "40/40 [==============================] - 0s 829us/step - loss: 0.2966 - accuracy: 0.8742\n",
      "Epoch 167/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.2942 - accuracy: 0.8882\n",
      "Epoch 168/1500\n",
      "40/40 [==============================] - 0s 707us/step - loss: 0.2886 - accuracy: 0.8826\n",
      "Epoch 169/1500\n",
      "40/40 [==============================] - 0s 713us/step - loss: 0.3019 - accuracy: 0.8754\n",
      "Epoch 170/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2978 - accuracy: 0.8762\n",
      "Epoch 171/1500\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.3021 - accuracy: 0.8782\n",
      "Epoch 172/1500\n",
      "40/40 [==============================] - 0s 712us/step - loss: 0.2990 - accuracy: 0.8794\n",
      "Epoch 173/1500\n",
      "40/40 [==============================] - 0s 722us/step - loss: 0.2949 - accuracy: 0.8782\n",
      "Epoch 174/1500\n",
      "40/40 [==============================] - 0s 723us/step - loss: 0.2776 - accuracy: 0.8834\n",
      "Epoch 175/1500\n",
      "40/40 [==============================] - 0s 729us/step - loss: 0.2955 - accuracy: 0.8786\n",
      "Epoch 176/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.2849 - accuracy: 0.8774\n",
      "Epoch 177/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.2900 - accuracy: 0.8794\n",
      "Epoch 178/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.2985 - accuracy: 0.8782\n",
      "Epoch 179/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.2857 - accuracy: 0.8930\n",
      "Epoch 180/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2816 - accuracy: 0.8814\n",
      "Epoch 181/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.2778 - accuracy: 0.8838\n",
      "Epoch 182/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2761 - accuracy: 0.8926\n",
      "Epoch 183/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2760 - accuracy: 0.8902\n",
      "Epoch 184/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2858 - accuracy: 0.8802\n",
      "Epoch 185/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.2969 - accuracy: 0.8778\n",
      "Epoch 186/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.2782 - accuracy: 0.8930\n",
      "Epoch 187/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.2842 - accuracy: 0.8806\n",
      "Epoch 188/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.3024 - accuracy: 0.8758\n",
      "Epoch 189/1500\n",
      "40/40 [==============================] - 0s 708us/step - loss: 0.2730 - accuracy: 0.8862\n",
      "Epoch 190/1500\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.2840 - accuracy: 0.8862\n",
      "Epoch 191/1500\n",
      "40/40 [==============================] - 0s 695us/step - loss: 0.2730 - accuracy: 0.8994\n",
      "Epoch 192/1500\n",
      "40/40 [==============================] - 0s 725us/step - loss: 0.2798 - accuracy: 0.8818\n",
      "Epoch 193/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.2884 - accuracy: 0.8870\n",
      "Epoch 194/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2759 - accuracy: 0.8838\n",
      "Epoch 195/1500\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.2781 - accuracy: 0.8782\n",
      "Epoch 196/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.2780 - accuracy: 0.8858\n",
      "Epoch 197/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.2765 - accuracy: 0.8946\n",
      "Epoch 198/1500\n",
      "40/40 [==============================] - 0s 846us/step - loss: 0.2599 - accuracy: 0.8986\n",
      "Epoch 199/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.2705 - accuracy: 0.8910\n",
      "Epoch 200/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.2627 - accuracy: 0.8922\n",
      "Epoch 201/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.2735 - accuracy: 0.8850\n",
      "Epoch 202/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.2548 - accuracy: 0.8930\n",
      "Epoch 203/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.2651 - accuracy: 0.8942\n",
      "Epoch 204/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.2664 - accuracy: 0.8890\n",
      "Epoch 205/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.2777 - accuracy: 0.8902\n",
      "Epoch 206/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2580 - accuracy: 0.8890\n",
      "Epoch 207/1500\n",
      "40/40 [==============================] - 0s 964us/step - loss: 0.2608 - accuracy: 0.9010\n",
      "Epoch 208/1500\n",
      "40/40 [==============================] - 0s 739us/step - loss: 0.2863 - accuracy: 0.8830\n",
      "Epoch 209/1500\n",
      "40/40 [==============================] - 0s 893us/step - loss: 0.2605 - accuracy: 0.8954\n",
      "Epoch 210/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2820 - accuracy: 0.8810\n",
      "Epoch 211/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.2532 - accuracy: 0.8946\n",
      "Epoch 212/1500\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.2572 - accuracy: 0.8982\n",
      "Epoch 213/1500\n",
      "40/40 [==============================] - 0s 903us/step - loss: 0.2859 - accuracy: 0.8846\n",
      "Epoch 214/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2692 - accuracy: 0.8914\n",
      "Epoch 215/1500\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.2588 - accuracy: 0.8930\n",
      "Epoch 216/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2787 - accuracy: 0.8842\n",
      "Epoch 217/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.2662 - accuracy: 0.8870\n",
      "Epoch 218/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.2445 - accuracy: 0.8978\n",
      "Epoch 219/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2781 - accuracy: 0.8950\n",
      "Epoch 220/1500\n",
      "40/40 [==============================] - 0s 719us/step - loss: 0.2586 - accuracy: 0.8938\n",
      "Epoch 221/1500\n",
      "40/40 [==============================] - 0s 924us/step - loss: 0.2595 - accuracy: 0.8902\n",
      "Epoch 222/1500\n",
      "40/40 [==============================] - 0s 940us/step - loss: 0.2751 - accuracy: 0.8950\n",
      "Epoch 223/1500\n",
      "40/40 [==============================] - 0s 918us/step - loss: 0.2583 - accuracy: 0.8994\n",
      "Epoch 224/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2725 - accuracy: 0.8958\n",
      "Epoch 225/1500\n",
      "40/40 [==============================] - 0s 906us/step - loss: 0.2465 - accuracy: 0.9026\n",
      "Epoch 226/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.2751 - accuracy: 0.8834\n",
      "Epoch 227/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.2568 - accuracy: 0.8938\n",
      "Epoch 228/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.2713 - accuracy: 0.8918\n",
      "Epoch 229/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.2415 - accuracy: 0.9069\n",
      "Epoch 230/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2559 - accuracy: 0.9002\n",
      "Epoch 231/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.2415 - accuracy: 0.9069\n",
      "Epoch 232/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.2528 - accuracy: 0.9018\n",
      "Epoch 233/1500\n",
      "40/40 [==============================] - 0s 960us/step - loss: 0.2580 - accuracy: 0.8922\n",
      "Epoch 234/1500\n",
      "40/40 [==============================] - 0s 990us/step - loss: 0.2504 - accuracy: 0.9038\n",
      "Epoch 235/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9038\n",
      "Epoch 236/1500\n",
      "40/40 [==============================] - 0s 959us/step - loss: 0.2646 - accuracy: 0.8946\n",
      "Epoch 237/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.2501 - accuracy: 0.8990\n",
      "Epoch 238/1500\n",
      "40/40 [==============================] - 0s 985us/step - loss: 0.2717 - accuracy: 0.8842\n",
      "Epoch 239/1500\n",
      "40/40 [==============================] - 0s 935us/step - loss: 0.2533 - accuracy: 0.9030\n",
      "Epoch 240/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.2592 - accuracy: 0.8918\n",
      "Epoch 241/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2577 - accuracy: 0.8918\n",
      "Epoch 242/1500\n",
      "40/40 [==============================] - 0s 996us/step - loss: 0.2453 - accuracy: 0.8950\n",
      "Epoch 243/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.9030\n",
      "Epoch 244/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.8986\n",
      "Epoch 245/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.8942\n",
      "Epoch 246/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2457 - accuracy: 0.9022\n",
      "Epoch 247/1500\n",
      "40/40 [==============================] - 0s 877us/step - loss: 0.2508 - accuracy: 0.8994\n",
      "Epoch 248/1500\n",
      "40/40 [==============================] - 0s 958us/step - loss: 0.2398 - accuracy: 0.9109\n",
      "Epoch 249/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.8994\n",
      "Epoch 250/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9058\n",
      "Epoch 251/1500\n",
      "40/40 [==============================] - 0s 970us/step - loss: 0.2422 - accuracy: 0.9014\n",
      "Epoch 252/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9050\n",
      "Epoch 253/1500\n",
      "40/40 [==============================] - 0s 987us/step - loss: 0.2467 - accuracy: 0.8986\n",
      "Epoch 254/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.2381 - accuracy: 0.9054\n",
      "Epoch 255/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.2330 - accuracy: 0.9081\n",
      "Epoch 256/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.2480 - accuracy: 0.8982\n",
      "Epoch 257/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.2458 - accuracy: 0.9010\n",
      "Epoch 258/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.2409 - accuracy: 0.8998\n",
      "Epoch 259/1500\n",
      "40/40 [==============================] - 0s 899us/step - loss: 0.2567 - accuracy: 0.9022\n",
      "Epoch 260/1500\n",
      "40/40 [==============================] - 0s 917us/step - loss: 0.2316 - accuracy: 0.9073\n",
      "Epoch 261/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.8998\n",
      "Epoch 262/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9073\n",
      "Epoch 263/1500\n",
      "40/40 [==============================] - 0s 989us/step - loss: 0.2341 - accuracy: 0.9065\n",
      "Epoch 264/1500\n",
      "40/40 [==============================] - 0s 896us/step - loss: 0.2372 - accuracy: 0.9030\n",
      "Epoch 265/1500\n",
      "40/40 [==============================] - 0s 961us/step - loss: 0.2323 - accuracy: 0.9081\n",
      "Epoch 266/1500\n",
      "40/40 [==============================] - 0s 952us/step - loss: 0.2327 - accuracy: 0.9121\n",
      "Epoch 267/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.2378 - accuracy: 0.9034\n",
      "Epoch 268/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.9046\n",
      "Epoch 269/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2354 - accuracy: 0.9097\n",
      "Epoch 270/1500\n",
      "40/40 [==============================] - 0s 880us/step - loss: 0.2361 - accuracy: 0.9014\n",
      "Epoch 271/1500\n",
      "40/40 [==============================] - 0s 989us/step - loss: 0.2370 - accuracy: 0.9062\n",
      "Epoch 272/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.2465 - accuracy: 0.9006\n",
      "Epoch 273/1500\n",
      "40/40 [==============================] - 0s 877us/step - loss: 0.2284 - accuracy: 0.9077\n",
      "Epoch 274/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.2429 - accuracy: 0.9018\n",
      "Epoch 275/1500\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.2354 - accuracy: 0.9081\n",
      "Epoch 276/1500\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.2387 - accuracy: 0.9077\n",
      "Epoch 277/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.9046\n",
      "Epoch 278/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9101\n",
      "Epoch 279/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9121\n",
      "Epoch 280/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.9065\n",
      "Epoch 281/1500\n",
      "40/40 [==============================] - 0s 971us/step - loss: 0.2215 - accuracy: 0.9169\n",
      "Epoch 282/1500\n",
      "40/40 [==============================] - 0s 910us/step - loss: 0.2254 - accuracy: 0.9137\n",
      "Epoch 283/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9073\n",
      "Epoch 284/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2184 - accuracy: 0.9113\n",
      "Epoch 285/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2395 - accuracy: 0.9081\n",
      "Epoch 286/1500\n",
      "40/40 [==============================] - 0s 864us/step - loss: 0.2281 - accuracy: 0.9077\n",
      "Epoch 287/1500\n",
      "40/40 [==============================] - 0s 935us/step - loss: 0.2362 - accuracy: 0.8982\n",
      "Epoch 288/1500\n",
      "40/40 [==============================] - 0s 932us/step - loss: 0.2194 - accuracy: 0.9149\n",
      "Epoch 289/1500\n",
      "40/40 [==============================] - 0s 944us/step - loss: 0.2214 - accuracy: 0.9161\n",
      "Epoch 290/1500\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.2398 - accuracy: 0.9030\n",
      "Epoch 291/1500\n",
      "40/40 [==============================] - 0s 908us/step - loss: 0.2017 - accuracy: 0.9229\n",
      "Epoch 292/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.2310 - accuracy: 0.9073\n",
      "Epoch 293/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2307 - accuracy: 0.9121\n",
      "Epoch 294/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.2209 - accuracy: 0.9097\n",
      "Epoch 295/1500\n",
      "40/40 [==============================] - 0s 883us/step - loss: 0.2407 - accuracy: 0.9062\n",
      "Epoch 296/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.2273 - accuracy: 0.9101\n",
      "Epoch 297/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.2318 - accuracy: 0.9002\n",
      "Epoch 298/1500\n",
      "40/40 [==============================] - 0s 884us/step - loss: 0.2380 - accuracy: 0.9006\n",
      "Epoch 299/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.2270 - accuracy: 0.9105\n",
      "Epoch 300/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.2226 - accuracy: 0.9125\n",
      "Epoch 301/1500\n",
      "40/40 [==============================] - 0s 885us/step - loss: 0.2389 - accuracy: 0.9018\n",
      "Epoch 302/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.2135 - accuracy: 0.9141\n",
      "Epoch 303/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.2188 - accuracy: 0.9161\n",
      "Epoch 304/1500\n",
      "40/40 [==============================] - 0s 852us/step - loss: 0.2295 - accuracy: 0.9089\n",
      "Epoch 305/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.2102 - accuracy: 0.9177\n",
      "Epoch 306/1500\n",
      "40/40 [==============================] - 0s 856us/step - loss: 0.2092 - accuracy: 0.9193\n",
      "Epoch 307/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2191 - accuracy: 0.9205\n",
      "Epoch 308/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2181 - accuracy: 0.9105\n",
      "Epoch 309/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2133 - accuracy: 0.9157\n",
      "Epoch 310/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2151 - accuracy: 0.9117\n",
      "Epoch 311/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2042 - accuracy: 0.9225\n",
      "Epoch 312/1500\n",
      "40/40 [==============================] - 0s 873us/step - loss: 0.2169 - accuracy: 0.9113\n",
      "Epoch 313/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2055 - accuracy: 0.9161\n",
      "Epoch 314/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.2235 - accuracy: 0.9085\n",
      "Epoch 315/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9093\n",
      "Epoch 316/1500\n",
      "40/40 [==============================] - 0s 982us/step - loss: 0.2114 - accuracy: 0.9181\n",
      "Epoch 317/1500\n",
      "40/40 [==============================] - 0s 896us/step - loss: 0.2167 - accuracy: 0.9065\n",
      "Epoch 318/1500\n",
      "40/40 [==============================] - 0s 916us/step - loss: 0.2167 - accuracy: 0.9141\n",
      "Epoch 319/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2086 - accuracy: 0.9153\n",
      "Epoch 320/1500\n",
      "40/40 [==============================] - 0s 936us/step - loss: 0.2249 - accuracy: 0.9109\n",
      "Epoch 321/1500\n",
      "14/40 [=========>....................] - ETA: 0s - loss: 0.1891 - accuracy: 0.9275Restoring model weights from the end of the best epoch: 291.\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2066 - accuracy: 0.9141\n",
      "Epoch 321: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.7639 - accuracy: 0.6863\n",
      "5/5 [==============================] - 0s 815us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.64 (16/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.763889491558075, Accuracy: 0.686274528503418, Precision: 0.6495596205962059, Recall: 0.6608458591217212, F1 Score: 0.6023060796645702\n",
      "Confusion Matrix:\n",
      " [[82  2 32]\n",
      " [10 14  0]\n",
      " [ 4  0  9]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'070A'}\n",
      "Moved to Test Set:\n",
      "{'070A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A'\n",
      " '011A' '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '031A' '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A'\n",
      " '045A' '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '059A' '060A' '062A' '064A' '065A' '069A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A' '039A'\n",
      " '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A' '067A'\n",
      " '068A' '070A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "700\n",
      "Length of y_train_val:\n",
      "700\n",
      "Length of groups_train_val:\n",
      "700\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     443\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     145\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group distribution: Counter({0: 886, 2: 685, 1: 600})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 1s 1ms/step - loss: 1.0153 - accuracy: 0.5348\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8544 - accuracy: 0.6168\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8123 - accuracy: 0.6545\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7818 - accuracy: 0.6624\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.7473 - accuracy: 0.6614\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.7475 - accuracy: 0.6766\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.7173 - accuracy: 0.6845\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 989us/step - loss: 0.7072 - accuracy: 0.6882\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6893 - accuracy: 0.6983\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.7034\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6579 - accuracy: 0.7098\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6411 - accuracy: 0.7250\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 943us/step - loss: 0.6427 - accuracy: 0.7158\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.6451 - accuracy: 0.7153\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.6571 - accuracy: 0.7080\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 0.6264 - accuracy: 0.7333\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.6029 - accuracy: 0.7416\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.5906 - accuracy: 0.7398\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 997us/step - loss: 0.5761 - accuracy: 0.7471\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.5845 - accuracy: 0.7411\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 965us/step - loss: 0.5950 - accuracy: 0.7388\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 945us/step - loss: 0.5999 - accuracy: 0.7379\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.5721 - accuracy: 0.7448\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.5690 - accuracy: 0.7545\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.5675 - accuracy: 0.7490\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.5540 - accuracy: 0.7563\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.5635 - accuracy: 0.7503\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.5578 - accuracy: 0.7605\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.5348 - accuracy: 0.7660\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.5346 - accuracy: 0.7688\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.5410 - accuracy: 0.7605\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.5182 - accuracy: 0.7669\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.5337 - accuracy: 0.7609\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.5258 - accuracy: 0.7715\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.5053 - accuracy: 0.7734\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.5100 - accuracy: 0.7665\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.5038 - accuracy: 0.7830\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.5146 - accuracy: 0.7738\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.5084 - accuracy: 0.7780\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7748\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.7895\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 971us/step - loss: 0.5000 - accuracy: 0.7697\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.4882 - accuracy: 0.7766\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.4782 - accuracy: 0.7959\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.4882 - accuracy: 0.7849\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.4766 - accuracy: 0.7904\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.4896 - accuracy: 0.7807\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.4880 - accuracy: 0.7835\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4733 - accuracy: 0.7987\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.4748 - accuracy: 0.7877\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.4662 - accuracy: 0.7932\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.4516 - accuracy: 0.8088\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.4602 - accuracy: 0.7955\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.4473 - accuracy: 0.8042\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.4442 - accuracy: 0.8070\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 935us/step - loss: 0.4576 - accuracy: 0.7996\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.4523 - accuracy: 0.7932\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.4299 - accuracy: 0.8107\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.4599 - accuracy: 0.7982\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.4451 - accuracy: 0.8033\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 992us/step - loss: 0.4379 - accuracy: 0.8047\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 926us/step - loss: 0.4450 - accuracy: 0.8070\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.4404 - accuracy: 0.8111\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.4368 - accuracy: 0.8079\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.4348 - accuracy: 0.8116\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.4320 - accuracy: 0.8079\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.4384 - accuracy: 0.8139\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.4184 - accuracy: 0.8254\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.4127 - accuracy: 0.8171\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.4300 - accuracy: 0.8107\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.4257 - accuracy: 0.8084\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.4158 - accuracy: 0.8185\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 973us/step - loss: 0.4046 - accuracy: 0.8245\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4148 - accuracy: 0.8222\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.7964\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.4107 - accuracy: 0.8287\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.4138 - accuracy: 0.8088\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.3988 - accuracy: 0.8213\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.3927 - accuracy: 0.8259\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.3929 - accuracy: 0.8277\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.4100 - accuracy: 0.8139\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 963us/step - loss: 0.3925 - accuracy: 0.8185\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.4037 - accuracy: 0.8245\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.3953 - accuracy: 0.8227\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 0.3877 - accuracy: 0.8310\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.4055 - accuracy: 0.8254\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.3863 - accuracy: 0.8369\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 981us/step - loss: 0.3935 - accuracy: 0.8365\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.3773 - accuracy: 0.8356\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.3817 - accuracy: 0.8360\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.3851 - accuracy: 0.8397\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.4002 - accuracy: 0.8208\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.3911 - accuracy: 0.8282\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.3825 - accuracy: 0.8397\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.3839 - accuracy: 0.8305\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.3862 - accuracy: 0.8351\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 917us/step - loss: 0.3657 - accuracy: 0.8392\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.3761 - accuracy: 0.8365\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.3828 - accuracy: 0.8323\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.3686 - accuracy: 0.8388\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.3745 - accuracy: 0.8383\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.3754 - accuracy: 0.8443\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.3653 - accuracy: 0.8475\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.3607 - accuracy: 0.8425\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.3740 - accuracy: 0.8356\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.3560 - accuracy: 0.8448\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.3539 - accuracy: 0.8494\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 971us/step - loss: 0.3495 - accuracy: 0.8462\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.3642 - accuracy: 0.8480\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.3557 - accuracy: 0.8471\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.3580 - accuracy: 0.8415\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.3697 - accuracy: 0.8466\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.3543 - accuracy: 0.8420\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.3508 - accuracy: 0.8521\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.3481 - accuracy: 0.8526\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.3408 - accuracy: 0.8466\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.3437 - accuracy: 0.8623\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.3480 - accuracy: 0.8462\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.3438 - accuracy: 0.8508\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.3350 - accuracy: 0.8604\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.3416 - accuracy: 0.8563\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.3400 - accuracy: 0.8600\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.3322 - accuracy: 0.8572\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.3468 - accuracy: 0.8540\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.3327 - accuracy: 0.8485\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.3360 - accuracy: 0.8540\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.3297 - accuracy: 0.8604\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.3314 - accuracy: 0.8508\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3449 - accuracy: 0.8512\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 963us/step - loss: 0.3251 - accuracy: 0.8591\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.3346 - accuracy: 0.8591\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.3324 - accuracy: 0.8498\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8595\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.3287 - accuracy: 0.8692\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.3273 - accuracy: 0.8623\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.3188 - accuracy: 0.8632\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.3212 - accuracy: 0.8567\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.3112 - accuracy: 0.8761\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.3292 - accuracy: 0.8517\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.8655\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.3165 - accuracy: 0.8660\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8563\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.3154 - accuracy: 0.8646\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.3130 - accuracy: 0.8623\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 926us/step - loss: 0.3002 - accuracy: 0.8770\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.3119 - accuracy: 0.8678\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.3142 - accuracy: 0.8655\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.3181 - accuracy: 0.8710\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.3213 - accuracy: 0.8646\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3179 - accuracy: 0.8632\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.3090 - accuracy: 0.8678\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.3204 - accuracy: 0.8641\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.8733\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8618\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3026 - accuracy: 0.8802\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.3010 - accuracy: 0.8807\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.2944 - accuracy: 0.8862\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 947us/step - loss: 0.2866 - accuracy: 0.8761\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 971us/step - loss: 0.3052 - accuracy: 0.8706\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.2888 - accuracy: 0.8756\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2936 - accuracy: 0.8733\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.2945 - accuracy: 0.8770\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.3024 - accuracy: 0.8766\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2968 - accuracy: 0.8724\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.3035 - accuracy: 0.8687\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2894 - accuracy: 0.8756\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2929 - accuracy: 0.8779\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2886 - accuracy: 0.8821\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.3009 - accuracy: 0.8770\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.2943 - accuracy: 0.8729\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.2702 - accuracy: 0.8881\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.2726 - accuracy: 0.8922\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2926 - accuracy: 0.8766\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2808 - accuracy: 0.8835\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.2791 - accuracy: 0.8784\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8798\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2807 - accuracy: 0.8871\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 975us/step - loss: 0.2740 - accuracy: 0.8881\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 945us/step - loss: 0.2643 - accuracy: 0.8982\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.2852 - accuracy: 0.8839\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.2743 - accuracy: 0.8862\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.2637 - accuracy: 0.8918\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 994us/step - loss: 0.2756 - accuracy: 0.8807\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.2648 - accuracy: 0.8908\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 945us/step - loss: 0.2772 - accuracy: 0.8835\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.2640 - accuracy: 0.8899\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 988us/step - loss: 0.2628 - accuracy: 0.8922\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2867 - accuracy: 0.8839\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 994us/step - loss: 0.2678 - accuracy: 0.8821\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8931\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.2671 - accuracy: 0.8899\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.2777 - accuracy: 0.8871\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.2546 - accuracy: 0.8954\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2595 - accuracy: 0.8867\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8895\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.8922\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.8987\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.8977\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.2538 - accuracy: 0.8977\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.2492 - accuracy: 0.8973\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.2621 - accuracy: 0.8904\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.2449 - accuracy: 0.9074\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 935us/step - loss: 0.2530 - accuracy: 0.8936\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2517 - accuracy: 0.8991\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.8973\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.2606 - accuracy: 0.8918\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.2577 - accuracy: 0.8959\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.2725 - accuracy: 0.8890\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2450 - accuracy: 0.9047\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.2498 - accuracy: 0.8977\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2557 - accuracy: 0.8945\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2495 - accuracy: 0.8954\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.2362 - accuracy: 0.9060\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.2496 - accuracy: 0.8977\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.2406 - accuracy: 0.9014\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.2642 - accuracy: 0.8876\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2577 - accuracy: 0.8936\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2391 - accuracy: 0.9010\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2414 - accuracy: 0.8987\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2505 - accuracy: 0.8945\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2490 - accuracy: 0.9070\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2615 - accuracy: 0.8941\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2311 - accuracy: 0.9047\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2363 - accuracy: 0.9079\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2477 - accuracy: 0.8977\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2504 - accuracy: 0.9033\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.2336 - accuracy: 0.9019\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2231 - accuracy: 0.9134\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2268 - accuracy: 0.9083\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.2431 - accuracy: 0.8987\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.2340 - accuracy: 0.9051\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.2353 - accuracy: 0.9033\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.2430 - accuracy: 0.9028\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.2327 - accuracy: 0.9051\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.2269 - accuracy: 0.9060\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2306 - accuracy: 0.9000\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.2290 - accuracy: 0.9056\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.8977\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.9037\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.2174 - accuracy: 0.9148\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 981us/step - loss: 0.2214 - accuracy: 0.9143\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.2346 - accuracy: 0.9106\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.2348 - accuracy: 0.9074\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2279 - accuracy: 0.9116\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.2229 - accuracy: 0.9120\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2153 - accuracy: 0.9212\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.2166 - accuracy: 0.9088\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.2312 - accuracy: 0.9134\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.2266 - accuracy: 0.9060\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.2212 - accuracy: 0.9125\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 993us/step - loss: 0.2329 - accuracy: 0.9028\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2303 - accuracy: 0.9134\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 995us/step - loss: 0.2129 - accuracy: 0.9152\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9217\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 984us/step - loss: 0.2096 - accuracy: 0.9254\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 981us/step - loss: 0.2218 - accuracy: 0.9129\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2194 - accuracy: 0.9120\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.2043 - accuracy: 0.9235\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.2180 - accuracy: 0.9175\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.2064 - accuracy: 0.9194\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2166 - accuracy: 0.9166\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9129\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.2075 - accuracy: 0.9189\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2197 - accuracy: 0.9106\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.2151 - accuracy: 0.9116\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 926us/step - loss: 0.2180 - accuracy: 0.9120\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.2129 - accuracy: 0.9171\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 984us/step - loss: 0.2026 - accuracy: 0.9208\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.2310 - accuracy: 0.9129\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9263\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 951us/step - loss: 0.2067 - accuracy: 0.9199\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2093 - accuracy: 0.9249\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.2082 - accuracy: 0.9134\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2025 - accuracy: 0.9240\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.2077 - accuracy: 0.9116\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.2160 - accuracy: 0.9120\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1957 - accuracy: 0.9226\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.1982 - accuracy: 0.9226\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1941 - accuracy: 0.9268\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2029 - accuracy: 0.9157\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.2139 - accuracy: 0.9129\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.1967 - accuracy: 0.9185\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9222\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9341\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.2000 - accuracy: 0.9226\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.1904 - accuracy: 0.9231\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.1957 - accuracy: 0.9304\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2083 - accuracy: 0.9162\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.2019 - accuracy: 0.9189\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2158 - accuracy: 0.9125\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2131 - accuracy: 0.9139\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.2116 - accuracy: 0.9120\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.1779 - accuracy: 0.9314\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.2156 - accuracy: 0.9162\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.1986 - accuracy: 0.9194\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2116 - accuracy: 0.9189\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1934 - accuracy: 0.9254\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.1944 - accuracy: 0.9208\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2031 - accuracy: 0.9157\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1825 - accuracy: 0.9346\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.2007 - accuracy: 0.9199\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.2051 - accuracy: 0.9212\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.1866 - accuracy: 0.9258\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9277\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1916 - accuracy: 0.9263\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.1939 - accuracy: 0.9263\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1866 - accuracy: 0.9258\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1995 - accuracy: 0.9208\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1933 - accuracy: 0.9203\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1987 - accuracy: 0.9166\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1910 - accuracy: 0.9272\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.1949 - accuracy: 0.9272\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.1656 - accuracy: 0.9323\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2000 - accuracy: 0.9185\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1796 - accuracy: 0.9268\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1941 - accuracy: 0.9268\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.1753 - accuracy: 0.9318\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.1852 - accuracy: 0.9258\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1969 - accuracy: 0.9245\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1864 - accuracy: 0.9217\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.1750 - accuracy: 0.9337\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1876 - accuracy: 0.9268\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1872 - accuracy: 0.9286\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.1853 - accuracy: 0.9249\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.1855 - accuracy: 0.9263\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.1875 - accuracy: 0.9245\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.1876 - accuracy: 0.9272\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.1938 - accuracy: 0.9240\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1633 - accuracy: 0.9369\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1838 - accuracy: 0.9263\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.1828 - accuracy: 0.9258\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1844 - accuracy: 0.9231\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.1757 - accuracy: 0.9314\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1738 - accuracy: 0.9281\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1974 - accuracy: 0.9231\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.1940 - accuracy: 0.9263\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.1751 - accuracy: 0.9355\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1827 - accuracy: 0.9272\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.1993 - accuracy: 0.9245\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.1724 - accuracy: 0.9351\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.1740 - accuracy: 0.9231\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.1700 - accuracy: 0.9374\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.1812 - accuracy: 0.9258\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.1815 - accuracy: 0.9281\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1629 - accuracy: 0.9346\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.1762 - accuracy: 0.9263\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.1780 - accuracy: 0.9291\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 929us/step - loss: 0.1806 - accuracy: 0.9300\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1755 - accuracy: 0.9351\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1697 - accuracy: 0.9332\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.1767 - accuracy: 0.9341\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.1584 - accuracy: 0.9364\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.1806 - accuracy: 0.9295\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.1609 - accuracy: 0.9360\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1806 - accuracy: 0.9295\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.1694 - accuracy: 0.9355\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1576 - accuracy: 0.9383\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1656 - accuracy: 0.9415\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1617 - accuracy: 0.9415\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1604 - accuracy: 0.9374\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.1734 - accuracy: 0.9323\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.1818 - accuracy: 0.9263\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.1692 - accuracy: 0.9351\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.1883 - accuracy: 0.9208\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1601 - accuracy: 0.9387\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.1734 - accuracy: 0.9341\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1840 - accuracy: 0.9249\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.1709 - accuracy: 0.9429\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9364\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 979us/step - loss: 0.1631 - accuracy: 0.9383\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 998us/step - loss: 0.1634 - accuracy: 0.9392\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 984us/step - loss: 0.1777 - accuracy: 0.9263\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9401\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.1712 - accuracy: 0.9360\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.1771 - accuracy: 0.9291\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.1655 - accuracy: 0.9318\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 945us/step - loss: 0.1635 - accuracy: 0.9424\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9332\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9401\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.1668 - accuracy: 0.9360\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1668 - accuracy: 0.9355\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.1475 - accuracy: 0.9480\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.1592 - accuracy: 0.9429\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.1522 - accuracy: 0.9355\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9318\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9415\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.1548 - accuracy: 0.9378\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 993us/step - loss: 0.1625 - accuracy: 0.9355\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1630 - accuracy: 0.9383\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.1670 - accuracy: 0.9360\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1648 - accuracy: 0.9397\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.1616 - accuracy: 0.9392\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1593 - accuracy: 0.9383\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1670 - accuracy: 0.9327\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.1545 - accuracy: 0.9410\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.1577 - accuracy: 0.9378\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1490 - accuracy: 0.9424\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1645 - accuracy: 0.9401\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1498 - accuracy: 0.9415\n",
      "Epoch 400/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1630 - accuracy: 0.9300\n",
      "Epoch 401/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.1528 - accuracy: 0.9415\n",
      "Epoch 402/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.1397 - accuracy: 0.9512\n",
      "Epoch 403/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.1538 - accuracy: 0.9401\n",
      "Epoch 404/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.1466 - accuracy: 0.9456\n",
      "Epoch 405/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.1540 - accuracy: 0.9364\n",
      "Epoch 406/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.1743 - accuracy: 0.9323\n",
      "Epoch 407/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.1642 - accuracy: 0.9433\n",
      "Epoch 408/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1513 - accuracy: 0.9443\n",
      "Epoch 409/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1588 - accuracy: 0.9420\n",
      "Epoch 410/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1562 - accuracy: 0.9360\n",
      "Epoch 411/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1563 - accuracy: 0.9410\n",
      "Epoch 412/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1431 - accuracy: 0.9507\n",
      "Epoch 413/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.1564 - accuracy: 0.9364\n",
      "Epoch 414/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1521 - accuracy: 0.9415\n",
      "Epoch 415/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1681 - accuracy: 0.9341\n",
      "Epoch 416/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.1534 - accuracy: 0.9420\n",
      "Epoch 417/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1443 - accuracy: 0.9424\n",
      "Epoch 418/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.1600 - accuracy: 0.9346\n",
      "Epoch 419/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1560 - accuracy: 0.9410\n",
      "Epoch 420/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1538 - accuracy: 0.9420\n",
      "Epoch 421/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.1561 - accuracy: 0.9332\n",
      "Epoch 422/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1623 - accuracy: 0.9351\n",
      "Epoch 423/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.1444 - accuracy: 0.9438\n",
      "Epoch 424/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1578 - accuracy: 0.9406\n",
      "Epoch 425/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1492 - accuracy: 0.9387\n",
      "Epoch 426/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1696 - accuracy: 0.9318\n",
      "Epoch 427/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1506 - accuracy: 0.9429\n",
      "Epoch 428/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.1427 - accuracy: 0.9470\n",
      "Epoch 429/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.1559 - accuracy: 0.9447\n",
      "Epoch 430/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1544 - accuracy: 0.9415\n",
      "Epoch 431/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1542 - accuracy: 0.9387\n",
      "Epoch 432/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.2478 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 402.\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.1583 - accuracy: 0.9433\n",
      "Epoch 432: early stopping\n",
      "8/8 [==============================] - 0s 812us/step - loss: 0.9532 - accuracy: 0.6667\n",
      "8/8 [==============================] - 0s 611us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.66 (19/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "Final Test Results - Loss: 0.9531576633453369, Accuracy: 0.6666666865348816, Precision: 0.6924683179723502, Recall: 0.5131944788007717, F1 Score: 0.5356772088226095\n",
      "Confusion Matrix:\n",
      " [[129   2  14]\n",
      " [ 39  12   0]\n",
      " [ 24   0  17]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 902, 1: 810, 2: 505})\n",
      "Epoch 1/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 1.0852 - accuracy: 0.5196\n",
      "Epoch 2/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.9084 - accuracy: 0.5931\n",
      "Epoch 3/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.7607 - accuracy: 0.6779\n",
      "Epoch 4/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.7247 - accuracy: 0.6964\n",
      "Epoch 5/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.6811 - accuracy: 0.7185\n",
      "Epoch 6/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.6532 - accuracy: 0.7163\n",
      "Epoch 7/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.6601 - accuracy: 0.7271\n",
      "Epoch 8/1500\n",
      "35/35 [==============================] - 0s 910us/step - loss: 0.6235 - accuracy: 0.7438\n",
      "Epoch 9/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.5922 - accuracy: 0.7488\n",
      "Epoch 10/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.5932 - accuracy: 0.7609\n",
      "Epoch 11/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.5693 - accuracy: 0.7582\n",
      "Epoch 12/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.5572 - accuracy: 0.7709\n",
      "Epoch 13/1500\n",
      "35/35 [==============================] - 0s 922us/step - loss: 0.5509 - accuracy: 0.7668\n",
      "Epoch 14/1500\n",
      "35/35 [==============================] - 0s 921us/step - loss: 0.5373 - accuracy: 0.7722\n",
      "Epoch 15/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.5390 - accuracy: 0.7821\n",
      "Epoch 16/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.5255 - accuracy: 0.7830\n",
      "Epoch 17/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.4948 - accuracy: 0.7957\n",
      "Epoch 18/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.5120 - accuracy: 0.7903\n",
      "Epoch 19/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.5114 - accuracy: 0.7898\n",
      "Epoch 20/1500\n",
      "35/35 [==============================] - 0s 919us/step - loss: 0.4826 - accuracy: 0.8060\n",
      "Epoch 21/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.4953 - accuracy: 0.7903\n",
      "Epoch 22/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.4901 - accuracy: 0.7853\n",
      "Epoch 23/1500\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.4775 - accuracy: 0.8078\n",
      "Epoch 24/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.4771 - accuracy: 0.8065\n",
      "Epoch 25/1500\n",
      "35/35 [==============================] - 0s 963us/step - loss: 0.4589 - accuracy: 0.8137\n",
      "Epoch 26/1500\n",
      "35/35 [==============================] - 0s 911us/step - loss: 0.4689 - accuracy: 0.8074\n",
      "Epoch 27/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.8029\n",
      "Epoch 28/1500\n",
      "35/35 [==============================] - 0s 972us/step - loss: 0.4537 - accuracy: 0.8047\n",
      "Epoch 29/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.4489 - accuracy: 0.8119\n",
      "Epoch 30/1500\n",
      "35/35 [==============================] - 0s 986us/step - loss: 0.4307 - accuracy: 0.8209\n",
      "Epoch 31/1500\n",
      "35/35 [==============================] - 0s 935us/step - loss: 0.4459 - accuracy: 0.8078\n",
      "Epoch 32/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.4393 - accuracy: 0.8110\n",
      "Epoch 33/1500\n",
      "35/35 [==============================] - 0s 957us/step - loss: 0.4356 - accuracy: 0.8223\n",
      "Epoch 34/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.4104 - accuracy: 0.8300\n",
      "Epoch 35/1500\n",
      "35/35 [==============================] - 0s 902us/step - loss: 0.4053 - accuracy: 0.8331\n",
      "Epoch 36/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.4197 - accuracy: 0.8232\n",
      "Epoch 37/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.4261 - accuracy: 0.8263\n",
      "Epoch 38/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.4231 - accuracy: 0.8200\n",
      "Epoch 39/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.4052 - accuracy: 0.8304\n",
      "Epoch 40/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.4046 - accuracy: 0.8327\n",
      "Epoch 41/1500\n",
      "35/35 [==============================] - 0s 894us/step - loss: 0.4022 - accuracy: 0.8281\n",
      "Epoch 42/1500\n",
      "35/35 [==============================] - 0s 840us/step - loss: 0.3973 - accuracy: 0.8268\n",
      "Epoch 43/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.3937 - accuracy: 0.8245\n",
      "Epoch 44/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.3944 - accuracy: 0.8403\n",
      "Epoch 45/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.3965 - accuracy: 0.8313\n",
      "Epoch 46/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.3783 - accuracy: 0.8462\n",
      "Epoch 47/1500\n",
      "35/35 [==============================] - 0s 923us/step - loss: 0.3840 - accuracy: 0.8412\n",
      "Epoch 48/1500\n",
      "35/35 [==============================] - 0s 918us/step - loss: 0.3950 - accuracy: 0.8390\n",
      "Epoch 49/1500\n",
      "35/35 [==============================] - 0s 946us/step - loss: 0.3771 - accuracy: 0.8444\n",
      "Epoch 50/1500\n",
      "35/35 [==============================] - 0s 931us/step - loss: 0.3657 - accuracy: 0.8498\n",
      "Epoch 51/1500\n",
      "35/35 [==============================] - 0s 978us/step - loss: 0.3840 - accuracy: 0.8426\n",
      "Epoch 52/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3827 - accuracy: 0.8412\n",
      "Epoch 53/1500\n",
      "35/35 [==============================] - 0s 975us/step - loss: 0.3696 - accuracy: 0.8471\n",
      "Epoch 54/1500\n",
      "35/35 [==============================] - 0s 924us/step - loss: 0.3798 - accuracy: 0.8439\n",
      "Epoch 55/1500\n",
      "35/35 [==============================] - 0s 924us/step - loss: 0.3732 - accuracy: 0.8403\n",
      "Epoch 56/1500\n",
      "35/35 [==============================] - 0s 910us/step - loss: 0.3673 - accuracy: 0.8525\n",
      "Epoch 57/1500\n",
      "35/35 [==============================] - 0s 915us/step - loss: 0.3632 - accuracy: 0.8539\n",
      "Epoch 58/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3567 - accuracy: 0.8539\n",
      "Epoch 59/1500\n",
      "35/35 [==============================] - 0s 996us/step - loss: 0.3570 - accuracy: 0.8507\n",
      "Epoch 60/1500\n",
      "35/35 [==============================] - 0s 915us/step - loss: 0.3579 - accuracy: 0.8521\n",
      "Epoch 61/1500\n",
      "35/35 [==============================] - 0s 928us/step - loss: 0.3494 - accuracy: 0.8480\n",
      "Epoch 62/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.3449 - accuracy: 0.8642\n",
      "Epoch 63/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.3602 - accuracy: 0.8516\n",
      "Epoch 64/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.3545 - accuracy: 0.8561\n",
      "Epoch 65/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.3312 - accuracy: 0.8597\n",
      "Epoch 66/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.3474 - accuracy: 0.8579\n",
      "Epoch 67/1500\n",
      "35/35 [==============================] - 0s 896us/step - loss: 0.3348 - accuracy: 0.8683\n",
      "Epoch 68/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.3351 - accuracy: 0.8633\n",
      "Epoch 69/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.3321 - accuracy: 0.8629\n",
      "Epoch 70/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.3370 - accuracy: 0.8570\n",
      "Epoch 71/1500\n",
      "35/35 [==============================] - 0s 878us/step - loss: 0.3412 - accuracy: 0.8579\n",
      "Epoch 72/1500\n",
      "35/35 [==============================] - 0s 922us/step - loss: 0.3447 - accuracy: 0.8570\n",
      "Epoch 73/1500\n",
      "35/35 [==============================] - 0s 958us/step - loss: 0.3295 - accuracy: 0.8660\n",
      "Epoch 74/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.8687\n",
      "Epoch 75/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.8466\n",
      "Epoch 76/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8611\n",
      "Epoch 77/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8737\n",
      "Epoch 78/1500\n",
      "35/35 [==============================] - 0s 911us/step - loss: 0.3260 - accuracy: 0.8611\n",
      "Epoch 79/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.3289 - accuracy: 0.8633\n",
      "Epoch 80/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.3201 - accuracy: 0.8728\n",
      "Epoch 81/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.3348 - accuracy: 0.8660\n",
      "Epoch 82/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.3324 - accuracy: 0.8647\n",
      "Epoch 83/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3282 - accuracy: 0.8642\n",
      "Epoch 84/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.3012 - accuracy: 0.8705\n",
      "Epoch 85/1500\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.3122 - accuracy: 0.8769\n",
      "Epoch 86/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8764\n",
      "Epoch 87/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8841\n",
      "Epoch 88/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.2979 - accuracy: 0.8755\n",
      "Epoch 89/1500\n",
      "35/35 [==============================] - 0s 924us/step - loss: 0.3147 - accuracy: 0.8674\n",
      "Epoch 90/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3266 - accuracy: 0.8629\n",
      "Epoch 91/1500\n",
      "35/35 [==============================] - 0s 975us/step - loss: 0.3170 - accuracy: 0.8742\n",
      "Epoch 92/1500\n",
      "35/35 [==============================] - 0s 935us/step - loss: 0.2817 - accuracy: 0.8854\n",
      "Epoch 93/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.3021 - accuracy: 0.8773\n",
      "Epoch 94/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2958 - accuracy: 0.8818\n",
      "Epoch 95/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.3006 - accuracy: 0.8742\n",
      "Epoch 96/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.3070 - accuracy: 0.8755\n",
      "Epoch 97/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.3069 - accuracy: 0.8737\n",
      "Epoch 98/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.2903 - accuracy: 0.8769\n",
      "Epoch 99/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.2978 - accuracy: 0.8809\n",
      "Epoch 100/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.2877 - accuracy: 0.8872\n",
      "Epoch 101/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.2814 - accuracy: 0.8899\n",
      "Epoch 102/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.3004 - accuracy: 0.8809\n",
      "Epoch 103/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2946 - accuracy: 0.8787\n",
      "Epoch 104/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.2984 - accuracy: 0.8714\n",
      "Epoch 105/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2849 - accuracy: 0.8872\n",
      "Epoch 106/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.2942 - accuracy: 0.8742\n",
      "Epoch 107/1500\n",
      "35/35 [==============================] - 0s 873us/step - loss: 0.2778 - accuracy: 0.8886\n",
      "Epoch 108/1500\n",
      "35/35 [==============================] - 0s 882us/step - loss: 0.2844 - accuracy: 0.8809\n",
      "Epoch 109/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.2713 - accuracy: 0.8850\n",
      "Epoch 110/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.2862 - accuracy: 0.8863\n",
      "Epoch 111/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.2752 - accuracy: 0.8751\n",
      "Epoch 112/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.2856 - accuracy: 0.8886\n",
      "Epoch 113/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.2945 - accuracy: 0.8859\n",
      "Epoch 114/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.2709 - accuracy: 0.8890\n",
      "Epoch 115/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.2797 - accuracy: 0.8868\n",
      "Epoch 116/1500\n",
      "35/35 [==============================] - 0s 907us/step - loss: 0.2691 - accuracy: 0.8958\n",
      "Epoch 117/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.2770 - accuracy: 0.8823\n",
      "Epoch 118/1500\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.2931 - accuracy: 0.8742\n",
      "Epoch 119/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2815 - accuracy: 0.8886\n",
      "Epoch 120/1500\n",
      "35/35 [==============================] - 0s 886us/step - loss: 0.2769 - accuracy: 0.8836\n",
      "Epoch 121/1500\n",
      "35/35 [==============================] - 0s 971us/step - loss: 0.2765 - accuracy: 0.8881\n",
      "Epoch 122/1500\n",
      "35/35 [==============================] - 0s 889us/step - loss: 0.2605 - accuracy: 0.8935\n",
      "Epoch 123/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2666 - accuracy: 0.8904\n",
      "Epoch 124/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.2740 - accuracy: 0.8832\n",
      "Epoch 125/1500\n",
      "35/35 [==============================] - 0s 918us/step - loss: 0.2650 - accuracy: 0.8931\n",
      "Epoch 126/1500\n",
      "35/35 [==============================] - 0s 921us/step - loss: 0.2519 - accuracy: 0.8895\n",
      "Epoch 127/1500\n",
      "35/35 [==============================] - 0s 913us/step - loss: 0.2565 - accuracy: 0.8990\n",
      "Epoch 128/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.2529 - accuracy: 0.9071\n",
      "Epoch 129/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.2624 - accuracy: 0.8926\n",
      "Epoch 130/1500\n",
      "35/35 [==============================] - 0s 901us/step - loss: 0.2526 - accuracy: 0.9021\n",
      "Epoch 131/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.2565 - accuracy: 0.8972\n",
      "Epoch 132/1500\n",
      "35/35 [==============================] - 0s 906us/step - loss: 0.2551 - accuracy: 0.9021\n",
      "Epoch 133/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.2574 - accuracy: 0.8958\n",
      "Epoch 134/1500\n",
      "35/35 [==============================] - 0s 883us/step - loss: 0.2405 - accuracy: 0.9030\n",
      "Epoch 135/1500\n",
      "35/35 [==============================] - 0s 882us/step - loss: 0.2673 - accuracy: 0.8967\n",
      "Epoch 136/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.2499 - accuracy: 0.8976\n",
      "Epoch 137/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.2465 - accuracy: 0.9039\n",
      "Epoch 138/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.2414 - accuracy: 0.8999\n",
      "Epoch 139/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.2564 - accuracy: 0.8963\n",
      "Epoch 140/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.2421 - accuracy: 0.9026\n",
      "Epoch 141/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2494 - accuracy: 0.9008\n",
      "Epoch 142/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.2428 - accuracy: 0.8981\n",
      "Epoch 143/1500\n",
      "35/35 [==============================] - 0s 907us/step - loss: 0.2378 - accuracy: 0.9044\n",
      "Epoch 144/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.2573 - accuracy: 0.8954\n",
      "Epoch 145/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.2335 - accuracy: 0.9071\n",
      "Epoch 146/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.2418 - accuracy: 0.9057\n",
      "Epoch 147/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.2520 - accuracy: 0.8935\n",
      "Epoch 148/1500\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.2458 - accuracy: 0.9012\n",
      "Epoch 149/1500\n",
      "35/35 [==============================] - 0s 950us/step - loss: 0.2411 - accuracy: 0.9030\n",
      "Epoch 150/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.2257 - accuracy: 0.9093\n",
      "Epoch 151/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.2477 - accuracy: 0.9021\n",
      "Epoch 152/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.2315 - accuracy: 0.9129\n",
      "Epoch 153/1500\n",
      "35/35 [==============================] - 0s 858us/step - loss: 0.2459 - accuracy: 0.9017\n",
      "Epoch 154/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.2452 - accuracy: 0.9008\n",
      "Epoch 155/1500\n",
      "35/35 [==============================] - 0s 883us/step - loss: 0.2294 - accuracy: 0.9111\n",
      "Epoch 156/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.2377 - accuracy: 0.8981\n",
      "Epoch 157/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.2504 - accuracy: 0.8922\n",
      "Epoch 158/1500\n",
      "35/35 [==============================] - 0s 892us/step - loss: 0.2379 - accuracy: 0.9035\n",
      "Epoch 159/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.2461 - accuracy: 0.9039\n",
      "Epoch 160/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.2385 - accuracy: 0.9062\n",
      "Epoch 161/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.2242 - accuracy: 0.9125\n",
      "Epoch 162/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.2328 - accuracy: 0.9021\n",
      "Epoch 163/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.2256 - accuracy: 0.9098\n",
      "Epoch 164/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2399 - accuracy: 0.9008\n",
      "Epoch 165/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.2340 - accuracy: 0.9107\n",
      "Epoch 166/1500\n",
      "35/35 [==============================] - 0s 858us/step - loss: 0.2445 - accuracy: 0.9066\n",
      "Epoch 167/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.2271 - accuracy: 0.9098\n",
      "Epoch 168/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2314 - accuracy: 0.9120\n",
      "Epoch 169/1500\n",
      "35/35 [==============================] - 0s 959us/step - loss: 0.2108 - accuracy: 0.9184\n",
      "Epoch 170/1500\n",
      "35/35 [==============================] - 0s 943us/step - loss: 0.2292 - accuracy: 0.8999\n",
      "Epoch 171/1500\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.2287 - accuracy: 0.9116\n",
      "Epoch 172/1500\n",
      "35/35 [==============================] - 0s 940us/step - loss: 0.2186 - accuracy: 0.9125\n",
      "Epoch 173/1500\n",
      "35/35 [==============================] - 0s 981us/step - loss: 0.2203 - accuracy: 0.9242\n",
      "Epoch 174/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9197\n",
      "Epoch 175/1500\n",
      "35/35 [==============================] - 0s 971us/step - loss: 0.2062 - accuracy: 0.9197\n",
      "Epoch 176/1500\n",
      "35/35 [==============================] - 0s 941us/step - loss: 0.2309 - accuracy: 0.9048\n",
      "Epoch 177/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.2300 - accuracy: 0.9089\n",
      "Epoch 178/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.2346 - accuracy: 0.9084\n",
      "Epoch 179/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.2341 - accuracy: 0.9080\n",
      "Epoch 180/1500\n",
      "35/35 [==============================] - 0s 902us/step - loss: 0.2194 - accuracy: 0.9093\n",
      "Epoch 181/1500\n",
      "35/35 [==============================] - 0s 924us/step - loss: 0.2180 - accuracy: 0.9157\n",
      "Epoch 182/1500\n",
      "35/35 [==============================] - 0s 936us/step - loss: 0.2298 - accuracy: 0.9098\n",
      "Epoch 183/1500\n",
      "35/35 [==============================] - 0s 889us/step - loss: 0.2130 - accuracy: 0.9170\n",
      "Epoch 184/1500\n",
      "35/35 [==============================] - 0s 949us/step - loss: 0.2265 - accuracy: 0.9089\n",
      "Epoch 185/1500\n",
      "35/35 [==============================] - 0s 981us/step - loss: 0.2322 - accuracy: 0.9057\n",
      "Epoch 186/1500\n",
      "35/35 [==============================] - 0s 992us/step - loss: 0.2197 - accuracy: 0.9157\n",
      "Epoch 187/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.2059 - accuracy: 0.9161\n",
      "Epoch 188/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.2138 - accuracy: 0.9138\n",
      "Epoch 189/1500\n",
      "35/35 [==============================] - 0s 896us/step - loss: 0.2061 - accuracy: 0.9206\n",
      "Epoch 190/1500\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.2279 - accuracy: 0.9057\n",
      "Epoch 191/1500\n",
      "35/35 [==============================] - 0s 852us/step - loss: 0.2219 - accuracy: 0.9102\n",
      "Epoch 192/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2136 - accuracy: 0.9138\n",
      "Epoch 193/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2159 - accuracy: 0.9152\n",
      "Epoch 194/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.2169 - accuracy: 0.9161\n",
      "Epoch 195/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.2064 - accuracy: 0.9251\n",
      "Epoch 196/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2090 - accuracy: 0.9206\n",
      "Epoch 197/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.2083 - accuracy: 0.9188\n",
      "Epoch 198/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.2099 - accuracy: 0.9175\n",
      "Epoch 199/1500\n",
      "35/35 [==============================] - 0s 985us/step - loss: 0.2166 - accuracy: 0.9157\n",
      "Epoch 200/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9179\n",
      "Epoch 201/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9125\n",
      "Epoch 202/1500\n",
      "35/35 [==============================] - 0s 898us/step - loss: 0.2187 - accuracy: 0.9211\n",
      "Epoch 203/1500\n",
      "35/35 [==============================] - 0s 908us/step - loss: 0.1879 - accuracy: 0.9215\n",
      "Epoch 204/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.2095 - accuracy: 0.9179\n",
      "Epoch 205/1500\n",
      "35/35 [==============================] - 0s 840us/step - loss: 0.2064 - accuracy: 0.9247\n",
      "Epoch 206/1500\n",
      "35/35 [==============================] - 0s 900us/step - loss: 0.2059 - accuracy: 0.9188\n",
      "Epoch 207/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2124 - accuracy: 0.9184\n",
      "Epoch 208/1500\n",
      "35/35 [==============================] - 0s 904us/step - loss: 0.1921 - accuracy: 0.9193\n",
      "Epoch 209/1500\n",
      "35/35 [==============================] - 0s 893us/step - loss: 0.1992 - accuracy: 0.9247\n",
      "Epoch 210/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.2162 - accuracy: 0.9116\n",
      "Epoch 211/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2002 - accuracy: 0.9224\n",
      "Epoch 212/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.1903 - accuracy: 0.9296\n",
      "Epoch 213/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.2017 - accuracy: 0.9206\n",
      "Epoch 214/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.1919 - accuracy: 0.9193\n",
      "Epoch 215/1500\n",
      "35/35 [==============================] - 0s 840us/step - loss: 0.1983 - accuracy: 0.9251\n",
      "Epoch 216/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.1976 - accuracy: 0.9197\n",
      "Epoch 217/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.1914 - accuracy: 0.9265\n",
      "Epoch 218/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1925 - accuracy: 0.9224\n",
      "Epoch 219/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.1968 - accuracy: 0.9274\n",
      "Epoch 220/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2067 - accuracy: 0.9161\n",
      "Epoch 221/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.2120 - accuracy: 0.9211\n",
      "Epoch 222/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.2059 - accuracy: 0.9215\n",
      "Epoch 223/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.1915 - accuracy: 0.9238\n",
      "Epoch 224/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.1962 - accuracy: 0.9197\n",
      "Epoch 225/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.1896 - accuracy: 0.9332\n",
      "Epoch 226/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.1900 - accuracy: 0.9202\n",
      "Epoch 227/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.1986 - accuracy: 0.9206\n",
      "Epoch 228/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.1950 - accuracy: 0.9260\n",
      "Epoch 229/1500\n",
      "35/35 [==============================] - 0s 885us/step - loss: 0.1809 - accuracy: 0.9332\n",
      "Epoch 230/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.1918 - accuracy: 0.9278\n",
      "Epoch 231/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.1957 - accuracy: 0.9224\n",
      "Epoch 232/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1999 - accuracy: 0.9260\n",
      "Epoch 233/1500\n",
      "35/35 [==============================] - 0s 912us/step - loss: 0.1824 - accuracy: 0.9323\n",
      "Epoch 234/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.1947 - accuracy: 0.9251\n",
      "Epoch 235/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.1852 - accuracy: 0.9238\n",
      "Epoch 236/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.1731 - accuracy: 0.9310\n",
      "Epoch 237/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1892 - accuracy: 0.9247\n",
      "Epoch 238/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1816 - accuracy: 0.9287\n",
      "Epoch 239/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.1831 - accuracy: 0.9269\n",
      "Epoch 240/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.1847 - accuracy: 0.9283\n",
      "Epoch 241/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.2004 - accuracy: 0.9260\n",
      "Epoch 242/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.1745 - accuracy: 0.9364\n",
      "Epoch 243/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.1801 - accuracy: 0.9296\n",
      "Epoch 244/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.1937 - accuracy: 0.9242\n",
      "Epoch 245/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.1731 - accuracy: 0.9314\n",
      "Epoch 246/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.1952 - accuracy: 0.9202\n",
      "Epoch 247/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.1744 - accuracy: 0.9323\n",
      "Epoch 248/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.1786 - accuracy: 0.9332\n",
      "Epoch 249/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.1810 - accuracy: 0.9274\n",
      "Epoch 250/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1826 - accuracy: 0.9310\n",
      "Epoch 251/1500\n",
      "35/35 [==============================] - 0s 892us/step - loss: 0.1720 - accuracy: 0.9337\n",
      "Epoch 252/1500\n",
      "35/35 [==============================] - 0s 898us/step - loss: 0.1799 - accuracy: 0.9287\n",
      "Epoch 253/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.1755 - accuracy: 0.9350\n",
      "Epoch 254/1500\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.1876 - accuracy: 0.9269\n",
      "Epoch 255/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9369\n",
      "Epoch 256/1500\n",
      "35/35 [==============================] - 0s 963us/step - loss: 0.1717 - accuracy: 0.9355\n",
      "Epoch 257/1500\n",
      "35/35 [==============================] - 0s 898us/step - loss: 0.1771 - accuracy: 0.9359\n",
      "Epoch 258/1500\n",
      "35/35 [==============================] - 0s 885us/step - loss: 0.1757 - accuracy: 0.9319\n",
      "Epoch 259/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.1771 - accuracy: 0.9355\n",
      "Epoch 260/1500\n",
      "35/35 [==============================] - 0s 898us/step - loss: 0.1740 - accuracy: 0.9323\n",
      "Epoch 261/1500\n",
      "35/35 [==============================] - 0s 896us/step - loss: 0.1898 - accuracy: 0.9224\n",
      "Epoch 262/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.1704 - accuracy: 0.9305\n",
      "Epoch 263/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.1699 - accuracy: 0.9332\n",
      "Epoch 264/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.1841 - accuracy: 0.9319\n",
      "Epoch 265/1500\n",
      "35/35 [==============================] - 0s 949us/step - loss: 0.1792 - accuracy: 0.9296\n",
      "Epoch 266/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.1872 - accuracy: 0.9278\n",
      "Epoch 267/1500\n",
      "35/35 [==============================] - 0s 954us/step - loss: 0.1646 - accuracy: 0.9373\n",
      "Epoch 268/1500\n",
      "35/35 [==============================] - 0s 934us/step - loss: 0.1563 - accuracy: 0.9423\n",
      "Epoch 269/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.1617 - accuracy: 0.9373\n",
      "Epoch 270/1500\n",
      "35/35 [==============================] - 0s 931us/step - loss: 0.1631 - accuracy: 0.9337\n",
      "Epoch 271/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.1715 - accuracy: 0.9341\n",
      "Epoch 272/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.1529 - accuracy: 0.9396\n",
      "Epoch 273/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.1776 - accuracy: 0.9364\n",
      "Epoch 274/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1725 - accuracy: 0.9314\n",
      "Epoch 275/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.1635 - accuracy: 0.9355\n",
      "Epoch 276/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.1696 - accuracy: 0.9305\n",
      "Epoch 277/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.1772 - accuracy: 0.9346\n",
      "Epoch 278/1500\n",
      "35/35 [==============================] - 0s 885us/step - loss: 0.1677 - accuracy: 0.9378\n",
      "Epoch 279/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1741 - accuracy: 0.9301\n",
      "Epoch 280/1500\n",
      "35/35 [==============================] - 0s 932us/step - loss: 0.1716 - accuracy: 0.9359\n",
      "Epoch 281/1500\n",
      "35/35 [==============================] - 0s 896us/step - loss: 0.1672 - accuracy: 0.9346\n",
      "Epoch 282/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.1598 - accuracy: 0.9323\n",
      "Epoch 283/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.1666 - accuracy: 0.9396\n",
      "Epoch 284/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.1637 - accuracy: 0.9418\n",
      "Epoch 285/1500\n",
      "35/35 [==============================] - 0s 858us/step - loss: 0.1691 - accuracy: 0.9341\n",
      "Epoch 286/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1639 - accuracy: 0.9369\n",
      "Epoch 287/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.1618 - accuracy: 0.9350\n",
      "Epoch 288/1500\n",
      "35/35 [==============================] - 0s 920us/step - loss: 0.1488 - accuracy: 0.9409\n",
      "Epoch 289/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.1623 - accuracy: 0.9405\n",
      "Epoch 290/1500\n",
      "35/35 [==============================] - 0s 973us/step - loss: 0.1812 - accuracy: 0.9328\n",
      "Epoch 291/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.1698 - accuracy: 0.9337\n",
      "Epoch 292/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1520 - accuracy: 0.9359\n",
      "Epoch 293/1500\n",
      "35/35 [==============================] - 0s 907us/step - loss: 0.1595 - accuracy: 0.9423\n",
      "Epoch 294/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1647 - accuracy: 0.9341\n",
      "Epoch 295/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.1642 - accuracy: 0.9350\n",
      "Epoch 296/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.1713 - accuracy: 0.9364\n",
      "Epoch 297/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.1576 - accuracy: 0.9409\n",
      "Epoch 298/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1611 - accuracy: 0.9423\n",
      "Epoch 299/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.1516 - accuracy: 0.9418\n",
      "Epoch 300/1500\n",
      "35/35 [==============================] - 0s 954us/step - loss: 0.1619 - accuracy: 0.9450\n",
      "Epoch 301/1500\n",
      "35/35 [==============================] - 0s 939us/step - loss: 0.1548 - accuracy: 0.9445\n",
      "Epoch 302/1500\n",
      "35/35 [==============================] - 0s 957us/step - loss: 0.1543 - accuracy: 0.9441\n",
      "Epoch 303/1500\n",
      "35/35 [==============================] - 0s 900us/step - loss: 0.1754 - accuracy: 0.9323\n",
      "Epoch 304/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.1492 - accuracy: 0.9468\n",
      "Epoch 305/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.1554 - accuracy: 0.9445\n",
      "Epoch 306/1500\n",
      "35/35 [==============================] - 0s 987us/step - loss: 0.1654 - accuracy: 0.9314\n",
      "Epoch 307/1500\n",
      "35/35 [==============================] - 0s 949us/step - loss: 0.1527 - accuracy: 0.9436\n",
      "Epoch 308/1500\n",
      "35/35 [==============================] - 0s 958us/step - loss: 0.1637 - accuracy: 0.9346\n",
      "Epoch 309/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.1646 - accuracy: 0.9328\n",
      "Epoch 310/1500\n",
      "35/35 [==============================] - 0s 979us/step - loss: 0.1617 - accuracy: 0.9378\n",
      "Epoch 311/1500\n",
      "35/35 [==============================] - 0s 915us/step - loss: 0.1686 - accuracy: 0.9364\n",
      "Epoch 312/1500\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.1496 - accuracy: 0.9423\n",
      "Epoch 313/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9391\n",
      "Epoch 314/1500\n",
      "35/35 [==============================] - 0s 984us/step - loss: 0.1510 - accuracy: 0.9472\n",
      "Epoch 315/1500\n",
      "35/35 [==============================] - 0s 939us/step - loss: 0.1688 - accuracy: 0.9387\n",
      "Epoch 316/1500\n",
      "35/35 [==============================] - 0s 972us/step - loss: 0.1646 - accuracy: 0.9283\n",
      "Epoch 317/1500\n",
      "35/35 [==============================] - 0s 962us/step - loss: 0.1600 - accuracy: 0.9341\n",
      "Epoch 318/1500\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.1245 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 288.\n",
      "35/35 [==============================] - 0s 982us/step - loss: 0.1535 - accuracy: 0.9409\n",
      "Epoch 318: early stopping\n",
      "7/7 [==============================] - 0s 939us/step - loss: 0.9228 - accuracy: 0.6278\n",
      "7/7 [==============================] - 0s 693us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.70 (21/30)\n",
      "Before appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 0.9228075742721558, Accuracy: 0.6278026700019836, Precision: 0.5858451536643026, Recall: 0.7146332985749044, F1 Score: 0.6288581442538276\n",
      "Confusion Matrix:\n",
      " [[98  6 33]\n",
      " [ 0  9  0]\n",
      " [43  1 33]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 864, 1: 730, 2: 655})\n",
      "Epoch 1/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.4211 - accuracy: 0.4042\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.0461 - accuracy: 0.5353\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.9233 - accuracy: 0.5989\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.8551 - accuracy: 0.6385\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.8300 - accuracy: 0.6443\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.7788 - accuracy: 0.6665\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 986us/step - loss: 0.7541 - accuracy: 0.6727\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.7398 - accuracy: 0.6763\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.7351 - accuracy: 0.6923\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.6990 - accuracy: 0.6963\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.6942 - accuracy: 0.7012\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.6839 - accuracy: 0.7016\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 882us/step - loss: 0.6694 - accuracy: 0.7177\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.6780 - accuracy: 0.7105\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.6380 - accuracy: 0.7212\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.6368 - accuracy: 0.7239\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.6185 - accuracy: 0.7257\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.6184 - accuracy: 0.7390\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.6079 - accuracy: 0.7399\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 893us/step - loss: 0.6004 - accuracy: 0.7332\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.6090 - accuracy: 0.7399\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.5819 - accuracy: 0.7488\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 0.5899 - accuracy: 0.7319\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 915us/step - loss: 0.5582 - accuracy: 0.7643\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 927us/step - loss: 0.5867 - accuracy: 0.7541\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.5422 - accuracy: 0.7599\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.5456 - accuracy: 0.7719\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 953us/step - loss: 0.5597 - accuracy: 0.7550\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 890us/step - loss: 0.5451 - accuracy: 0.7590\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.5410 - accuracy: 0.7675\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.5358 - accuracy: 0.7723\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.5068 - accuracy: 0.7790\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.5371 - accuracy: 0.7652\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 927us/step - loss: 0.5239 - accuracy: 0.7763\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.5157 - accuracy: 0.7799\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 0.5017 - accuracy: 0.7866\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.5072 - accuracy: 0.7839\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.4946 - accuracy: 0.7826\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.5024 - accuracy: 0.7866\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.5027 - accuracy: 0.7866\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.4898 - accuracy: 0.7946\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.4902 - accuracy: 0.7937\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.4887 - accuracy: 0.7843\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.4921 - accuracy: 0.7897\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 889us/step - loss: 0.4864 - accuracy: 0.7924\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.4967 - accuracy: 0.7817\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.4725 - accuracy: 0.7959\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.5091 - accuracy: 0.7843\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - 0s 938us/step - loss: 0.4668 - accuracy: 0.7981\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.4685 - accuracy: 0.7977\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.4442 - accuracy: 0.8155\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 889us/step - loss: 0.4690 - accuracy: 0.8070\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 915us/step - loss: 0.4714 - accuracy: 0.7999\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.4557 - accuracy: 0.8133\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.4670 - accuracy: 0.8057\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 927us/step - loss: 0.4354 - accuracy: 0.8195\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.4412 - accuracy: 0.8106\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.4425 - accuracy: 0.8155\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.4277 - accuracy: 0.8141\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 916us/step - loss: 0.4403 - accuracy: 0.8039\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.4322 - accuracy: 0.8101\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.4323 - accuracy: 0.8181\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 901us/step - loss: 0.4253 - accuracy: 0.8079\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.4322 - accuracy: 0.8115\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.4342 - accuracy: 0.8155\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.4184 - accuracy: 0.8284\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.4357 - accuracy: 0.8164\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4237 - accuracy: 0.8141\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8199\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 998us/step - loss: 0.4303 - accuracy: 0.8141\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.3998 - accuracy: 0.8301\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.4174 - accuracy: 0.8270\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.4130 - accuracy: 0.8204\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.4014 - accuracy: 0.8333\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.4229 - accuracy: 0.8159\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 905us/step - loss: 0.3883 - accuracy: 0.8364\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.4115 - accuracy: 0.8230\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.3986 - accuracy: 0.8359\n",
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.4075 - accuracy: 0.8275\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.4007 - accuracy: 0.8279\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 893us/step - loss: 0.3931 - accuracy: 0.8346\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.3930 - accuracy: 0.8306\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.3837 - accuracy: 0.8377\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.3880 - accuracy: 0.8426\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.3929 - accuracy: 0.8395\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.3719 - accuracy: 0.8453\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.3863 - accuracy: 0.8484\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.3761 - accuracy: 0.8382\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 916us/step - loss: 0.4141 - accuracy: 0.8235\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 893us/step - loss: 0.3870 - accuracy: 0.8350\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 925us/step - loss: 0.3775 - accuracy: 0.8453\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 980us/step - loss: 0.3758 - accuracy: 0.8448\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 994us/step - loss: 0.3739 - accuracy: 0.8386\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8417\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3651 - accuracy: 0.8475\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.3857 - accuracy: 0.8435\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3594 - accuracy: 0.8470\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.3619 - accuracy: 0.8493\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 905us/step - loss: 0.3912 - accuracy: 0.8359\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 911us/step - loss: 0.3632 - accuracy: 0.8462\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.3637 - accuracy: 0.8475\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.3696 - accuracy: 0.8404\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 921us/step - loss: 0.3707 - accuracy: 0.8470\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 968us/step - loss: 0.3642 - accuracy: 0.8488\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 938us/step - loss: 0.3507 - accuracy: 0.8582\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 945us/step - loss: 0.3417 - accuracy: 0.8497\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.3443 - accuracy: 0.8595\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.3529 - accuracy: 0.8577\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.3561 - accuracy: 0.8519\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.3521 - accuracy: 0.8515\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.3473 - accuracy: 0.8564\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.3555 - accuracy: 0.8555\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 953us/step - loss: 0.3416 - accuracy: 0.8506\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 921us/step - loss: 0.3323 - accuracy: 0.8688\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.3497 - accuracy: 0.8550\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.3660 - accuracy: 0.8488\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 915us/step - loss: 0.3542 - accuracy: 0.8568\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.3390 - accuracy: 0.8608\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.3619 - accuracy: 0.8528\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 905us/step - loss: 0.3289 - accuracy: 0.8644\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.3397 - accuracy: 0.8662\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 905us/step - loss: 0.3561 - accuracy: 0.8510\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3318 - accuracy: 0.8662\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.3338 - accuracy: 0.8644\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.3239 - accuracy: 0.8697\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.3346 - accuracy: 0.8639\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.3385 - accuracy: 0.8582\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3383 - accuracy: 0.8599\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.3400 - accuracy: 0.8595\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.3316 - accuracy: 0.8582\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.3232 - accuracy: 0.8666\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.3390 - accuracy: 0.8608\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.3412 - accuracy: 0.8568\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.3238 - accuracy: 0.8697\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.3309 - accuracy: 0.8604\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.3304 - accuracy: 0.8595\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.3138 - accuracy: 0.8702\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.3404 - accuracy: 0.8568\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.3222 - accuracy: 0.8684\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.3280 - accuracy: 0.8617\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.3148 - accuracy: 0.8724\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8657\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.3273 - accuracy: 0.8666\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 915us/step - loss: 0.3072 - accuracy: 0.8711\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.3291 - accuracy: 0.8626\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 910us/step - loss: 0.2996 - accuracy: 0.8791\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.3109 - accuracy: 0.8773\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.3340 - accuracy: 0.8657\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.3177 - accuracy: 0.8719\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 968us/step - loss: 0.3016 - accuracy: 0.8853\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.3216 - accuracy: 0.8657\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.3150 - accuracy: 0.8764\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.3041 - accuracy: 0.8746\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.3190 - accuracy: 0.8679\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.2955 - accuracy: 0.8826\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.3239 - accuracy: 0.8706\n",
      "Epoch 157/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2997 - accuracy: 0.8755\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3154 - accuracy: 0.8755\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.3030 - accuracy: 0.8768\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.2919 - accuracy: 0.8871\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.3064 - accuracy: 0.8751\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.3227 - accuracy: 0.8599\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 922us/step - loss: 0.2964 - accuracy: 0.8813\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 937us/step - loss: 0.3185 - accuracy: 0.8706\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.3115 - accuracy: 0.8684\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 976us/step - loss: 0.2919 - accuracy: 0.8813\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 960us/step - loss: 0.3130 - accuracy: 0.8719\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.3036 - accuracy: 0.8799\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 974us/step - loss: 0.2987 - accuracy: 0.8835\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - 0s 933us/step - loss: 0.3008 - accuracy: 0.8702\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2771 - accuracy: 0.8857\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.3014 - accuracy: 0.8773\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 930us/step - loss: 0.2943 - accuracy: 0.8782\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 983us/step - loss: 0.2895 - accuracy: 0.8782\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 971us/step - loss: 0.2884 - accuracy: 0.8839\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.2814 - accuracy: 0.8862\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.2931 - accuracy: 0.8786\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.2895 - accuracy: 0.8755\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2918 - accuracy: 0.8839\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 901us/step - loss: 0.2960 - accuracy: 0.8817\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2839 - accuracy: 0.8884\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.2993 - accuracy: 0.8733\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.2829 - accuracy: 0.8853\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.2894 - accuracy: 0.8893\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.2894 - accuracy: 0.8880\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.2804 - accuracy: 0.8839\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 912us/step - loss: 0.2759 - accuracy: 0.8795\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2802 - accuracy: 0.8791\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.2823 - accuracy: 0.8920\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.8888\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.2892 - accuracy: 0.8880\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.2679 - accuracy: 0.8977\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.2723 - accuracy: 0.8893\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 827us/step - loss: 0.2739 - accuracy: 0.8884\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 800us/step - loss: 0.2802 - accuracy: 0.8871\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2731 - accuracy: 0.8906\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.2841 - accuracy: 0.8920\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 933us/step - loss: 0.2790 - accuracy: 0.8928\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.2818 - accuracy: 0.8906\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.2592 - accuracy: 0.8933\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.2755 - accuracy: 0.8875\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2801 - accuracy: 0.8875\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.2716 - accuracy: 0.8955\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.2954 - accuracy: 0.8791\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2618 - accuracy: 0.8920\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.2653 - accuracy: 0.8964\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.2719 - accuracy: 0.8937\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.2601 - accuracy: 0.8960\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 928us/step - loss: 0.2694 - accuracy: 0.8911\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 931us/step - loss: 0.2572 - accuracy: 0.8986\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.2616 - accuracy: 0.8942\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 917us/step - loss: 0.2517 - accuracy: 0.9022\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 887us/step - loss: 0.2558 - accuracy: 0.8960\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.2794 - accuracy: 0.8857\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 894us/step - loss: 0.2742 - accuracy: 0.8955\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 889us/step - loss: 0.2637 - accuracy: 0.8893\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.2674 - accuracy: 0.8995\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.2564 - accuracy: 0.9022\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.2707 - accuracy: 0.8928\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.2793 - accuracy: 0.8795\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2462 - accuracy: 0.9057\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.8884\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2615 - accuracy: 0.8968\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 959us/step - loss: 0.2720 - accuracy: 0.8951\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8880\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.9004\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 974us/step - loss: 0.2543 - accuracy: 0.8902\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 953us/step - loss: 0.2581 - accuracy: 0.8991\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2730 - accuracy: 0.8857\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.2666 - accuracy: 0.8924\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.2783 - accuracy: 0.8853\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 937us/step - loss: 0.2712 - accuracy: 0.8911\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 971us/step - loss: 0.2574 - accuracy: 0.8946\n",
      "Epoch 234/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.2611 - accuracy: 0.8968\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.2542 - accuracy: 0.9013\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 890us/step - loss: 0.2545 - accuracy: 0.8964\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.2442 - accuracy: 0.9031\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 915us/step - loss: 0.2563 - accuracy: 0.8968\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.2575 - accuracy: 0.8906\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.2536 - accuracy: 0.8933\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2524 - accuracy: 0.9000\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 937us/step - loss: 0.2531 - accuracy: 0.9000\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.2633 - accuracy: 0.8946\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.2544 - accuracy: 0.8991\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2547 - accuracy: 0.8951\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.2570 - accuracy: 0.9008\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2208 - accuracy: 0.9106\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.2420 - accuracy: 0.9093\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.2363 - accuracy: 0.9066\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.2514 - accuracy: 0.8995\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2370 - accuracy: 0.9026\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.2595 - accuracy: 0.8991\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.2410 - accuracy: 0.9044\n",
      "Epoch 254/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2361 - accuracy: 0.9115\n",
      "Epoch 255/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.2360 - accuracy: 0.9057\n",
      "Epoch 256/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2430 - accuracy: 0.9040\n",
      "Epoch 257/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2663 - accuracy: 0.8906\n",
      "Epoch 258/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.2340 - accuracy: 0.9066\n",
      "Epoch 259/1500\n",
      "36/36 [==============================] - 0s 889us/step - loss: 0.2373 - accuracy: 0.9022\n",
      "Epoch 260/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.2397 - accuracy: 0.9057\n",
      "Epoch 261/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2231 - accuracy: 0.9075\n",
      "Epoch 262/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2248 - accuracy: 0.9088\n",
      "Epoch 263/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2276 - accuracy: 0.9053\n",
      "Epoch 264/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.2509 - accuracy: 0.8973\n",
      "Epoch 265/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.2418 - accuracy: 0.9000\n",
      "Epoch 266/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.2353 - accuracy: 0.9048\n",
      "Epoch 267/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.2518 - accuracy: 0.8915\n",
      "Epoch 268/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.2420 - accuracy: 0.9088\n",
      "Epoch 269/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.2201 - accuracy: 0.9173\n",
      "Epoch 270/1500\n",
      "36/36 [==============================] - 0s 894us/step - loss: 0.2425 - accuracy: 0.9026\n",
      "Epoch 271/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.2492 - accuracy: 0.9044\n",
      "Epoch 272/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2249 - accuracy: 0.9093\n",
      "Epoch 273/1500\n",
      "36/36 [==============================] - 0s 890us/step - loss: 0.2435 - accuracy: 0.9142\n",
      "Epoch 274/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.2221 - accuracy: 0.9164\n",
      "Epoch 275/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.2274 - accuracy: 0.9075\n",
      "Epoch 276/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2285 - accuracy: 0.9111\n",
      "Epoch 277/1500\n",
      " 1/36 [..............................] - ETA: 0s - loss: 0.2874 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 247.\n",
      "36/36 [==============================] - 0s 945us/step - loss: 0.2309 - accuracy: 0.9120\n",
      "Epoch 277: early stopping\n",
      "8/8 [==============================] - 0s 796us/step - loss: 0.6850 - accuracy: 0.7193\n",
      "8/8 [==============================] - 0s 662us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.71 (20/28)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n",
      "Final Test Results - Loss: 0.6849704384803772, Accuracy: 0.719298243522644, Precision: 0.6663266867812322, Recall: 0.770192762320422, F1 Score: 0.6980439347820958\n",
      "Confusion Matrix:\n",
      " [[107  13  36]\n",
      " [  2  22   1]\n",
      " [ 12   0  35]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6162213418807758\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8312062919139862\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.6750105321407318\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6485499447535227\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6647165997044548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.67 (74/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[kitten, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, kitten,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, kitten, senior, adult, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[senior, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, senior, adult, senior, adult, adult, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, senior, senior, senior, adult,...        senior           senior                   True\n",
       "43    037A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, senior, senior, senior,...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "25    023A       [kitten, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, senior, adult, se...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "10    009A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, kitten, adult, adult, ki...         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "101   106A  [adult, senior, senior, senior, senior, adult,...         adult           senior                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, kitten,...        senior            adult                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "93    097B  [senior, adult, kitten, senior, adult, kitten,...        kitten            adult                  False\n",
       "12    011A                                    [adult, senior]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "40    034A           [senior, senior, senior, senior, senior]        senior            adult                  False\n",
       "47    041A                                    [adult, kitten]         adult           kitten                  False\n",
       "48    042A  [adult, adult, adult, adult, adult, adult, adu...         adult           kitten                  False\n",
       "50    044A              [adult, adult, adult, kitten, kitten]         adult           kitten                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, adult, adu...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "36    029A  [adult, adult, adult, senior, senior, senior, ...        senior            adult                  False\n",
       "57    051B  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "64    058A                            [senior, adult, kitten]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "69    063A  [senior, senior, senior, adult, adult, senior,...        senior            adult                  False\n",
       "30    025C             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "109   117A  [senior, senior, adult, adult, senior, adult, ...         adult           senior                  False"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     56\n",
      "kitten     7\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             56  76.712329\n",
      "1           kitten           15              7  46.666667\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          554            416  75.090253\n",
      "1           kitten          109             57  52.293578\n",
      "2           senior          178             94  52.808989\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    218      167     76.61\n",
      "1          M    337      237     70.33\n",
      "2          X    286      163     56.99\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
